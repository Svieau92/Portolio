[
  {
    "objectID": "test.html",
    "href": "test.html",
    "title": "Data Checking Report",
    "section": "",
    "text": "The aim of the current study is to assess how treatment response differs for HIV+ patients 2 years after initiating Highly Active Antiretroviral Therapy (HAART) based on hard drug usage (such as heroin or cocaine). This study is of particular scientific interest because it is unclear whether the use of hard drugs inhibits the immune system in humans; treatment strategies may differ based on these results. The researchers are interested in comparing subjects who never used hard drugs to current hard drug users (those that use hard drugs at year 2) or previous hard drug users (those who used drugs at year 0 or 1). Outcomes of interest are: viral load (HIV copies in a mL of blood), CD4+ T cell count (a measure of immunologic health), and aggregate physical and quality of life scores from the SF-36.\nThe clinical hypothesis is that, if hard drugs inhibit the immune system in humans, subjects who currently or previously used hard drugs will have higher viral load and lower CD4+ T cell counts than those who never used hard drugs. Additionally, the researchers are interested in knowing if potential differences between the drug use groups can be explained by differences in adherence to the treatment regimen. The researchers are agnostic on how quality of life changes after treatment, since side effects of the treatment are significant.\nThe project description provided by the PI is available below:"
  },
  {
    "objectID": "test.html#data-preparation",
    "href": "test.html#data-preparation",
    "title": "Data Checking Report",
    "section": "Data Preparation",
    "text": "Data Preparation\nFirst we load the necessary packages\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(tidyverse)\nlibrary(knitr)\nlibrary(naniar)\nlibrary(kableExtra)\nlibrary(table1)\n\nThen we import the data set.\n\ndata &lt;- read_csv(\"C:/Users/sviea/Documents/Portfolio/Project_2/Project_2_R/RawData/hiv_dataset.csv\")\n\nRows: 3632 Columns: 33\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (33): newid, AGG_MENT, AGG_PHYS, HASHV, HASHF, income, BMI, HBP, DIAB, L...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nAnd take a look.\n\nglimpse(data)\n\nRows: 3,632\nColumns: 33\n$ newid      &lt;dbl&gt; 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4,…\n$ AGG_MENT   &lt;dbl&gt; 44.90710, 58.20754, 59.65136, 56.80657, 46.34190, 48.71791,…\n$ AGG_PHYS   &lt;dbl&gt; 52.52557, 41.29347, 48.54453, 46.73991, 27.92331, 38.03807,…\n$ HASHV      &lt;dbl&gt; 1, 1, 1, 1, 1, 2, 1, 2, 1, 2, 1, 1, 2, 1, 2, 1, 1, 2, 2, 1,…\n$ HASHF      &lt;dbl&gt; NA, 4, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 0, 0, 0, 0, 0, 4, 2…\n$ income     &lt;dbl&gt; 4, 4, 4, 5, 2, 1, 2, 1, 9, 2, 6, 6, 6, NA, 7, 7, 7, NA, 1, …\n$ BMI        &lt;dbl&gt; 24.71756, 26.06801, 27.16421, 25.71786, 26.66936, 25.96576,…\n$ HBP        &lt;dbl&gt; 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 9, 9, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ DIAB       &lt;dbl&gt; 1, 1, 1, 1, 9, 9, 9, 1, 1, 1, 9, 9, 1, 9, 9, 9, 9, 9, 1, 3,…\n$ LIV34      &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1,…\n$ KID        &lt;dbl&gt; 1, 1, 1, 1, 9, 9, 9, 2, 2, 2, 1, 1, 1, 2, 1, 1, 1, 2, 9, 9,…\n$ FRP        &lt;dbl&gt; 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2,…\n$ FP         &lt;dbl&gt; 1, 1, 1, 1, 9, 9, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2,…\n$ TCHOL      &lt;dbl&gt; 133, 131, 180, 171, 125, NA, 134, 105, 141, 153, 170, 170, …\n$ TRIG       &lt;dbl&gt; 176, 107, 233, 139, NA, NA, NA, 104, 162, 127, NA, NA, 82, …\n$ LDL        &lt;dbl&gt; 62, 66, 86, 96, NA, NA, NA, 44, 73, 84, NA, NA, 127, NA, NA…\n$ DYSLIP     &lt;dbl&gt; 2, 1, 2, 1, 2, 9, 4, 1, 2, 1, 2, 2, 2, 9, 2, 2, 2, 2, 2, 4,…\n$ CESD       &lt;dbl&gt; 14, 2, 1, 18, 20, 18, 18, 21, 23, 17, 18, 22, 23, 14, 1, 1,…\n$ SMOKE      &lt;dbl&gt; 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3,…\n$ DKGRP      &lt;dbl&gt; 0, 3, 0, 1, 0, 3, 2, 1, 0, 1, 0, 0, 0, 1, 2, 1, 1, 2, 1, 2,…\n$ HEROPIATE  &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2,…\n$ IDU        &lt;dbl&gt; 2, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1,…\n$ LEU3N      &lt;dbl&gt; 104.1659, 262.0061, 345.4010, 292.3271, 257.8278, 459.4562,…\n$ VLOAD      &lt;dbl&gt; 1.020130e+05, 2.700000e+01, 6.000000e+01, 9.000000e+00, 8.1…\n$ ADH        &lt;dbl&gt; NA, 2, 1, 1, NA, 1, 1, 1, 1, 1, NA, 1, 1, 2, 1, 1, 2, 1, NA…\n$ RACE       &lt;dbl&gt; 1, 1, 1, 1, 3, 3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ EDUCBAS    &lt;dbl&gt; 4, 4, 4, 4, 2, 2, 2, 2, 2, 2, 7, 7, 7, 7, 7, 7, 7, 7, 5, 5,…\n$ hivpos     &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ age        &lt;dbl&gt; 52, 53, 54, 55, 54, 55, 56, 60, 61, 62, 47, 48, 49, 51, 52,…\n$ ART        &lt;dbl&gt; 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1,…\n$ everART    &lt;dbl&gt; 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1,…\n$ years      &lt;dbl&gt; 0, 1, 2, 3, 0, 1, 2, 6, 7, 8, 0, 1, 2, 4, 5, 6, 7, 8, 0, 1,…\n$ hard_drugs &lt;dbl&gt; 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1,…\n\n\nEverything appears properly formatted,"
  },
  {
    "objectID": "Project_1_Regression/Project_1_Regression_SAS/untitled.html",
    "href": "Project_1_Regression/Project_1_Regression_SAS/untitled.html",
    "title": "Quarto stuff",
    "section": "",
    "text": "Quarto stuff\nblah blah blah"
  },
  {
    "objectID": "Project_1_Regression/Project_1_Regression_R/Project_1_Regression.html",
    "href": "Project_1_Regression/Project_1_Regression_R/Project_1_Regression.html",
    "title": "Advanced Data Analysis - Project 1",
    "section": "",
    "text": "The aim of the current study is to assess whether a new gel treatment for gum disease results in lower whole-mouth average pocket depth and attachment loss after one year. Average pocket depth and attachment loss were taken at baseline, and participants were assigned into one of five groups (1 = placebo, 2 = no treatment, 3 = low concentration, 4 = medium concentration, 5 = high concentration) and instructed to apply the gel to their gums twice a day. After 1-year, average pocket depth and attachment loss were recorded again. Data was received as a .csv file containing treatment level, average pocket depth and attachment loss at baseline and at one-year, demographic information, gender, age, number of sites measured, and smoking status. The clinical hypothesis is that average pocket depth and attachment loss in participants who applied the gel will be lower compared to participants who did not. Gender, age, ethnicity, and smoking status will be investigated as potential covariates.\nThe project description provided by the PI is available below:\n\n\n\nFirst we begin by loading in the necessary packages\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(tidyverse)\nlibrary(knitr)\nlibrary(kableExtra) # This lets me pretty print the tables\nlibrary(naniar) # Used to visualize missing data\nlibrary(glue) # This is used for f-strings in R\nlibrary(purrr) # Used for summary tables\nlibrary(corrplot) # Used to make the correlation matrix\nlibrary(corrtable) # used to make the table for the correlation matrix\nlibrary(xtable) # Used to make the table for the correlation matrix\nlibrary(htmlTable) # Used to make Table 1\nlibrary(boot) # Used to make Table 1\nlibrary(table1) # Used to make Table 1\nlibrary(sjPlot) # Used to generate publication quality tables for regressions\nlibrary(mice) # Used for multiple imputation\nlibrary(car) # Used for outlier diagnostics with jackknife residuals\n\nThen we will import the dataset\n\n# Import dataset\ndata &lt;- read.csv(\"C:\\\\Users\\\\sviea\\\\Documents\\\\Advanced Data Analysis\\\\Project 1 MLR with Covariates\\\\Project1_data.csv\")\nnames(data)[1] &lt;- \"id\" # Renaming the weird character out of this colname\n\nAnd visualize the dataset\n\n# Create a nicely formatted table, code adapted from ChatGPT\nkable(head(data), format = \"html\") %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\"))\n\n\n\n\nid\ntrtgroup\ngender\nrace\nage\nsmoker\nsites\nattachbase\nattach1year\npdbase\npd1year\n\n\n\n\n101\n4\n2\n5\n44.57221\n1\n162\n2.432099\n2.577640\n3.246914\n3.407407\n\n\n102\n5\n2\n5\n35.57290\n1\n162\n2.543210\nNA\n3.006173\nNA\n\n\n103\n2\n2\n5\n47.94524\n1\n144\n2.881944\n3.076389\n3.118056\n3.125000\n\n\n104\n3\n2\n5\n55.17864\n1\n138\n4.956522\n5.304348\n5.217391\n4.891304\n\n\n105\n1\n2\n2\n43.79740\n1\n168\n1.773810\n1.452381\n3.363095\n2.898810\n\n\n106\n4\n2\n5\n42.14921\n1\n168\n2.369048\n1.922619\n3.910714\n3.083333\n\n\n\n\n\n\n# Acquire number of variables and observations of the data set\ndim(data)\n\n[1] 130  11\n\n\nOur data set is comprised of 130 observations, with 11 variables\n\n\nThe variables in the provided data set are as follows:\n\nid: Patient ID\ntrtgroup: Treatment group (1 = placebo, 2 = no treatment control, 3 = low concentration, 4 = medium concentration, 5 = high concentration gel)\ngender: Gender (1 = male, 2 = female)\nrace: Race, (1 = Native American, 2 = African American, 3 = Not used, 4 = Asian, 5 = White)\nage: Age in years\nsmoker: Smoking status (0 = No, 1 = Yes)\nsites: Number of sites gum measurements were averaged from\nattachbase: Whole-mouth average attachment loss taken at base timepoint\nattach1year: Whole-mouth average attachment loss after 1 year\npdbase: Whole-mouth average pocket loss at base timepoint\npd1year: Whole-mouth average pocket loss after 1 year\n\n\n\n\n\n\n\nNote\n\n\n\nPocket depth and attachment loss are both measurements of how far the gums have pulled away from the teeth, hence smaller values are better"
  },
  {
    "objectID": "Project_1_Regression/Project_1_Regression_R/Project_1_Regression.html#data-preparation",
    "href": "Project_1_Regression/Project_1_Regression_R/Project_1_Regression.html#data-preparation",
    "title": "Advanced Data Analysis - Project 1",
    "section": "",
    "text": "First we begin by loading in the necessary packages\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(tidyverse)\nlibrary(knitr)\nlibrary(kableExtra) # This lets me pretty print the tables\nlibrary(naniar) # Used to visualize missing data\nlibrary(glue) # This is used for f-strings in R\nlibrary(purrr) # Used for summary tables\nlibrary(corrplot) # Used to make the correlation matrix\nlibrary(corrtable) # used to make the table for the correlation matrix\nlibrary(xtable) # Used to make the table for the correlation matrix\nlibrary(htmlTable) # Used to make Table 1\nlibrary(boot) # Used to make Table 1\nlibrary(table1) # Used to make Table 1\nlibrary(sjPlot) # Used to generate publication quality tables for regressions\nlibrary(mice) # Used for multiple imputation\nlibrary(car) # Used for outlier diagnostics with jackknife residuals\n\nThen we will import the dataset\n\n# Import dataset\ndata &lt;- read.csv(\"C:\\\\Users\\\\sviea\\\\Documents\\\\Advanced Data Analysis\\\\Project 1 MLR with Covariates\\\\Project1_data.csv\")\nnames(data)[1] &lt;- \"id\" # Renaming the weird character out of this colname\n\nAnd visualize the dataset\n\n# Create a nicely formatted table, code adapted from ChatGPT\nkable(head(data), format = \"html\") %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\"))\n\n\n\n\nid\ntrtgroup\ngender\nrace\nage\nsmoker\nsites\nattachbase\nattach1year\npdbase\npd1year\n\n\n\n\n101\n4\n2\n5\n44.57221\n1\n162\n2.432099\n2.577640\n3.246914\n3.407407\n\n\n102\n5\n2\n5\n35.57290\n1\n162\n2.543210\nNA\n3.006173\nNA\n\n\n103\n2\n2\n5\n47.94524\n1\n144\n2.881944\n3.076389\n3.118056\n3.125000\n\n\n104\n3\n2\n5\n55.17864\n1\n138\n4.956522\n5.304348\n5.217391\n4.891304\n\n\n105\n1\n2\n2\n43.79740\n1\n168\n1.773810\n1.452381\n3.363095\n2.898810\n\n\n106\n4\n2\n5\n42.14921\n1\n168\n2.369048\n1.922619\n3.910714\n3.083333\n\n\n\n\n\n\n# Acquire number of variables and observations of the data set\ndim(data)\n\n[1] 130  11\n\n\nOur data set is comprised of 130 observations, with 11 variables\n\n\nThe variables in the provided data set are as follows:\n\nid: Patient ID\ntrtgroup: Treatment group (1 = placebo, 2 = no treatment control, 3 = low concentration, 4 = medium concentration, 5 = high concentration gel)\ngender: Gender (1 = male, 2 = female)\nrace: Race, (1 = Native American, 2 = African American, 3 = Not used, 4 = Asian, 5 = White)\nage: Age in years\nsmoker: Smoking status (0 = No, 1 = Yes)\nsites: Number of sites gum measurements were averaged from\nattachbase: Whole-mouth average attachment loss taken at base timepoint\nattach1year: Whole-mouth average attachment loss after 1 year\npdbase: Whole-mouth average pocket loss at base timepoint\npd1year: Whole-mouth average pocket loss after 1 year\n\n\n\n\n\n\n\nNote\n\n\n\nPocket depth and attachment loss are both measurements of how far the gums have pulled away from the teeth, hence smaller values are better"
  },
  {
    "objectID": "Project_1_Regression/Project_1_Regression_R/Project_1_Regression.html#Descriptives",
    "href": "Project_1_Regression/Project_1_Regression_R/Project_1_Regression.html#Descriptives",
    "title": "Advanced Data Analysis - Project 1",
    "section": "Descriptive Statistics",
    "text": "Descriptive Statistics\nHere we will acquire the descriptive statistics of our data set and create Table 1. Code modified from cran.r-project.org\n\n# Duplicate the dataset so we are not modifying the original\ndata2 &lt;- data\n\n# Factor the basic variables that we're interested in\ndata2$trtgroup &lt;- factor(data2$trtgroup,\n                                levels = c(1,2,3,4,5),\n                                labels = c(\"Placebo\", \"Control\", \"Low\", \"Medium\", \"High\"))\n\ndata2$gender &lt;- factor(data2$gender,\n                              levels = c(1,2),\n                              labels = c(\"Male\", \"Female\"))\n\ndata2$race &lt;- factor(data2$race,\n                             levels = c(1,2,4,5),\n                             labels = c(\"Native American\", \"African American\", \"White\", \"Asian\"))\n                             \ndata2$smoker &lt;- factor(data2$smoker,\n                               levels = c(0,1),\n                               labels = c(\"Non-Smoker\", \"Smoker\"))\n\n# Create labels to make the names of each variable more professional\nlabel(data2$gender) &lt;- \"Gender\"\nlabel(data2$race) &lt;- \"Race\"\nlabel(data2$age) &lt;- \"Age (Years)\"\nlabel(data2$smoker) &lt;- \"Smoking Status\"\nlabel(data2$sites) &lt;- \"Sites\"\nlabel(data2$attachbase) &lt;- \"Attachment Loss at Baseline\"\nlabel(data2$attach1year) &lt;- \"Attachment Loss at 1 Year\"\nlabel(data2$pdbase) &lt;- \"Pocket Depth at Baseline\"\nlabel(data2$pd1year) &lt;- \"Pocket Depth at 1 Year\"\nlabel(data2$attachchange) &lt;- \"Attachment Loss Change\"\nlabel(data2$pdchange) &lt;- \"Pocket Depth Change\"\n\n\n# Create table 1\ntable1 &lt;- table1(~ gender + race + age + smoker + sites + attachbase + attach1year + pdbase + pd1year + attachchange + pdchange| trtgroup,  data = data2, caption = \"Descriptive Statistics\", overall = c(left=\"Total\"))\ntable1\n\n\n\nDescriptive Statistics\n\n\n\n\n\n\n\n\n\n\n\n\nTotal\n(N=130)\nPlacebo\n(N=26)\nControl\n(N=26)\nLow\n(N=26)\nMedium\n(N=26)\nHigh\n(N=26)\n\n\n\n\nGender\n\n\n\n\n\n\n\n\nMale\n54 (41.5%)\n11 (42.3%)\n10 (38.5%)\n11 (42.3%)\n11 (42.3%)\n11 (42.3%)\n\n\nFemale\n76 (58.5%)\n15 (57.7%)\n16 (61.5%)\n15 (57.7%)\n15 (57.7%)\n15 (57.7%)\n\n\nRace\n\n\n\n\n\n\n\n\nNative American\n4 (3.1%)\n0 (0%)\n1 (3.8%)\n1 (3.8%)\n0 (0%)\n2 (7.7%)\n\n\nAfrican American\n9 (6.9%)\n2 (7.7%)\n1 (3.8%)\n5 (19.2%)\n0 (0%)\n1 (3.8%)\n\n\nWhite\n3 (2.3%)\n1 (3.8%)\n1 (3.8%)\n0 (0%)\n1 (3.8%)\n0 (0%)\n\n\nAsian\n114 (87.7%)\n23 (88.5%)\n23 (88.5%)\n20 (76.9%)\n25 (96.2%)\n23 (88.5%)\n\n\nAge (Years)\n\n\n\n\n\n\n\n\nMean (SD)\n49.9 (10.0)\n47.1 (8.61)\n50.7 (9.90)\n51.9 (10.8)\n49.0 (9.49)\n50.8 (11.2)\n\n\nMedian [Min, Max]\n48.6 [28.6, 74.5]\n44.7 [30.4, 67.1]\n49.2 [36.1, 73.3]\n51.5 [36.9, 71.9]\n48.1 [28.6, 70.9]\n49.9 [34.1, 74.5]\n\n\nMissing\n1 (0.8%)\n1 (3.8%)\n0 (0%)\n0 (0%)\n0 (0%)\n0 (0%)\n\n\nSmoking Status\n\n\n\n\n\n\n\n\nNon-Smoker\n81 (62.3%)\n15 (57.7%)\n17 (65.4%)\n18 (69.2%)\n14 (53.8%)\n17 (65.4%)\n\n\nSmoker\n48 (36.9%)\n11 (42.3%)\n9 (34.6%)\n8 (30.8%)\n11 (42.3%)\n9 (34.6%)\n\n\nMissing\n1 (0.8%)\n0 (0%)\n0 (0%)\n0 (0%)\n1 (3.8%)\n0 (0%)\n\n\nSites\n\n\n\n\n\n\n\n\nMean (SD)\n158 (11.3)\n160 (10.1)\n154 (10.9)\n161 (8.54)\n155 (15.7)\n157 (9.65)\n\n\nMedian [Min, Max]\n162 [114, 168]\n162 [138, 168]\n159 [126, 168]\n162 [138, 168]\n162 [114, 168]\n159 [138, 168]\n\n\nAttachment Loss at Baseline\n\n\n\n\n\n\n\n\nMean (SD)\n2.15 (0.797)\n1.79 (0.646)\n2.46 (0.687)\n2.07 (0.987)\n2.17 (0.656)\n2.24 (0.858)\n\n\nMedian [Min, Max]\n2.03 [0.895, 5.09]\n1.71 [0.899, 3.64]\n2.48 [1.22, 4.39]\n1.77 [0.895, 4.96]\n2.12 [1.02, 4.01]\n1.97 [1.26, 5.09]\n\n\nAttachment Loss at 1 Year\n\n\n\n\n\n\n\n\nMean (SD)\n2.10 (0.772)\n1.74 (0.542)\n2.33 (0.551)\n2.08 (1.06)\n2.24 (0.652)\n2.15 (0.915)\n\n\nMedian [Min, Max]\n1.98 [0.865, 5.30]\n1.64 [0.964, 3.10]\n2.23 [1.46, 3.49]\n1.74 [0.865, 5.30]\n2.25 [1.35, 3.83]\n1.71 [1.22, 4.04]\n\n\nMissing\n27 (20.8%)\n3 (11.5%)\n3 (11.5%)\n5 (19.2%)\n6 (23.1%)\n10 (38.5%)\n\n\nPocket Depth at Baseline\n\n\n\n\n\n\n\n\nMean (SD)\n3.14 (0.437)\n3.09 (0.372)\n3.28 (0.473)\n3.17 (0.593)\n3.05 (0.402)\n3.11 (0.273)\n\n\nMedian [Min, Max]\n3.10 [2.26, 5.22]\n3.11 [2.47, 4.08]\n3.11 [2.65, 4.77]\n3.07 [2.26, 5.22]\n3.09 [2.42, 3.91]\n3.14 [2.62, 3.60]\n\n\nPocket Depth at 1 Year\n\n\n\n\n\n\n\n\nMean (SD)\n2.88 (0.488)\n2.75 (0.482)\n2.95 (0.455)\n3.02 (0.578)\n2.84 (0.469)\n2.80 (0.423)\n\n\nMedian [Min, Max]\n2.90 [1.96, 4.89]\n2.70 [1.96, 3.75]\n2.90 [2.24, 4.07]\n2.97 [2.16, 4.89]\n2.90 [2.05, 3.78]\n2.87 [2.04, 3.40]\n\n\nMissing\n27 (20.8%)\n3 (11.5%)\n3 (11.5%)\n5 (19.2%)\n6 (23.1%)\n10 (38.5%)\n\n\nAttachment Loss Change\n\n\n\n\n\n\n\n\nMean (SD)\n-0.0995 (0.276)\n-0.0871 (0.242)\n-0.222 (0.280)\n-0.0178 (0.266)\n-0.00656 (0.231)\n-0.165 (0.326)\n\n\nMedian [Min, Max]\n-0.0679 [-1.05, 0.452]\n-0.0247 [-0.599, 0.452]\n-0.123 [-0.901, 0.194]\n0.0298 [-0.705, 0.348]\n-0.0160 [-0.446, 0.339]\n-0.0579 [-1.05, 0.199]\n\n\nMissing\n27 (20.8%)\n3 (11.5%)\n3 (11.5%)\n5 (19.2%)\n6 (23.1%)\n10 (38.5%)\n\n\nPocket Depth Change\n\n\n\n\n\n\n\n\nMean (SD)\n-0.294 (0.268)\n-0.350 (0.277)\n-0.338 (0.232)\n-0.206 (0.279)\n-0.203 (0.272)\n-0.382 (0.245)\n\n\nMedian [Min, Max]\n-0.284 [-0.858, 0.455]\n-0.383 [-0.858, 0.161]\n-0.367 [-0.759, 0.0145]\n-0.244 [-0.661, 0.455]\n-0.200 [-0.827, 0.175]\n-0.347 [-0.845, 0.0536]\n\n\nMissing\n27 (20.8%)\n3 (11.5%)\n3 (11.5%)\n5 (19.2%)\n6 (23.1%)\n10 (38.5%)"
  },
  {
    "objectID": "Project_1_Regression/Project_1_Regression_R/Project_1_Regression.html#Preliminary",
    "href": "Project_1_Regression/Project_1_Regression_R/Project_1_Regression.html#Preliminary",
    "title": "Advanced Data Analysis - Project 1",
    "section": "Preliminary Evaluation of Assumptions",
    "text": "Preliminary Evaluation of Assumptions\nBefore we begin digging into the data, let’s take a closer look at the relationships between our variables.\nWe will first run a correlation matrix to assess the correlations between our IVs. Then we will explore any high correlations that which could affect our analysis.\nFinally we will make histograms of attachment loss and pocket depth change to gauge whether they will be normally distributed or if we need to run some transformations.\n\nCorrelation MatrixGender and MissingnessNormality of Dependent Variables\n\n\nFirst we will begin by making a correlation matrix to assess whether any of our IVs are related to each other (multicollinearity). This will inform which variables to incorporate into the final model.\n\n# Since we made dummy codes, we will get spurious correlations that will obfuscate the main relationships we are interested in (e.g. between 'medium' and 'trtgroup'. So we will first make a separate dataset excluding the dummy coded variables\ndata_for_matrix &lt;- select(data_missing, -placebo, -control, -low, -medium,  -high, -trt, -trt3groups)\n\n# Make a correlation matrix with all variables of the trimmed data set\ncorrelation_matrix &lt;- cor(data_for_matrix, use = \"complete.obs\")\n\n# Plot the matrix\ncorrplot(correlation_matrix, method = \"circle\")\n\n\n\n\n\n\n\n# Trim the matrix\ncorrelation_matrix[upper.tri(correlation_matrix)] &lt;- NA\n\n# Save the matrix as a LaTex file for paper\ncor_table &lt;- xtable(correlation_matrix, caption = \"Correlation Matrix\", label = \"tab:correlation\")\nprint(cor_table, type = \"latex\", file = \"correlation_matrix.tex\")\n\nVisually, we can see a cluster of high positive correlations between all of attachment loss and pocket depth at baseline and 1 year. This makes sense, as all measurements were taken from the same sites in the gums. This will be explored later (See Exploratory Data Analysis - Dependent Variables)\nJust for sanity (and practice), let’s make a table of our correlation coefficients.\n\n# Convert the matrix to a dataframe for better formatting\ncorrelation_df &lt;- as.data.frame(correlation_matrix)\n\n# Use Kable to pretty print the table\nkable(correlation_df, caption = \"Correlation Matrix\", format = \"html\") %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\"))\n\n\nCorrelation Matrix\n\n\n\nid\ntrtgroup\ngender\nrace\nage\nsmoker\nsites\nattachbase\nattach1year\npdbase\npd1year\nattachchange\npdchange\n\n\n\n\nid\n1.0000000\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\ntrtgroup\n-0.1452850\n1.0000000\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\ngender\n-0.8306185\n0.1428866\n1.0000000\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nrace\n0.1319767\n-0.0314771\n-0.0955531\n1.0000000\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nage\n0.0593032\n0.1100051\n-0.0453862\n0.1787040\n1.0000000\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nsmoker\n-0.2742888\n-0.0495029\n0.0206456\n0.0377211\n-0.2229620\n1.0000000\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nsites\n0.0520821\n-0.0393742\n-0.1291127\n0.0505164\n-0.0274695\n-0.0226381\n1.0000000\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nattachbase\n-0.1641513\n0.1440999\n0.2416242\n0.0313438\n0.1354434\n0.1456234\n-0.4237813\n1.0000000\nNA\nNA\nNA\nNA\nNA\n\n\nattach1year\n-0.1440182\n0.1671499\n0.2021284\n0.0671713\n0.0850851\n0.1987327\n-0.4042743\n0.9450189\n1.0000000\nNA\nNA\nNA\nNA\n\n\npdbase\n-0.2010672\n-0.0224838\n0.2645908\n0.0182289\n-0.0884687\n0.2614919\n-0.1805543\n0.6047132\n0.6093346\n1.0000000\nNA\nNA\nNA\n\n\npd1year\n-0.0685783\n0.0126731\n0.1362804\n0.0606735\n-0.1246106\n0.2447030\n-0.1901940\n0.5601052\n0.6685457\n0.8436653\n1.0000000\nNA\nNA\n\n\nattachchange\n0.0964391\n0.0296043\n-0.1696216\n0.0928969\n-0.1742586\n0.1135740\n0.1578683\n-0.3976360\n-0.0757224\n-0.1342018\n0.1679506\n1.0000000\nNA\n\n\npdchange\n0.2251487\n0.0622762\n-0.2123192\n0.0789059\n-0.0731704\n-0.0091784\n-0.0323859\n-0.0317726\n0.1579534\n-0.2031295\n0.3543035\n0.5400668\n1\n\n\n\n\n\n\n\nOther than that, there is a correlation between gender and ID which seems spurious. Let’s investigate that in the next tab.\nBack to top of tabset\n\n\nThere is a correlation between gender and ID. Let’s make a simple plot to investigate.\n\n# Creating a simple plot of id and gender\nggplot(data_missing, aes(x = factor(gender), y = id)) + \n  geom_point() + \n  labs(title = \"ID by Gender\")\n\n\n\n\n\n\n\n\nInterestingly, it appears that the experimenters assigned ID based on gender. That is, females received ID’s starting at 101, and males received ID’s starting at 201 (for some reason there’s a few females with ID’s &gt; 200).\nIt will be important to double check with the PI’s how they assigned participants to treatment group to ensure it was in fact random.\nLet’s make a contingency table to see what the breakdown between gender and treatment group is.\n\n# First make a contingency table of both variables\ncontingency_table &lt;- table(data_missing$gender, data_missing$trtgroup)\n\n# Set the row and column names\ndimnames(contingency_table) &lt;- list(\n  \"Gender\" = c(\"Male\", \"Female\"),\n  \"Treatment Condition\" = c(\"Placebo\", \"Control\", \"Low\", \"Medium\", \"High\"))\n\n# Convert the table to a dataframe for better formatting (from ChatGPT)\ncontingency_df &lt;- as.data.frame.matrix(contingency_table)\n\n# Pretty print the table using kable\nkable(contingency_df, format = \"html\") %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\"))\n\n\n\n\n\nPlacebo\nControl\nLow\nMedium\nHigh\n\n\n\n\nMale\n10\n7\n9\n7\n3\n\n\nFemale\n13\n16\n12\n13\n13\n\n\n\n\n\n\n\nThat’s not good! It looks like males were less likely to be in the high treatment condition compared to females.\nThis could be because males were more likely to drop out then females. Let’s make a quick table using the original data set before we dropped the missing variables.\n\n# First make a contingency table of both variables\ncontingency_table_clean &lt;- table(data$gender, data$trtgroup)\n\n# Set the row and column names\ndimnames(contingency_table_clean) &lt;- list(\n  \"Gender\" = c(\"Male\", \"Female\"),\n  \"Treatment Condition\" = c(\"Placebo\", \"Control\", \"Low\", \"Medium\", \"High\"))\n\n# Convert the table to a dataframe for better formatting (from ChatGPT)\ncontingency_df_clean &lt;- as.data.frame.matrix(contingency_table_clean)\n\n# Pretty print the table using kable\nkable(contingency_df_clean, format = \"html\") %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\"))\n\n\n\n\n\nPlacebo\nControl\nLow\nMedium\nHigh\n\n\n\n\nMale\n11\n10\n11\n11\n11\n\n\nFemale\n15\n16\n15\n15\n15\n\n\n\n\n\n\n\nIt looks more balanced before I took out participants with missing data.\nChi-square is known to be unsuitable if a cell has &lt; 5 counts, which we have in this case (3 males in high concentration condition). So I will run Fisher’s test to see if that difference is statistically significant.\n\nfisher_test &lt;- fisher.test(contingency_table)\nfisher_test\n\n\n    Fisher's Exact Test for Count Data\n\ndata:  contingency_table\np-value = 0.506\nalternative hypothesis: two.sided\n\n\nThe p-value is not signficant (p = 0.506).\nWe know from visualizing the missing data that there were 27 missing data points for the gum measurement DVs, and these all belong to the same people. Furthermore, it appears that males in the treatment groups were more likely to have missing values than in the placebo (and maybe control) group. Is it possible that the gel was having an adverse effect on these participants? Does the gel have an adverse effect only on males and not females for some reason? Let’s explore.\nFirst, I want to investigate if males were more likely to have missing data points. It’s possible if their gums were hurting they simply rejected or avoided having these measurements taken.\nLet’s repeat this process and make a contingency table of gender and missing variables.\n\n# First let's add a new dummy code for if a participant is missing any data points\ndata$missing &lt;- ifelse(apply(data, 1, function(row) any(is.na(row))), 1, 0)\n\n# First make a contingency table of both variables\ncontingency_table_missing &lt;- table(data$gender, data$missing)\n\n# Set the row and column names\ndimnames(contingency_table_missing) &lt;- list(\n  \"Gender\" = c(\"Male\", \"Female\"),\n  \"Missing\" = c(\"Not Missing\", \"Missing\"))\n\n# Convert the table to a dataframe for better formatting (from ChatGPT)\ncontingency_df_missing &lt;- as.data.frame.matrix(contingency_table_missing)\n\n# Pretty print the table using kable\nkable(contingency_df_missing, format = \"html\") %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\"))\n\n\n\n\n\nNot Missing\nMissing\n\n\n\n\nMale\n35\n19\n\n\nFemale\n66\n10\n\n\n\n\n\n\n\nProportionally, it appears that males may be more likely to have missing variables than females. Let’s run a chi-square to check.\n\nchi_square_test &lt;- chisq.test(contingency_df_missing)\nchi_square_test\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  contingency_df_missing\nX-squared = 7.6127, df = 1, p-value = 0.005796\n\n\nSuccess! Males were more likely to have missing values compared to females (p = 0.005796). This could be a problem (counfound) if something was causing males to avoid having their gums measured compared to females (such as adverse reactions from the gel)\nLet’s do a quick chi square test to check if there is a relationship between missing values and treatment condition.\nWe start off the same way by making a contingency table and running a chi-square test.\n\n# First make a contingency table of both variables\ncontingency_table_missing2 &lt;- table(data$missing, data$trtgroup)\n\n# Set the row and column names\ndimnames(contingency_table_missing2) &lt;- list(\"Missing\" = c(\"Not Missing\", \"Missing\"),\n                                             \"Treatmtent Condition\" = c(\"Placebo\", \"Control\", \"Low\", \"Medium\", \"High\"))\n\n# Convert the table to a dataframe for better formatting (from ChatGPT)\ncontingency_df_missing2 &lt;- as.data.frame.matrix(contingency_table_missing2)\n\n# Pretty print the table using kable\nkable(contingency_df_missing2, format = \"html\") %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\"))\n\n\n\n\n\nPlacebo\nControl\nLow\nMedium\nHigh\n\n\n\n\nNot Missing\n22\n23\n21\n19\n16\n\n\nMissing\n4\n3\n5\n7\n10\n\n\n\n\n\n\n\nIt does appear that there are more missing variables in the high concentration condition. Is it statistically significant?\n\n# Run a Fisher's Exact Test (since we have &lt; 5 observations in cells)\nfisher_test &lt;- fisher.test(contingency_df_missing2)\nfisher_test\n\n\n    Fisher's Exact Test for Count Data\n\ndata:  contingency_df_missing2\np-value = 0.1675\nalternative hypothesis: two.sided\n\n\nNot significant (p = 0.1675). So we can conclude that there is no difference in gender or missing values based on treatment condition (i.e., participants in all treatment conditions were equally likely to be male or female, or have missing values)\nHowever, across the board, males were more likely to have missing values than females. This will be important to note as a caveat during interpretation of the final results.\nBack to top of tabset\n\n\nHere we will simply plot the histograms of attachment loss and pocket depth changes scores, to assess if they appear normally distributed or if we will have to perform a transformation of some kind.\n\n# Plot simple histogram of attachment loss change score\nhist(data_missing$attachchange,\n     main  = \"Histogram of Attachment Loss Change\",\n     xlab = \"Attachment Loss Change\",\n     ylab = \"Frequency\")\n\n\n\n\n\n\n\n# Plot simple histogram of pocket depth change score\nhist(data_missing$pdchange,\n     main  = \"Histogram of Attachment Loss Change\",\n     xlab = \"Pocket Depth Change\",\n     ylab = \"Frequency\")\n\n\n\n\n\n\n\n\nBoth attachment loss and pocket depth change appear to be normally distributed and will not need to be transformed. Attachment loss change is slightly left-tailed but this could be due to outliers, or may just not impact the analysis."
  },
  {
    "objectID": "Project_1_Regression/Project_1_Regression_R/Project_1_Regression.html#Exploratory",
    "href": "Project_1_Regression/Project_1_Regression_R/Project_1_Regression.html#Exploratory",
    "title": "Advanced Data Analysis - Project 1",
    "section": "Exploratory Data Analysis",
    "text": "Exploratory Data Analysis\nHere I will plot the data and perform a number of simple linear regressions to examine relationships between variables in order to determine which covariates to include in the model.\n\nPrimary Explanatory VariableCovariatesDependent VariablesSummary\n\n\n\n5 Treatment Groups3 Treatment Groups\n\n\nLet’s examine if there appears to be a difference in the average attachment loss and pocket depth change according to treatment level\n\nggplot(data_missing, aes(x = factor(trtgroup), y = attachchange)) + \n  geom_boxplot() +\n  labs(title = \"Boxplot of Attachment Loss change by Treatment\")\n\n\n\n\n\n\n\nggplot(data_missing, aes(x = factor(trtgroup), y = pdchange)) + \n  geom_boxplot()  +\n  labs(title = \"Boxplot of Pocket Depth Change by treatment\")\n\n\n\n\n\n\n\n\nAttachment Loss Change\nVisually, there does not appear to be a difference between the treatment group levels (low, medium, high concentration gel) compared to each other. Additionally, there does not seem to be a difference between the treatment groups and the placebo for attachment loss change.\nPocket Depth Change\nInterestingly, the high concentration condition appears to have had a negative effect. Additionally, there seems to be a difference in the treatment groups compared to the no treatment control, but NOT when compared to the placebo. The exception is the low concentration condition compared to the placebo when looking at pocket depth change. Further analysis will reveal whether these differences are statistically significant or not.\nBack to top of tabset\n\n\nThe relationship between low vs medium vs high gel concentration does not look very strong. Furthermore, we technically do not have a large enough sample size in the high concentration condition to include it.\nFor those reasons, let’s make the same comparisons but while combining all treatment levels into one group called treatment.\n\nggplot(data_missing, aes(x = factor(trt3groups), y = attachchange)) + \n  geom_boxplot()\n\n\n\n\n\n\n\nggplot(data_missing, aes(x = factor(trt3groups), y = attachchange)) +\n  geom_boxplot() +\n  labs(title = \"Boxplot of Attachment Loss Change by Treatment\")\n\n\n\n\n\n\n\nggplot(data_missing, aes(x = factor(trt3groups), y = pdchange)) + \n  geom_boxplot() +\n  labs(title = \"Boxplot of Pocket Depth Change by Treatment\")\n\n\n\n\n\n\n\n\nBy eye, it appears as if there is no difference between the placebo and collapsed treatment groups in attachment loss change or pocket depth change. However, both placebo and any treatment conditions appear to have decreased (?) attachment loss and pocket depth. It may be that this study has null results, unless including one of the covariates changes the results. We will see come the analysis section.\nBack to top of tabset\n\n\n\n\n\nNow we will exlore the relationships between our potential covariates and attachment loss and pocket depth change\n\nAgeSitesGenderRaceSmoking Status\n\n\n\n# Plot age vs attachment loss change\nggplot(data_missing, aes(x = age, y = attachchange)) + \n  geom_point() + \n  labs(title = \"Scatterplot of Age vs Attachment Loss Change\")\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n# Plot age vs pocket depth change\nggplot(data_missing, aes(x = age, y = pdchange)) +\n  geom_point() + \n  labs(title = \"Scatterplot of Age vs Pocket Depth Change\")\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nThere does not appear to be any kind of linear relationship between age and attachment loss change or pocket depth change. I will run a model where I include age, but it will likely be removed in the final model.\nBack to top of tabset\n\n\nIs there any relationship between the number of sites measured from and attachment loss change or pocket depth change (e.g. a lower number of sites could lead to less accurate readings)? If so this could be something to include in our data analysis\n\n# Plot sites vs attachment change loss\nggplot(data_missing, aes(x = sites, y = attachchange)) + \n  geom_point() +\n  labs(title = \"Scatterplot of Attachment Loss Change vs Sites\")\n\n\n\n\n\n\n\n# Plot sites vs pocket depth chagne\nggplot(data_missing, aes(x = sites, y = pdchange)) + \n  geom_point() +\n  labs(title = \"Scatterplot of Pocket Depth Change by Sites\")\n\n\n\n\n\n\n\n\nThere does not seem to be a dramatic difference in attachment loss or pocket depth change based on the number of sites measured from. It’s a bit hard to tell with those two low site subjects, but the points are similar enough to the rest of the dataset that I do not think we have to worry about site number in our analysis.\nBack to top of tabset\n\n\nIt is conceivable that there are sex differences in regards to gum health. Let’s examine if there is a difference in attachment loss or pocket depth change based on gender\n\n# Plot gender vs attachment loss change\nggplot(data_missing, aes(x = factor(gender), y = attachchange)) +\n  geom_boxplot() + \n  labs(title = \"Boxplot of Attachment Loss Change by Gender\",\n      x = \"Gender\")\n\n\n\n\n\n\n\n# Plot gender vs pocket depth change\nggplot(data_missing, aes(x = factor(gender), y = pdchange)) +\n  geom_boxplot() + \n  labs(title = \"Boxplot of Pocket Depth Change by Gender\",\n      x = \"Gender\")\n\n\n\n\n\n\n\n\nBy eye, it appears that males may have more pocket depth loss compared with females. This will be a good variable to include as a covariate.\nLet’s try a t-test to see if there is any difference in attachment loss or pocket depth change based on gender\n\n# Running a t-test on attachment loss change by gender\nmale &lt;- data_missing$attachchange[data_missing$gender == 1]\nfemale &lt;- data_missing$attachchange[data_missing$gender == 2]\nt_test_result &lt;- t.test(male, female)\nt_test_result\n\n\n    Welch Two Sample t-test\n\ndata:  male and female\nt = 2.1969, df = 100.8, p-value = 0.03032\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n 0.01003279 0.19682842\nsample estimates:\n  mean of x   mean of y \n-0.03217094 -0.13560154 \n\n\nThere is a significant difference in attachment loss change score based on gender (t = 2.0502, p = 0.04299)\nLet’s repeat for pocket depth change\n\n# Running a t-test on pocket depth change by gender\nmale &lt;- data_missing$pdchange[data_missing$gender == 1]\nfemale &lt;- data_missing$pdchange[data_missing$gender == 2]\nt_test_result &lt;- t.test(male, female)\nt_test_result\n\n\n    Welch Two Sample t-test\n\ndata:  male and female\nt = 2.3534, df = 81.683, p-value = 0.02101\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n 0.01884469 0.22488042\nsample estimates:\n mean of x  mean of y \n-0.2150844 -0.3369470 \n\n\nThere is also a statistically significant difference in pocket depth change based on gender (t = 2.2626, p = 0.02641).\nTherefore I will include gender as a covariate in the analysis!\nBack to top of tabset\n\n\n\n# Plot race vs attachment loss change\nggplot(data_missing, aes(x = factor(race), y = attachchange)) +\n  geom_boxplot() +\n  labs(title = \"Boxplot of Attachment Change by Race\",\n       x = \"Race\")\n\n\n\n\n\n\n\nggplot(data_missing, aes(x = factor(race), y = pdchange)) + \n  geom_boxplot() +\n  labs(title = \"Boxplot of Pocket Depth Change by Race\",\n  x = \"Race\")\n\n\n\n\n\n\n\n\nThere does not seem to be much of a difference in attachment loss or pocket depth based on race. MAYBE African Americans (2) have more pocket depth loss compared to Asians(4), but the sample size was very small for both races (88.1% of the sample identified as White)\nBack to top of tabset\n\n\nWhether the participant is a smoker or not likely has a dramatic effect on gum health. Let’s assess\n\n# Plot smoking status vs attachment loss change\nggplot(data_missing, aes(x = factor(smoker), y = attachchange)) +\n  geom_boxplot() +\n  labs(title = \"Attachment loss by smoking status\",\n       x = \"Smoking Status\")\n\n\n\n\n\n\n\n# Plot smoking status vs pocket depth change\nggplot(data_missing, aes(x = factor(smoker), y = pdchange)) +\n  geom_boxplot() +\n  labs(title = \"Attachment loss by smoking status\",\n       x = \"Smoking Status\")\n\n\n\n\n\n\n\n\nThere does not appear to be any difference in attachment loss or pocket depth change based on smoking status. I will test a model with smoking status included, but it will likely be dropped in the final model.\nBack to top of tabset\n\n\n\n\n\n\nBaseline vs 1 YearAttachment Loss vs Pocket Depth\n\n\nI am curious what the relationship between base line and 1 year measurements are. The study is an RCT so even if baseline measurements affect 1 year measurements, participants should have been randomly assigned to groups so it is essentially controlled for in the study design. It will still be important to assess this relationship as a moderator however. Maybe treatment only worked for those with high attachment loss or pocket depth at the beginning?\nFirst let’s make a simple plot of attachment loss at baseline and at 1 year\n\n# Creating a scatter plot of attachment loss at baseline and 1 year\nggplot(data_missing, aes(x = attachbase, y = attach1year)) + \n  geom_point() +\n  labs(title = \"Scatterplot of Attachment Loss at Baseline and 1 Year\")\n\n\n\n\n\n\n\n\nLet’s check the correlation coefficient.\n\n# Run a correlation test between attachment at base and 1 year\ncorrelation &lt;- cor.test(data_missing$attachbase, data_missing$attach1year)\ncorrelation\n\n\n    Pearson's product-moment correlation\n\ndata:  data_missing$attachbase and data_missing$attach1year\nt = 29.198, df = 101, p-value &lt; 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.9204657 0.9628839\nsample estimates:\n      cor \n0.9455558 \n\n\nThose are highly correlated (R = 0.946, p &lt;.0001)! Let’s run a simple linear regression\n\n# Run a regression with attachment loss at 1 year as the DV and attachment at base as the IV\nmodel &lt;- lm(attach1year ~ attachbase, data = data_missing)\nsummary(model)\n\n\nCall:\nlm(formula = attach1year ~ attachbase, data = data_missing)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.55683 -0.15641 -0.01841  0.15202  0.82062 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.19872    0.06975   2.849  0.00531 ** \nattachbase   0.86452    0.02961  29.198  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2525 on 101 degrees of freedom\nMultiple R-squared:  0.8941,    Adjusted R-squared:  0.893 \nF-statistic: 852.5 on 1 and 101 DF,  p-value: &lt; 2.2e-16\n\n# Plot the model\nggplot(data_missing, aes(x = attachbase, y = attach1year)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", col = \"blue\") +\n  labs(title = \"Simple Linear Regression of Attachment Loss at Baseline and 1 Year\",\n      x = \"Attachment Loss at Baseline\",\n      y = \"Attachment Loss at 1 Year\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nAttachment loss at baseline is a significant predictor of attachment loss at 1 year (t = 29.20, p &lt;.0001). This is important! We should account for attachment loss at baseline by including it as a covariate in our final model!\nLet’s do the same process of pocket depth at baseline and 1 year\n\n# Create a scatterplot of pocket depth at base vs 1 year\nggplot(data_missing, aes(x = pdbase, y = pd1year)) +\n  geom_point() +\n  labs(title = \"Scatterplot of Pocket Depth at Baseline and 1 Year\")\n\n\n\n\n\n\n\n\nSimilar to attachment loss, we see a relationship between pocket depth at baseline and 1 year. Let’s run the correlation.\n\n# Run a correlation between pocket depth at baseline and 1 year\ncorrelation &lt;- cor.test(data_missing$pdbase, data_missing$pd1year)\ncorrelation\n\n\n    Pearson's product-moment correlation\n\ndata:  data_missing$pdbase and data_missing$pd1year\nt = 15.767, df = 101, p-value &lt; 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.7764571 0.8913340\nsample estimates:\n      cor \n0.8432691 \n\n\nWhile not as strong as attachment loss, there is still a strong relationship between pocket depth at baseline and 1 year (R = 0.84, p &lt;.0001). Let’s run the SLR.\n\n# Run an SLR with pocket depth at 1 year as the DV and pocket depth at baseline as the DV\nmodel &lt;- lm(pd1year ~ pdbase, data = data_missing)\nsummary(model)\n\n\nCall:\nlm(formula = pd1year ~ pdbase, data = data_missing)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.55105 -0.17484  0.01996  0.19627  0.64381 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.07504    0.17948   0.418    0.677    \npdbase       0.88346    0.05603  15.767   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2634 on 101 degrees of freedom\nMultiple R-squared:  0.7111,    Adjusted R-squared:  0.7082 \nF-statistic: 248.6 on 1 and 101 DF,  p-value: &lt; 2.2e-16\n\n# Plot the model\nggplot(data_missing, aes(x = pdbase, y = pd1year)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", col = \"blue\") +\n  labs(title = \"Simple Linear Regression of Pocket Depth at Baseline and 1 Year\",\n       x = \"Pocket Depth at Baseline\",\n       y = \"Pocket Depth at 1 year\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nPocket depth at baseline is also a significant predictor of pocket depth at 1 year (t = 15.78, p = &lt;.0001). We should also therefore include pocket depth at baseline as a covariate in our model to control for it!\nBack to top of tabset\n\n\nI am interested in how attachment loss and pocket depth change are related. Since they are both measurements taken from the same sites in the gums, they are likely to be highly correlated. This could have implications on how we perform the analysis and interpret the results.\nFirst, we plot attachment loss change against pocket depth change\n\n# Creating a scatter plot of attachment loss change against pocket depth change \nggplot(data_missing, aes(x = attachchange, y = pdchange)) + \n  geom_point() +\n  labs(title = \"Scatterplot of Attachment Loss Change and Pocket Depth Change\")\n\n\n\n\n\n\n\n\nThat looks like a linear relationship! Let’s run a correlation and an SLR.\n\n# Running the correlation\ncorrelation &lt;- cor.test(data_missing$attachchange, data_missing$pdchange)\ncorrelation\n\n\n    Pearson's product-moment correlation\n\ndata:  data_missing$attachchange and data_missing$pdchange\nt = 6.3717, df = 101, p-value = 5.621e-09\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.3814636 0.6605362\nsample estimates:\n      cor \n0.5354593 \n\n# Running the regression\nmodel &lt;- lm(attachchange ~ pdchange, data = data_missing)\nsummary(model)\n\n\nCall:\nlm(formula = attachchange ~ pdchange, data = data_missing)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.83130 -0.12683  0.00563  0.14913  0.46480 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.06312    0.03441   1.835   0.0695 .  \npdchange     0.55231    0.08668   6.372 5.62e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2343 on 101 degrees of freedom\nMultiple R-squared:  0.2867,    Adjusted R-squared:  0.2797 \nF-statistic:  40.6 on 1 and 101 DF,  p-value: 5.621e-09\n\n# Creating the plot\nggplot(data_missing, aes(x = attachchange, y = pdchange)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", col = \"blue\") +\n  labs(title = \"Simple Linear Regression of Attachment Loss and Pocket Depth Change\",\n       x = \"Pocket Depth Change\",\n       y = \"Attachment Loss Change\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nThe correlation shows that attachment loss and pocket depth change are very correlated (R = 0.54, p &lt; .0001). The simple linear regression shows that pocket depth change significantly predicts attachment loss change (t = 6.37, p &lt;.0001).\nHowever, multicollinearity is only an issue when IVs are correlated with each other. We can still run a multivariate multiple linear regression even though the DVs are correlated. In fact this is often the case, and is one of the justifications for using a multivariate MLR in the first place! Back to top of tabset\n\n\n\n\n\nTreatment Condition\nCollapsing the low, medium, and high concentration gel groups into 1 group does not seem to improve the relationship between treatment and attachment loss or pocket depth change. Only models including all 5 treatment groups will therefore be considered from here on out to keep in alignment with the original study design.\nGender\nGender is related to attachment loss and pocket depth change, and as such will be included in the final model.\nSupressor / Counfound Variables\nA model will be tested with all covariates to assess for possible suppressor / confound variables. But the other potential covariates of race, age, sites, and smoking status were not related to attachment loss or pocket depth change by themselves.\nBaseline vs 1 Year Measurements\nThe baseline measurements of attachment loss and pocket depth were significant predictors of attachment loss and pocket depth at 1 year, respectively. While the RCT nature of the study should ensure that participants were randomly assigned into treatment condition regardless of their baseline measurements, it will still be good practice to include baseline attachment loss and pocket depth into the final model.\nAttachment Loss vs Pocket Depth Change Scores\nAttachment loss and pocket depth change are highly related to each other, but this should not impact the analysis. PI’s will need to be consulted to interpret the clinical significance of findings, and to help fully understand the implications of any possible differences that may arise between attachment loss and pocket depth change in the analysis.\nBack to top of tabset"
  },
  {
    "objectID": "Project_1_Regression/Project_1_Regression_R/Project_1_Regression.html#Model_1",
    "href": "Project_1_Regression/Project_1_Regression_R/Project_1_Regression.html#Model_1",
    "title": "Advanced Data Analysis - Project 1",
    "section": "Running the Model",
    "text": "Running the Model\nFor ease of interpretation I will be performing this analysis as two separate linear regressions, one for each outcome variable of either attachment loss or pocket depth change. I will begin with a simple linear regression only including the PEV and either attachment loss change or pocket depth change.\n\nAttachment Loss ChangePocket Depth ChangePost-Hoc Analysis\n\n\nWe start by constructing a model with attachment loss change as the dependent variable, and treatment group as the independent variable.\n\n# Running a SLR with attachment loss change as the DV, treatment group as the IV with no treatment as the reference group\nmodel_attach1 &lt;- lm(attachchange ~ placebo + low +  medium + high, data = data_missing)\nsummary(model_attach1)\n\n\nCall:\nlm(formula = attachchange ~ placebo + low + medium + high, data = data_missing)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.88283 -0.14813  0.05115  0.17174  0.53945 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.22169    0.05590  -3.966 0.000139 ***\nplacebo      0.13462    0.07906   1.703 0.091771 .  \nlow          0.20388    0.08092   2.520 0.013365 *  \nmedium       0.21514    0.08197   2.625 0.010063 *  \nhigh         0.05690    0.08728   0.652 0.515950    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2681 on 98 degrees of freedom\nMultiple R-squared:  0.09368,   Adjusted R-squared:  0.05669 \nF-statistic: 2.532 on 4 and 98 DF,  p-value: 0.0451\n\n# Apply Bonferroni correction\np_values &lt;- summary(model_attach1)$coefficients[,4] # This line selects the fourth column of the resulting coefficients table from summary(model_attach), which is the p-values\np_adjusted &lt;- p.adjust(p_values, method = \"bonferroni\")\n\n# Compare adjusted p-values to unadjusted p-values\np_comparison &lt;- cbind(p_values, p_adjusted)\np_comparison\n\n                p_values   p_adjusted\n(Intercept) 0.0001392148 0.0006960742\nplacebo     0.0917709125 0.4588545624\nlow         0.0133650749 0.0668253747\nmedium      0.0100625287 0.0503126435\nhigh        0.5159498143 1.0000000000\n\n# Get confidence intervals\nconf_intervals &lt;- confint(model_attach1)\nconf_intervals\n\n                  2.5 %     97.5 %\n(Intercept) -0.33262557 -0.1107577\nplacebo     -0.02226569  0.2915028\nlow          0.04330175  0.3644540\nmedium       0.05247446  0.3777966\nhigh        -0.11629489  0.2300962\n\n\nThe overall model is significant (F(4,98) = 2.53, p = 0.0451). Looking at the individual t-statistics, we see that - following adjustment for multiple pairwise comparisons - the placebo group (p = 0.549), low concentration group (p = 0.0668), and high concentration group (p = 1.000) are not statistically different from the control group. The medium concentration group is approaching significance (p = 0.0503).\nThat was with the no treatment control group as the reference. Let’s see if any of our treatment conditions were different from the placebo group.\n\n# Running a SLR with attachment loss change as the DV, treatment group as the IV with placebo as the reference group\nmodel_attach2 &lt;- lm(attachchange ~ control + low +  medium + high, data = data_missing)\nsummary(model_attach2)\n\n\nCall:\nlm(formula = attachchange ~ control + low + medium + high, data = data_missing)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.88283 -0.14813  0.05115  0.17174  0.53945 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept) -0.08707    0.05590  -1.558   0.1225  \ncontrol     -0.13462    0.07906  -1.703   0.0918 .\nlow          0.06926    0.08092   0.856   0.3941  \nmedium       0.08052    0.08197   0.982   0.3284  \nhigh        -0.07772    0.08728  -0.890   0.3754  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2681 on 98 degrees of freedom\nMultiple R-squared:  0.09368,   Adjusted R-squared:  0.05669 \nF-statistic: 2.532 on 4 and 98 DF,  p-value: 0.0451\n\n# Apply Bonferroni correction\np_values &lt;- summary(model_attach2)$coefficients[,4] # This line selects the fourth column of the resulting coefficients table from summary(model_attach), which is the p-values\np_adjusted &lt;- p.adjust(p_values, method = \"bonferroni\")\n\n# Compare adjusted p-values to unadjusted p-values\np_comparison &lt;- cbind(p_values, p_adjusted)\np_comparison\n\n              p_values p_adjusted\n(Intercept) 0.12254523  0.6127262\ncontrol     0.09177091  0.4588546\nlow         0.39412117  1.0000000\nmedium      0.32836700  1.0000000\nhigh        0.37538435  1.0000000\n\n# Get confidence intervals\nconf_intervals &lt;- confint(model_attach2)\nconf_intervals\n\n                  2.5 %     97.5 %\n(Intercept) -0.19800701 0.02386082\ncontrol     -0.29150281 0.02226569\nlow         -0.09131681 0.22983549\nmedium      -0.08214410 0.24317801\nhigh        -0.25091345 0.09547760\n\n\nAfter applying a bonferroni correction, none of the groups are significantly different from the placebo group (p &gt; .05).\nConclusion\nWhile the overall model was significant, the only groups that were statistically different from the no treatment control were the low and medium concentration gel groups (p &lt; 0.05). However, since this was a placebo-controlled RCT, and none of the treatment groups were significantly different from the placebo group, we can conclude that we fail to reject the null hypothesis that the average attachment loss over 1 year is the same between all groups.\nTables\n\n\nNote: p-values are unadjusted.\nBack to top of tabset\n\n\nNow we will create our model with pocket depth change as the dependent variable and treatment group as the independent variable.\n\n# Running the regression with pocket depth change as the DV, treatment group as the IV with no treatment as the reference group\nmodel_pd &lt;- lm(pdchange ~ placebo + low + medium + high, data = data_missing)\nsummary(model_pd)\n\n\nCall:\nlm(formula = pdchange ~ placebo + low + medium + high, data = data_missing)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.62483 -0.14595 -0.01768  0.16029  0.66130 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.33817    0.05466  -6.187 1.43e-08 ***\nplacebo     -0.01152    0.07730  -0.149   0.8818    \nlow          0.13200    0.07912   1.668   0.0984 .  \nmedium       0.13562    0.08015   1.692   0.0938 .  \nhigh        -0.04413    0.08534  -0.517   0.6063    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2621 on 98 degrees of freedom\nMultiple R-squared:  0.07806,   Adjusted R-squared:  0.04043 \nF-statistic: 2.074 on 4 and 98 DF,  p-value: 0.08994\n\n\nThe overall model is not statistically significant (F(4,98)= 2.074, p = 0.0899). Based on this model, it appears that the gel treatment has no effect on pocket depth change after 1 year.\nConclusion\nNo groups were significantly different from each other in pocket depth change after 1 year (F(4,98)= 2.074, p = 0.0899), and we fail to reject the null hypothesis that the average pocket depth change over 1 year is the same between all groups.\nTables\n\nNote: p-values are unadjusted\nBack to top of tabset\n\n\nWe did not find a main effect based on treatment group. However I am still interested if including any of the covariates changes the results.\n\n# Run a SLR with attachment loss change as the DV and gender as a covariate\nmodel2_attach &lt;- lm(attachchange ~ placebo + low + medium + high + gender, data = data_missing) \nsummary(model2_attach)\n\n\nCall:\nlm(formula = attachchange ~ placebo + low + medium + high + gender, \n    data = data_missing)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.86656 -0.17533  0.02334  0.16395  0.57717 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept) -0.07460    0.10988  -0.679   0.4988  \nplacebo      0.12330    0.07883   1.564   0.1210  \nlow          0.19310    0.08064   2.395   0.0186 *\nmedium       0.21118    0.08143   2.593   0.0110 *\nhigh         0.06704    0.08690   0.771   0.4423  \ngender      -0.08675    0.05593  -1.551   0.1242  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2662 on 97 degrees of freedom\nMultiple R-squared:  0.1156,    Adjusted R-squared:  0.07003 \nF-statistic: 2.536 on 5 and 97 DF,  p-value: 0.03346\n\n# Apply Bonferroni correction\np_values &lt;- summary(model2_attach)$coefficients[,4] # This line selects the fourth column of the resulting coefficients table from summary(model_attach), which is the p-values\np_adjusted &lt;- p.adjust(p_values, method = \"bonferroni\")\n\n# Compare adjusted p-values to unadjusted p-values\np_comparison &lt;- cbind(p_values, p_adjusted)\np_comparison\n\n              p_values p_adjusted\n(Intercept) 0.49883151 1.00000000\nplacebo     0.12104977 0.72629859\nlow         0.01856222 0.11137333\nmedium      0.01097151 0.06582903\nhigh        0.44234244 1.00000000\ngender      0.12415264 0.74491582\n\n# Get confidence intervals\nconf_intervals &lt;- confint(model2_attach)\nconf_intervals\n\n                  2.5 %     97.5 %\n(Intercept) -0.29268916 0.14349242\nplacebo     -0.03315881 0.27976619\nlow          0.03304940 0.35315426\nmedium       0.04956814 0.37278247\nhigh        -0.10544029 0.23951403\ngender      -0.19775102 0.02425639\n\n\nA model including gender does not seem to help anything. What about with all covariates (just for fun)?\n\n# Run a SLR with attachment loss change as the DV and gender as a covariate\nmodel3_attach &lt;- lm(attachchange ~ placebo + low + medium + high + gender + race + age + smoker + sites + attachbase + pdbase , data = data_missing) \nsummary(model3_attach)\n\n\nCall:\nlm(formula = attachchange ~ placebo + low + medium + high + gender + \n    race + age + smoker + sites + attachbase + pdbase, data = data_missing)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.60415 -0.16360  0.02131  0.16645  0.57481 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.160630   0.480855   0.334 0.739127    \nplacebo      0.017898   0.079113   0.226 0.821541    \nlow          0.151477   0.077598   1.952 0.054074 .  \nmedium       0.172196   0.079066   2.178 0.032060 *  \nhigh         0.045948   0.081336   0.565 0.573557    \ngender      -0.050360   0.054953  -0.916 0.361924    \nrace         0.032325   0.026871   1.203 0.232181    \nage         -0.002472   0.002709  -0.912 0.364067    \nsmoker       0.069384   0.055052   1.260 0.210844    \nsites       -0.001501   0.002539  -0.591 0.555885    \nattachbase  -0.165154   0.043233  -3.820 0.000246 ***\npdbase       0.094444   0.072226   1.308 0.194370    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2456 on 89 degrees of freedom\n  (2 observations deleted due to missingness)\nMultiple R-squared:  0.2908,    Adjusted R-squared:  0.2032 \nF-statistic: 3.318 on 11 and 89 DF,  p-value: 0.0007435\n\n# Apply Bonferroni correction\np_values &lt;- summary(model3_attach)$coefficients[,4] # This line selects the fourth column of the resulting coefficients table from summary(model_attach), which is the p-values\np_adjusted &lt;- p.adjust(p_values, method = \"bonferroni\")\n\n# Compare adjusted p-values to unadjusted p-values\np_comparison &lt;- cbind(p_values, p_adjusted)\np_comparison\n\n               p_values  p_adjusted\n(Intercept) 0.739127379 1.000000000\nplacebo     0.821540779 1.000000000\nlow         0.054074263 0.648891153\nmedium      0.032060232 0.384722790\nhigh        0.573556835 1.000000000\ngender      0.361924208 1.000000000\nrace        0.232180658 1.000000000\nage         0.364067210 1.000000000\nsmoker      0.210844428 1.000000000\nsites       0.555885079 1.000000000\nattachbase  0.000246493 0.002957916\npdbase      0.194369803 1.000000000\n\n# Get confidence intervals\nconf_intervals &lt;- confint(model3_attach)\nconf_intervals\n\n                   2.5 %       97.5 %\n(Intercept) -0.794818756  1.116078333\nplacebo     -0.139298371  0.175094025\nlow         -0.002709329  0.305662938\nmedium       0.015093048  0.329298305\nhigh        -0.115665991  0.207561289\ngender      -0.159551369  0.058830891\nrace        -0.021067104  0.085716311\nage         -0.007855313  0.002911679\nsmoker      -0.040003508  0.178771331\nsites       -0.006546011  0.003543891\nattachbase  -0.251056725 -0.079251471\npdbase      -0.049067286  0.237955593\n\n\nNope. Interestingly the best predictor of attachment loss at 1 year is attachment loss at baseline. The results for pocket depth are likely the same.\nI also want to assess if controlling for baseline attachment loss or pocket depth change affects things, since those were strong predictors of each measurement at 1 year (still need to add those SLRs).\n\nmodel4_attach &lt;- lm(attachchange ~ placebo + low + medium + high + attachbase, data = data_missing)\nsummary(model4_attach)\n\n\nCall:\nlm(formula = attachchange ~ placebo + low + medium + high + attachbase, \n    data = data_missing)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.52469 -0.16400  0.02084  0.15231  0.73422 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.10707    0.09304   1.151   0.2527    \nplacebo      0.04202    0.07616   0.552   0.5824    \nlow          0.14605    0.07592   1.924   0.0573 .  \nmedium       0.17586    0.07622   2.307   0.0232 *  \nhigh         0.02665    0.08087   0.330   0.7425    \nattachbase  -0.12902    0.03039  -4.246 4.99e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2475 on 97 degrees of freedom\nMultiple R-squared:  0.2357,    Adjusted R-squared:  0.1963 \nF-statistic: 5.984 on 5 and 97 DF,  p-value: 7.236e-05\n\n# Apply Bonferroni correction\np_values &lt;- summary(model4_attach)$coefficients[,4] # This line selects the fourth column of the resulting coefficients table from summary(model_attach), which is the p-values\np_adjusted &lt;- p.adjust(p_values, method = \"bonferroni\")\n\n# Compare adjusted p-values to unadjusted p-values\np_comparison &lt;- cbind(p_values, p_adjusted)\np_comparison\n\n                p_values   p_adjusted\n(Intercept) 2.526878e-01 1.0000000000\nplacebo     5.824313e-01 1.0000000000\nlow         5.731590e-02 0.3438953943\nmedium      2.317045e-02 0.1390227138\nhigh        7.424826e-01 1.0000000000\nattachbase  4.989769e-05 0.0002993861\n\n\nAfter controlling for treatment group, the only significant predictor of attachment loss change is baseline attachment loss scores (padj = 0.001)\nThese models were just for exploration, fun, and practice. The final models selected are the SLR’s with treatment group as the IV and either attachment loss change or pocket depth change as the DV. Back to top of tabset"
  },
  {
    "objectID": "Project_1_Regression/Project_1_Regression_R/Project_1_Regression.html#Assumptions",
    "href": "Project_1_Regression/Project_1_Regression_R/Project_1_Regression.html#Assumptions",
    "title": "Advanced Data Analysis - Project 1",
    "section": "Evaluating Assumptions",
    "text": "Evaluating Assumptions\nIn order to evaluate the assumptions of our models, we will first gather the residuals of the model predicting attachment loss change score and the model predicting pocket depth change score.\n\n# Calculate the jackknife residuals of model_attach\njackknife_residuals_attach &lt;- rstudent(model_attach1)\n\n# Calculate the jackknife residuals of model_pd\njackknife_residuals_pd &lt;- rstudent(model_pd)\n\nNow that we have our residuals, we can take a closer look at the assumptions.\n\nLinearityIndependenceNormalityEqual Variances (Homoscedasticity)Residuals Centered Around ZeroSummary\n\n\nSince the IV is categorical, we do not need to assess linearity.\nBack to top of tabset\n\n\nIndependence can be assessed in part by the study design and how the data was collected. Based on the information provided by the PI, I will assume subjects are independent from each other (e.g. not siblings).\nAdditionally, we can examine a scatter plot of the model’s residuals against any time point variable (such as ID). Let’s do that.\n\n# Create a scatterplot of jackknife residuals vs ID to assess independence for pocket depth change\nggplot(data_missing, aes(x = id, y = jackknife_residuals_attach)) + \n  geom_point() +\n  geom_hline(yintercept = 0) +\n  labs(title = \"Scatterplot of Jackknife Residuals vs ID for Attachment Loss Change\",\n       x = \"ID\",\n       y = \"Jackknife Residuals\")\n\n\n\n\n\n\n\n# Create a scatterplot of jackknife residuals vs ID to assess independence for pocket depth change\nggplot(data_missing, aes(x = id, y = jackknife_residuals_pd)) + \n  geom_point() +\n  geom_hline(yintercept = 0) +\n  labs(title = \"Scatterplot of Residuals vs ID for Pocket Depth Change\",\n       x = \"ID\",\n       y = \"Jackknife Residuals\")\n\n\n\n\n\n\n\n\nThe pattern appears random, suggesting independence.\nNote: The gap between ID’s 170 and 200 looks odd, but is an artifact from how the experimenters assigned ID, with females starting at 101, and males starting at 201.\nBack to top of tabset\n\n\nHere we will asses that, for any fixed value of X, Y has a normal distribution. We will do this using Q-Q plots and histograms of the residuals.\n\n# Make the Q-Q plots using the jackknife residuals for attachment loss change\nqqnorm(jackknife_residuals_attach, main = \"Q-Q plots of Jackknife Residuals for model_attach\")\nqqline(jackknife_residuals_attach, col = \"black\")\n\n\n\n\n\n\n\n# Create histogram of jackknife residuals for model_attach\nhist(jackknife_residuals_attach, main = \"Histogram of Jackknife Residuals\",\n     xlab = \"Jackknife Residuals\",\n     col = \"lightblue\",\n     border = \"black\")\n\n\n\n\n\n\n\n# Make the Q-Q plots using the jackknife residuals for pocket depth change\nqqnorm(jackknife_residuals_pd, main = \"Q-Q plots of Jackknife Residuals for model_pd\")\nqqline(jackknife_residuals_pd, col = \"black\")\n\n\n\n\n\n\n\n# Create histogram of jackknife residuals for model_pd\nhist(jackknife_residuals_pd, main = \"Histogram of Jackknife Residuals\",\n     xlab = \"Jackknife Residuals\",\n     col = \"lightblue\",\n     border = \"black\")\n\n\n\n\n\n\n\n\nThe histogram for attachment loss change is a little left-tailed. This could be from an outlier. Comparatively, the Q-Q plot and histogram of the residuals for pocket depth change are normally distributed.\nWe can also include the Shapiro-Wilk test of normality, which provides a p-value and allows us to numerically establish that the assumption of normality is met. If the p-value is &lt; 0.05 you conclude that the assumption of normality is not met.\n\nshapiro.test(jackknife_residuals_attach)\n\n\n    Shapiro-Wilk normality test\n\ndata:  jackknife_residuals_attach\nW = 0.9604, p-value = 0.003601\n\nshapiro.test(jackknife_residuals_pd)\n\n\n    Shapiro-Wilk normality test\n\ndata:  jackknife_residuals_pd\nW = 0.99059, p-value = 0.6933\n\n\nThe assumption of normality is violated for the attachment loss change model (p = 0.0036), but not for the pocket depth change model (p = 0.6933). Looking at the histogram of the residuals for attachment loss change, this is likely due to an outlier.\nSummary\nFor attachment loss change, we have a slight violation of normality, which could be due to the presence of an outlier. However, regressions are robust to violations of assumptions and this may not actually be an issue. Outliers in the model will be assessed using jackknife residuals to confirm that these points do not have an excessive amount of influence on the model.\nBased on the Q-Q plots and histograms of the residuals for pocket depth change, we can conclude that we satisfy the assumption of normality.\nBack to top of tabset\n\n\nUsing the scale-location plot will allow us to evaluate the constant variance assumption. This will allow us to see whether the variability of the residuals is roughly constant between each group (Source).\nTo assess homoscedasticity we examine the residual scatterplots by treatment group. The warning sign to look for here is if the variance differs greatly across groups.\n\n# Make the residual scatterplot using the jackknife residuals for model_attach\nggplot(data_missing, aes(x = trtgroup, y = jackknife_residuals_attach)) +\n  geom_point() + \n  labs(title = \"Jackknife Residuals vs Treatment Condition for Attachment Loss Change\",\n       x = \"Treatment Condition\",\n       y = \"JackKnife Residuals\")\n\n\n\n\n\n\n\n# Make the residual scatterplot using the jackknife residuals for model_pd\nggplot(data_missing, aes(x = trtgroup, y = jackknife_residuals_pd)) +\n  geom_point() + \n  labs(title = \"Jackknife Residuals vs Treatment Condition for Pocket Depth Change\",\n       x = \"Treatment Condition\",\n       y = \"JackKnife Residuals\")\n\n\n\n\n\n\n\n\nIt appears that our variances in all groups are equal!\nAdditionally, while it is not recommended to perform a statistical test to assess for equality of variances (because formal tests of equality of variance are not very powerful), we can still do this using Bartlett’s test.\n\nbartlett.test(attachchange ~ trtgroup, data = data_missing)\n\n\n    Bartlett test of homogeneity of variances\n\ndata:  attachchange by trtgroup\nBartlett's K-squared = 2.5462, df = 4, p-value = 0.6364\n\n\nThe null hypothesis of the Bartlett test is that the variances are equal. Thus failing to reject the null (p &gt; 0.05) indicates that the data are consistent with the equal variance assumption.\nSo we meet the assumption of equality of variances, looking good!\nBack to top of tabset\n\n\nTo start, we can simply check that the mean of our residuals is close to 0.\n\n#### For attachment loss change score\n# Generate the mean of the jackknife residuals for attachment loss change. Should be close to 0.\nmean(jackknife_residuals_attach)\n\n[1] -0.004132486\n\n\nWe are looking good for attachment loss change score. What about for pocket depth change?\n\n#### For pocket depth change score\n# Generate the mean of the jackknife residuals for attachment loss change. Should be close to 0.\nmean(jackknife_residuals_pd)\n\n[1] -6.695748e-06\n\n\nA more sophisticated approach is to plot the fitted values vs jackknife residuals. We can then compare the trend between groups to see if they are random. The fitted line should gravitate around 0 with no obvious trends.\n\n# Generate vectors containing the fitted values vs jackknife residuals so we can plot them \nfitted_values_pd &lt;- fitted(model_pd)\n\nggplot(data_missing, aes(x = fitted_values_pd, y = jackknife_residuals_pd)) + \n  geom_point() +\n  geom_smooth(method = \"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n  labs(title = \"Jackknife Residuals vs Fitted Values for Attachment Loss Change\",\n       x = \"Fitted Values\",\n       y = \"Jackknife Residuals\")\n\n$x\n[1] \"Fitted Values\"\n\n$y\n[1] \"Jackknife Residuals\"\n\n$title\n[1] \"Jackknife Residuals vs Fitted Values for Attachment Loss Change\"\n\nattr(,\"class\")\n[1] \"labels\"\n\n\nThe pattern looks random and we can conclude we meet this assumption.\nBack to top of tabset\n\n\nWe meet the assumptions of independence, equal variances, and errors centered around zero required for this analysis. There is a slight violation of normality for the attachment loss change model, but this could be due to an outlier."
  },
  {
    "objectID": "Project_1_Regression/Project_1_Regression_R/Project_1_Regression.html#Multiple_imputation",
    "href": "Project_1_Regression/Project_1_Regression_R/Project_1_Regression.html#Multiple_imputation",
    "title": "Advanced Data Analysis - Project 1",
    "section": "Multiple Imputation",
    "text": "Multiple Imputation\n\nBackgroundCoding ExamplePerforming the Multiple Imputation\n\n\nNote: This information is taken from Biostats II (BIOS 6612) slides, week 13 lecture 20, and from Flexible Imputation of Missing Data 2nd edition.\nMissing Data\nTo review, there are several assumptions for missing data. The first is MCAR (Missing Completely at Random). This is when the probability of having a missing value is the same for everyone, and is rarely ever the case with real data.\nThe second is MAR (missing at random). This is when the probability of having a missing value is the same within groups defined by the observed data. That is, if we know that second variable by which the data is NOT MCAR, in addition to the measurements, we can assume MCAR within that second variable. For instance, if we know the missing data is not equal between the sexes, then we can assume MCAR within groups defined by sex. MAR is often observed in situations such as males missing more data than females, or older participants missing more data than younger ones, etc. MAR is more general and more realistic than MCAR. Modern missing data assumptions typically start from the MAR assumption.\nMNAR (Missing Not At Random) is the final category of missingness, and is when the the probability of having a missing value varies for reasons that are unknown to you. For example, in public opinion research, those with weaker opinions may respond less than those with stronger opinions, and you may not know this ahead of time. MNAR is the most complex and if you have it, you have your work cut out for you. In that situation, you can find more data about the causes for the missingness, or run lots and lots of sensitivity analyses.\nAnd as they say, the best way to handle missing data is not to have it.\nMultiple Imputation\nIn the current experiment, we know that data is not MCAR (males are missing more data than females), and thus cannot simply throw out subjects with missing values. If the data are not MCAR, listwise deletion (deleting any subject with a missing value) can severely bias estimates of means, regression coefficients and correlations.\nMultiple Imputation is a commonly used method to handle missing data when data is not MCAR, but is MAR. It is a process by which you use observed data to “predict” missing data, then use those “imputed” values in further analyses. Multiple imputation builds upon and pools together “single” imputation approaches.\nFor example, there exists very simple ways of imputing data, such as plugging in the average or median values in place of missing values. This is not sophisticated and kind of sketchy.\nThen there exists regression imputation, where for each variable you fit a regression model of it to the observed data, and then use that model to predict missing values, and use those predictions in place of the missing values. However, this injects bias into the estimate for the correlation between X and Y (since the values fall perfectly in line with the hypothesis that X and Y have a non zero correlation. (and are in a perfect line)).\n\nThen there is stochastic regression imputation, which is like regression imputation but adds noise back into the imputations based on the variance of their residuals. This helps account for variance in the results due to missing data.\n\nMultiple imputation works by fitting multiple stochastic imputation models (can be 100’s or 1000’s), analyzing each dataset separately to produce estimates of interest, and finally pooling together these statistics of interest. If done properly, the pooled statistics are unbiased under MAR, and the SE’s will be correct!\n\nMultiple Imputation is “simple, elegant and powerful. It is simple because it fills the holes in the data with plausible values. It is elegant because the uncertainty about the unknown data is coded in the data itself. And it is powerful because it can solve “other” problems that are actually missing data problems in disguise.” - Stef van Buuren\nBack to top of tabset\n\n\nFirst we will begin with an example of multiple imputation using the built in airquality dataset in R.\n\n# Examining data set and missingness\nhead(airquality)\n\n  Ozone Solar.R Wind Temp Month Day\n1    41     190  7.4   67     5   1\n2    36     118  8.0   72     5   2\n3    12     149 12.6   74     5   3\n4    18     313 11.5   62     5   4\n5    NA      NA 14.3   56     5   5\n6    28      NA 14.9   66     5   6\n\ncolSums(is.na(airquality))\n\n  Ozone Solar.R    Wind    Temp   Month     Day \n     37       7       0       0       0       0 \n\nvis_miss(airquality)\n\n\n\n\n\n\n\n\nWe’re missing 37 values in the Ozone column (24% of values!). We will have to address this missing data somehow.\nMean Imputation\nWe could quickly fill in missing data with mean imputation…\n\n# Here's how you do mean imputation in mice\nimp &lt;- mice(airquality, method = \"mean\", m = 1, maxit = 1)\n\n\n iter imp variable\n  1   1  Ozone  Solar.R\n\n\nBut mean imputation will “underestimate the variance, disturb the relations between variables, bias almost any estimate other than the mean and bias the estimate of the mean when data are not MCAR.” You can use mean imputation as a quick fix when there’s a handful of missing values, but it should be avoided in general.\nRegression Imputation\nWe could alternatively do regression imputation, where we fit a model and then use that model to predict the missing values. In regression imputation, you are essentially using the observed data to predict the missing data.\n\n# Performing regression imputation using the mice package\ndata_example &lt;- airquality[, c(\"Ozone\", \"Solar.R\")]\nimp &lt;- mice(data_example, method = \"norm.predict\", seed = 1,\n           m = 1, print = FALSE)\n\n# Plotting missing vs observed values\nxyplot(imp, Ozone ~ Solar.R,\n       main = \"Missing vs Observed values for Ozone ~ Solar.R\",\n       auto.key = list(corner = c(0,1),\n                  points = TRUE, \n                  text = c(\"Observed\", \"Missing\"), col = c(\"steelblue\", \"red3\")))\n\n\n\n\n\n\n\n\nYou predict the values of the missing datapoints with the regression equation resulting from your model. That is, the imputed values correspond to the most likely values under that model. However, the imputed values (red) vary less than the observed values (blue). So each value is the best under the model, but it is very unlikely that the real values would have had this distribution.\nRegression imputation yields unbiased estimates of the means and of the regression weights of the model under MCAR. It also does so under MAR, provided that the factors that influence missinginess are part of the model. However, correlations are biased to be greater/higher.\n“Regression imputation, as well as its modern incarnations in machine learning is probably the most dangerous of all methods described here. We may be led to believe that we’re to do a good job by preserving the relations between the variables. In reality however, regression imputation artificially strengthens the relations in the data. Correlations are biased upwards. Variability is underestimated. Imputations are too good to be true. Regression imputation is a recipe for false positive and spurious relations.”\nStochastic regression imputation\nStochastic regression imputation is a refinement of regression imputation that attempts to address correlation bias by adding noise back into the predictions of the missing values.\n\n# Impute Ozone from Solar.R by stochastic regression imputation using the mice package.\ndata_example &lt;- airquality[, c(\"Ozone\", \"Solar.R\")]\n\n# Perform stochastic regression imputation\nimp &lt;- mice(data_example, method = \"norm.nob\", m = 1, maxit = 1, seed = 1, print = FALSE)\n\n# Plotting missing vs observed values\nxyplot(imp, Ozone ~ Solar.R,\n       main = \"Missing vs Observed values for Ozone ~ Solar.R\",\n       auto.key = list(corner = c(0,1),\n                  points = TRUE, \n                  text = c(\"Observed\", \"Missing\"), col = c(\"steelblue\", \"red3\")))\n\n\n\n\n\n\n\n\n“The method = norm.nob argument requests a plain, non-Bayesian, stochastic regression method. This method first estimates the intercept, slope and residual variance under the linear model, then calculates the predicted value for each missing value, and adds a random draw from the residual to the prediction”.\nThis can create issues though, such as we have one imputed value that is negative! Which might not be plausible (such as in this case, there is no such thing as a negative ozone level).\nA more convenient solution is multiple imputation\nMultiple Imputation\nMultiple imputation creates m &gt; 1 complete data sets. The m results are then pooled into a final point estimate plus standard error. So each data set is identical in observed values, but differs in imputed values.\nWe then estimate the parameters of interest from each dataset. This is done by applying the analytic method you would have used if the dataset was complete in the first place (here, a regression). The results of the model on each dataset will differ because the data is different. These differences are caused by the uncertainty of what value to impute.\nThe last step is that these parameter estimates are then pooled together into a single value, and its variance estimated. The variance is assessed by combining the “within-imputation variance with the extra variance caused by the missing data (between-imputation variance)”. So under the appropriate conditions the pooled estimates are unbiased. MI solves the problem of ‘too small’ SEs of other imputation methods we just covered.\nNow let’s perform the multiple imputation on the airquality data set.\n\n# Perform multiple imputation on the airquality data set\nimp &lt;- mice(airquality, seed = 1, m = 20, print = FALSE) # This line imputes the missing data 20 times\n\n# Fit a linear regression \nmodel_imp &lt;- with(imp, lm(Ozone ~ Wind + Temp + Solar.R)) # You have to run the regression with \"with(imp, lm(etc))\n                  \nsummary(model_imp) # This runs a regression on each imputed dataset (so 20 different regressions)\n\n# A tibble: 80 × 6\n   term        estimate std.error statistic  p.value  nobs\n   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;int&gt;\n 1 (Intercept) -60.7      17.9        -3.40 8.78e- 4   153\n 2 Wind         -2.95      0.514      -5.75 4.96e- 8   153\n 3 Temp          1.53      0.197       7.74 1.42e-12   153\n 4 Solar.R       0.0671    0.0184      3.64 3.70e- 4   153\n 5 (Intercept) -56.8      18.5        -3.07 2.56e- 3   153\n 6 Wind         -3.33      0.532      -6.25 4.04e- 9   153\n 7 Temp          1.56      0.205       7.58 3.49e-12   153\n 8 Solar.R       0.0554    0.0191      2.89 4.37e- 3   153\n 9 (Intercept) -74.0      18.7        -3.95 1.19e- 4   153\n10 Wind         -2.69      0.540      -4.99 1.67e- 6   153\n# ℹ 70 more rows\n\nsummary(pool(model_imp)) # This pools together the parameters of all 20 regressions (statistic is Wald's test) into a single model.\n\n         term     estimate   std.error statistic       df      p.value\n1 (Intercept) -65.87829658 23.09377412 -2.852643 69.97033 5.696702e-03\n2        Wind  -3.01897171  0.66252377 -4.556775 70.51194 2.125022e-05\n3        Temp   1.63483547  0.25107557  6.511328 75.99913 7.203792e-09\n4     Solar.R   0.05861581  0.02267832  2.584662 90.10797 1.135441e-02\n\n\n\n# mids workflow using pipes\nest3 &lt;- airquality %&gt;%\n  mice(seed = 1, print = FALSE, m = 20) %&gt;%\n  with(lm(formula = Ozone ~ Wind + Temp + Solar.R)) %&gt;%\n  pool()\nsummary(est3)\n\n         term     estimate   std.error statistic       df      p.value\n1 (Intercept) -65.87829658 23.09377412 -2.852643 69.97033 5.696702e-03\n2        Wind  -3.01897171  0.66252377 -4.556775 70.51194 2.125022e-05\n3        Temp   1.63483547  0.25107557  6.511328 75.99913 7.203792e-09\n4     Solar.R   0.05861581  0.02267832  2.584662 90.10797 1.135441e-02\n\n\n\n# Run a regression on the imputed airquality data set. \nimp &lt;- mice(airquality, seed = 1, print = FALSE)\n\nfit &lt;- with(imp, lm(Ozone ~ Solar.R))\n\n# Print out the estimates for the first and second data set\ncoef(fit$analyses[[1]])\n\n(Intercept)     Solar.R \n 22.2963201   0.1057437 \n\ncoef(fit$analyses[[2]])\n\n(Intercept)     Solar.R \n 20.7891622   0.1140081 \n\n\nNotice that the paremter estimates differ because of the uncertainty created by the missing data.\nApplying the standard pooling rules is done with\n\nest &lt;- pool(fit)\nsummary(est)\n\n         term   estimate  std.error statistic       df      p.value\n1 (Intercept) 21.5321395 5.67991951  3.790923 136.3544 0.0002245626\n2     Solar.R  0.1091546 0.02846503  3.834693 102.7169 0.0002170873\n\n\nAny R expression produced by expression() can be used on the multiply imputed data…\n\n# Extract residuals and fitted values\n\n# This gets you ALL the residuals for ALL the models/dataset\nimp_residuals &lt;- sapply(model_imp[[4]], residuals)\n\n# Same thing but for the fitted\nimp_fitted &lt;- sapply(model_imp[[4]],fitted)\n\n# Don't know where to go from here. Apparently you can take the average of all of these to get the average residuals, but it's not published anywhere.\n\nWe can compare the results of our multiple imputation model with the listwise deletion model to see how different they came out.\n\nmodel_non_imp &lt;- lm(Ozone ~ Wind + Temp + Solar.R, data = airquality, na.action = na.omit)\nsummary(model_non_imp)\n\n\nCall:\nlm(formula = Ozone ~ Wind + Temp + Solar.R, data = airquality, \n    na.action = na.omit)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-40.485 -14.219  -3.551  10.097  95.619 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -64.34208   23.05472  -2.791  0.00623 ** \nWind         -3.33359    0.65441  -5.094 1.52e-06 ***\nTemp          1.65209    0.25353   6.516 2.42e-09 ***\nSolar.R       0.05982    0.02319   2.580  0.01124 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 21.18 on 107 degrees of freedom\n  (42 observations deleted due to missingness)\nMultiple R-squared:  0.6059,    Adjusted R-squared:  0.5948 \nF-statistic: 54.83 on 3 and 107 DF,  p-value: &lt; 2.2e-16\n\nsummary(pool(model_imp))\n\n         term     estimate   std.error statistic       df      p.value\n1 (Intercept) -65.87829658 23.09377412 -2.852643 69.97033 5.696702e-03\n2        Wind  -3.01897171  0.66252377 -4.556775 70.51194 2.125022e-05\n3        Temp   1.63483547  0.25107557  6.511328 75.99913 7.203792e-09\n4     Solar.R   0.05861581  0.02267832  2.584662 90.10797 1.135441e-02\n\n\nOur regression using MI and the complete case (i.e. listwise deletion) data set are comparable!\nNote: what we are NOT doing in multiple imputation is taking an average of the imputed data and running a model on that as if its a single, complete data set. “Researchers are often tempted to average the multiply imputed data, and analyze the averaged data as if it were complete. This method yields incorrect standard errors, confidence intervals and p-values, and thus should not be used if any form of statistical testing or uncertainty analysis is to be done on the imputed data. The reason is that the procedure ignores the between-imputation variability, and hence shares all the drawbacks of single imputation”.\nNote: It’s recommended to impute then transform, because if you create variables based on other variables, there are relationships between those variables that MI doesn’t account for (it might create combinations of variables that are unrealistic)\nYou can do this as\n\n# Example of impute then transform method (i.e. how to add variables to an imputed dataset)\ndata_ex &lt;- boys[, c(\"age\", \"hgt\", \"wgt\", \"hc\", \"reg\")]\nimp &lt;- mice(data_ex, print = FALSE, seed = 1)\n \n# put the data in long format\nlong &lt;- mice::complete(imp, \"long\", include = TRUE)\n\nlong$new_var &lt;- with(long, 100 * wgt / hgt)\n\nimp.itt &lt;- as.mids(long)\n\nBack to top of tabset\n\n\nNow that we have reviewed the background and gone through coding examples of multiple imputation, we are ready to perform it for the dataset for Project 1.\n\n# Pool the parameters of interest for a regression on the imputed values for attach change control as reference\nmodel_imp &lt;- data %&gt;%\n  mice(seed = 1, print = FALSE, m = 20) %&gt;%\n  with(lm(attachchange ~ placebo + low + medium + high)) %&gt;%\n  pool()\n\nWarning: Number of logged events: 1158\n\n\nNone of the p-values for the pooled regressions on the MI data are significant (p &gt; 0.05).\nHow do those parameter estimates compare to the complete case analysis model?\n\n# Compare coefficients to complete case analysis for attachment loss with control as reference category\nMI &lt;- summary(model_imp)\nMI\n\n         term   estimate  std.error  statistic       df     p.value\n1 (Intercept) -0.1853493 0.06920536 -2.6782512 65.79196 0.009337056\n2     placebo  0.1158204 0.09076258  1.2760813 91.37003 0.205161055\n3         low  0.1608502 0.10027097  1.6041549 59.48514 0.113976747\n4      medium  0.1612700 0.11393844  1.4154130 37.61151 0.165174849\n5        high  0.0930271 0.15435067  0.6026997 18.41109 0.554060413\n\ncomplete_case &lt;- summary(model_attach1)\ncomplete_case$coefficients\n\n               Estimate Std. Error    t value     Pr(&gt;|t|)\n(Intercept) -0.22169165 0.05590110 -3.9657832 0.0001392148\nplacebo      0.13461856 0.07905610  1.7028232 0.0917709125\nlow          0.20387790 0.08091650  2.5196086 0.0133650749\nmedium       0.21513551 0.08196711  2.6246567 0.0100625287\nhigh         0.05690063 0.08727557  0.6519652 0.5159498143\n\n\nThey are drastically different, although still not significant.\nWe can also plot the imputed values for a sample of m dataset (not too many or it’s hard to see!) in order to visually assess that our imputed values are comparabe to the observed values.\n\n# Plot imputed values for m = 5 data set\nimpy &lt;- mice(data, seed = 1, print = FALSE, m = 5)\n\nWarning: Number of logged events: 285\n\nmodel_impy &lt;- with(imp, attachchange ~ trtgroup)\n\n# This is cool, here's how I can plot the imputed values for attach change \nxyplot(impy, attachchange ~ trtgroup,\n       main = \"Example of Imputed Values for m = 5 Dataset\",\n       auto.key = list(space = \"right\",\n        points = TRUE, \n        text = c(\"Observed\", \"Missing\"), col = c(\"blue4\", \"red3\")))\n\n\n\n\n\n\n\n\nWhat about for the same model but with placebo as the reference group?\n\n# Pool the parameters of interest for a regression on the imputed values for attach change placebo as reference\nmodel_imp &lt;- data %&gt;%\n  mice(seed = 1, print = FALSE, m = 20) %&gt;%\n  with(lm(attachchange ~ control + low + medium + high)) %&gt;%\n  pool()\n\nWarning: Number of logged events: 1158\n\n# Compare coefficients for complete case analysis with attachment with placebo as reference category\nsummary(model_imp)\n\n         term    estimate  std.error  statistic       df   p.value\n1 (Intercept) -0.06952890 0.06697153 -1.0381859 75.81298 0.3024834\n2     control -0.11582043 0.09076258 -1.2760813 91.37003 0.2051611\n3         low  0.04502973 0.11027780  0.4083299 41.79227 0.6851161\n4      medium  0.04544952 0.11870540  0.3828766 33.28878 0.7042453\n5        high -0.02279333 0.16350701 -0.1394028 16.62753 0.8908064\n\nsummary(model_attach2)$coefficients\n\n               Estimate Std. Error    t value   Pr(&gt;|t|)\n(Intercept) -0.08707309 0.05590110 -1.5576275 0.12254523\ncontrol     -0.13461856 0.07905610 -1.7028232 0.09177091\nlow          0.06925934 0.08091650  0.8559360 0.39412117\nmedium       0.08051695 0.08196711  0.9823081 0.32836700\nhigh        -0.07771793 0.08727557 -0.8904889 0.37538435\n\n\nThe estimates are more comparable, although still pretty different. Still not significant with both methods.\nAnd finally for pocket depth change.\n\n# Pool the parameters of interest for a regression on the imputed values for pocket depth  change placebo as reference\nmodel_imp &lt;- data %&gt;%\n  mice(seed = 1, print = FALSE, m = 20) %&gt;%\n  with(lm(pdchange ~ placebo + low + medium + high)) %&gt;%\n  pool()\n\nWarning: Number of logged events: 1158\n\n# Compare coefficients with complete case analysis for pocket depth change\nMI &lt;- summary(model_imp)\nMI\n\n         term    estimate  std.error  statistic       df      p.value\n1 (Intercept) -0.33597389 0.06144440 -5.4679330 66.87925 7.296278e-07\n2     placebo -0.00982096 0.08061544 -0.1218248 92.74234 9.033013e-01\n3         low  0.11415425 0.07995226  1.4277802 96.05307 1.565984e-01\n4      medium  0.11260463 0.08900896  1.2650932 60.47912 2.106907e-01\n5        high -0.02704497 0.10936047 -0.2473011 30.38938 8.063384e-01\n\n\nHere the models are a LOT more comparable, although still not significant. It seems we can conclude that missing values impact the attachment loss measurements more than pocket depth change.\nLet’s finish by plotting the imputed values for pocket depth change\n\n# Plot imputed values for m = 5 data set\nimpy &lt;- mice(data, seed = 1, print = FALSE, m = 5)\n\nWarning: Number of logged events: 285\n\nmodel_impy &lt;- with(imp, pdchange ~ trtgroup)\n\n# This is cool, here's how I can plot the imputed values for attach change \nxyplot(impy, pdchange ~ trtgroup,\n       main = \"Example of Imputed Values for m = 5 Dataset\",\n       auto.key = list(space = \"right\",\n        points = TRUE, \n        text = c(\"Observed\", \"Missing\"), col = c(\"blue4\", \"red3\")))\n\n\n\n\n\n\n\n\nLooks pretty good.\nBack to top of tabset"
  },
  {
    "objectID": "Project_1_Regression/Project_1_Regression_R/Project_1_Regression.html#Jackknife",
    "href": "Project_1_Regression/Project_1_Regression_R/Project_1_Regression.html#Jackknife",
    "title": "Advanced Data Analysis - Project 1",
    "section": "Identify Outliers using Jackknife Residuals",
    "text": "Identify Outliers using Jackknife Residuals\nIn this section we will explore how our analysis would have changed if we identified outliers using the jackknife residuals.\n\nBackgroundIdentifying OutliersRerunning the Model with Outliers Removed\n\n\n*Note: This information is from BIOS 6602 Week 6 Lecture 10\nJackknife residuals are a type of residual used in regressions that allows us to detect outliers that have a significant impact on the regression coefficients of the model.\nJackknife residuals are calculated by systematically leaving out one observation (i) at a time from a dataset, fitting the regression model to the remaining dataset, and predicting the left out observation. The residual for each observation is then computed based on that prediction.\nThis process follows three step:\n\nFit the regression model to the data, excluding observation (i)\nUse the model to predict the left-out observation (i)\nCalculate the jackknife residual as the difference between the actual value and that fitted value\n\nJackknife residuals are similar to standardized residuals. Standardized residuals are standardized by the standard error of the regression, and are primarily used for identifying outliers. Jackknife residuals on the other hand allow you to assess the leverage of individuals data points, to see how much they actually influence the model (Source)\n\nJackknife residuals follow exactly a t(n-p-2) distribution, where approximately 5% of residuals are expected to exceed 1.96 in absolute value. Jackknife residuals make suspicious values more obvious compared to other residuals.\nDefinitions vary, but we generally consider a residual to be an outlier if the jackknife residual is +- 3.\nHowever, a potential outlier value may not actually have that dramatic of an impact on the model (which is what we are concerned that outliers will do). That is why we use jackknife residuals to investigate leverage and influence.\n\nExtreme X values can have high leverage.\nExtreme X values can have high influence\nA high-leverage point becomes an influential point if its Y value doesn’t follow the pattern of the rest of the data (i.e. is too low or too high)\n\n\nAn observation is influential if removing it substantially changes the estimate of the coefficients for that model. We use five measurements to assess influence: Jackknife residuals, Leverage, Cook’s Distance (Cook’s D), DFFITS, and DFBETAS.\n\nLeverage: Measures how far a measurement deviates from the mean. You want to examine values greater than 2(p+1)/n\nCook’s D: Measures the influence of an observation on regression predictions. You want to examine observations with d_i &gt; 1.0\nDFFITS: Measures the influence of an observation on regression predictions (related to Cook’s D). You want to examine observations outside the range of +-2(sqrt(p+10n)). If h_i is near zero, then that observation has little effect.\nDDFBETAS: Measure the influence of an observation on *individual* coefficient estimates. You want to examine estimates outside the range of +- 2/sqrt(n). A large DFBETA for variable k indicates that the i-th observation has a sizeable impact on the k-th regression coefficient.\n\nNote: p = the number of variables in the model\n\nBack to top of tabset\n\n\nThis website was used for coding information for this section, alongside the notes from BIOS 6602\nNow that we have covered the background for jackknife residuals, we can apply that knowledge to assess for outliers in our dataset for this project!\n\nAttachment LossPocket Depth\n\n\n\nSet upJackknife ResidualsLeverageCook’s DDFFITSDFBETASAdditional PlotsFinal Look and Conclusion\n\n\nRecall that we computed the jackknife residuals earlier.\n\n# Calculate the jackknife residuals of model_attach\njackknife_residuals_attach &lt;- rstudent(model_attach1)\n\n# Calculate the jackknife residuals of model_pd\njackknife_residuals_pd &lt;- rstudent(model_pd)\n\nLet’s generate a table so we can handily compare: ID, jackknife residual, leverage (diagonal hat), DFFITS, and DFBETAs for each participant.\n\n# Get leverage values (hat values)\nhat_values_attach &lt;- hatvalues(model_attach1)\n\n# Get Cook's D values\ncooks_d_attach &lt;- cooks.distance(model_attach1)\n\n# Get the DFFITS values\ndffits_attach &lt;- dffits(model_attach1)\n\n# Get the DFBETAS\ndfbetas_attach &lt;- dfbetas(model_attach1)\n\n# Make a table with ID and all diagnostic values\ndiagnostics_attach &lt;- data.frame(id = data_missing$id, jackknife_residuals = jackknife_residuals_attach, leverage = hat_values_attach, cooks_D = cooks_d_attach, dffits = dffits_attach, dfbetas = dfbetas_attach)\n\nkable(head(diagnostics_attach), caption = \"Diagnostics for Attachment Loss Change Model\", format = \"html\") %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\"))\n\n\nDiagnostics for Attachment Loss Change Model\n\n\nid\njackknife_residuals\nleverage\ncooks_D\ndffits\ndfbetas..Intercept.\ndfbetas.placebo\ndfbetas.low\ndfbetas.medium\ndfbetas.high\n\n\n\n\n101\n0.5800959\n0.0500000\n0.0035664\n0.1330831\n0.0000000\n0.0000000\n0.0000000\n0.0973313\n0.0000000\n\n\n103\n1.5996714\n0.0434783\n0.0228989\n0.3410511\n0.3410511\n-0.2411595\n-0.2356149\n-0.2325949\n-0.2184475\n\n\n104\n1.4044559\n0.0476190\n0.0195311\n0.3140459\n0.0000000\n0.0000000\n0.2270548\n0.0000000\n0.0000000\n\n\n105\n-0.8928811\n0.0434783\n0.0072626\n-0.1903629\n0.0000000\n-0.1346069\n0.0000000\n0.0000000\n0.0000000\n\n\n106\n-1.6995151\n0.0500000\n0.0298289\n-0.3898955\n0.0000000\n0.0000000\n0.0000000\n-0.2851530\n0.0000000\n\n\n107\n-2.7107885\n0.0476190\n0.0690131\n-0.6061507\n0.0000000\n0.0000000\n-0.4382463\n0.0000000\n0.0000000\n\n\n\n\n\n\n\nBack to top of tabset\n\n\nWe can start by making a plot of the jackknife residuals to assess if there are outliers &lt; -3 or &gt; 3 SDs).\n\n# Plot jackknife outiers and label any values that are &gt; 3 or &lt; -3.\nggplot(data_missing, aes(x = id, y = jackknife_residuals_attach)) + geom_point() + \n  geom_hline(yintercept = c(-3,0,3)) + ylim(-4,4) + \n  labs(y = \"Jackknife Residuals\",\n       title = \"Jackknife Residuals vs ID\") + \n  geom_text(aes(label = ifelse(jackknife_residuals_attach &gt; 3 | jackknife_residuals_attach &lt; -3, id, '')), \n            vjust = -1)\n\n\n\n\n\n\n\n\nParticipant 168 is a potential outlier based on the residual value.\nLet’s look at this participant more closely.\n\n# Examine participant 168 to assess what values could make them an outlier\ndata_missing[data_missing$id == 168,]\n\n    id trtgroup gender race      age smoker sites attachbase attach1year\n58 168        5      2    5 54.20397      0   168   5.089286    4.041667\n     pdbase  pd1year attachchange   pdchange placebo control low medium high\n58 3.410714 2.904762    -1.047619 -0.5059524       0       0   0      0    1\n   trt trt3groups\n58   1          3\n\n# Sort our dataset by descending order of attachment at baseline to see if participant 168 is the highest\n\nkable(head(data_missing[order(-data_missing$attachbase),]), caption = \"Diagnostics for Attachment Loss Change Model\", format = \"html\") %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\"))\n\n\nDiagnostics for Attachment Loss Change Model\n\n\n\nid\ntrtgroup\ngender\nrace\nage\nsmoker\nsites\nattachbase\nattach1year\npdbase\npd1year\nattachchange\npdchange\nplacebo\ncontrol\nlow\nmedium\nhigh\ntrt\ntrt3groups\n\n\n\n\n58\n168\n5\n2\n5\n54.20397\n0\n168\n5.089286\n4.041667\n3.410714\n2.904762\n-1.0476190\n-0.5059524\n0\n0\n0\n0\n1\n1\n3\n\n\n3\n104\n3\n2\n5\n55.17864\n1\n138\n4.956522\n5.304348\n5.217391\n4.891304\n0.3478261\n-0.3260870\n0\n0\n1\n0\n0\n1\n3\n\n\n40\n144\n2\n2\n2\n49.07598\n0\n126\n4.388889\n3.488000\n3.666667\n3.230159\n-0.9008889\n-0.4365079\n0\n1\n0\n0\n0\n0\n2\n\n\n102\n269\n3\n1\n5\n64.90075\n0\n162\n4.080247\n3.685185\n3.370370\n3.265432\n-0.3950617\n-0.1049383\n0\n0\n1\n0\n0\n1\n3\n\n\n18\n121\n4\n2\n5\n54.49692\n1\n138\n4.014493\n3.825000\n3.608696\n3.783333\n-0.1894928\n0.1746377\n0\n0\n0\n1\n0\n1\n3\n\n\n17\n120\n3\n2\n5\n41.83710\n1\n150\n3.886667\n3.920000\n4.200000\n3.880000\n0.0333333\n-0.3200000\n0\n0\n1\n0\n0\n1\n3\n\n\n\n\n\n\n\nWe have confirmed that participant 168 has the largest values for attachment loss at baseline (5.09). If that’s the case however, participant 3 is not far behind them (4.96) (who incidentally has the highest baseline pocket depth measurement), followed by participant 40 a large amount lower (4.39).\nWe can also (apparently) run a test statistic to test if we have an outlier using the ‘car’ package\n\noutlierTest(model_attach1)\n\nNo Studentized residuals with Bonferroni p &lt; 0.05\nLargest |rstudent|:\n    rstudent unadjusted p-value Bonferroni p\n58 -3.602896         0.00049863     0.051359\n\n\nThis is a test of a hypothesis that we do not have an outlier. We reject that hypothesis (p &lt; 0.05) so we have an outlier (I think).\nBack to top of tabset\n\n\nLeverage is a measure of geometric distance of an observation’s predictor point from the center point of the predictor space. In other words, leverage is a measurement of how far that observation deviates from the mean of that variable. High leverage observations have the potential to be very influential, but they are not necessarily influential. It’s possible for a high leverage point to not be influential, but very difficult for a low leverage point to be influential.\nLeverage is calculated as\n&lt;img src=“Media/leverage.png” width=“90%&gt;\nIf X _i is close to Xbar, then h_i is small (i.e. the i-th point has low leverage).\n\n# Caculate cutoff for leverage\ncutoff_leverage &lt;- 2*((4+1)/103)\ncutoff_leverage\n\n[1] 0.09708738\n\n# Make leverage plot in ggplot. Set a horizontal line at the cutoff point, and label ID's for participants that pass that line\nggplot(diagnostics_attach, aes(x = id, y = leverage)) +\n  geom_point() + \n  geom_hline(yintercept = cutoff_leverage) +\n  labs(title = \"Leverage Plot with Cutoff Line\",\n       x = \"ID\",\n       y = \"Hat Values\") + \n  geom_text(aes(label = ifelse(leverage &gt; cutoff_leverage, id, \"\")), vjust = -0.5)\n\n\n\n\n\n\n\n\nNone of our data points pass the cutoff point for leverage/hat values. I think this may be because the IV is categorical which changes the pattern and interpretation. Will check with a professor.\nBack to top of tabset\n\n\nCook’s D combines information about the residuals and leverage into a single value. A higher Cook’s D value signifies that the data point woud greatly change the regression coefficients, and is therefore influential and may impact the model’s accuracy.\nApparently we actually want to look at a cutoff of Cook’s D as 4/(n-p-1). Let’s calculate that and store it.\n\n# Calculate cutoff for Cook's D\ncutoff_d &lt;- 4/((nrow(data_missing) - length(model_attach1$coefficients) - 1))\ncutoff_d\n\n[1] 0.04123711\n\n# Make Cook's D plot in ggplot. Set a horizontal line at the cutoff point, and label ID's for participants that pass that line\nggplot(diagnostics_attach, aes(x = id, y = cooks_D)) +\n  geom_point() + \n  geom_hline(yintercept = cutoff_d) +\n  labs(title = \"Cook's D with Cutoff Line\",\n       x = \"ID\",\n       y = \"Cook's D\") + \n  geom_text(aes(label = ifelse(cooks_D &gt; cutoff_d, id, \"\")), vjust = -0.5)\n\n\n\n\n\n\n\n\nWe see that participants 107, 144, 146 and 168 are past the cutoff for Cook’s D. As when looking at the residuals plot, participant 168 is drastically different compared to the other potential outliers.\nBack to top of tabset\n\n\nNow we will examine DFFITS (Difference in fits). DFFITS is a measure used to identify influential data points in a regression analysis. It quantifes how much the predicted values (Y) of a model change when that particular observation is left out from the analysis. So in this case it is the difference in attachment loss change if we take out observation i from the analysis.\nLet’s make a plot where we label the values that are past the cutoff.\n\n# Calculate the cutoff for DFFITS\ncutoff_dffits &lt;- 2 * sqrt((4+1)/103)\ncutoff_dffits\n\n[1] 0.4406526\n\n# Make DFFITS plot in ggplot. Set a horizontal line at the cutoff points, and label ID's for participants that pass those lines.\nggplot(diagnostics_attach, aes(x = id, y = dffits)) + \n  geom_point() +\n  geom_hline(yintercept = c(cutoff_dffits, -cutoff_dffits)) + \n  geom_text(aes(label = ifelse(dffits &lt; -cutoff_dffits | dffits &gt; cutoff_dffits, id, \"\")), vjust = -0.5) +\n  labs(title = \"DFFITS by ID\",\n       x = \"ID\",\n       y = \"DFFITS\")\n\n\n\n\n\n\n\n\nAgain we are flagging participants 107, 113, 144, 168. The new addition is 113, who is right on the cutoff line. Again participant 168 is the most egregious, and the other potential outliers are a lot closer to the cutoff.\nBack to top of tabset\n\n\nNow we can look at the DFBETAs (Difference in Betas). DFBETAS quantify how much the beta coefficients for each variable in the model change when you exclude observation i from the analysis.\n\n# Calculate the cutoff for DFBETAS\ncutoff_dfbetas &lt;- 2/sqrt(103)\ncutoff_dfbetas\n\n[1] 0.1970659\n\n# Make DFBETAS plot in ggplot. Set a horizontal line at the cutoff points, and label ID's for participants that pass those lines.\n\n# For placebo group\nggplot(diagnostics_attach, aes(x = id, y = dfbetas.placebo)) + \n  geom_point() +\n  geom_hline(yintercept = c(cutoff_dfbetas, -cutoff_dfbetas)) + \n  geom_text(aes(label = ifelse(dfbetas.placebo &lt; -cutoff_dfbetas | dfbetas.placebo &gt; cutoff_dfbetas, id, \"\")), vjust = -0.5) +\n  labs(title = \"DFBETAS by ID for Placebo\",\n       x = \"ID\",\n       y = \"DFBETA\")\n\n\n\n\n\n\n\n# For low dose group\nggplot(diagnostics_attach, aes(x = id, y = dfbetas.low)) + \n  geom_point() +\n  geom_hline(yintercept = c(cutoff_dfbetas, -cutoff_dfbetas)) + \n  geom_text(aes(label = ifelse(dfbetas.low &lt; -cutoff_dfbetas | dfbetas.low &gt; cutoff_dfbetas, id, \"\")), vjust = -0.5) +\n  labs(title = \"DFBETAS by ID for Low Dose\",\n       x = \"ID\",\n       y = \"DFBETA\")\n\n\n\n\n\n\n\n# For medium dose group\nggplot(diagnostics_attach, aes(x = id, y = dfbetas.medium)) + \n  geom_point() +\n  geom_hline(yintercept = c(cutoff_dfbetas, -cutoff_dfbetas)) + \n  geom_text(aes(label = ifelse(dfbetas.medium &lt; -cutoff_dfbetas | dfbetas.medium &gt; cutoff_dfbetas, id, \"\")), vjust = -0.5) +\n  labs(title = \"DFBETAS by ID for Medium Dose\",\n       x = \"ID\",\n       y = \"DFBETA\")\n\n\n\n\n\n\n\n# For high does group\nggplot(diagnostics_attach, aes(x = id, y = dfbetas.high)) + \n  geom_point() +\n  geom_hline(yintercept = c(cutoff_dfbetas, -cutoff_dfbetas)) + \n  geom_text(aes(label = ifelse(dfbetas.high &lt; -cutoff_dfbetas | dfbetas.high &gt; cutoff_dfbetas, id, \"\")), vjust = -0.5) +\n  labs(title = \"DFBETAS by ID for High Dose\",\n       x = \"ID\",\n       y = \"DFBETA\")\n\n\n\n\n\n\n\n\nThat’s a lot of points that are past the cutoff! I think for these plots it might matter less if we’re pass that point. I think the idea here is we get a fine tune look at, for instance, how participant 168 drastically changes the beta for high, compared to how much any other point changes the betas. Other possible values of concern are 144 and 146. All the other data points are still roughly around the cutoff of .20, but 168 is around .7!\nBack to top of tabset\n\n\nThe car package in R has some handy plots that we can use to help us with diagnosing outliers.\nThe first is the influence plot, which essentially combines the jackknife residuals plot with the Cook’s D plot.\n\n# Influence plot\ninfluencePlot(model_attach1, main = \"Influence Plot\",\n              sub = \"Circle size is proportional to Cook's Distance\")\n\n\n\n\n\n\n\n\n      StudRes        Hat       CookD\n6  -2.7107885 0.04761905 0.069013121\n8  -0.5238664 0.06250000 0.003686439\n12  1.4073671 0.06250000 0.026147438\n58 -3.6028957 0.06250000 0.154223694\n\n\nUsing this plot, we can see that obseration 58 (ID 168) has the largest residual value and Cook’s D value by a large margin!\nAdditionally, a single line of code provides us with some useful diagnostic plots.\n\n# infIndexPlot gives us a series of plots that we need to investigate influence points\ninfIndexPlot(model_attach1)\n\n\n\n\n\n\n\n\nThe first plot is the Cook’s D plot, the second plot is the studentized residuals plot, and the leverage/hat values plot, all of which we plotted before. New is the third plot which is the Bonferroni P-value plot.\nBased on these plots we can see pretty readily that participant 168 is a true outlier. That is, they are a point that has high leverage AND influence in this model.\nBack to top of tabset\n\n\nWe saw that participant 168 is the most egregious offender, and participants 107, 144, 146, and MAYBE 113 are flagging past our different cutoff points.\nLet’s take one last look at the table of these participants\n\n# Pretty print diagnostics table of potential outliers\nkable(diagnostics_attach[diagnostics_attach$id %in% c(107,113,144,146,168),], caption = \"Correlation Matrix\", format = \"html\") %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\"))\n\n\nCorrelation Matrix\n\n\n\nid\njackknife_residuals\nleverage\ncooks_D\ndffits\ndfbetas..Intercept.\ndfbetas.placebo\ndfbetas.low\ndfbetas.medium\ndfbetas.high\n\n\n\n\n6\n107\n-2.710789\n0.0476190\n0.0690131\n-0.6061507\n0.0000000\n0.0000000\n-0.4382463\n0.0000000\n0.0000000\n\n\n11\n113\n2.092587\n0.0434783\n0.0384816\n0.4461411\n0.0000000\n0.3154694\n0.0000000\n0.0000000\n0.0000000\n\n\n40\n144\n-2.670168\n0.0434783\n0.0610008\n-0.5692818\n-0.5692818\n0.4025431\n0.3932880\n0.3882470\n0.3646322\n\n\n42\n146\n-2.291912\n0.0434783\n0.0457672\n-0.4886373\n-0.4886373\n0.3455188\n0.3375748\n0.3332479\n0.3129784\n\n\n58\n168\n-3.602896\n0.0625000\n0.1542237\n-0.9302637\n0.0000000\n0.0000000\n0.0000000\n0.0000000\n-0.7143938\n\n\n\n\n\n\n\nAcross the board we can see that participant 168 has worse values for almost everything, in particular the jackknife residual, Cook’s D, and DFFITS values. I can confidently conclude that we should remove participant 168 as an outlier, and feel justified in keeping participants 107, 113, 144, and 146 based on the closeness to the rest of the data points on the previous plots.\n\n\n\n\n\n\nSet UpJackknife ResidualsLeverageCook’s DDFFITSDFBETASAdditional PlotsFinal Look and Conclusion\n\n\nLet’s generate a table so we can handily compare: ID, jackknife residual, leverage (diagonal hat), DFFITS, and DFBETAs for each participant.\n\n# Get leverage values (hat values)\nhat_values_pd &lt;- hatvalues(model_pd)\n\n# Get Cook's D values\ncooks_d_pd &lt;- cooks.distance(model_pd)\n\n# Get the DFFITS values\ndffits_pd &lt;- dffits(model_pd)\n\n# Get the DFBETAS\ndfbetas_pd &lt;- dfbetas(model_pd)\n\n# Make a table with ID and all diagnostic values\ndiagnostics_pd &lt;- data.frame(id = data_missing$id, jackknife_residuals = jackknife_residuals_pd, leverage = hat_values_pd, cooks_D = cooks_d_pd, dffits = dffits_pd, dfbetas = dfbetas_pd)\n\nkable(head(diagnostics_pd), caption = \"Diagnostics for Attachment Loss Change Model\", format = \"html\") %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\"))\n\n\nDiagnostics for Attachment Loss Change Model\n\n\nid\njackknife_residuals\nleverage\ncooks_D\ndffits\ndfbetas..Intercept.\ndfbetas.placebo\ndfbetas.low\ndfbetas.medium\ndfbetas.high\n\n\n\n\n101\n1.4284091\n0.0500000\n0.0212518\n0.3276995\n0.0000000\n0.0000000\n0.0000000\n0.2396655\n0.0000000\n\n\n103\n1.3517762\n0.0434783\n0.0164727\n0.2881997\n0.2881997\n-0.2037879\n-0.1991025\n-0.1965505\n-0.1845955\n\n\n104\n-0.4668576\n0.0476190\n0.0021971\n-0.1043925\n0.0000000\n0.0000000\n-0.0754757\n0.0000000\n0.0000000\n\n\n105\n-0.4451251\n0.0434783\n0.0018161\n-0.0949010\n0.0000000\n-0.0671051\n0.0000000\n0.0000000\n0.0000000\n\n\n106\n-2.5107398\n0.0500000\n0.0629491\n-0.5760032\n0.0000000\n0.0000000\n0.0000000\n-0.4212642\n0.0000000\n\n\n107\n-1.7949702\n0.0476190\n0.0315049\n-0.4013675\n0.0000000\n0.0000000\n-0.2901882\n0.0000000\n0.0000000\n\n\n\n\n\n\n\nBack to top of tabset\n\n\nWe can start by making a plot of the jackknife residuals to assess if there are outliers &lt; -3 or &gt; 3 SDs).\n\n# Plot jackknife outiers and label any values that are &gt; 3 or &lt; -3.\nggplot(data_missing, aes(x = id, y = jackknife_residuals_pd)) + geom_point() + \n  geom_hline(yintercept = c(-3,0,3)) + ylim(-4,4) + \n  labs(y = \"Jackknife Residuals\",\n       title = \"Jackknife Residuals vs ID\") + \n  geom_text(aes(label = ifelse(jackknife_residuals_pd &gt; 3 | jackknife_residuals_pd &lt; -3, id, '')), \n            vjust = -1)\n\n\n\n\n\n\n\n\nWe have no jackknife residuals +- 3. That’s a good sign!\nLet’s sort our dataset and see who has the highest pocket depth at baseline.\n\n# Sort our dataset by descending order of pocket depth at baseline to see who is the highest\nkable(head(data_missing[order(-data_missing$pdbase),]), caption = \"Diagnostics for Attachment Loss Change Model\", format = \"html\") %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\"))\n\n\nDiagnostics for Attachment Loss Change Model\n\n\n\nid\ntrtgroup\ngender\nrace\nage\nsmoker\nsites\nattachbase\nattach1year\npdbase\npd1year\nattachchange\npdchange\nplacebo\ncontrol\nlow\nmedium\nhigh\ntrt\ntrt3groups\n\n\n\n\n3\n104\n3\n2\n5\n55.17864\n1\n138\n4.956522\n5.304348\n5.217391\n4.891304\n0.3478261\n-0.3260870\n0\n0\n1\n0\n0\n1\n3\n\n\n20\n124\n2\n2\n5\n41.01574\n1\n162\n2.901235\n2.808642\n4.771605\n4.067901\n-0.0925926\n-0.7037037\n0\n1\n0\n0\n0\n0\n2\n\n\n17\n120\n3\n2\n5\n41.83710\n1\n150\n3.886667\n3.920000\n4.200000\n3.880000\n0.0333333\n-0.3200000\n0\n0\n1\n0\n0\n1\n3\n\n\n77\n234\n1\n2\n5\n51.12115\n0\n144\n2.756944\n2.527778\n4.083333\n3.631944\n-0.2291667\n-0.4513889\n1\n0\n0\n0\n0\n0\n1\n\n\n5\n106\n4\n2\n5\n42.14921\n1\n168\n2.369048\n1.922619\n3.910714\n3.083333\n-0.4464286\n-0.8273810\n0\n0\n0\n1\n0\n1\n3\n\n\n6\n107\n3\n2\n5\n37.15811\n1\n156\n3.544872\n2.839744\n3.897436\n3.237179\n-0.7051282\n-0.6602564\n0\n0\n1\n0\n0\n1\n3\n\n\n\n\n\n\n\nParticipant 104 has the highest baseline pocket depth. Interestingly it is not 168, who was our outlier for attachment loss.\nLet’s run that test we did earlier to see if there’s an outlier in the pocket depth model.\n\noutlierTest(model_pd)\n\nNo Studentized residuals with Bonferroni p &lt; 0.05\nLargest |rstudent|:\n   rstudent unadjusted p-value Bonferroni p\n82 2.664155          0.0090393      0.93105\n\n\nThis is a test of a hypothesis that we do not have an outlier. We fail to reject the null and thus have more evidence that we do not have outliers for the pocket depth model.\nBack to top of tabset\n\n\nLet’s plot leverage for our model on pocket depth change.\n\n# Caculate cutoff for leverage\ncutoff_leverage &lt;- 2*((4+1)/103)\ncutoff_leverage\n\n[1] 0.09708738\n\n# Make leverage plot in ggplot. Set a horizontal line at the cutoff point, and label ID's for participants that pass that line\nggplot(diagnostics_pd, aes(x = id, y = leverage)) +\n  geom_point() + \n  geom_hline(yintercept = cutoff_leverage) +\n  labs(title = \"Leverage Plot with Cutoff Line\",\n       x = \"ID\",\n       y = \"Hat Values\") + \n  geom_text(aes(label = ifelse(leverage &gt; cutoff_leverage, id, \"\")), vjust = -0.5)\n\n\n\n\n\n\n\n\nAgain we are good on leverage.\nBack to top of tabset\n\n\nLet’s make our plot for Cook’s D for pocket depth change.\n\n# Calculate cutoff for Cook's D\ncutoff_d &lt;- 4/((nrow(data_missing) - length(model_attach1$coefficients) - 1))\ncutoff_d\n\n[1] 0.04123711\n\n# Make Cook's D plot in ggplot. Set a horizontal line at the cutoff point, and label ID's for participants that pass that line\nggplot(diagnostics_pd, aes(x = id, y = cooks_D)) +\n  geom_point() + \n  geom_hline(yintercept = cutoff_d) +\n  labs(title = \"Cook's D with Cutoff Line\",\n       x = \"ID\",\n       y = \"Cook's D\") + \n  geom_text(aes(label = ifelse(cooks_D &gt; cutoff_d, id, \"\")), vjust = -0.5)\n\n\n\n\n\n\n\n\nWe see that participants 106, 118, and 239 have Cook’s D values past the cutoff. Interestingly participant 104 who had the highest baseline pocket depth did not flag here.\nBack to top of tabset\n\n\nLet’s make the DFFITS plot for pocket depth change.\n\n# Calculate the cutoff for DFFITS\ncutoff_dffits &lt;- 2 * sqrt((4+1)/103)\ncutoff_dffits\n\n[1] 0.4406526\n\n# Make DFFITS plot in ggplot. Set a horizontal line at the cutoff points, and label ID's for participants that pass those lines.\nggplot(diagnostics_pd, aes(x = id, y = dffits)) + \n  geom_point() +\n  geom_hline(yintercept = c(cutoff_dffits, -cutoff_dffits)) + \n  geom_text(aes(label = ifelse(dffits &lt; -cutoff_dffits | dffits &gt; cutoff_dffits, id, \"\")), vjust = -0.5) +\n  labs(title = \"DFFITS by ID\",\n       x = \"ID\",\n       y = \"DFFITS\")\n\n\n\n\n\n\n\n\nParticipants 106, 118, and 239 are flagging here again. In addition we have 137 and 233, which are near enough to the cutoff that we can ignore them.\nBack to top of tabset\n\n\nNow we can look at the DFBETAS plot for pocket depth change.\n\n# Calculate the cutoff for DFBETAS\ncutoff_dfbetas &lt;- 2/sqrt(103)\ncutoff_dfbetas\n\n[1] 0.1970659\n\n# Make DFBETAS plot in ggplot. Set a horizontal line at the cutoff points, and label ID's for participants that pass those lines.\n\n# For placebo group\nggplot(diagnostics_pd, aes(x = id, y = dfbetas.placebo)) + \n  geom_point() +\n  geom_hline(yintercept = c(cutoff_dfbetas, -cutoff_dfbetas)) + \n  geom_text(aes(label = ifelse(dfbetas.placebo &lt; -cutoff_dfbetas | dfbetas.placebo &gt; cutoff_dfbetas, id, \"\")), vjust = -0.5) +\n  labs(title = \"DFBETAS by ID for Placebo\",\n       x = \"ID\",\n       y = \"DFBETA\")\n\n\n\n\n\n\n\n# For low dose group\nggplot(diagnostics_pd, aes(x = id, y = dfbetas.low)) + \n  geom_point() +\n  geom_hline(yintercept = c(cutoff_dfbetas, -cutoff_dfbetas)) + \n  geom_text(aes(label = ifelse(dfbetas.low &lt; -cutoff_dfbetas | dfbetas.low &gt; cutoff_dfbetas, id, \"\")), vjust = -0.5) +\n  labs(title = \"DFBETAS by ID for Low Dose\",\n       x = \"ID\",\n       y = \"DFBETA\")\n\n\n\n\n\n\n\n# For medium dose group\nggplot(diagnostics_pd, aes(x = id, y = dfbetas.medium)) + \n  geom_point() +\n  geom_hline(yintercept = c(cutoff_dfbetas, -cutoff_dfbetas)) + \n  geom_text(aes(label = ifelse(dfbetas.medium &lt; -cutoff_dfbetas | dfbetas.medium &gt; cutoff_dfbetas, id, \"\")), vjust = -0.5) +\n  labs(title = \"DFBETAS by ID for Medium Dose\",\n       x = \"ID\",\n       y = \"DFBETA\")\n\n\n\n\n\n\n\n# For high does group\nggplot(diagnostics_pd, aes(x = id, y = dfbetas.high)) + \n  geom_point() +\n  geom_hline(yintercept = c(cutoff_dfbetas, -cutoff_dfbetas)) + \n  geom_text(aes(label = ifelse(dfbetas.high &lt; -cutoff_dfbetas | dfbetas.high &gt; cutoff_dfbetas, id, \"\")), vjust = -0.5) +\n  labs(title = \"DFBETAS by ID for High Dose\",\n       x = \"ID\",\n       y = \"DFBETA\")\n\n\n\n\n\n\n\n\nAgain we have a lot of points that are past the cutoff here. None as far off as 168 in the attachment loss model, 239 is the only one that is concerning.\nBack to top of tabset\n\n\nThe car package in R has some handy plots that we can use to help us with diagnosing outliers.\nThe first is the influence plot, which essentially combines the jackknife residuals plot with the Cook’s D plot.\n\n# Influence plot\ninfluencePlot(model_pd, main = \"Influence Plot\",\n              sub = \"Circle size is proportional to Cook's Distance\")\n\n\n\n\n\n\n\n\n      StudRes        Hat       CookD\n5  -2.5107398 0.05000000 0.062949101\n8  -0.8158788 0.06250000 0.008905825\n12  0.4184503 0.06250000 0.002354494\n82  2.6641548 0.04761905 0.066819586\n\n\nHere we can see that none of our residuals are +- 3, but we do have some concerns with large Cook’s distance, particularly with observation 82 (ID = 239).\nAdditionally, a single line of code provides us with some useful diagnostic plots.\n\n# infIndexPlot gives us a series of plots that we need to investigate influence points\ninfIndexPlot(model_pd)\n\n\n\n\n\n\n\n\nBased on these plots, it is arguable that participant 239 is an outlier for pocket depth. Though they do not have an extreme jackknife residual, the large Cook’s distance suggests that this data point has substantial infuence on the model’s parameters.\nHowever, a closer look reveals that the influence and leverage values for the pocket depth model are all comparable to the potential outliers in the attachmnet loss model we chose to keep (Cook’s D ~0.06). Specifically, while it appears that participant 239 here has a large Cook’s D (0.068) compared to the rest of the values, it is nowhere near as high as participant 168 was in the attachment loss model (0.15)!\nBack to top of tabset\n\n\nParticipants 106 and 239 were the only potential concerns here. However, as noted, their Cook’s D is comparable to the values we kept in the attachment loss model, and are &lt; 1/2 of what the Cook’s D was for participant 168 who we plan to remove as an outlier in the attachment loss model!\n\n# Pretty print diagnostics table of potential outliers\nkable(diagnostics_pd[diagnostics_attach$id %in% c(106,239),], caption = \"Correlation Matrix\", format = \"html\") %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\"))\n\n\nCorrelation Matrix\n\n\n\nid\njackknife_residuals\nleverage\ncooks_D\ndffits\ndfbetas..Intercept.\ndfbetas.placebo\ndfbetas.low\ndfbetas.medium\ndfbetas.high\n\n\n\n\n5\n106\n-2.510740\n0.050000\n0.0629491\n-0.5760032\n0\n0\n0.0000000\n-0.4212642\n0\n\n\n82\n239\n2.664155\n0.047619\n0.0668196\n0.5957231\n0\n0\n0.4307071\n0.0000000\n0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Remove participant 168 from the dataset\ndata_missing_outlier &lt;- data_missing[data_missing$id != 168,]\n\n# Run model 1 attachment loss\nmodel_attach1_outlier &lt;- lm(attachchange ~ placebo + low + medium + high, data = data_missing_outlier)\nsummary(model_attach1_outlier)\n\n\nCall:\nlm(formula = attachchange ~ placebo + low + medium + high, data = data_missing_outlier)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.68731 -0.16279  0.04936  0.16823  0.53945 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.22169    0.05277  -4.201  5.9e-05 ***\nplacebo      0.13462    0.07463   1.804  0.07435 .  \nlow          0.20388    0.07638   2.669  0.00891 ** \nmedium       0.21514    0.07737   2.780  0.00652 ** \nhigh         0.11576    0.08399   1.378  0.17130    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2531 on 97 degrees of freedom\nMultiple R-squared:  0.09493,   Adjusted R-squared:  0.05761 \nF-statistic: 2.543 on 4 and 97 DF,  p-value: 0.04442\n\n# Run model 2 attachmnent loss\nmodel_attach2_outlier &lt;- lm(attachchange ~ control + low + medium + high, data = data_missing_outlier)\nsummary(model_attach2_outlier)\n\n\nCall:\nlm(formula = attachchange ~ control + low + medium + high, data = data_missing_outlier)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.68731 -0.16279  0.04936  0.16823  0.53945 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept) -0.08707    0.05277  -1.650   0.1022  \ncontrol     -0.13462    0.07463  -1.804   0.0743 .\nlow          0.06926    0.07638   0.907   0.3668  \nmedium       0.08052    0.07737   1.041   0.3006  \nhigh        -0.01886    0.08399  -0.225   0.8228  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2531 on 97 degrees of freedom\nMultiple R-squared:  0.09493,   Adjusted R-squared:  0.05761 \nF-statistic: 2.543 on 4 and 97 DF,  p-value: 0.04442\n\n# Run the pocket depth model\nmodel_pd_outlier &lt;- lm(pdchange ~ control + low + medium + high, data = data_missing_outlier)\nsummary(model_pd_outlier)\n\n\nCall:\nlm(formula = pdchange ~ control + low + medium + high, data = data_missing_outlier)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-0.6248 -0.1513 -0.0129  0.1616  0.6613 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.34969    0.05488  -6.372 6.26e-09 ***\ncontrol      0.01152    0.07761   0.148   0.8823    \nlow          0.14352    0.07943   1.807   0.0739 .  \nmedium       0.14714    0.08046   1.829   0.0705 .  \nhigh        -0.02437    0.08734  -0.279   0.7809    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2632 on 97 degrees of freedom\nMultiple R-squared:  0.07456,   Adjusted R-squared:  0.0364 \nF-statistic: 1.954 on 4 and 97 DF,  p-value: 0.1077\n\n\nThere is no difference in our models after removing outlier 168."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Sean Vieau",
    "section": "",
    "text": "Graduate student in the Masters in Applied Biostatistics program at the Colorado School of Public Health. I have compiled this portfolio to showcase my most significant projects and skills.\nIn my spare time I am an avid dancer, jiu jitsu practitioner, and drummer!"
  },
  {
    "objectID": "Project_1_Regression/Project_1_Regression_SAS/Project_1_Regression_SAS.html",
    "href": "Project_1_Regression/Project_1_Regression_SAS/Project_1_Regression_SAS.html",
    "title": "This is markdown",
    "section": "",
    "text": "# This is code\nprint(\"Hello world\")"
  },
  {
    "objectID": "Project_2/Project_2_R/Code/Project2.html",
    "href": "Project_2/Project_2_R/Code/Project2.html",
    "title": "Advanced Data Analysis - Project 2",
    "section": "",
    "text": "The aim of the current study is to assess how treatment response differs for HIV+ patients 2 years after initiating Highly Active Antiretroviral Therapy (HAART) based on hard drug usage (such as heroin or cocaine). This study is of particular scientific interest because it is unclear whether the use of hard drugs inhibits the immune system in humans; treatment strategies may differ based on these results. The researchers are interested in comparing subjects who never used hard drugs to current hard drug users (those that use hard drugs at year 2) or previous hard drug users (those who used drugs at year 0 or 1). Outcomes of interest are: viral load (HIV copies in a mL of blood), CD4+ T cell count (a measure of immunologic health), and aggregate physical and quality of life scores from the SF-36.\nThe clinical hypothesis is that, if hard drugs inhibit the immune system in humans, subjects who currently or previously used hard drugs will have higher viral load and lower CD4+ T cell counts than those who never used hard drugs. Additionally, the researchers are interested in knowing if potential differences between the drug use groups can be explained by differences in adherence to the treatment regimen. The researchers are agnostic on how quality of life changes after treatment, since side effects of the treatment are significant.\nThe project description provided by the PI is available below:"
  },
  {
    "objectID": "Project_2/Project_2_R/Code/Project2.html#labeling-categorical-variables",
    "href": "Project_2/Project_2_R/Code/Project2.html#labeling-categorical-variables",
    "title": "Advanced Data Analysis - Project 2",
    "section": "Labeling Categorical Variables",
    "text": "Labeling Categorical Variables\nLet’s factor and label our categorical variables so they are appropriately represented (and not doubles, which will yield incorrect results in models)\n\n# Converting all appropriate variables from doubles to categorical variables\n\ndata$HASHV &lt;- factor(data$HASHV,\n                     levels = c(1, 2),\n                     labels = c(\"No\", \"Yes\"))\n\ndata$HASHF &lt;- factor(data$HASHF,\n                     levels = c(0, 1, 2, 3, 4),\n                     labels = c(\"Never\", \"Daily\", \"Weekly\", \"Monthly\", \"Less Often\"))\n\ndata$income &lt;- factor(data$income,\n                      levels = c(1, 2, 3, 4, 5, 6, 7, 9),\n                      labels = c(\"Less than $10,000\", \"$10,000-$19,999\", \"$20,000-$29,999\", \"$30,000-$39,999\", \"$40,000-$49,999\", \"$50,000-$59,999\", \"$60,000 or more\", \"Do not wish to answer\"))\n\ndata$HBP &lt;- factor(data$HBP,\n                   levels = c(1, 2, 3, 4, 9, -1),\n                   labels = c(\"No\", \"Yes\", \"No, based on data trajectory\", \"Yes, based on data trajectory\", \"Insufficient data, may include reported treatment without diagnosis\", \"Improbable Value\"))\n\ndata$DIAB &lt;- factor(data$DIAB,\n                    levels = c(1, 2, 3, 4, 9),\n                    labels = c(\"No\", \"Yes\", \"No, based on data trajectory\", \"Yes, based on data trajectory\", \"Insufficient data\"))\n                      \ndata$LIV34 &lt;- factor(data$LIV34,\n                     levels = c(1, 2, 9),\n                     labels = c(\"No\", \"Yes\", \"Insufficient Data\"))\n\ndata$KID &lt;- factor(data$KID,\n                   levels = c(1, 2, 3, 4, 9),\n                   labels = c(\"No\", \"Yes\", \"No, based on data trajectory\", \"Yes, based on data trajectory\", \"Insufficient data\"))\n\ndata$FRP &lt;- factor(data$FRP,\n                   levels = c(1,2,9),\n                   labels = c(\"No\", \"Yes\", \"Insufficient Data\"))\n\ndata$FP &lt;- factor(data$FP,\n                  levels = c(1,2,9),\n                  labels = c(\"No\", \"Yes\", \"Insufficient Data\"))\n\ndata$DYSLIP &lt;- factor(data$DYSLIP,\n                      levels = c(1, 2, 3, 4, 9),\n                      labels = c(\"No\", \"Yes\", \"No, based on data trajectory\", \"Yes, based on data trajectory\", \"Insufficient data\"))\n\ndata$SMOKE &lt;- factor(data$SMOKE,\n                     levels = c(1, 2, 3),\n                     labels = c(\"Never Smoked\", \"Former Smoker\", \"Current Smoker\"))\n\ndata$DKGRP &lt;- factor(data$DKGRP,\n                     levels = c(0, 1, 2, 3),\n                     labels = c(\"None\", \"1-3 drinks/week\", \"4-13 drinks/week\", \"&gt;13 drinks/week\"))\n\ndata$HEROPIATE &lt;- factor(data$HEROPIATE,\n                         levels = c(1, 2, -9),\n                         labels = c(\"No\", \"Yes\", \"Not Specified\"))\n\ndata$IDU &lt;- factor(data$IDU,\n                   levels = c(1, 2),\n                   labels = c(\"No\", \"Yes\"))\n\ndata$ADH &lt;- factor(data$ADH,\n                   levels = c(1, 2, 3, 4),\n                   labels = c(\"100%\", \"95-99%\", \"75-94%\", \"&lt;75%\"))\n\ndata$RACE &lt;- factor(data$RACE,\n                    levels = c(1, 2, 3, 4, 5, 6, 7),\n                    labels = c(\"White, non-Hispanic\", \"White, Hispanic\", \"Black, non-Hispanic \", \"Black, Hispanic\",  \"American Indian or Alaskan Native\", \"Asian or Pacific Islander\", \"Other Hispanic\"))\n\ndata$EDUCBAS &lt;- factor(data$EDUCBAS,\n                       levels = c(1, 2, 3, 4, 5, 6, 7),\n                       labels = c(\"8th grade or less \", \"9,10, or 11th grade\", \"12th grade\", \"At least one year college but no degree\", \"Four years college or got degree\", \"Some graduate work\", \"Post-graduate degree\"))\n\ndata$hard_drugs &lt;- factor(data$hard_drugs,\n                          levels = c(0, 1),\n                          labels = c(\"No\", \"Yes\"))\n\n# Create labels for variables to make the names of each variable more professional in outputs\nlabel(data$newid) &lt;- \"ID\"\nlabel(data$AGG_MENT) &lt;- \"Aggregate Mental QOL Score\"\nlabel(data$AGG_PHYS) &lt;- \"Aggregate Physical QOL Score\"\nlabel(data$HASHF) &lt;- \"Hash/Marijuana Use Since Last Visit\"\nlabel(data$HASHV) &lt;- \"Frequency of Hash/Marijuana Use\"\nlabel(data$income) &lt;- \"Income\"\nlabel(data$HBP) &lt;- \"High Blood Pressure\"\nlabel(data$DIAB) &lt;- \"Diabetes\"\nlabel(data$LIV34) &lt;- \"Liver Disease Stage 3/4\"\nlabel(data$KID) &lt;-\"Kidney Disease\"\nlabel(data$FRP) &lt;- \"Frailty Related Phenotype\"\nlabel(data$FP) &lt;- \"Fraily Phenotype\"\nlabel(data$BMI) &lt;- \"BMI\"\nlabel(data$TCHOL) &lt;- \"Total Cholesterol\"\nlabel(data$TRIG) &lt;- \"Triglycerides\"\nlabel(data$LDL) &lt;- \"LDL\"\nlabel(data$DYSLIP) &lt; \"Dyslipidemia\"\nlabel(data$SMOKE) &lt; \"Smoking Status\"\nlabel(data$CESD) &lt;- \"CESD Depression Score\"\nlabel(data$SMOKE) &lt; \"Smoking Status\"\nlabel(data$DKGRP) &lt;- \"Drinking Group\"\nlabel(data$HEROPIATE) &lt;- \"Heroin or Opiate Use Since Last Visit\"\nlabel(data$IDU) &lt;- \"Intravenous Drug Usage Since Last Visit\"\nlabel(data$LEU3N) &lt;- \"CD4+ T Cell Count\"\nlabel(data$VLOAD) &lt;- \"Viral Load\"\nlabel(data$ADH) &lt;- \"Adherence to Treatment Regimen\"\nlabel(data$RACE) &lt;- \"Race\"\nlabel(data$EDUCBAS) &lt;- \"Education at Baseline\"\nlabel(data$hivpos) &lt;- \"HIV Serostatus\"\nlabel(data$age) &lt;- \"Age\"\nlabel(data$ART) &lt;- \"Antiretroviral Therapy\"\nlabel(data$years) &lt;- \"Year of Visit\"\nlabel(data$hard_drugs) &lt;- \"Hard Drug Usage\"\n\nLet’s take another look to check that those variables are no longer doubles.\n\n# Examine data\nglimpse(data)\n\nRows: 3,632\nColumns: 33\n$ newid      &lt;dbl&gt; 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4,…\n$ AGG_MENT   &lt;dbl&gt; 44.90710, 58.20754, 59.65136, 56.80657, 46.34190, 48.71791,…\n$ AGG_PHYS   &lt;dbl&gt; 52.52557, 41.29347, 48.54453, 46.73991, 27.92331, 38.03807,…\n$ HASHV      &lt;fct&gt; No, No, No, No, No, Yes, No, Yes, No, Yes, No, No, Yes, No,…\n$ HASHF      &lt;fct&gt; NA, Less Often, Never, Never, Never, Never, Never, Never, N…\n$ income     &lt;fct&gt; \"$30,000-$39,999\", \"$30,000-$39,999\", \"$30,000-$39,999\", \"$…\n$ BMI        &lt;dbl&gt; 24.71756, 26.06801, 27.16421, 25.71786, 26.66936, 25.96576,…\n$ HBP        &lt;fct&gt; \"No\", \"No\", \"No\", \"No\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", …\n$ DIAB       &lt;fct&gt; \"No\", \"No\", \"No\", \"No\", \"Insufficient data\", \"Insufficient …\n$ LIV34      &lt;fct&gt; No, No, No, No, No, No, No, No, No, No, No, No, No, No, Yes…\n$ KID        &lt;fct&gt; \"No\", \"No\", \"No\", \"No\", \"Insufficient data\", \"Insufficient …\n$ FRP        &lt;fct&gt; No, No, No, No, Yes, No, No, No, No, No, No, No, No, No, No…\n$ FP         &lt;fct&gt; No, No, No, No, Insufficient Data, Insufficient Data, No, N…\n$ TCHOL      &lt;dbl&gt; 133, 131, 180, 171, 125, NA, 134, 105, 141, 153, 170, 170, …\n$ TRIG       &lt;dbl&gt; 176, 107, 233, 139, NA, NA, NA, 104, 162, 127, NA, NA, 82, …\n$ LDL        &lt;dbl&gt; 62, 66, 86, 96, NA, NA, NA, 44, 73, 84, NA, NA, 127, NA, NA…\n$ DYSLIP     &lt;fct&gt; \"Yes\", \"No\", \"Yes\", \"No\", \"Yes\", \"Insufficient data\", \"Yes,…\n$ CESD       &lt;dbl&gt; 14, 2, 1, 18, 20, 18, 18, 21, 23, 17, 18, 22, 23, 14, 1, 1,…\n$ SMOKE      &lt;fct&gt; Current Smoker, Current Smoker, Current Smoker, Current Smo…\n$ DKGRP      &lt;fct&gt; None, &gt;13 drinks/week, None, 1-3 drinks/week, None, &gt;13 dri…\n$ HEROPIATE  &lt;fct&gt; No, No, No, No, No, No, No, No, No, No, No, No, No, No, No,…\n$ IDU        &lt;fct&gt; Yes, No, No, No, Yes, No, No, No, No, No, Yes, Yes, Yes, Ye…\n$ LEU3N      &lt;dbl&gt; 104.1659, 262.0061, 345.4010, 292.3271, 257.8278, 459.4562,…\n$ VLOAD      &lt;dbl&gt; 102013.000000, 27.000000, 60.000000, 9.000000, 8121.000000,…\n$ ADH        &lt;fct&gt; NA, 95-99%, 100%, 100%, NA, 100%, 100%, 100%, 100%, 100%, N…\n$ RACE       &lt;fct&gt; \"White, non-Hispanic\", \"White, non-Hispanic\", \"White, non-H…\n$ EDUCBAS    &lt;fct&gt; \"At least one year college but no degree\", \"At least one ye…\n$ hivpos     &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ age        &lt;dbl&gt; 52, 53, 54, 55, 54, 55, 56, 60, 61, 62, 47, 48, 49, 51, 52,…\n$ ART        &lt;dbl&gt; 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1,…\n$ everART    &lt;dbl&gt; 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1,…\n$ years      &lt;dbl&gt; 0, 1, 2, 3, 0, 1, 2, 6, 7, 8, 0, 1, 2, 4, 5, 6, 7, 8, 0, 1,…\n$ hard_drugs &lt;fct&gt; Yes, No, No, No, Yes, No, No, No, No, No, Yes, Yes, Yes, Ye…\n\n\nLooks good."
  },
  {
    "objectID": "Project_2/Project_2_R/Code/Project2.html#filtering-data-set",
    "href": "Project_2/Project_2_R/Code/Project2.html#filtering-data-set",
    "title": "Advanced Data Analysis - Project 2",
    "section": "Filtering Data Set",
    "text": "Filtering Data Set\nNow let’s take a look at the header to get a good feeling for our data.\n\n# Pretty print data header\npretty_print(head(data))\n\n\n\n\nnewid\nAGG_MENT\nAGG_PHYS\nHASHV\nHASHF\nincome\nBMI\nHBP\nDIAB\nLIV34\nKID\nFRP\nFP\nTCHOL\nTRIG\nLDL\nDYSLIP\nCESD\nSMOKE\nDKGRP\nHEROPIATE\nIDU\nLEU3N\nVLOAD\nADH\nRACE\nEDUCBAS\nhivpos\nage\nART\neverART\nyears\nhard_drugs\n\n\n\n\n1\n44.90710\n52.52557\nNo\nNA\n$30,000-$39,999\n24.71756\nNo\nNo\nNo\nNo\nNo\nNo\n133\n176\n62\nYes\n14\nCurrent Smoker\nNone\nNo\nYes\n104.1659\n102013\nNA\nWhite, non-Hispanic\nAt least one year college but no degree\n1\n52\n0\n0\n0\nYes\n\n\n1\n58.20754\n41.29346\nNo\nLess Often\n$30,000-$39,999\n26.06801\nNo\nNo\nNo\nNo\nNo\nNo\n131\n107\n66\nNo\n2\nCurrent Smoker\n&gt;13 drinks/week\nNo\nNo\n262.0061\n27\n95-99%\nWhite, non-Hispanic\nAt least one year college but no degree\n1\n53\n1\n1\n1\nNo\n\n\n1\n59.65136\n48.54453\nNo\nNever\n$30,000-$39,999\n27.16421\nNo\nNo\nNo\nNo\nNo\nNo\n180\n233\n86\nYes\n1\nCurrent Smoker\nNone\nNo\nNo\n345.4010\n60\n100%\nWhite, non-Hispanic\nAt least one year college but no degree\n1\n54\n1\n1\n2\nNo\n\n\n1\n56.80657\n46.73991\nNo\nNever\n$40,000-$49,999\n25.71786\nNo\nNo\nNo\nNo\nNo\nNo\n171\n139\n96\nNo\n18\nCurrent Smoker\n1-3 drinks/week\nNo\nNo\n292.3271\n9\n100%\nWhite, non-Hispanic\nAt least one year college but no degree\n1\n55\n1\n1\n3\nNo\n\n\n2\n46.34190\n27.92331\nNo\nNever\n$10,000-$19,999\n26.66936\nYes\nInsufficient data\nNo\nInsufficient data\nYes\nInsufficient Data\n125\nNA\nNA\nYes\n20\nCurrent Smoker\nNone\nNo\nYes\n257.8278\n8121\nNA\nBlack, non-Hispanic\n9,10, or 11th grade\n1\n54\n0\n0\n0\nYes\n\n\n2\n48.71791\n38.03807\nYes\nNever\nLess than $10,000\n25.96576\nYes\nInsufficient data\nNo\nInsufficient data\nNo\nInsufficient Data\nNA\nNA\nNA\nInsufficient data\n18\nCurrent Smoker\n&gt;13 drinks/week\nNo\nNo\n459.4562\n21\n100%\nBlack, non-Hispanic\n9,10, or 11th grade\n1\n55\n1\n1\n1\nNo\n\n\n\n\n\n\n\nHmm, we have 8 years worth of data points, but the experimenters are only interested in the first 2 years.\nOut of curiosity, let’s look at how many participants they had each year.\n\n# Visualize patient drop off over 8 years of study\nbarplot(table(data$years))\n\n\n\n\n\n\n\n# Check number of patients in each year\npretty_print(table(data$years))\n\n\n\n\nVar1\nFreq\n\n\n\n\n0\n550\n\n\n1\n550\n\n\n2\n550\n\n\n3\n414\n\n\n4\n381\n\n\n5\n338\n\n\n6\n325\n\n\n7\n272\n\n\n8\n252\n\n\n\n\n\n\n\nThis is interesting, we don’t seem to have as drastic a drop off as I expected. The researchers managed to retain all participants for the first 2 years, and 50% by the end of the 8-year study.\nLet’s filter to only include values from the first 2 years, as this is the timeframe the researchers are interested in.\n\n# Filter long form data set to be include only first 2 years\ndata_2 &lt;- data[data$years &lt;= 2,]\n\n# Check how many visits we have in the filtered data set.\ndim(data_2)\n\n[1] 1650   33\n\n\nWe went from 3632 visits in the 8 year data set to 1650 in the 2 year filtered data set.\n\n# Double check if any patients dropped out within the first 2 years\nany(is.na(data_2$years))\n\n[1] FALSE\n\n\nLuckily, no patients dropped out within the first 2 years of the study!"
  },
  {
    "objectID": "Project_2/Project_2_R/Code/Project2.html#transpose-to-wideform",
    "href": "Project_2/Project_2_R/Code/Project2.html#transpose-to-wideform",
    "title": "Advanced Data Analysis - Project 2",
    "section": "Transpose to Wideform",
    "text": "Transpose to Wideform\nWe can also see that the provided data set is in longform. Let’s convert that to wideform.\n\n# Create new wideform data set for first 2 years of study              \ndata_wide_2 &lt;- pivot_wider(data_2, id_cols = newid, names_from = years, values_from = -c(newid, years))\n\nAnd take a look at the header to check that was done correctly.\n\n# Pretty print header of wideform data\npretty_print(head(data_wide_2))\n\n\n\n\nnewid\nAGG_MENT_0\nAGG_MENT_1\nAGG_MENT_2\nAGG_PHYS_0\nAGG_PHYS_1\nAGG_PHYS_2\nHASHV_0\nHASHV_1\nHASHV_2\nHASHF_0\nHASHF_1\nHASHF_2\nincome_0\nincome_1\nincome_2\nBMI_0\nBMI_1\nBMI_2\nHBP_0\nHBP_1\nHBP_2\nDIAB_0\nDIAB_1\nDIAB_2\nLIV34_0\nLIV34_1\nLIV34_2\nKID_0\nKID_1\nKID_2\nFRP_0\nFRP_1\nFRP_2\nFP_0\nFP_1\nFP_2\nTCHOL_0\nTCHOL_1\nTCHOL_2\nTRIG_0\nTRIG_1\nTRIG_2\nLDL_0\nLDL_1\nLDL_2\nDYSLIP_0\nDYSLIP_1\nDYSLIP_2\nCESD_0\nCESD_1\nCESD_2\nSMOKE_0\nSMOKE_1\nSMOKE_2\nDKGRP_0\nDKGRP_1\nDKGRP_2\nHEROPIATE_0\nHEROPIATE_1\nHEROPIATE_2\nIDU_0\nIDU_1\nIDU_2\nLEU3N_0\nLEU3N_1\nLEU3N_2\nVLOAD_0\nVLOAD_1\nVLOAD_2\nADH_0\nADH_1\nADH_2\nRACE_0\nRACE_1\nRACE_2\nEDUCBAS_0\nEDUCBAS_1\nEDUCBAS_2\nhivpos_0\nhivpos_1\nhivpos_2\nage_0\nage_1\nage_2\nART_0\nART_1\nART_2\neverART_0\neverART_1\neverART_2\nhard_drugs_0\nhard_drugs_1\nhard_drugs_2\n\n\n\n\n1\n44.90710\n58.20754\n59.65136\n52.52557\n41.29346\n48.54453\nNo\nNo\nNo\nNA\nLess Often\nNever\n$30,000-$39,999\n$30,000-$39,999\n$30,000-$39,999\n24.71756\n26.06801\n27.16421\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\n133\n131\n180\n176\n107\n233\n62\n66\n86\nYes\nNo\nYes\n14\n2\n1\nCurrent Smoker\nCurrent Smoker\nCurrent Smoker\nNone\n&gt;13 drinks/week\nNone\nNo\nNo\nNo\nYes\nNo\nNo\n104.1659\n262.0061\n345.4010\n102013.000\n27.00000\n60.00000\nNA\n95-99%\n100%\nWhite, non-Hispanic\nWhite, non-Hispanic\nWhite, non-Hispanic\nAt least one year college but no degree\nAt least one year college but no degree\nAt least one year college but no degree\n1\n1\n1\n52\n53\n54\n0\n1\n1\n0\n1\n1\nYes\nNo\nNo\n\n\n2\n46.34190\n48.71791\n45.41483\n27.92331\n38.03807\n37.32204\nNo\nYes\nNo\nNever\nNever\nNever\n$10,000-$19,999\nLess than $10,000\n$10,000-$19,999\n26.66936\n25.96576\n26.96037\nYes\nYes\nYes\nInsufficient data\nInsufficient data\nInsufficient data\nNo\nNo\nNo\nInsufficient data\nInsufficient data\nInsufficient data\nYes\nNo\nNo\nInsufficient Data\nInsufficient Data\nNo\n125\nNA\n134\nNA\nNA\nNA\nNA\nNA\nNA\nYes\nInsufficient data\nYes, based on data trajectory\n20\n18\n18\nCurrent Smoker\nCurrent Smoker\nCurrent Smoker\nNone\n&gt;13 drinks/week\n4-13 drinks/week\nNo\nNo\nNo\nYes\nNo\nNo\n257.8278\n459.4562\n263.0693\n8121.000\n21.00000\n48.00000\nNA\n100%\n100%\nBlack, non-Hispanic\nBlack, non-Hispanic\nBlack, non-Hispanic\n9,10, or 11th grade\n9,10, or 11th grade\n9,10, or 11th grade\n1\n1\n1\n54\n55\n56\n0\n1\n1\n0\n1\n1\nYes\nNo\nNo\n\n\n3\n40.22337\n44.42011\n41.70079\n60.06970\n62.71705\n58.51450\nNo\nNo\nYes\nLess Often\nLess Often\nLess Often\n$50,000-$59,999\n$50,000-$59,999\n$50,000-$59,999\n28.59085\n28.35320\n28.18510\nInsufficient data, may include reported treatment without diagnosis\nInsufficient data, may include reported treatment without diagnosis\nNo\nInsufficient data\nInsufficient data\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\n170\n170\n180\nNA\nNA\n82\nNA\nNA\n127\nYes\nYes\nYes\n18\n22\n23\nFormer Smoker\nFormer Smoker\nFormer Smoker\nNone\nNone\nNone\nNo\nNo\nNo\nYes\nYes\nYes\n563.1223\n488.9100\n405.1816\n4001.556\n2020.00000\n27.50917\nNA\n100%\n100%\nWhite, non-Hispanic\nWhite, non-Hispanic\nWhite, non-Hispanic\nPost-graduate degree\nPost-graduate degree\nPost-graduate degree\n1\n1\n1\n47\n48\n49\n0\n1\n1\n0\n1\n1\nYes\nYes\nYes\n\n\n4\n42.90638\n31.15971\n52.68223\n50.78850\n44.62883\n51.50533\nYes\nNo\nNo\nLess Often\nWeekly\nNever\nLess than $10,000\nLess than $10,000\nNA\n20.36451\n18.21865\n20.28485\nNo\nNo\nNo\nNo\nNo, based on data trajectory\nNo\nNo\nNo\nNo\nInsufficient data\nInsufficient data\nInsufficient data\nNo\nYes\nNo\nNo\nYes\nNo\n214\n197\n251\n97\nNA\n260\n147\nNA\n152\nYes\nYes, based on data trajectory\nYes\n14\n25\n13\nCurrent Smoker\nCurrent Smoker\nCurrent Smoker\n1-3 drinks/week\n4-13 drinks/week\nNone\nYes\nYes\nNo\nNo\nNo\nNo\n110.4218\n159.6297\n179.6409\n740.000\n26.64732\n27.00000\nNA\n75-94%\n95-99%\nWhite, non-Hispanic\nWhite, non-Hispanic\nWhite, non-Hispanic\nFour years college or got degree\nFour years college or got degree\nFour years college or got degree\n1\n1\n1\n44\n45\n46\n0\n1\n1\n0\n1\n1\nYes\nYes\nNo\n\n\n5\n56.42904\n56.21993\n66.50629\n43.75671\n30.47055\n18.82350\nNo\nNA\nYes\nMonthly\nMonthly\nLess Often\n$50,000-$59,999\n$10,000-$19,999\nDo not wish to answer\n22.26986\n24.97865\n20.80193\nNo\nNo\nNo\nInsufficient data\nInsufficient data\nInsufficient data\nNo\nNo\nInsufficient Data\nInsufficient data\nInsufficient data\nInsufficient data\nNo\nNo\nYes\nInsufficient Data\nInsufficient Data\nYes\n196\n204\nNA\n162\n192\nNA\n135\nNA\nNA\nYes\nInsufficient data\nYes, based on data trajectory\n1\n0\n1\nCurrent Smoker\nCurrent Smoker\nCurrent Smoker\nNone\n1-3 drinks/week\n1-3 drinks/week\nNot Specified\nNA\nNo\nYes\nYes\nYes\n252.6634\n92.6634\n59.6219\n62727.039\n30389.00000\n419.50000\nNA\n100%\n100%\nWhite, non-Hispanic\nWhite, non-Hispanic\nWhite, non-Hispanic\nPost-graduate degree\nPost-graduate degree\nPost-graduate degree\n1\n1\n1\n53\n54\n55\n0\n1\n1\n0\n1\n1\nYes\nYes\nYes\n\n\n6\n59.74437\n53.84956\n50.26010\n56.86261\n57.91396\n55.95668\nNo\nNo\nNo\nNA\nNA\nLess Often\n$30,000-$39,999\n$30,000-$39,999\nNA\n23.22166\n23.75318\n22.41001\nNo\nNo\nNo\nInsufficient data\nInsufficient data\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\n216\n216\n151\nNA\nNA\n125\nNA\nNA\n81\nInsufficient data\nInsufficient data\nNo\n3\n3\n4\nNever Smoked\nNever Smoked\nNever Smoked\n1-3 drinks/week\n1-3 drinks/week\n1-3 drinks/week\nNo\nNo\nNo\nNo\nNo\nYes\n634.1246\n745.6517\n893.4328\n15745.000\n7870.00000\n53.50000\nNA\n95-99%\n95-99%\nWhite, non-Hispanic\nWhite, non-Hispanic\nWhite, non-Hispanic\nSome graduate work\nSome graduate work\nSome graduate work\n1\n1\n1\n36\n37\n38\n0\n1\n1\n0\n1\n1\nNo\nNo\nYes\n\n\n\n\n\n\n\nGood. now we have a long and wide form of the data set for the first two years of the study.\nFinally, let’s just clean that wide data set up a bit to drop repeat measures of variables that are constant over time (race, education at baseline, HIV serostatus, everART)\n\n# Clean up the wide data set a bit by deleting multiple observations across time for constant variables such as race\ndata_wide_2 &lt;- data_wide_2 %&gt;% select(-RACE_1, -RACE_2, -EDUCBAS_1, -EDUCBAS_2, -hivpos_1, - hivpos_2, -everART_1, -everART_2)\n\nNow that our data sets are adequately prepared, we can move on to performing our data checks to ensure fidelity of the data set."
  },
  {
    "objectID": "Project_2/Project_2_R/Code/Project2.html#missingness-redux",
    "href": "Project_2/Project_2_R/Code/Project2.html#missingness-redux",
    "title": "Advanced Data Analysis - Project 2",
    "section": "Missingness Redux",
    "text": "Missingness Redux\nWe first examined missingness before performing data cleaning just to get a sense of the data set.\nLet’s compare what our missingness looked like pre- and post-data cleaning.\n\n# Visualize missingness for pre-cleaned data\ngg_miss_var(data)\n\n\n\n\n\n\n\n# Visualize missingness for post-cleaned data\ngg_miss_var(data_2)\n\n\n\n\n\n\n\n\nThe order for missingness has changed, now with KID at the top, followed by DIAB, LDL, TRIG, and DYSLIP.\nTCHOL, LIV34, and income are further behind, with levels of missingness that may be salvageable (~30%).\n\n# Visualize missingness for pre-cleaned dataset\nvis_miss(data)\n\n\n\n\n\n\n\n# Visualize missingness for post-cleaned dataset\nvis_miss(data_2)\n\n\n\n\n\n\n\n\nAnd for good measure let’s now examine missingness in the wide form data set.\n\n# Create new wideform data set for first 2 years of study              \ndata_wide_2 &lt;- pivot_wider(data_2, id_cols = newid, names_from = years, values_from = -c(newid, years))\n\n# Visualize missing values in the wideform data set\nvis_miss(data_wide_2)\n\n\n\n\n\n\n\n\nThere are no real trends that become apparent when looking at this plot for the wideform data set.\nTo summarize, it appears that diagnoses that were determined by algorithm (such as KID, DIAB, DYSLIP, and LIV34, often had insufficient data to make a diagnosis, so perhaps this is an issue with those algorithms. Additionally, lab measurements of LDL and TRIG seem to have been too onerous for participants to have had collected. Maybe they opted out of those tests, or maybe the tests were only ordered under certain circumstances.\nThese would be valuable questions to bring forth to the PI. But for now it appear as if we won’t be able to use these variables.\nWe may want to impute income, LIV34, and TCHOL, as these have missingness of 24%, 31%, and 32%, respectively.\n\nVariables with minimal missing data (&lt;5%) that can be disregarded without affecting analysis integrity\n\nAGG_MENT\nAGG_PHYS\nHASH_V\nHASHF\nFRP\nSMOKE\nDKGRP\nHEROPIATE\nIDU\nLEU3N\nVLOAD\nADH\nEDUCBASE\nAGE\nART\nyears\nhard-drugs\n\n\n\nVariables with moderate missing data (5-20%) that necessitate intervention\n\nBMI\nHBP\n\n\n\nVariables with &gt;20% missing data that are edge cases and may require exclusion or imputation\n\nTCHOL\nincome\nLIV34\n\n\n\nVariables with an excess of missing data (&gt;40%) that necessitate exclusion from the analysis\n\nLDL\nTRIG\nDIAB\nKID\nDYSLIP"
  },
  {
    "objectID": "Project_2/Project_2_R/Code/Project2.html#removing-superfluous-variables",
    "href": "Project_2/Project_2_R/Code/Project2.html#removing-superfluous-variables",
    "title": "Advanced Data Analysis - Project 2",
    "section": "Removing Superfluous Variables",
    "text": "Removing Superfluous Variables\nWe previously determined that LDL, TRIG, DIAB, KID, and DYSLIP had excessive missing values (&gt;40%).\nNow that we have seen that they are not strongly related to the outcome variables, we can be assured that we can safely remove them with no need for imputation.\n\n# Drop variables with excessive missing values from the wideform data set\ndata_2 &lt;- data_2 %&gt;%\n  select(-LDL, -TRIG, -DIAB, -KID, -DYSLIP)\n\nFRP and FP are both precision variables for AGG_PHYS, but highly correlated to each other.\nFP has more missing values (22%) than FRP(~0%), and will thus be dropped.\n\n# Drop FP\ndata_2 &lt;- data_2 %&gt;%\n  select(-FP)\n\nEDUCBASE is highly correlated with income, TCHOL, SMOKE, and RACE.\nincome has 27% missing values and thus will be dropped from further analysis.\nTCHOL has 32% missing values and will thus be dropped.\nSMOKE and RACE will be dropped to avoid issues of multicollinearity, and EDUCBASE used as the covariate of choice.\n\n# Drop income, total cholesterol, smoke, and race\ndata_2 &lt;- data_2 %&gt;%\n  select(-income, -TCHOL, -SMOKE, -RACE)\n\nHard_drugs is highly correlated with HEROPIATE and IDU.\nHard_drugs is our main independent variable of interest and thus we drop HEROPIATE and IDU.\n\n# Drop income, total cholesterol, smoke, and race\ndata_2 &lt;- data_2 %&gt;%\n  select(-HEROPIATE, -IDU)\n\nHBP is lightly correlated with age and BMI. Let’s drop it to avoid multicollinearity.\n\n# Drop high blood pressure.\ndata_2 &lt;- data_2 %&gt;%\n  select(-HBP)"
  },
  {
    "objectID": "Project_2/Project_2_R/Code/Project2.html#correlation-matrix-redux",
    "href": "Project_2/Project_2_R/Code/Project2.html#correlation-matrix-redux",
    "title": "Advanced Data Analysis - Project 2",
    "section": "Correlation Matrix Redux",
    "text": "Correlation Matrix Redux\nLet’s take another look at that correlation matrix now that we have cleaned up our data set to remove variables with excessive missing values and issues of multicollinearity.\n\n# Let's clean our output by making a trimmed dataset excluding extraneous variables\ndata_for_matrix &lt;- select(data_2, -newid, -ART, -everART, -LEU3N_log, -LEU3N_yeojohnson, -LEU3N_boxcox, -LEU3N_orderNorm, - LEU3N_standard, -AGG_MENT_orderNorm, -AGG_MENT_orderNorm, -BMI_outlier, -CESD_sqrt, -hivpos, -AGG_PHYS_orderNorm, - LIV34)\n\n# We factored our variables at the start. To make a correlation matrix we must reconvert those back to numeric\ndata_for_matrix &lt;- data.frame(lapply(data_for_matrix, function(x) if (is.factor(x)) as.numeric(x) else x))\n\n# Make a correlation matrix with all variables of the trimmed data set\ncorrelation_matrix &lt;- cor(data_for_matrix, use = \"complete.obs\")\n\n# Plot the matrix\ncorrplot(correlation_matrix, method = \"circle\")\n\n\n\n\n\n\n\n\nLooking MUCH better! Here we can see some potentially strong relationships emerge.\n\nCESD as mentioned will be included as a precision variable for AGG_MENT\nFRP as mentioned will be included as a precision variable for AGG_PHYS\nEDUCBASE looks like it will be a predictor for all outcome variables except AGG_MENT\nBMI appears to have a weak correlation with all outcome variables and will likely be included in the final models.\nage also looks weakly correlated to all outcome variables except VLOAD_log\n\nLet’s run some individual regression and assess these relationships more closely.\np-values of &lt; 0.1 will be considered for the final models.\nI just want to try making a correlation matrix of the wideform data set and see what it looks like\n\n# Let's clean our output by making a trimmed dataset excluding extraneous variables\ndata_for_matrix &lt;- select(data_wide_2, AGG_MENT_CHANGE, AGG_PHYS_CHANGE, LEU3N_CHANGE, VLOAD_log_CHANGE, BMI_2, FRP_2, CESD_2, DKGRP_2, ADH_2, ADH_HIGHVSLOW, EDUCBAS_2, EDUC_COLLEGE, age_2, hard_drugs_grp)\n\n# We factored our variables at the start. To make a correlation matrix we must reconvert those back to numeric\ndata_for_matrix &lt;- data.frame(lapply(data_for_matrix, function(x) if (is.factor(x)) as.numeric(x) else x))\n\n# Make a correlation matrix with all variables of the trimmed data set\ncorrelation_matrix &lt;- cor(data_for_matrix, use = \"complete.obs\")\n\n# Plot the matrix\ncorrplot(correlation_matrix, method = \"circle\")"
  },
  {
    "objectID": "Project_2/Project_2_R/Code/Project2.html#log-viral-load-change-1",
    "href": "Project_2/Project_2_R/Code/Project2.html#log-viral-load-change-1",
    "title": "Advanced Data Analysis - Project 2",
    "section": "Log Viral Load Change",
    "text": "Log Viral Load Change\n\nLog Viral Load ChangeTest\n\n\n\n# Refactor reference level of hard drug use group back to Never User\ndata_wide_2$hard_drugs_grp &lt;- relevel(data_wide_2$hard_drugs_grp, ref = \"Never User\")\n\n# Perform final model regression predicting log viral load change by hard drug use group, adherence group, and college education\nmodel_log_VLOAD1 &lt;- lm(VLOAD_log_CHANGE ~ hard_drugs_grp + ADH_HIGHVSLOW + EDUC_COLLEGE, data = data_wide_2)\n\n# Examine summary\nsummary(model_log_VLOAD1)\n\n\nCall:\nlm(formula = VLOAD_log_CHANGE ~ hard_drugs_grp + ADH_HIGHVSLOW + \n    EDUC_COLLEGE, data = data_wide_2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.7941 -1.7418 -0.2149  1.2043  9.3073 \n\nCoefficients:\n                            Estimate Std. Error t value             Pr(&gt;|t|)\n(Intercept)                  -4.0871     0.4910  -8.324 0.000000000000000771\nhard_drugs_grpPrevious User   0.7190     0.4336   1.658               0.0978\nhard_drugs_grpCurrent User    0.9170     0.3755   2.442               0.0149\nADH_HIGHVSLOWHigh Adherence  -1.8429     0.4377  -4.211 0.000030041916341918\nEDUC_COLLEGECollege          -0.6897     0.2970  -2.322               0.0206\n                               \n(Intercept)                 ***\nhard_drugs_grpPrevious User .  \nhard_drugs_grpCurrent User  *  \nADH_HIGHVSLOWHigh Adherence ***\nEDUC_COLLEGECollege         *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.714 on 515 degrees of freedom\n  (30 observations deleted due to missingness)\nMultiple R-squared:  0.06016,   Adjusted R-squared:  0.05286 \nF-statistic: 8.241 on 4 and 515 DF,  p-value: 0.000001901\n\n# Refactor reference level of hard drug use group to Previous User\ndata_wide_2$hard_drugs_grp &lt;- relevel(data_wide_2$hard_drugs_grp, ref = \"Previous User\")\n\n# Perform final model regression predicting log viral load change by hard drug use group, adherence group, and college education\nmodel_log_VLOAD2 &lt;- lm(VLOAD_log_CHANGE ~ hard_drugs_grp + ADH_HIGHVSLOW + EDUC_COLLEGE, data = data_wide_2)\n\n# Examine summary\nsummary(model_log_VLOAD2)\n\n\nCall:\nlm(formula = VLOAD_log_CHANGE ~ hard_drugs_grp + ADH_HIGHVSLOW + \n    EDUC_COLLEGE, data = data_wide_2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.7941 -1.7418 -0.2149  1.2043  9.3073 \n\nCoefficients:\n                            Estimate Std. Error t value      Pr(&gt;|t|)    \n(Intercept)                  -3.3680     0.5718  -5.890 0.00000000697 ***\nhard_drugs_grpNever User     -0.7190     0.4336  -1.658        0.0978 .  \nhard_drugs_grpCurrent User    0.1980     0.5451   0.363        0.7166    \nADH_HIGHVSLOWHigh Adherence  -1.8429     0.4377  -4.211 0.00003004192 ***\nEDUC_COLLEGECollege          -0.6897     0.2970  -2.322        0.0206 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.714 on 515 degrees of freedom\n  (30 observations deleted due to missingness)\nMultiple R-squared:  0.06016,   Adjusted R-squared:  0.05286 \nF-statistic: 8.241 on 4 and 515 DF,  p-value: 0.000001901\n\n# Testing with an interaction term. I don't believe this is the appropriate method.\n\nmodel_log_VLOAD &lt;- lm(VLOAD_log_CHANGE ~ hard_drugs_grp + ADH_HIGHVSLOW + EDUC_COLLEGE + hard_drugs_grp*ADH_HIGHVSLOW, data = data_wide_2)\n\nsummary(model_log_VLOAD)\n\n\nCall:\nlm(formula = VLOAD_log_CHANGE ~ hard_drugs_grp + ADH_HIGHVSLOW + \n    EDUC_COLLEGE + hard_drugs_grp * ADH_HIGHVSLOW, data = data_wide_2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.2508 -1.6120 -0.2134  1.1308  9.2569 \n\nCoefficients:\n                                                       Estimate Std. Error\n(Intercept)                                              0.8757     1.1260\nhard_drugs_grpNever User                                -5.8083     1.1857\nhard_drugs_grpCurrent User                              -2.3325     1.7242\nADH_HIGHVSLOWHigh Adherence                             -6.6356     1.1785\nEDUC_COLLEGECollege                                     -0.8489     0.2955\nhard_drugs_grpNever User:ADH_HIGHVSLOWHigh Adherence     5.8479     1.2773\nhard_drugs_grpCurrent User:ADH_HIGHVSLOWHigh Adherence   3.1050     1.8278\n                                                       t value     Pr(&gt;|t|)    \n(Intercept)                                              0.778      0.43710    \nhard_drugs_grpNever User                                -4.898 0.0000012967 ***\nhard_drugs_grpCurrent User                              -1.353      0.17672    \nADH_HIGHVSLOWHigh Adherence                             -5.630 0.0000000297 ***\nEDUC_COLLEGECollege                                     -2.872      0.00424 ** \nhard_drugs_grpNever User:ADH_HIGHVSLOWHigh Adherence     4.578 0.0000058885 ***\nhard_drugs_grpCurrent User:ADH_HIGHVSLOWHigh Adherence   1.699      0.08997 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.661 on 513 degrees of freedom\n  (30 observations deleted due to missingness)\nMultiple R-squared:  0.1001,    Adjusted R-squared:  0.08956 \nF-statistic:  9.51 on 6 and 513 DF,  p-value: 0.0000000006388\n\n\nI guess the interaction term shows that there is an interaction for never and previous users, based on adherence. Look into this more later. It may be that never users with low adherence are different from previous users with high adherence!! HOW TO INTERPRET??\nSo it seems that adherence IS a confounder for drug use.\nWhen we ran drug_use_grp by itself as a predictor on log viral load, the overall model was significant, but after correcting for multiple pairwise comparisons, none of the between-group comparisons was significant (p &gt; 0.05).\nHowever, after including ADH_HIGHLOW in the model, the overall model was still significant, but now the current vs never drug use comparison is significant, even after performing a bonferroni correction (p-adjusted = 0.0447). This is in comparison to the model without ADH, where this same p-adjusted value was 0.1377. The beta coefficient is not &gt; 10% different, however based on the shifts in the p-values, I will conclude that adherence is a confounder for the relationship between hard drug use and log viral load.\nTake a look at the unadjusted vs adjusted CIs for hard drug use groups.\nAdd stuff here about pre-inclusion and post-inclusion SEs and parameter estimates.\n2.50 2.65\n\nmodel_anova &lt;- aov(VLOAD_log_CHANGE ~ hard_drugs_grp * ADH_HIGHVSLOW, data = data_wide_2)\n\nsummary(model_anova)\n\n                              Df Sum Sq Mean Sq F value    Pr(&gt;F)    \nhard_drugs_grp                 2     75   37.41   5.208   0.00577 ** \nADH_HIGHVSLOW                  1    128  128.34  17.868 0.0000280 ***\nhard_drugs_grp:ADH_HIGHVSLOW   2    143   71.27   9.922 0.0000592 ***\nResiduals                    514   3692    7.18                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n30 observations deleted due to missingness\n\nTukeyHSD(model_anova)\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = VLOAD_log_CHANGE ~ hard_drugs_grp * ADH_HIGHVSLOW, data = data_wide_2)\n\n$hard_drugs_grp\n                                 diff         lwr         upr     p adj\nNever User-Previous User   -1.0418369 -2.02089200 -0.06278185 0.0338682\nCurrent User-Previous User -0.1874073 -1.42194879  1.04713417 0.9322394\nCurrent User-Never User     0.8544296 -0.01577357  1.72463280 0.0555904\n\n$ADH_HIGHVSLOW\n                                 diff      lwr        upr     p adj\nHigh Adherence-Low Adherence -1.82015 -2.66753 -0.9727702 0.0000289\n\n$`hard_drugs_grp:ADH_HIGHVSLOW`\n                                                               diff        lwr\nNever User:Low Adherence-Previous User:Low Adherence     -5.6226305 -9.0331996\nCurrent User:Low Adherence-Previous User:Low Adherence   -1.9080830 -6.8566568\nPrevious User:High Adherence-Previous User:Low Adherence -6.1262114 -9.4824975\nNever User:High Adherence-Previous User:Low Adherence    -6.4314985 -9.5857348\nCurrent User:High Adherence-Previous User:Low Adherence  -5.7721523 -9.0653051\nCurrent User:Low Adherence-Never User:Low Adherence       3.7145475 -0.3511207\nPrevious User:High Adherence-Never User:Low Adherence    -0.5035809 -2.3218030\nNever User:High Adherence-Never User:Low Adherence       -0.8088680 -2.2197126\nCurrent User:High Adherence-Never User:Low Adherence     -0.1495218 -1.8483834\nPrevious User:High Adherence-Current User:Low Adherence  -4.2181285 -8.2383688\nNever User:High Adherence-Current User:Low Adherence     -4.5234155 -8.3765809\nCurrent User:High Adherence-Current User:Low Adherence   -3.8640693 -7.8317552\nNever User:High Adherence-Previous User:High Adherence   -0.3052871 -1.5793187\nCurrent User:High Adherence-Previous User:High Adherence  0.3540591 -1.2330152\nCurrent User:High Adherence-Never User:High Adherence     0.6593462 -0.4376296\n                                                                upr     p adj\nNever User:Low Adherence-Previous User:Low Adherence     -2.2120614 0.0000455\nCurrent User:Low Adherence-Previous User:Low Adherence    3.0404908 0.8801123\nPrevious User:High Adherence-Previous User:Low Adherence -2.7699254 0.0000038\nNever User:High Adherence-Previous User:Low Adherence    -3.2772622 0.0000001\nCurrent User:High Adherence-Previous User:Low Adherence  -2.4789996 0.0000109\nCurrent User:Low Adherence-Never User:Low Adherence       7.7802157 0.0958216\nPrevious User:High Adherence-Never User:Low Adherence     1.3146412 0.9687938\nNever User:High Adherence-Never User:Low Adherence        0.6019766 0.5723343\nCurrent User:High Adherence-Never User:Low Adherence      1.5493398 0.9998623\nPrevious User:High Adherence-Current User:Low Adherence  -0.1978881 0.0333990\nNever User:High Adherence-Current User:Low Adherence     -0.6702502 0.0108555\nCurrent User:High Adherence-Current User:Low Adherence    0.1036166 0.0613473\nNever User:High Adherence-Previous User:High Adherence    0.9687446 0.9835298\nCurrent User:High Adherence-Previous User:High Adherence  1.9411335 0.9880816\nCurrent User:High Adherence-Never User:High Adherence     1.7563219 0.5195563\n\n\nBased on the results of the ANOVA, we can see that at high adherence, there is no difference between the never and previous drug use groups (p = .9835298). However, there is a difference when adherence for both groups is low (p = 0.0000455) and when never users have high adherence and previous users have low adherence (p = 0.0000001), but NOT when Previous Users have High Adherence and Never User have Low Adherence (p = 0.9687938).\nIn other words, the relationship between never and previous drug users depends on adherence. If adherence is high, then there is no statistical difference in log viral load change over 2 years between both groups. However, if adherence is low\nIn other words, previous hard drug users can make up for the impact of hard drug use on log viral load by having high adherence to the treatment regiment. Where if they have high adherence, their log viral loads are comparable to those who never used hard drugs.\nHow to show that in a plot??\nCHECK HERE IF THERE IS AN ASSOCIATION BETWEEN HARD DRUG USE GROUP AND ADHERENCE\nIF YES THEN ADHERENCE IS A CONFOUNDER\nUnder the classical but not but not operational definition (% change not &gt; 20%). It is therefore a Maverick variable!!!\nUnder the classical definition of a confounder, a variable Z is a confounder if:\n\n\nIt is associated with outcome Y\n\n\nIt is associated with PEV X\n\n\nIt is not on the causal pathway (not a mediator)\n\n\nTo assess if Adherence is associated with hard drug use group, we can run a chi-square test\n\n# Make a contingency table for hard drug use and adherence high vs low\ncontingency_table &lt;- table(data_wide_2$hard_drugs_grp, data_wide_2$ADH_HIGHVSLOW)\ncontingency_table\n\n               \n                Low Adherence High Adherence\n  Previous User             6             40\n  Never User               33            399\n  Current User              4             56\n\n# We have to run Fisher's Exact test since we have fewer than 5 observations in some categories\nfisher_test &lt;- fisher.test(contingency_table)\n\nfisher_test\n\n\n    Fisher's Exact Test for Count Data\n\ndata:  contingency_table\np-value = 0.4168\nalternative hypothesis: two.sided\n\n\nThey are NOT associated. Which means that adherence is NOT a confounder????? But it basically is.\nHowever, it is NOT necessary for this associaton to be statistically significant for there to be important confounding present.\nIn our case I think we conclude adherence IS a confounder for the relationship between hard drug use and log viral load.\nADD INTERPRETATIONS HERE.\nTop of Tabset"
  },
  {
    "objectID": "Project_2/Project_2_R/Code/Project2.html#cd4-t-cell-count-change-1",
    "href": "Project_2/Project_2_R/Code/Project2.html#cd4-t-cell-count-change-1",
    "title": "Advanced Data Analysis - Project 2",
    "section": "CD4+ T Cell Count Change",
    "text": "CD4+ T Cell Count Change\nThe second outcome variable of interest was change in CD4+ T Cell Count over the first 2 years of the study.\n\nFull ModelReduced Model\n\n\nThe candidate variables for inclusion in our model predicting LEU3N_CHANGE were LEU3N_CHANGE arehard_drugs_grp,ADH_HIGHLOW,BMI_2,FRP_2, andEDUC_COLLEGE`.\nWe begin by examining the full model with the PEV, potential confounder, and all potential covariates.\n\n# Relevel to change to the reference group to never user\ndata_wide_2$hard_drugs_grp&lt;- relevel(data_wide_2$hard_drugs_grp, ref = \"Never User\")\n\n# Perform regression on CD4+ T Cell count change by hard drugs group, include adherence as confounder, and BMI, frailty related phenotype, and college education as precision variables.\nmodel_LEU3N_full &lt;- lm(LEU3N_CHANGE ~ hard_drugs_grp + ADH_HIGHVSLOW + BMI_2 + FRP_2 + EDUC_COLLEGE, data = data_wide_2)\n\n# Exmamine the summary\nsummary(model_LEU3N_full)\n\n\nCall:\nlm(formula = LEU3N_CHANGE ~ hard_drugs_grp + ADH_HIGHVSLOW + \n    BMI_2 + FRP_2 + EDUC_COLLEGE, data = data_wide_2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-660.13 -119.14   -0.47  110.28 1078.44 \n\nCoefficients:\n                            Estimate Std. Error t value   Pr(&gt;|t|)    \n(Intercept)                  -52.753     61.507  -0.858    0.39152    \nhard_drugs_grpPrevious User  -94.636     31.395  -3.014    0.00272 ** \nhard_drugs_grpCurrent User  -118.453     25.688  -4.611 0.00000521 ***\nADH_HIGHVSLOWHigh Adherence   75.017     33.271   2.255    0.02463 *  \nBMI_2                          5.842      2.117   2.759    0.00603 ** \nFRP_2Yes                    -106.137     36.076  -2.942    0.00343 ** \nEDUC_COLLEGECollege           30.584     21.416   1.428    0.15395    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 180 on 454 degrees of freedom\n  (89 observations deleted due to missingness)\nMultiple R-squared:  0.1128,    Adjusted R-squared:  0.1011 \nF-statistic:  9.62 on 6 and 454 DF,  p-value: 0.0000000005661\n\n\n\n# Gather p-values of the model\np_values &lt;- summary(model_LEU3N_full)$coefficients[,4] # This line selects the fourth column of the resulting coefficients table from summary(model_attach), which is the p-values\n\n# Perform Bonferroni correction \np_adjusted &lt;- p.adjust(p_values, method = \"bonferroni\")\n\n# Compare adjusted p-values to unadjusted p-values\np_comparison &lt;- cbind(p_values, p_adjusted)\npretty_print(p_comparison)\n\n\n\n\n\np_values\np_adjusted\n\n\n\n\n(Intercept)\n0.3915196\n1.0000000\n\n\nhard_drugs_grpPrevious User\n0.0027196\n0.0190369\n\n\nhard_drugs_grpCurrent User\n0.0000052\n0.0000365\n\n\nADH_HIGHVSLOWHigh Adherence\n0.0246269\n0.1723884\n\n\nBMI_2\n0.0060313\n0.0422191\n\n\nFRP_2Yes\n0.0034277\n0.0239937\n\n\nEDUC_COLLEGECollege\n0.1539464\n1.0000000\n\n\n\n\n\n\n\nRelevel to set previous user as the reference category and re-run the model.\n\n# Relevel to change to the reference group to previous user\ndata_wide_2$hard_drugs_grp&lt;- relevel(data_wide_2$hard_drugs_grp, ref = \"Previous User\")\n\n# Perform regression on CD4+ T Cell count change by hard drugs group, include adherence as confounder, and BMI, frailty related phenotype, and college education as precision variables.\nmodel_LEU3N_full &lt;- lm(LEU3N_CHANGE ~ hard_drugs_grp + ADH_HIGHVSLOW + BMI_2 + FRP_2 + EDUC_COLLEGE, data = data_wide_2)\n\n# Exmamine the summary\nsummary(model_LEU3N_full)\n\n\nCall:\nlm(formula = LEU3N_CHANGE ~ hard_drugs_grp + ADH_HIGHVSLOW + \n    BMI_2 + FRP_2 + EDUC_COLLEGE, data = data_wide_2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-660.13 -119.14   -0.47  110.28 1078.44 \n\nCoefficients:\n                            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)                 -147.390     68.228  -2.160  0.03128 * \nhard_drugs_grpNever User      94.636     31.395   3.014  0.00272 **\nhard_drugs_grpCurrent User   -23.817     38.689  -0.616  0.53846   \nADH_HIGHVSLOWHigh Adherence   75.017     33.271   2.255  0.02463 * \nBMI_2                          5.842      2.117   2.759  0.00603 **\nFRP_2Yes                    -106.137     36.076  -2.942  0.00343 **\nEDUC_COLLEGECollege           30.584     21.416   1.428  0.15395   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 180 on 454 degrees of freedom\n  (89 observations deleted due to missingness)\nMultiple R-squared:  0.1128,    Adjusted R-squared:  0.1011 \nF-statistic:  9.62 on 6 and 454 DF,  p-value: 0.0000000005661\n\n\n\n# Gather p-values of the model\np_values &lt;- summary(model_LEU3N_full)$coefficients[,4] # This line selects the fourth column of the resulting coefficients table from summary(model_attach), which is the p-values\n\n# Perform Bonferroni correction \np_adjusted &lt;- p.adjust(p_values, method = \"bonferroni\")\n\n# Compare adjusted p-values to unadjusted p-values\np_comparison &lt;- cbind(p_values, p_adjusted)\npretty_print(p_comparison)\n\n\n\n\n\np_values\np_adjusted\n\n\n\n\n(Intercept)\n0.0312767\n0.2189370\n\n\nhard_drugs_grpNever User\n0.0027196\n0.0190369\n\n\nhard_drugs_grpCurrent User\n0.5384636\n1.0000000\n\n\nADH_HIGHVSLOWHigh Adherence\n0.0246269\n0.1723884\n\n\nBMI_2\n0.0060313\n0.0422191\n\n\nFRP_2Yes\n0.0034277\n0.0239937\n\n\nEDUC_COLLEGECollege\n0.1539464\n1.0000000\n\n\n\n\n\n\n\nThe overall model is significant (F(6,454)= 9.62, p &lt; .0001).\nCurrent hard drug users have an average change in CD4+ T Cell count that is 118.45 cells lower than never drug users (p-adjusted &lt; 0.0001), and previous drug users have an average change in CD4+ T Cell count that is 94.636 cells lower (p-adjusted = 0.0190) than never drug users, while controlling for all the other variables in the model. The other between group comparisons were not significant (p-adjusted &gt; 0.05).\nADH_HIGHLOW is now significant in the full model (p = 0.02463), when it was not by itself. This means that, while controlling for hard drug use and the precision variables in the model, those who had high adherence to the treatment regiment had a change in CD4+ T Cell count that was 75.017 cells higher than those with low adherence on average.\nFor precision variables, we can see that BMI is now statistically significant (p = 0.00603), when it was not so by itself. FRP_2 is still a significant predictor (p = 0.00343).\nHowever, EDUC_COLLEGE is not longer a predictor of LEU3N_CHANGE when controlling for the other variables in the model (p = 0.15395).\nTop of Tabset\n\n\nLet’s examine a reduced model excluding EDUC_COLLEGE., since it was no longer significant in the full model.\n\n# Relevel to change to the reference group to Never user\ndata_wide_2$hard_drugs_grp&lt;- relevel(data_wide_2$hard_drugs_grp, ref = \"Never User\")\n\n# Perform regression on CD4+ T Cell count change by hard drugs group, include adherence as confounder, and BMI, FRP, as precision variables.\nmodel_LEU3N_red &lt;- lm(LEU3N_CHANGE ~ hard_drugs_grp + ADH_HIGHVSLOW + BMI_2 + FRP_2, data = data_wide_2)\n\n# Exmamine the summary\nsummary(model_LEU3N_red)\n\n\nCall:\nlm(formula = LEU3N_CHANGE ~ hard_drugs_grp + ADH_HIGHVSLOW + \n    BMI_2 + FRP_2, data = data_wide_2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-650.61 -114.34   -1.86  115.92 1085.18 \n\nCoefficients:\n                            Estimate Std. Error t value   Pr(&gt;|t|)    \n(Intercept)                  -40.005     60.925  -0.657   0.511756    \nhard_drugs_grpPrevious User -107.272     30.157  -3.557   0.000414 ***\nhard_drugs_grpCurrent User  -115.502     25.634  -4.506 0.00000842 ***\nADH_HIGHVSLOWHigh Adherence   76.596     33.291   2.301   0.021853 *  \nBMI_2                          6.247      2.101   2.974   0.003099 ** \nFRP_2Yes                    -104.737     36.104  -2.901   0.003900 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 180.2 on 455 degrees of freedom\n  (89 observations deleted due to missingness)\nMultiple R-squared:  0.1088,    Adjusted R-squared:  0.09901 \nF-statistic: 11.11 on 5 and 455 DF,  p-value: 0.0000000004093\n\n# Gather p-values of the model\np_values &lt;- summary(model_LEU3N_red)$coefficients[,4] # This line selects the fourth column of the resulting coefficients table from summary(model_attach), which is the p-values\n\n# Perform Bonferroni correction \np_adjusted &lt;- p.adjust(p_values, method = \"bonferroni\")\n\n# Compare adjusted p-values to unadjusted p-values\np_comparison &lt;- cbind(p_values, p_adjusted)\npretty_print(p_comparison)\n\n\n\n\n\np_values\np_adjusted\n\n\n\n\n(Intercept)\n0.5117564\n1.0000000\n\n\nhard_drugs_grpPrevious User\n0.0004143\n0.0024857\n\n\nhard_drugs_grpCurrent User\n0.0000084\n0.0000505\n\n\nADH_HIGHVSLOWHigh Adherence\n0.0218533\n0.1311196\n\n\nBMI_2\n0.0030987\n0.0185925\n\n\nFRP_2Yes\n0.0039005\n0.0234029\n\n\n\n\n\n\n\n\n# Relevel to change to the reference group to previous user\ndata_wide_2$hard_drugs_grp&lt;- relevel(data_wide_2$hard_drugs_grp, ref = \"Previous User\")\n\n# Perform regression on CD4+ T Cell count change by hard drugs group, include adherence as confounder, and BMI, FRP, as precision variables.\nmodel_LEU3N_red &lt;- lm(LEU3N_CHANGE ~ hard_drugs_grp + ADH_HIGHVSLOW + BMI_2 + FRP_2, data = data_wide_2)\n\n# Exmamine the summary\nsummary(model_LEU3N_red)\n\n\nCall:\nlm(formula = LEU3N_CHANGE ~ hard_drugs_grp + ADH_HIGHVSLOW + \n    BMI_2 + FRP_2, data = data_wide_2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-650.61 -114.34   -1.86  115.92 1085.18 \n\nCoefficients:\n                            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                 -147.277     68.306  -2.156 0.031597 *  \nhard_drugs_grpNever User     107.272     30.157   3.557 0.000414 ***\nhard_drugs_grpCurrent User    -8.230     37.160  -0.221 0.824825    \nADH_HIGHVSLOWHigh Adherence   76.596     33.291   2.301 0.021853 *  \nBMI_2                          6.247      2.101   2.974 0.003099 ** \nFRP_2Yes                    -104.737     36.104  -2.901 0.003900 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 180.2 on 455 degrees of freedom\n  (89 observations deleted due to missingness)\nMultiple R-squared:  0.1088,    Adjusted R-squared:  0.09901 \nF-statistic: 11.11 on 5 and 455 DF,  p-value: 0.0000000004093\n\n# Get the 95% CIs\nconf_intervals &lt;- confint(model_LEU3N_red)\npretty_print(conf_intervals)\n\n\n\n\n\n2.5 %\n97.5 %\n\n\n\n\n(Intercept)\n-281.511775\n-13.04207\n\n\nhard_drugs_grpNever User\n48.007758\n166.53668\n\n\nhard_drugs_grpCurrent User\n-81.255944\n64.79629\n\n\nADH_HIGHVSLOWHigh Adherence\n11.173265\n142.01870\n\n\nBMI_2\n2.118601\n10.37534\n\n\nFRP_2Yes\n-175.688700\n-33.78534\n\n\n\n\n\n\n\n\n# Relevel to change to the reference group to previous user\ndata_wide_2$hard_drugs_grp&lt;- relevel(data_wide_2$hard_drugs_grp, ref = \"Previous User\")\n\n# Perform regression on CD4+ T Cell count change by hard drugs group, include adherence as confounder, and BMI, FRP, as precision variables.\nmodel_LEU3N_red &lt;- lm(LEU3N_CHANGE ~ hard_drugs_grp + ADH_HIGHVSLOW + BMI_2 + FRP_2, data = data_wide_2)\n\n# Exmamine the summary\nsummary(model_LEU3N_red)\n\n\nCall:\nlm(formula = LEU3N_CHANGE ~ hard_drugs_grp + ADH_HIGHVSLOW + \n    BMI_2 + FRP_2, data = data_wide_2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-650.61 -114.34   -1.86  115.92 1085.18 \n\nCoefficients:\n                            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                 -147.277     68.306  -2.156 0.031597 *  \nhard_drugs_grpNever User     107.272     30.157   3.557 0.000414 ***\nhard_drugs_grpCurrent User    -8.230     37.160  -0.221 0.824825    \nADH_HIGHVSLOWHigh Adherence   76.596     33.291   2.301 0.021853 *  \nBMI_2                          6.247      2.101   2.974 0.003099 ** \nFRP_2Yes                    -104.737     36.104  -2.901 0.003900 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 180.2 on 455 degrees of freedom\n  (89 observations deleted due to missingness)\nMultiple R-squared:  0.1088,    Adjusted R-squared:  0.09901 \nF-statistic: 11.11 on 5 and 455 DF,  p-value: 0.0000000004093\n\n\nThe previous relationships all hold, with a similar adjusted R-squared (0.1011 in the full model, now 0.09901).\nWe can perform a nested F-Test to determine if the additional variable of EDUC_COLLEGE in the full model significantly improves the model fit compared to the reduced model without it.\n\n# Perform a nested/partial F-Test to compare the full and reduced model\nanova(model_LEU3N_full, model_LEU3N_red)\n\nAnalysis of Variance Table\n\nModel 1: LEU3N_CHANGE ~ hard_drugs_grp + ADH_HIGHVSLOW + BMI_2 + FRP_2 + \n    EDUC_COLLEGE\nModel 2: LEU3N_CHANGE ~ hard_drugs_grp + ADH_HIGHVSLOW + BMI_2 + FRP_2\n  Res.Df      RSS Df Sum of Sq      F Pr(&gt;F)\n1    454 14714229                           \n2    455 14780330 -1    -66101 2.0395 0.1539\n\n\nThe partial F-test is not signifant (p = 0.154), and we conclude that the addition of EDUC_COLLEGE does not significantly improve the model fit. Thus the reduced model without it will be considered the final model for LEU3N_CHANGE.\nTop of Tabset\n\nConclusion:\nThe overall model is significant (F(6,455)= 11.11, p &lt; .0001).\nCurrent hard drug users have an average change in CD4+ T Cell count that is 115.502 cells lower than never drug users (95% CI: 165.88 cells to 65.13 cells lower; p-adjusted &lt; 0.0001), and previous drug users have an average change in CD4+ T Cell count that is 107.272 cells lower (95% CI: 166.54 cells lower to 48.00 cells lower; p-adjusted = 0.00249) than never drug users, while controlling for all the other variables in the model. Previous and current drug users did not differ in CD4+ T Cell count change over 2 years (p-adjusted &gt; 0.05).\nAdherence to the treatment regiment was a significant predictor for change in CD4+ T Cell count over 2 years, while controlling for hard drug use and the precision variables in the model (t = 2.30, p = 0.0219). Those who had high adherence to the treatment regiment had a change in CD4+ T Cell count that was 76.60 cells higher than those with low adherence on average (95% CI: 11.17 to 142.019 cells higher).\nBMI is a significant predictor of change in CD4+ T cell count over 2 years, while controlling for the other variables in the model (t = 2.974, p = 0.00310). On average, every 1 unit increase in BMI is associated with a 6.24 cell increase in CD4+ T cell count (95% CI: 2.12 to 10.38 cells higher).\nFrailty related phenotype is a significant predictor for change in CD4+ T Cell count over 2 years, while controlling for the other variables in the model (t = -2.901, p = 0.0039). On average, those with a frailty related phenotype had a change in CD4+ T Cell count that was 104.74 cells lower than those without a frailty related phenotype (95% CI: 175.69 to 33.79 cells lower).\nTop of Tabset"
  },
  {
    "objectID": "Project_2/Project_2_R/Code/Project2.html#mental-qol-change",
    "href": "Project_2/Project_2_R/Code/Project2.html#mental-qol-change",
    "title": "Advanced Data Analysis - Project 2",
    "section": "Mental QOL Change",
    "text": "Mental QOL Change\nThe candidate variables for mental QOL change as determined by interactive variable selection are hard_drug_grp, ADH_HIGHLOW, CESD_2.\n\nFull ModelReduced Model 1\n\n\n\n# Relevel to change to the reference group to never user\ndata_wide_2$hard_drugs_grp &lt;- relevel(data_wide_2$hard_drugs_grp, ref = \"Never User\")\n\n# Perform regression on mental QOL change by hard drugs group, include adherence highlow and depression score.\nmodel_MENT_full &lt;- lm(AGG_MENT_CHANGE ~ hard_drugs_grp + ADH_HIGHVSLOW + CESD_2, data = data_wide_2)\n\n# Exmamine the summary\nsummary(model_MENT_full)\n\n\nCall:\nlm(formula = AGG_MENT_CHANGE ~ hard_drugs_grp + ADH_HIGHVSLOW + \n    CESD_2, data = data_wide_2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-32.274  -6.421  -1.574   5.343  39.297 \n\nCoefficients:\n                            Estimate Std. Error t value        Pr(&gt;|t|)    \n(Intercept)                  3.47571    2.00874   1.730          0.0842 .  \nhard_drugs_grpPrevious User  2.70142    1.85281   1.458          0.1454    \nhard_drugs_grpCurrent User  -0.73939    1.63662  -0.452          0.6516    \nADH_HIGHVSLOWHigh Adherence  2.94279    1.90898   1.542          0.1238    \nCESD_2                      -0.32881    0.04803  -6.845 0.0000000000214 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 11.72 on 523 degrees of freedom\n  (22 observations deleted due to missingness)\nMultiple R-squared:  0.09648,   Adjusted R-squared:  0.08957 \nF-statistic: 13.96 on 4 and 523 DF,  p-value: 0.00000000007883\n\n\n\n\nLet’s try that removing hard_drugs_grp\n\n# Relevel to change to the reference group to never user\ndata_wide_2$hard_drugs_grp &lt;- relevel(data_wide_2$hard_drugs_grp, ref = \"Never User\")\n\n# Perform regression on mental QOL change by adherence highlow and depression score.\nmodel_MENT_red1 &lt;- lm(AGG_MENT_CHANGE ~ ADH_HIGHVSLOW + CESD_2, data = data_wide_2)\n\n# Exmamine the summary\nsummary(model_MENT_red1)\n\n\nCall:\nlm(formula = AGG_MENT_CHANGE ~ ADH_HIGHVSLOW + CESD_2, data = data_wide_2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-32.502  -6.420  -1.588   5.540  38.941 \n\nCoefficients:\n                            Estimate Std. Error t value        Pr(&gt;|t|)    \n(Intercept)                  3.63424    2.00551   1.812          0.0705 .  \nADH_HIGHVSLOWHigh Adherence  2.81705    1.90788   1.477          0.1404    \nCESD_2                      -0.32031    0.04685  -6.838 0.0000000000224 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 11.72 on 525 degrees of freedom\n  (22 observations deleted due to missingness)\nMultiple R-squared:  0.09207,   Adjusted R-squared:  0.08861 \nF-statistic: 26.62 on 2 and 525 DF,  p-value: 0.000000000009752\n\n\nAdherence is no longer significant.\nWhat about with an interaction term?\n\n# Relevel to change to the reference group to never user\ndata_wide_2$hard_drugs_grp &lt;- relevel(data_wide_2$hard_drugs_grp, ref = \"Never User\")\n\n# Perform regression on mental QOL change by adherence highlow and depression score.\nmodel_MENT_red2 &lt;- lm(AGG_MENT_CHANGE ~ ADH_HIGHVSLOW + CESD_2 + ADH_HIGHVSLOW*CESD_2, data = data_wide_2)\n\n# Exmamine the summary\nsummary(model_MENT_red2)\n\n\nCall:\nlm(formula = AGG_MENT_CHANGE ~ ADH_HIGHVSLOW + CESD_2 + ADH_HIGHVSLOW * \n    CESD_2, data = data_wide_2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-32.768  -6.266  -1.609   5.564  38.302 \n\nCoefficients:\n                                   Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                          6.3906     2.9163   2.191 0.028864 *  \nADH_HIGHVSLOWHigh Adherence         -0.2448     3.0287  -0.081 0.935616    \nCESD_2                              -0.4695     0.1238  -3.791 0.000168 ***\nADH_HIGHVSLOWHigh Adherence:CESD_2   0.1741     0.1338   1.301 0.193784    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 11.72 on 524 degrees of freedom\n  (22 observations deleted due to missingness)\nMultiple R-squared:  0.09499,   Adjusted R-squared:  0.08981 \nF-statistic: 18.33 on 3 and 524 DF,  p-value: 0.00000000002522\n\n\nthere’s no interaction\n\n# Perform regression on mental QOL change by adherence highlow and depression score.\nmodel_MENT_red3 &lt;- lm(AGG_MENT_CHANGE ~ hard_drugs_grp + CESD_2, data = data_wide_2)\n\n# Exmamine the summary\nsummary(model_MENT_red3)\n\n\nCall:\nlm(formula = AGG_MENT_CHANGE ~ hard_drugs_grp + CESD_2, data = data_wide_2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-31.870  -6.425  -1.667   5.329  39.913 \n\nCoefficients:\n                            Estimate Std. Error t value            Pr(&gt;|t|)    \n(Intercept)                   6.3410     0.7841   8.087 0.00000000000000413 ***\nhard_drugs_grpPrevious User   2.6620     1.8584   1.432               0.153    \nhard_drugs_grpCurrent User   -0.6203     1.6408  -0.378               0.706    \nCESD_2                       -0.3430     0.0475  -7.220 0.00000000000179755 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 11.77 on 534 degrees of freedom\n  (12 observations deleted due to missingness)\nMultiple R-squared:  0.09257,   Adjusted R-squared:  0.08747 \nF-statistic: 18.16 on 3 and 534 DF,  p-value: 0.00000000003115\n\n\n\n# Relevel to change to the reference group to previous user\ndata_wide_2$hard_drugs_grp &lt;- relevel(data_wide_2$hard_drugs_grp, ref = \"Previous User\")\n\n# Perform regression on mental QOL change by adherence highlow and depression score.\nmodel_MENT_red3 &lt;- lm(AGG_MENT_CHANGE ~ hard_drugs_grp + CESD_2, data = data_wide_2)\n\n# Exmamine the summary\nsummary(model_MENT_red3)\n\n\nCall:\nlm(formula = AGG_MENT_CHANGE ~ hard_drugs_grp + CESD_2, data = data_wide_2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-31.870  -6.425  -1.667   5.329  39.913 \n\nCoefficients:\n                           Estimate Std. Error t value        Pr(&gt;|t|)    \n(Intercept)                  9.0030     1.9517   4.613 0.0000049773093 ***\nhard_drugs_grpNever User    -2.6620     1.8584  -1.432           0.153    \nhard_drugs_grpCurrent User  -3.2824     2.3079  -1.422           0.156    \nCESD_2                      -0.3430     0.0475  -7.220 0.0000000000018 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 11.77 on 534 degrees of freedom\n  (12 observations deleted due to missingness)\nMultiple R-squared:  0.09257,   Adjusted R-squared:  0.08747 \nF-statistic: 18.16 on 3 and 534 DF,  p-value: 0.00000000003115\n\n\n\n# Relevel to change to the reference group to Never user\ndata_wide_2$hard_drugs_grp &lt;- relevel(data_wide_2$hard_drugs_grp, ref = \"Never User\")\n\n# Perform regression on mental QOL change by adherence highlow and depression score.\nmodel_MENT_red4 &lt;- lm(AGG_MENT_CHANGE ~ hard_drugs_grp + CESD_2 + hard_drugs_grp*CESD_2, data = data_wide_2)\n\n# Exmamine the summary\nsummary(model_MENT_red4)\n\n\nCall:\nlm(formula = AGG_MENT_CHANGE ~ hard_drugs_grp + CESD_2 + hard_drugs_grp * \n    CESD_2, data = data_wide_2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-34.141  -6.071  -1.545   5.758  37.779 \n\nCoefficients:\n                                   Estimate Std. Error t value       Pr(&gt;|t|)\n(Intercept)                         5.10534    0.81668   6.251 0.000000000836\nhard_drugs_grpPrevious User        16.97744    2.91062   5.833 0.000000009463\nhard_drugs_grpCurrent User         -3.04165    2.90324  -1.048          0.295\nCESD_2                             -0.23480    0.05313  -4.419 0.000011998960\nhard_drugs_grpPrevious User:CESD_2 -0.80296    0.13010  -6.172 0.000000001341\nhard_drugs_grpCurrent User:CESD_2   0.10992    0.15093   0.728          0.467\n                                      \n(Intercept)                        ***\nhard_drugs_grpPrevious User        ***\nhard_drugs_grpCurrent User            \nCESD_2                             ***\nhard_drugs_grpPrevious User:CESD_2 ***\nhard_drugs_grpCurrent User:CESD_2     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 11.36 on 532 degrees of freedom\n  (12 observations deleted due to missingness)\nMultiple R-squared:  0.1571,    Adjusted R-squared:  0.1492 \nF-statistic: 19.84 on 5 and 532 DF,  p-value: &lt; 0.00000000000000022\n\n\n\n# Relevel to change to the reference group to Previous user\ndata_wide_2$hard_drugs_grp &lt;- relevel(data_wide_2$hard_drugs_grp, ref = \"Previous User\")\n\n# Perform regression on mental QOL change by adherence highlow and depression score.\nmodel_MENT_red4 &lt;- lm(AGG_MENT_CHANGE ~ hard_drugs_grp + CESD_2 + hard_drugs_grp*CESD_2, data = data_wide_2)\n\n# Exmamine the summary\nsummary(model_MENT_red4)\n\n\nCall:\nlm(formula = AGG_MENT_CHANGE ~ hard_drugs_grp + CESD_2 + hard_drugs_grp * \n    CESD_2, data = data_wide_2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-34.141  -6.071  -1.545   5.758  37.779 \n\nCoefficients:\n                                  Estimate Std. Error t value\n(Intercept)                        22.0828     2.7937   7.904\nhard_drugs_grpNever User          -16.9774     2.9106  -5.833\nhard_drugs_grpCurrent User        -20.0191     3.9455  -5.074\nCESD_2                             -1.0378     0.1188  -8.738\nhard_drugs_grpNever User:CESD_2     0.8030     0.1301   6.172\nhard_drugs_grpCurrent User:CESD_2   0.9129     0.1846   4.946\n                                              Pr(&gt;|t|)    \n(Intercept)                         0.0000000000000156 ***\nhard_drugs_grpNever User            0.0000000094634100 ***\nhard_drugs_grpCurrent User          0.0000005392622284 ***\nCESD_2                            &lt; 0.0000000000000002 ***\nhard_drugs_grpNever User:CESD_2     0.0000000013409353 ***\nhard_drugs_grpCurrent User:CESD_2   0.0000010165357517 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 11.36 on 532 degrees of freedom\n  (12 observations deleted due to missingness)\nMultiple R-squared:  0.1571,    Adjusted R-squared:  0.1492 \nF-statistic: 19.84 on 5 and 532 DF,  p-value: &lt; 0.00000000000000022\n\n\nWe have interaction. That just looks really crazy to interpret…\nLet’s try plotting it.\n\n# Create a scatterplot of mental QOLchange by depression score, colored by hard drug use group.\nggplot(data_wide_2, aes(x = CESD_2, y = AGG_MENT_CHANGE, color = hard_drugs_grp)) + \n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  theme_minimal() +\n  labs(title = \"Mental QOL Change by Depression at 2 years, Colored by Hard Drug Use Group\",\n       y = \"Mental QOL Change\",\n       x = \"Depression Score\") + \n  scale_fill_brewer(palette = \"Pastel2\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 12 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 12 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nDoes this show previous drug users are dealing with withdrawls and mental pains of quitting still?\nYeah I guess so.\nMaybe the downsides of the drug are so bad that taking hard drugs helps with it??\n\n# Relevel to change to the reference group to Never user\ndata_wide_2$hard_drugs_grp &lt;- relevel(data_wide_2$hard_drugs_grp, ref = \"Never User\")\n\n# Perform regression on mental QOL change by adherence highlow and depression score.\nmodel_MENT_red5 &lt;- lm(AGG_MENT_CHANGE ~ hard_drugs_grp + CESD_2 + ADH_HIGHVSLOW + hard_drugs_grp*CESD_2, data = data_wide_2)\n\n# Exmamine the summary\nsummary(model_MENT_red5)\n\n\nCall:\nlm(formula = AGG_MENT_CHANGE ~ hard_drugs_grp + CESD_2 + ADH_HIGHVSLOW + \n    hard_drugs_grp * CESD_2, data = data_wide_2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-33.946  -5.959  -1.450   5.733  37.717 \n\nCoefficients:\n                                   Estimate Std. Error t value      Pr(&gt;|t|)\n(Intercept)                         4.67063    1.96272   2.380        0.0177\nhard_drugs_grpPrevious User        16.94196    2.93963   5.763 0.00000001413\nhard_drugs_grpCurrent User         -2.98384    2.89763  -1.030        0.3036\nCESD_2                             -0.22865    0.05334  -4.287 0.00002157547\nADH_HIGHVSLOWHigh Adherence         0.39837    1.88979   0.211        0.8331\nhard_drugs_grpPrevious User:CESD_2 -0.80253    0.13286  -6.040 0.00000000293\nhard_drugs_grpCurrent User:CESD_2   0.10408    0.15058   0.691        0.4898\n                                      \n(Intercept)                        *  \nhard_drugs_grpPrevious User        ***\nhard_drugs_grpCurrent User            \nCESD_2                             ***\nADH_HIGHVSLOWHigh Adherence           \nhard_drugs_grpPrevious User:CESD_2 ***\nhard_drugs_grpCurrent User:CESD_2     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 11.33 on 521 degrees of freedom\n  (22 observations deleted due to missingness)\nMultiple R-squared:  0.1592,    Adjusted R-squared:  0.1496 \nF-statistic: 16.45 on 6 and 521 DF,  p-value: &lt; 0.00000000000000022\n\n\nSo we def don’t include adherence into this model.\nThe main driver of mental QOL change is not adherence to the protocol, but drug use and depression scores!!!\nDo I have to predict depression scores with mental QOL??? Or adherence???\n\n\n\n\nTO DO: PERFORM BACKWARDS SELECTION AND COMPARE THE RESULTING MODEL WITH MY MANUALLY SELECTED MODEL USING AICC AND BIC AND PICK BEST MODEL. _____"
  }
]