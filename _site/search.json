[
  {
    "objectID": "test.html",
    "href": "test.html",
    "title": "Data Checking Report",
    "section": "",
    "text": "The aim of the current study is to assess how treatment response differs for HIV+ patients 2 years after initiating Highly Active Antiretroviral Therapy (HAART) based on hard drug usage (such as heroin or cocaine). This study is of particular scientific interest because it is unclear whether the use of hard drugs inhibits the immune system in humans; treatment strategies may differ based on these results. The researchers are interested in comparing subjects who never used hard drugs to current hard drug users (those that use hard drugs at year 2) or previous hard drug users (those who used drugs at year 0 or 1). Outcomes of interest are: viral load (HIV copies in a mL of blood), CD4+ T cell count (a measure of immunologic health), and aggregate physical and quality of life scores from the SF-36.\nThe clinical hypothesis is that, if hard drugs inhibit the immune system in humans, subjects who currently or previously used hard drugs will have higher viral load and lower CD4+ T cell counts than those who never used hard drugs. Additionally, the researchers are interested in knowing if potential differences between the drug use groups can be explained by differences in adherence to the treatment regimen. The researchers are agnostic on how quality of life changes after treatment, since side effects of the treatment are significant.\nThe project description provided by the PI is available below:"
  },
  {
    "objectID": "test.html#data-preparation",
    "href": "test.html#data-preparation",
    "title": "Data Checking Report",
    "section": "Data Preparation",
    "text": "Data Preparation\nFirst we load the necessary packages\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(tidyverse)\nlibrary(knitr)\nlibrary(naniar)\nlibrary(kableExtra)\nlibrary(table1)\n\nThen we import the data set.\n\ndata &lt;- read_csv(\"C:/Users/sviea/Documents/Portfolio/Project_2/Project_2_R/RawData/hiv_dataset.csv\")\n\nRows: 3632 Columns: 33\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (33): newid, AGG_MENT, AGG_PHYS, HASHV, HASHF, income, BMI, HBP, DIAB, L...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nAnd take a look.\n\nglimpse(data)\n\nRows: 3,632\nColumns: 33\n$ newid      &lt;dbl&gt; 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4,…\n$ AGG_MENT   &lt;dbl&gt; 44.90710, 58.20754, 59.65136, 56.80657, 46.34190, 48.71791,…\n$ AGG_PHYS   &lt;dbl&gt; 52.52557, 41.29347, 48.54453, 46.73991, 27.92331, 38.03807,…\n$ HASHV      &lt;dbl&gt; 1, 1, 1, 1, 1, 2, 1, 2, 1, 2, 1, 1, 2, 1, 2, 1, 1, 2, 2, 1,…\n$ HASHF      &lt;dbl&gt; NA, 4, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 0, 0, 0, 0, 0, 4, 2…\n$ income     &lt;dbl&gt; 4, 4, 4, 5, 2, 1, 2, 1, 9, 2, 6, 6, 6, NA, 7, 7, 7, NA, 1, …\n$ BMI        &lt;dbl&gt; 24.71756, 26.06801, 27.16421, 25.71786, 26.66936, 25.96576,…\n$ HBP        &lt;dbl&gt; 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 9, 9, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ DIAB       &lt;dbl&gt; 1, 1, 1, 1, 9, 9, 9, 1, 1, 1, 9, 9, 1, 9, 9, 9, 9, 9, 1, 3,…\n$ LIV34      &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1,…\n$ KID        &lt;dbl&gt; 1, 1, 1, 1, 9, 9, 9, 2, 2, 2, 1, 1, 1, 2, 1, 1, 1, 2, 9, 9,…\n$ FRP        &lt;dbl&gt; 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2,…\n$ FP         &lt;dbl&gt; 1, 1, 1, 1, 9, 9, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2,…\n$ TCHOL      &lt;dbl&gt; 133, 131, 180, 171, 125, NA, 134, 105, 141, 153, 170, 170, …\n$ TRIG       &lt;dbl&gt; 176, 107, 233, 139, NA, NA, NA, 104, 162, 127, NA, NA, 82, …\n$ LDL        &lt;dbl&gt; 62, 66, 86, 96, NA, NA, NA, 44, 73, 84, NA, NA, 127, NA, NA…\n$ DYSLIP     &lt;dbl&gt; 2, 1, 2, 1, 2, 9, 4, 1, 2, 1, 2, 2, 2, 9, 2, 2, 2, 2, 2, 4,…\n$ CESD       &lt;dbl&gt; 14, 2, 1, 18, 20, 18, 18, 21, 23, 17, 18, 22, 23, 14, 1, 1,…\n$ SMOKE      &lt;dbl&gt; 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3,…\n$ DKGRP      &lt;dbl&gt; 0, 3, 0, 1, 0, 3, 2, 1, 0, 1, 0, 0, 0, 1, 2, 1, 1, 2, 1, 2,…\n$ HEROPIATE  &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2,…\n$ IDU        &lt;dbl&gt; 2, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1,…\n$ LEU3N      &lt;dbl&gt; 104.1659, 262.0061, 345.4010, 292.3271, 257.8278, 459.4562,…\n$ VLOAD      &lt;dbl&gt; 1.020130e+05, 2.700000e+01, 6.000000e+01, 9.000000e+00, 8.1…\n$ ADH        &lt;dbl&gt; NA, 2, 1, 1, NA, 1, 1, 1, 1, 1, NA, 1, 1, 2, 1, 1, 2, 1, NA…\n$ RACE       &lt;dbl&gt; 1, 1, 1, 1, 3, 3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ EDUCBAS    &lt;dbl&gt; 4, 4, 4, 4, 2, 2, 2, 2, 2, 2, 7, 7, 7, 7, 7, 7, 7, 7, 5, 5,…\n$ hivpos     &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ age        &lt;dbl&gt; 52, 53, 54, 55, 54, 55, 56, 60, 61, 62, 47, 48, 49, 51, 52,…\n$ ART        &lt;dbl&gt; 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1,…\n$ everART    &lt;dbl&gt; 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1,…\n$ years      &lt;dbl&gt; 0, 1, 2, 3, 0, 1, 2, 6, 7, 8, 0, 1, 2, 4, 5, 6, 7, 8, 0, 1,…\n$ hard_drugs &lt;dbl&gt; 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1,…\n\n\nEverything appears properly formatted,"
  },
  {
    "objectID": "Project_1_Regression/Project_1_Regression_SAS/untitled.html",
    "href": "Project_1_Regression/Project_1_Regression_SAS/untitled.html",
    "title": "Quarto stuff",
    "section": "",
    "text": "Quarto stuff\nblah blah blah"
  },
  {
    "objectID": "Project_1_Regression/Project_1_Regression_R/Project_1_Regression.html",
    "href": "Project_1_Regression/Project_1_Regression_R/Project_1_Regression.html",
    "title": "Advanced Data Analysis - Project 1",
    "section": "",
    "text": "The aim of the current study is to assess whether a new gel treatment for gum disease results in lower whole-mouth average pocket depth and attachment loss after one year. Average pocket depth and attachment loss were taken at baseline, and participants were assigned into one of five groups (1 = placebo, 2 = no treatment, 3 = low concentration, 4 = medium concentration, 5 = high concentration) and instructed to apply the gel to their gums twice a day. After 1-year, average pocket depth and attachment loss were recorded again. Data was received as a .csv file containing treatment level, average pocket depth and attachment loss at baseline and at one-year, demographic information, gender, age, number of sites measured, and smoking status. The clinical hypothesis is that average pocket depth and attachment loss in participants who applied the gel will be lower compared to participants who did not. Gender, age, ethnicity, and smoking status will be investigated as potential covariates.\nThe project description provided by the PI is available below:\n\n\n\nFirst we begin by loading in the necessary packages\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(tidyverse)\nlibrary(knitr)\nlibrary(kableExtra) # This lets me pretty print the tables\nlibrary(naniar) # Used to visualize missing data\nlibrary(glue) # This is used for f-strings in R\nlibrary(purrr) # Used for summary tables\nlibrary(corrplot) # Used to make the correlation matrix\nlibrary(corrtable) # used to make the table for the correlation matrix\nlibrary(xtable) # Used to make the table for the correlation matrix\nlibrary(htmlTable) # Used to make Table 1\nlibrary(boot) # Used to make Table 1\nlibrary(table1) # Used to make Table 1\nlibrary(sjPlot) # Used to generate publication quality tables for regressions\nlibrary(mice) # Used for multiple imputation\nlibrary(car) # Used for outlier diagnostics with jackknife residuals\n\nThen we will import the dataset\n\n# Import dataset\ndata &lt;- read.csv(\"C:\\\\Users\\\\sviea\\\\Documents\\\\Advanced Data Analysis\\\\Project 1 MLR with Covariates\\\\Project1_data.csv\")\nnames(data)[1] &lt;- \"id\" # Renaming the weird character out of this colname\n\nAnd visualize the dataset\n\n# Create a nicely formatted table, code adapted from ChatGPT\nkable(head(data), format = \"html\") %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\"))\n\n\n\n\nid\ntrtgroup\ngender\nrace\nage\nsmoker\nsites\nattachbase\nattach1year\npdbase\npd1year\n\n\n\n\n101\n4\n2\n5\n44.57221\n1\n162\n2.432099\n2.577640\n3.246914\n3.407407\n\n\n102\n5\n2\n5\n35.57290\n1\n162\n2.543210\nNA\n3.006173\nNA\n\n\n103\n2\n2\n5\n47.94524\n1\n144\n2.881944\n3.076389\n3.118056\n3.125000\n\n\n104\n3\n2\n5\n55.17864\n1\n138\n4.956522\n5.304348\n5.217391\n4.891304\n\n\n105\n1\n2\n2\n43.79740\n1\n168\n1.773810\n1.452381\n3.363095\n2.898810\n\n\n106\n4\n2\n5\n42.14921\n1\n168\n2.369048\n1.922619\n3.910714\n3.083333\n\n\n\n\n\n\n# Acquire number of variables and observations of the data set\ndim(data)\n\n[1] 130  11\n\n\nOur data set is comprised of 130 observations, with 11 variables\n\n\nThe variables in the provided data set are as follows:\n\nid: Patient ID\ntrtgroup: Treatment group (1 = placebo, 2 = no treatment control, 3 = low concentration, 4 = medium concentration, 5 = high concentration gel)\ngender: Gender (1 = male, 2 = female)\nrace: Race, (1 = Native American, 2 = African American, 3 = Not used, 4 = Asian, 5 = White)\nage: Age in years\nsmoker: Smoking status (0 = No, 1 = Yes)\nsites: Number of sites gum measurements were averaged from\nattachbase: Whole-mouth average attachment loss taken at base timepoint\nattach1year: Whole-mouth average attachment loss after 1 year\npdbase: Whole-mouth average pocket loss at base timepoint\npd1year: Whole-mouth average pocket loss after 1 year\n\n\n\n\n\n\n\nNote\n\n\n\nPocket depth and attachment loss are both measurements of how far the gums have pulled away from the teeth, hence smaller values are better"
  },
  {
    "objectID": "Project_1_Regression/Project_1_Regression_R/Project_1_Regression.html#data-preparation",
    "href": "Project_1_Regression/Project_1_Regression_R/Project_1_Regression.html#data-preparation",
    "title": "Advanced Data Analysis - Project 1",
    "section": "",
    "text": "First we begin by loading in the necessary packages\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(tidyverse)\nlibrary(knitr)\nlibrary(kableExtra) # This lets me pretty print the tables\nlibrary(naniar) # Used to visualize missing data\nlibrary(glue) # This is used for f-strings in R\nlibrary(purrr) # Used for summary tables\nlibrary(corrplot) # Used to make the correlation matrix\nlibrary(corrtable) # used to make the table for the correlation matrix\nlibrary(xtable) # Used to make the table for the correlation matrix\nlibrary(htmlTable) # Used to make Table 1\nlibrary(boot) # Used to make Table 1\nlibrary(table1) # Used to make Table 1\nlibrary(sjPlot) # Used to generate publication quality tables for regressions\nlibrary(mice) # Used for multiple imputation\nlibrary(car) # Used for outlier diagnostics with jackknife residuals\n\nThen we will import the dataset\n\n# Import dataset\ndata &lt;- read.csv(\"C:\\\\Users\\\\sviea\\\\Documents\\\\Advanced Data Analysis\\\\Project 1 MLR with Covariates\\\\Project1_data.csv\")\nnames(data)[1] &lt;- \"id\" # Renaming the weird character out of this colname\n\nAnd visualize the dataset\n\n# Create a nicely formatted table, code adapted from ChatGPT\nkable(head(data), format = \"html\") %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\"))\n\n\n\n\nid\ntrtgroup\ngender\nrace\nage\nsmoker\nsites\nattachbase\nattach1year\npdbase\npd1year\n\n\n\n\n101\n4\n2\n5\n44.57221\n1\n162\n2.432099\n2.577640\n3.246914\n3.407407\n\n\n102\n5\n2\n5\n35.57290\n1\n162\n2.543210\nNA\n3.006173\nNA\n\n\n103\n2\n2\n5\n47.94524\n1\n144\n2.881944\n3.076389\n3.118056\n3.125000\n\n\n104\n3\n2\n5\n55.17864\n1\n138\n4.956522\n5.304348\n5.217391\n4.891304\n\n\n105\n1\n2\n2\n43.79740\n1\n168\n1.773810\n1.452381\n3.363095\n2.898810\n\n\n106\n4\n2\n5\n42.14921\n1\n168\n2.369048\n1.922619\n3.910714\n3.083333\n\n\n\n\n\n\n# Acquire number of variables and observations of the data set\ndim(data)\n\n[1] 130  11\n\n\nOur data set is comprised of 130 observations, with 11 variables\n\n\nThe variables in the provided data set are as follows:\n\nid: Patient ID\ntrtgroup: Treatment group (1 = placebo, 2 = no treatment control, 3 = low concentration, 4 = medium concentration, 5 = high concentration gel)\ngender: Gender (1 = male, 2 = female)\nrace: Race, (1 = Native American, 2 = African American, 3 = Not used, 4 = Asian, 5 = White)\nage: Age in years\nsmoker: Smoking status (0 = No, 1 = Yes)\nsites: Number of sites gum measurements were averaged from\nattachbase: Whole-mouth average attachment loss taken at base timepoint\nattach1year: Whole-mouth average attachment loss after 1 year\npdbase: Whole-mouth average pocket loss at base timepoint\npd1year: Whole-mouth average pocket loss after 1 year\n\n\n\n\n\n\n\nNote\n\n\n\nPocket depth and attachment loss are both measurements of how far the gums have pulled away from the teeth, hence smaller values are better"
  },
  {
    "objectID": "Project_1_Regression/Project_1_Regression_R/Project_1_Regression.html#Descriptives",
    "href": "Project_1_Regression/Project_1_Regression_R/Project_1_Regression.html#Descriptives",
    "title": "Advanced Data Analysis - Project 1",
    "section": "Descriptive Statistics",
    "text": "Descriptive Statistics\nHere we will acquire the descriptive statistics of our data set and create Table 1. Code modified from cran.r-project.org\n\n# Duplicate the dataset so we are not modifying the original\ndata2 &lt;- data\n\n# Factor the basic variables that we're interested in\ndata2$trtgroup &lt;- factor(data2$trtgroup,\n                                levels = c(1,2,3,4,5),\n                                labels = c(\"Placebo\", \"Control\", \"Low\", \"Medium\", \"High\"))\n\ndata2$gender &lt;- factor(data2$gender,\n                              levels = c(1,2),\n                              labels = c(\"Male\", \"Female\"))\n\ndata2$race &lt;- factor(data2$race,\n                             levels = c(1,2,4,5),\n                             labels = c(\"Native American\", \"African American\", \"White\", \"Asian\"))\n                             \ndata2$smoker &lt;- factor(data2$smoker,\n                               levels = c(0,1),\n                               labels = c(\"Non-Smoker\", \"Smoker\"))\n\n# Create labels to make the names of each variable more professional\nlabel(data2$gender) &lt;- \"Gender\"\nlabel(data2$race) &lt;- \"Race\"\nlabel(data2$age) &lt;- \"Age (Years)\"\nlabel(data2$smoker) &lt;- \"Smoking Status\"\nlabel(data2$sites) &lt;- \"Sites\"\nlabel(data2$attachbase) &lt;- \"Attachment Loss at Baseline\"\nlabel(data2$attach1year) &lt;- \"Attachment Loss at 1 Year\"\nlabel(data2$pdbase) &lt;- \"Pocket Depth at Baseline\"\nlabel(data2$pd1year) &lt;- \"Pocket Depth at 1 Year\"\nlabel(data2$attachchange) &lt;- \"Attachment Loss Change\"\nlabel(data2$pdchange) &lt;- \"Pocket Depth Change\"\n\n\n# Create table 1\ntable1 &lt;- table1(~ gender + race + age + smoker + sites + attachbase + attach1year + pdbase + pd1year + attachchange + pdchange| trtgroup,  data = data2, caption = \"Descriptive Statistics\", overall = c(left=\"Total\"))\ntable1\n\n\n\nDescriptive Statistics\n\n\n\n\n\n\n\n\n\n\n\n\nTotal\n(N=130)\nPlacebo\n(N=26)\nControl\n(N=26)\nLow\n(N=26)\nMedium\n(N=26)\nHigh\n(N=26)\n\n\n\n\nGender\n\n\n\n\n\n\n\n\nMale\n54 (41.5%)\n11 (42.3%)\n10 (38.5%)\n11 (42.3%)\n11 (42.3%)\n11 (42.3%)\n\n\nFemale\n76 (58.5%)\n15 (57.7%)\n16 (61.5%)\n15 (57.7%)\n15 (57.7%)\n15 (57.7%)\n\n\nRace\n\n\n\n\n\n\n\n\nNative American\n4 (3.1%)\n0 (0%)\n1 (3.8%)\n1 (3.8%)\n0 (0%)\n2 (7.7%)\n\n\nAfrican American\n9 (6.9%)\n2 (7.7%)\n1 (3.8%)\n5 (19.2%)\n0 (0%)\n1 (3.8%)\n\n\nWhite\n3 (2.3%)\n1 (3.8%)\n1 (3.8%)\n0 (0%)\n1 (3.8%)\n0 (0%)\n\n\nAsian\n114 (87.7%)\n23 (88.5%)\n23 (88.5%)\n20 (76.9%)\n25 (96.2%)\n23 (88.5%)\n\n\nAge (Years)\n\n\n\n\n\n\n\n\nMean (SD)\n49.9 (10.0)\n47.1 (8.61)\n50.7 (9.90)\n51.9 (10.8)\n49.0 (9.49)\n50.8 (11.2)\n\n\nMedian [Min, Max]\n48.6 [28.6, 74.5]\n44.7 [30.4, 67.1]\n49.2 [36.1, 73.3]\n51.5 [36.9, 71.9]\n48.1 [28.6, 70.9]\n49.9 [34.1, 74.5]\n\n\nMissing\n1 (0.8%)\n1 (3.8%)\n0 (0%)\n0 (0%)\n0 (0%)\n0 (0%)\n\n\nSmoking Status\n\n\n\n\n\n\n\n\nNon-Smoker\n81 (62.3%)\n15 (57.7%)\n17 (65.4%)\n18 (69.2%)\n14 (53.8%)\n17 (65.4%)\n\n\nSmoker\n48 (36.9%)\n11 (42.3%)\n9 (34.6%)\n8 (30.8%)\n11 (42.3%)\n9 (34.6%)\n\n\nMissing\n1 (0.8%)\n0 (0%)\n0 (0%)\n0 (0%)\n1 (3.8%)\n0 (0%)\n\n\nSites\n\n\n\n\n\n\n\n\nMean (SD)\n158 (11.3)\n160 (10.1)\n154 (10.9)\n161 (8.54)\n155 (15.7)\n157 (9.65)\n\n\nMedian [Min, Max]\n162 [114, 168]\n162 [138, 168]\n159 [126, 168]\n162 [138, 168]\n162 [114, 168]\n159 [138, 168]\n\n\nAttachment Loss at Baseline\n\n\n\n\n\n\n\n\nMean (SD)\n2.15 (0.797)\n1.79 (0.646)\n2.46 (0.687)\n2.07 (0.987)\n2.17 (0.656)\n2.24 (0.858)\n\n\nMedian [Min, Max]\n2.03 [0.895, 5.09]\n1.71 [0.899, 3.64]\n2.48 [1.22, 4.39]\n1.77 [0.895, 4.96]\n2.12 [1.02, 4.01]\n1.97 [1.26, 5.09]\n\n\nAttachment Loss at 1 Year\n\n\n\n\n\n\n\n\nMean (SD)\n2.10 (0.772)\n1.74 (0.542)\n2.33 (0.551)\n2.08 (1.06)\n2.24 (0.652)\n2.15 (0.915)\n\n\nMedian [Min, Max]\n1.98 [0.865, 5.30]\n1.64 [0.964, 3.10]\n2.23 [1.46, 3.49]\n1.74 [0.865, 5.30]\n2.25 [1.35, 3.83]\n1.71 [1.22, 4.04]\n\n\nMissing\n27 (20.8%)\n3 (11.5%)\n3 (11.5%)\n5 (19.2%)\n6 (23.1%)\n10 (38.5%)\n\n\nPocket Depth at Baseline\n\n\n\n\n\n\n\n\nMean (SD)\n3.14 (0.437)\n3.09 (0.372)\n3.28 (0.473)\n3.17 (0.593)\n3.05 (0.402)\n3.11 (0.273)\n\n\nMedian [Min, Max]\n3.10 [2.26, 5.22]\n3.11 [2.47, 4.08]\n3.11 [2.65, 4.77]\n3.07 [2.26, 5.22]\n3.09 [2.42, 3.91]\n3.14 [2.62, 3.60]\n\n\nPocket Depth at 1 Year\n\n\n\n\n\n\n\n\nMean (SD)\n2.88 (0.488)\n2.75 (0.482)\n2.95 (0.455)\n3.02 (0.578)\n2.84 (0.469)\n2.80 (0.423)\n\n\nMedian [Min, Max]\n2.90 [1.96, 4.89]\n2.70 [1.96, 3.75]\n2.90 [2.24, 4.07]\n2.97 [2.16, 4.89]\n2.90 [2.05, 3.78]\n2.87 [2.04, 3.40]\n\n\nMissing\n27 (20.8%)\n3 (11.5%)\n3 (11.5%)\n5 (19.2%)\n6 (23.1%)\n10 (38.5%)\n\n\nAttachment Loss Change\n\n\n\n\n\n\n\n\nMean (SD)\n-0.0995 (0.276)\n-0.0871 (0.242)\n-0.222 (0.280)\n-0.0178 (0.266)\n-0.00656 (0.231)\n-0.165 (0.326)\n\n\nMedian [Min, Max]\n-0.0679 [-1.05, 0.452]\n-0.0247 [-0.599, 0.452]\n-0.123 [-0.901, 0.194]\n0.0298 [-0.705, 0.348]\n-0.0160 [-0.446, 0.339]\n-0.0579 [-1.05, 0.199]\n\n\nMissing\n27 (20.8%)\n3 (11.5%)\n3 (11.5%)\n5 (19.2%)\n6 (23.1%)\n10 (38.5%)\n\n\nPocket Depth Change\n\n\n\n\n\n\n\n\nMean (SD)\n-0.294 (0.268)\n-0.350 (0.277)\n-0.338 (0.232)\n-0.206 (0.279)\n-0.203 (0.272)\n-0.382 (0.245)\n\n\nMedian [Min, Max]\n-0.284 [-0.858, 0.455]\n-0.383 [-0.858, 0.161]\n-0.367 [-0.759, 0.0145]\n-0.244 [-0.661, 0.455]\n-0.200 [-0.827, 0.175]\n-0.347 [-0.845, 0.0536]\n\n\nMissing\n27 (20.8%)\n3 (11.5%)\n3 (11.5%)\n5 (19.2%)\n6 (23.1%)\n10 (38.5%)"
  },
  {
    "objectID": "Project_1_Regression/Project_1_Regression_R/Project_1_Regression.html#Preliminary",
    "href": "Project_1_Regression/Project_1_Regression_R/Project_1_Regression.html#Preliminary",
    "title": "Advanced Data Analysis - Project 1",
    "section": "Preliminary Evaluation of Assumptions",
    "text": "Preliminary Evaluation of Assumptions\nBefore we begin digging into the data, let’s take a closer look at the relationships between our variables.\nWe will first run a correlation matrix to assess the correlations between our IVs. Then we will explore any high correlations that which could affect our analysis.\nFinally we will make histograms of attachment loss and pocket depth change to gauge whether they will be normally distributed or if we need to run some transformations.\n\nCorrelation MatrixGender and MissingnessNormality of Dependent Variables\n\n\nFirst we will begin by making a correlation matrix to assess whether any of our IVs are related to each other (multicollinearity). This will inform which variables to incorporate into the final model.\n\n# Since we made dummy codes, we will get spurious correlations that will obfuscate the main relationships we are interested in (e.g. between 'medium' and 'trtgroup'. So we will first make a separate dataset excluding the dummy coded variables\ndata_for_matrix &lt;- select(data_missing, -placebo, -control, -low, -medium,  -high, -trt, -trt3groups)\n\n# Make a correlation matrix with all variables of the trimmed data set\ncorrelation_matrix &lt;- cor(data_for_matrix, use = \"complete.obs\")\n\n# Plot the matrix\ncorrplot(correlation_matrix, method = \"circle\")\n\n\n\n\n\n\n\n# Trim the matrix\ncorrelation_matrix[upper.tri(correlation_matrix)] &lt;- NA\n\n# Save the matrix as a LaTex file for paper\ncor_table &lt;- xtable(correlation_matrix, caption = \"Correlation Matrix\", label = \"tab:correlation\")\nprint(cor_table, type = \"latex\", file = \"correlation_matrix.tex\")\n\nVisually, we can see a cluster of high positive correlations between all of attachment loss and pocket depth at baseline and 1 year. This makes sense, as all measurements were taken from the same sites in the gums. This will be explored later (See Exploratory Data Analysis - Dependent Variables)\nJust for sanity (and practice), let’s make a table of our correlation coefficients.\n\n# Convert the matrix to a dataframe for better formatting\ncorrelation_df &lt;- as.data.frame(correlation_matrix)\n\n# Use Kable to pretty print the table\nkable(correlation_df, caption = \"Correlation Matrix\", format = \"html\") %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\"))\n\n\nCorrelation Matrix\n\n\n\nid\ntrtgroup\ngender\nrace\nage\nsmoker\nsites\nattachbase\nattach1year\npdbase\npd1year\nattachchange\npdchange\n\n\n\n\nid\n1.0000000\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\ntrtgroup\n-0.1452850\n1.0000000\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\ngender\n-0.8306185\n0.1428866\n1.0000000\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nrace\n0.1319767\n-0.0314771\n-0.0955531\n1.0000000\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nage\n0.0593032\n0.1100051\n-0.0453862\n0.1787040\n1.0000000\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nsmoker\n-0.2742888\n-0.0495029\n0.0206456\n0.0377211\n-0.2229620\n1.0000000\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nsites\n0.0520821\n-0.0393742\n-0.1291127\n0.0505164\n-0.0274695\n-0.0226381\n1.0000000\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nattachbase\n-0.1641513\n0.1440999\n0.2416242\n0.0313438\n0.1354434\n0.1456234\n-0.4237813\n1.0000000\nNA\nNA\nNA\nNA\nNA\n\n\nattach1year\n-0.1440182\n0.1671499\n0.2021284\n0.0671713\n0.0850851\n0.1987327\n-0.4042743\n0.9450189\n1.0000000\nNA\nNA\nNA\nNA\n\n\npdbase\n-0.2010672\n-0.0224838\n0.2645908\n0.0182289\n-0.0884687\n0.2614919\n-0.1805543\n0.6047132\n0.6093346\n1.0000000\nNA\nNA\nNA\n\n\npd1year\n-0.0685783\n0.0126731\n0.1362804\n0.0606735\n-0.1246106\n0.2447030\n-0.1901940\n0.5601052\n0.6685457\n0.8436653\n1.0000000\nNA\nNA\n\n\nattachchange\n0.0964391\n0.0296043\n-0.1696216\n0.0928969\n-0.1742586\n0.1135740\n0.1578683\n-0.3976360\n-0.0757224\n-0.1342018\n0.1679506\n1.0000000\nNA\n\n\npdchange\n0.2251487\n0.0622762\n-0.2123192\n0.0789059\n-0.0731704\n-0.0091784\n-0.0323859\n-0.0317726\n0.1579534\n-0.2031295\n0.3543035\n0.5400668\n1\n\n\n\n\n\n\n\nOther than that, there is a correlation between gender and ID which seems spurious. Let’s investigate that in the next tab.\nBack to top of tabset\n\n\nThere is a correlation between gender and ID. Let’s make a simple plot to investigate.\n\n# Creating a simple plot of id and gender\nggplot(data_missing, aes(x = factor(gender), y = id)) + \n  geom_point() + \n  labs(title = \"ID by Gender\")\n\n\n\n\n\n\n\n\nInterestingly, it appears that the experimenters assigned ID based on gender. That is, females received ID’s starting at 101, and males received ID’s starting at 201 (for some reason there’s a few females with ID’s &gt; 200).\nIt will be important to double check with the PI’s how they assigned participants to treatment group to ensure it was in fact random.\nLet’s make a contingency table to see what the breakdown between gender and treatment group is.\n\n# First make a contingency table of both variables\ncontingency_table &lt;- table(data_missing$gender, data_missing$trtgroup)\n\n# Set the row and column names\ndimnames(contingency_table) &lt;- list(\n  \"Gender\" = c(\"Male\", \"Female\"),\n  \"Treatment Condition\" = c(\"Placebo\", \"Control\", \"Low\", \"Medium\", \"High\"))\n\n# Convert the table to a dataframe for better formatting (from ChatGPT)\ncontingency_df &lt;- as.data.frame.matrix(contingency_table)\n\n# Pretty print the table using kable\nkable(contingency_df, format = \"html\") %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\"))\n\n\n\n\n\nPlacebo\nControl\nLow\nMedium\nHigh\n\n\n\n\nMale\n10\n7\n9\n7\n3\n\n\nFemale\n13\n16\n12\n13\n13\n\n\n\n\n\n\n\nThat’s not good! It looks like males were less likely to be in the high treatment condition compared to females.\nThis could be because males were more likely to drop out then females. Let’s make a quick table using the original data set before we dropped the missing variables.\n\n# First make a contingency table of both variables\ncontingency_table_clean &lt;- table(data$gender, data$trtgroup)\n\n# Set the row and column names\ndimnames(contingency_table_clean) &lt;- list(\n  \"Gender\" = c(\"Male\", \"Female\"),\n  \"Treatment Condition\" = c(\"Placebo\", \"Control\", \"Low\", \"Medium\", \"High\"))\n\n# Convert the table to a dataframe for better formatting (from ChatGPT)\ncontingency_df_clean &lt;- as.data.frame.matrix(contingency_table_clean)\n\n# Pretty print the table using kable\nkable(contingency_df_clean, format = \"html\") %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\"))\n\n\n\n\n\nPlacebo\nControl\nLow\nMedium\nHigh\n\n\n\n\nMale\n11\n10\n11\n11\n11\n\n\nFemale\n15\n16\n15\n15\n15\n\n\n\n\n\n\n\nIt looks more balanced before I took out participants with missing data.\nChi-square is known to be unsuitable if a cell has &lt; 5 counts, which we have in this case (3 males in high concentration condition). So I will run Fisher’s test to see if that difference is statistically significant.\n\nfisher_test &lt;- fisher.test(contingency_table)\nfisher_test\n\n\n    Fisher's Exact Test for Count Data\n\ndata:  contingency_table\np-value = 0.506\nalternative hypothesis: two.sided\n\n\nThe p-value is not signficant (p = 0.506).\nWe know from visualizing the missing data that there were 27 missing data points for the gum measurement DVs, and these all belong to the same people. Furthermore, it appears that males in the treatment groups were more likely to have missing values than in the placebo (and maybe control) group. Is it possible that the gel was having an adverse effect on these participants? Does the gel have an adverse effect only on males and not females for some reason? Let’s explore.\nFirst, I want to investigate if males were more likely to have missing data points. It’s possible if their gums were hurting they simply rejected or avoided having these measurements taken.\nLet’s repeat this process and make a contingency table of gender and missing variables.\n\n# First let's add a new dummy code for if a participant is missing any data points\ndata$missing &lt;- ifelse(apply(data, 1, function(row) any(is.na(row))), 1, 0)\n\n# First make a contingency table of both variables\ncontingency_table_missing &lt;- table(data$gender, data$missing)\n\n# Set the row and column names\ndimnames(contingency_table_missing) &lt;- list(\n  \"Gender\" = c(\"Male\", \"Female\"),\n  \"Missing\" = c(\"Not Missing\", \"Missing\"))\n\n# Convert the table to a dataframe for better formatting (from ChatGPT)\ncontingency_df_missing &lt;- as.data.frame.matrix(contingency_table_missing)\n\n# Pretty print the table using kable\nkable(contingency_df_missing, format = \"html\") %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\"))\n\n\n\n\n\nNot Missing\nMissing\n\n\n\n\nMale\n35\n19\n\n\nFemale\n66\n10\n\n\n\n\n\n\n\nProportionally, it appears that males may be more likely to have missing variables than females. Let’s run a chi-square to check.\n\nchi_square_test &lt;- chisq.test(contingency_df_missing)\nchi_square_test\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  contingency_df_missing\nX-squared = 7.6127, df = 1, p-value = 0.005796\n\n\nSuccess! Males were more likely to have missing values compared to females (p = 0.005796). This could be a problem (counfound) if something was causing males to avoid having their gums measured compared to females (such as adverse reactions from the gel)\nLet’s do a quick chi square test to check if there is a relationship between missing values and treatment condition.\nWe start off the same way by making a contingency table and running a chi-square test.\n\n# First make a contingency table of both variables\ncontingency_table_missing2 &lt;- table(data$missing, data$trtgroup)\n\n# Set the row and column names\ndimnames(contingency_table_missing2) &lt;- list(\"Missing\" = c(\"Not Missing\", \"Missing\"),\n                                             \"Treatmtent Condition\" = c(\"Placebo\", \"Control\", \"Low\", \"Medium\", \"High\"))\n\n# Convert the table to a dataframe for better formatting (from ChatGPT)\ncontingency_df_missing2 &lt;- as.data.frame.matrix(contingency_table_missing2)\n\n# Pretty print the table using kable\nkable(contingency_df_missing2, format = \"html\") %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\"))\n\n\n\n\n\nPlacebo\nControl\nLow\nMedium\nHigh\n\n\n\n\nNot Missing\n22\n23\n21\n19\n16\n\n\nMissing\n4\n3\n5\n7\n10\n\n\n\n\n\n\n\nIt does appear that there are more missing variables in the high concentration condition. Is it statistically significant?\n\n# Run a Fisher's Exact Test (since we have &lt; 5 observations in cells)\nfisher_test &lt;- fisher.test(contingency_df_missing2)\nfisher_test\n\n\n    Fisher's Exact Test for Count Data\n\ndata:  contingency_df_missing2\np-value = 0.1675\nalternative hypothesis: two.sided\n\n\nNot significant (p = 0.1675). So we can conclude that there is no difference in gender or missing values based on treatment condition (i.e., participants in all treatment conditions were equally likely to be male or female, or have missing values)\nHowever, across the board, males were more likely to have missing values than females. This will be important to note as a caveat during interpretation of the final results.\nBack to top of tabset\n\n\nHere we will simply plot the histograms of attachment loss and pocket depth changes scores, to assess if they appear normally distributed or if we will have to perform a transformation of some kind.\n\n# Plot simple histogram of attachment loss change score\nhist(data_missing$attachchange,\n     main  = \"Histogram of Attachment Loss Change\",\n     xlab = \"Attachment Loss Change\",\n     ylab = \"Frequency\")\n\n\n\n\n\n\n\n# Plot simple histogram of pocket depth change score\nhist(data_missing$pdchange,\n     main  = \"Histogram of Attachment Loss Change\",\n     xlab = \"Pocket Depth Change\",\n     ylab = \"Frequency\")\n\n\n\n\n\n\n\n\nBoth attachment loss and pocket depth change appear to be normally distributed and will not need to be transformed. Attachment loss change is slightly left-tailed but this could be due to outliers, or may just not impact the analysis."
  },
  {
    "objectID": "Project_1_Regression/Project_1_Regression_R/Project_1_Regression.html#Exploratory",
    "href": "Project_1_Regression/Project_1_Regression_R/Project_1_Regression.html#Exploratory",
    "title": "Advanced Data Analysis - Project 1",
    "section": "Exploratory Data Analysis",
    "text": "Exploratory Data Analysis\nHere I will plot the data and perform a number of simple linear regressions to examine relationships between variables in order to determine which covariates to include in the model.\n\nPrimary Explanatory VariableCovariatesDependent VariablesSummary\n\n\n\n5 Treatment Groups3 Treatment Groups\n\n\nLet’s examine if there appears to be a difference in the average attachment loss and pocket depth change according to treatment level\n\nggplot(data_missing, aes(x = factor(trtgroup), y = attachchange)) + \n  geom_boxplot() +\n  labs(title = \"Boxplot of Attachment Loss change by Treatment\")\n\n\n\n\n\n\n\nggplot(data_missing, aes(x = factor(trtgroup), y = pdchange)) + \n  geom_boxplot()  +\n  labs(title = \"Boxplot of Pocket Depth Change by treatment\")\n\n\n\n\n\n\n\n\nAttachment Loss Change\nVisually, there does not appear to be a difference between the treatment group levels (low, medium, high concentration gel) compared to each other. Additionally, there does not seem to be a difference between the treatment groups and the placebo for attachment loss change.\nPocket Depth Change\nInterestingly, the high concentration condition appears to have had a negative effect. Additionally, there seems to be a difference in the treatment groups compared to the no treatment control, but NOT when compared to the placebo. The exception is the low concentration condition compared to the placebo when looking at pocket depth change. Further analysis will reveal whether these differences are statistically significant or not.\nBack to top of tabset\n\n\nThe relationship between low vs medium vs high gel concentration does not look very strong. Furthermore, we technically do not have a large enough sample size in the high concentration condition to include it.\nFor those reasons, let’s make the same comparisons but while combining all treatment levels into one group called treatment.\n\nggplot(data_missing, aes(x = factor(trt3groups), y = attachchange)) + \n  geom_boxplot()\n\n\n\n\n\n\n\nggplot(data_missing, aes(x = factor(trt3groups), y = attachchange)) +\n  geom_boxplot() +\n  labs(title = \"Boxplot of Attachment Loss Change by Treatment\")\n\n\n\n\n\n\n\nggplot(data_missing, aes(x = factor(trt3groups), y = pdchange)) + \n  geom_boxplot() +\n  labs(title = \"Boxplot of Pocket Depth Change by Treatment\")\n\n\n\n\n\n\n\n\nBy eye, it appears as if there is no difference between the placebo and collapsed treatment groups in attachment loss change or pocket depth change. However, both placebo and any treatment conditions appear to have decreased (?) attachment loss and pocket depth. It may be that this study has null results, unless including one of the covariates changes the results. We will see come the analysis section.\nBack to top of tabset\n\n\n\n\n\nNow we will exlore the relationships between our potential covariates and attachment loss and pocket depth change\n\nAgeSitesGenderRaceSmoking Status\n\n\n\n# Plot age vs attachment loss change\nggplot(data_missing, aes(x = age, y = attachchange)) + \n  geom_point() + \n  labs(title = \"Scatterplot of Age vs Attachment Loss Change\")\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n# Plot age vs pocket depth change\nggplot(data_missing, aes(x = age, y = pdchange)) +\n  geom_point() + \n  labs(title = \"Scatterplot of Age vs Pocket Depth Change\")\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nThere does not appear to be any kind of linear relationship between age and attachment loss change or pocket depth change. I will run a model where I include age, but it will likely be removed in the final model.\nBack to top of tabset\n\n\nIs there any relationship between the number of sites measured from and attachment loss change or pocket depth change (e.g. a lower number of sites could lead to less accurate readings)? If so this could be something to include in our data analysis\n\n# Plot sites vs attachment change loss\nggplot(data_missing, aes(x = sites, y = attachchange)) + \n  geom_point() +\n  labs(title = \"Scatterplot of Attachment Loss Change vs Sites\")\n\n\n\n\n\n\n\n# Plot sites vs pocket depth chagne\nggplot(data_missing, aes(x = sites, y = pdchange)) + \n  geom_point() +\n  labs(title = \"Scatterplot of Pocket Depth Change by Sites\")\n\n\n\n\n\n\n\n\nThere does not seem to be a dramatic difference in attachment loss or pocket depth change based on the number of sites measured from. It’s a bit hard to tell with those two low site subjects, but the points are similar enough to the rest of the dataset that I do not think we have to worry about site number in our analysis.\nBack to top of tabset\n\n\nIt is conceivable that there are sex differences in regards to gum health. Let’s examine if there is a difference in attachment loss or pocket depth change based on gender\n\n# Plot gender vs attachment loss change\nggplot(data_missing, aes(x = factor(gender), y = attachchange)) +\n  geom_boxplot() + \n  labs(title = \"Boxplot of Attachment Loss Change by Gender\",\n      x = \"Gender\")\n\n\n\n\n\n\n\n# Plot gender vs pocket depth change\nggplot(data_missing, aes(x = factor(gender), y = pdchange)) +\n  geom_boxplot() + \n  labs(title = \"Boxplot of Pocket Depth Change by Gender\",\n      x = \"Gender\")\n\n\n\n\n\n\n\n\nBy eye, it appears that males may have more pocket depth loss compared with females. This will be a good variable to include as a covariate.\nLet’s try a t-test to see if there is any difference in attachment loss or pocket depth change based on gender\n\n# Running a t-test on attachment loss change by gender\nmale &lt;- data_missing$attachchange[data_missing$gender == 1]\nfemale &lt;- data_missing$attachchange[data_missing$gender == 2]\nt_test_result &lt;- t.test(male, female)\nt_test_result\n\n\n    Welch Two Sample t-test\n\ndata:  male and female\nt = 2.1969, df = 100.8, p-value = 0.03032\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n 0.01003279 0.19682842\nsample estimates:\n  mean of x   mean of y \n-0.03217094 -0.13560154 \n\n\nThere is a significant difference in attachment loss change score based on gender (t = 2.0502, p = 0.04299)\nLet’s repeat for pocket depth change\n\n# Running a t-test on pocket depth change by gender\nmale &lt;- data_missing$pdchange[data_missing$gender == 1]\nfemale &lt;- data_missing$pdchange[data_missing$gender == 2]\nt_test_result &lt;- t.test(male, female)\nt_test_result\n\n\n    Welch Two Sample t-test\n\ndata:  male and female\nt = 2.3534, df = 81.683, p-value = 0.02101\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n 0.01884469 0.22488042\nsample estimates:\n mean of x  mean of y \n-0.2150844 -0.3369470 \n\n\nThere is also a statistically significant difference in pocket depth change based on gender (t = 2.2626, p = 0.02641).\nTherefore I will include gender as a covariate in the analysis!\nBack to top of tabset\n\n\n\n# Plot race vs attachment loss change\nggplot(data_missing, aes(x = factor(race), y = attachchange)) +\n  geom_boxplot() +\n  labs(title = \"Boxplot of Attachment Change by Race\",\n       x = \"Race\")\n\n\n\n\n\n\n\nggplot(data_missing, aes(x = factor(race), y = pdchange)) + \n  geom_boxplot() +\n  labs(title = \"Boxplot of Pocket Depth Change by Race\",\n  x = \"Race\")\n\n\n\n\n\n\n\n\nThere does not seem to be much of a difference in attachment loss or pocket depth based on race. MAYBE African Americans (2) have more pocket depth loss compared to Asians(4), but the sample size was very small for both races (88.1% of the sample identified as White)\nBack to top of tabset\n\n\nWhether the participant is a smoker or not likely has a dramatic effect on gum health. Let’s assess\n\n# Plot smoking status vs attachment loss change\nggplot(data_missing, aes(x = factor(smoker), y = attachchange)) +\n  geom_boxplot() +\n  labs(title = \"Attachment loss by smoking status\",\n       x = \"Smoking Status\")\n\n\n\n\n\n\n\n# Plot smoking status vs pocket depth change\nggplot(data_missing, aes(x = factor(smoker), y = pdchange)) +\n  geom_boxplot() +\n  labs(title = \"Attachment loss by smoking status\",\n       x = \"Smoking Status\")\n\n\n\n\n\n\n\n\nThere does not appear to be any difference in attachment loss or pocket depth change based on smoking status. I will test a model with smoking status included, but it will likely be dropped in the final model.\nBack to top of tabset\n\n\n\n\n\n\nBaseline vs 1 YearAttachment Loss vs Pocket Depth\n\n\nI am curious what the relationship between base line and 1 year measurements are. The study is an RCT so even if baseline measurements affect 1 year measurements, participants should have been randomly assigned to groups so it is essentially controlled for in the study design. It will still be important to assess this relationship as a moderator however. Maybe treatment only worked for those with high attachment loss or pocket depth at the beginning?\nFirst let’s make a simple plot of attachment loss at baseline and at 1 year\n\n# Creating a scatter plot of attachment loss at baseline and 1 year\nggplot(data_missing, aes(x = attachbase, y = attach1year)) + \n  geom_point() +\n  labs(title = \"Scatterplot of Attachment Loss at Baseline and 1 Year\")\n\n\n\n\n\n\n\n\nLet’s check the correlation coefficient.\n\n# Run a correlation test between attachment at base and 1 year\ncorrelation &lt;- cor.test(data_missing$attachbase, data_missing$attach1year)\ncorrelation\n\n\n    Pearson's product-moment correlation\n\ndata:  data_missing$attachbase and data_missing$attach1year\nt = 29.198, df = 101, p-value &lt; 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.9204657 0.9628839\nsample estimates:\n      cor \n0.9455558 \n\n\nThose are highly correlated (R = 0.946, p &lt;.0001)! Let’s run a simple linear regression\n\n# Run a regression with attachment loss at 1 year as the DV and attachment at base as the IV\nmodel &lt;- lm(attach1year ~ attachbase, data = data_missing)\nsummary(model)\n\n\nCall:\nlm(formula = attach1year ~ attachbase, data = data_missing)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.55683 -0.15641 -0.01841  0.15202  0.82062 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.19872    0.06975   2.849  0.00531 ** \nattachbase   0.86452    0.02961  29.198  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2525 on 101 degrees of freedom\nMultiple R-squared:  0.8941,    Adjusted R-squared:  0.893 \nF-statistic: 852.5 on 1 and 101 DF,  p-value: &lt; 2.2e-16\n\n# Plot the model\nggplot(data_missing, aes(x = attachbase, y = attach1year)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", col = \"blue\") +\n  labs(title = \"Simple Linear Regression of Attachment Loss at Baseline and 1 Year\",\n      x = \"Attachment Loss at Baseline\",\n      y = \"Attachment Loss at 1 Year\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nAttachment loss at baseline is a significant predictor of attachment loss at 1 year (t = 29.20, p &lt;.0001). This is important! We should account for attachment loss at baseline by including it as a covariate in our final model!\nLet’s do the same process of pocket depth at baseline and 1 year\n\n# Create a scatterplot of pocket depth at base vs 1 year\nggplot(data_missing, aes(x = pdbase, y = pd1year)) +\n  geom_point() +\n  labs(title = \"Scatterplot of Pocket Depth at Baseline and 1 Year\")\n\n\n\n\n\n\n\n\nSimilar to attachment loss, we see a relationship between pocket depth at baseline and 1 year. Let’s run the correlation.\n\n# Run a correlation between pocket depth at baseline and 1 year\ncorrelation &lt;- cor.test(data_missing$pdbase, data_missing$pd1year)\ncorrelation\n\n\n    Pearson's product-moment correlation\n\ndata:  data_missing$pdbase and data_missing$pd1year\nt = 15.767, df = 101, p-value &lt; 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.7764571 0.8913340\nsample estimates:\n      cor \n0.8432691 \n\n\nWhile not as strong as attachment loss, there is still a strong relationship between pocket depth at baseline and 1 year (R = 0.84, p &lt;.0001). Let’s run the SLR.\n\n# Run an SLR with pocket depth at 1 year as the DV and pocket depth at baseline as the DV\nmodel &lt;- lm(pd1year ~ pdbase, data = data_missing)\nsummary(model)\n\n\nCall:\nlm(formula = pd1year ~ pdbase, data = data_missing)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.55105 -0.17484  0.01996  0.19627  0.64381 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.07504    0.17948   0.418    0.677    \npdbase       0.88346    0.05603  15.767   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2634 on 101 degrees of freedom\nMultiple R-squared:  0.7111,    Adjusted R-squared:  0.7082 \nF-statistic: 248.6 on 1 and 101 DF,  p-value: &lt; 2.2e-16\n\n# Plot the model\nggplot(data_missing, aes(x = pdbase, y = pd1year)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", col = \"blue\") +\n  labs(title = \"Simple Linear Regression of Pocket Depth at Baseline and 1 Year\",\n       x = \"Pocket Depth at Baseline\",\n       y = \"Pocket Depth at 1 year\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nPocket depth at baseline is also a significant predictor of pocket depth at 1 year (t = 15.78, p = &lt;.0001). We should also therefore include pocket depth at baseline as a covariate in our model to control for it!\nBack to top of tabset\n\n\nI am interested in how attachment loss and pocket depth change are related. Since they are both measurements taken from the same sites in the gums, they are likely to be highly correlated. This could have implications on how we perform the analysis and interpret the results.\nFirst, we plot attachment loss change against pocket depth change\n\n# Creating a scatter plot of attachment loss change against pocket depth change \nggplot(data_missing, aes(x = attachchange, y = pdchange)) + \n  geom_point() +\n  labs(title = \"Scatterplot of Attachment Loss Change and Pocket Depth Change\")\n\n\n\n\n\n\n\n\nThat looks like a linear relationship! Let’s run a correlation and an SLR.\n\n# Running the correlation\ncorrelation &lt;- cor.test(data_missing$attachchange, data_missing$pdchange)\ncorrelation\n\n\n    Pearson's product-moment correlation\n\ndata:  data_missing$attachchange and data_missing$pdchange\nt = 6.3717, df = 101, p-value = 5.621e-09\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.3814636 0.6605362\nsample estimates:\n      cor \n0.5354593 \n\n# Running the regression\nmodel &lt;- lm(attachchange ~ pdchange, data = data_missing)\nsummary(model)\n\n\nCall:\nlm(formula = attachchange ~ pdchange, data = data_missing)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.83130 -0.12683  0.00563  0.14913  0.46480 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.06312    0.03441   1.835   0.0695 .  \npdchange     0.55231    0.08668   6.372 5.62e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2343 on 101 degrees of freedom\nMultiple R-squared:  0.2867,    Adjusted R-squared:  0.2797 \nF-statistic:  40.6 on 1 and 101 DF,  p-value: 5.621e-09\n\n# Creating the plot\nggplot(data_missing, aes(x = attachchange, y = pdchange)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", col = \"blue\") +\n  labs(title = \"Simple Linear Regression of Attachment Loss and Pocket Depth Change\",\n       x = \"Pocket Depth Change\",\n       y = \"Attachment Loss Change\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nThe correlation shows that attachment loss and pocket depth change are very correlated (R = 0.54, p &lt; .0001). The simple linear regression shows that pocket depth change significantly predicts attachment loss change (t = 6.37, p &lt;.0001).\nHowever, multicollinearity is only an issue when IVs are correlated with each other. We can still run a multivariate multiple linear regression even though the DVs are correlated. In fact this is often the case, and is one of the justifications for using a multivariate MLR in the first place! Back to top of tabset\n\n\n\n\n\nTreatment Condition\nCollapsing the low, medium, and high concentration gel groups into 1 group does not seem to improve the relationship between treatment and attachment loss or pocket depth change. Only models including all 5 treatment groups will therefore be considered from here on out to keep in alignment with the original study design.\nGender\nGender is related to attachment loss and pocket depth change, and as such will be included in the final model.\nSupressor / Counfound Variables\nA model will be tested with all covariates to assess for possible suppressor / confound variables. But the other potential covariates of race, age, sites, and smoking status were not related to attachment loss or pocket depth change by themselves.\nBaseline vs 1 Year Measurements\nThe baseline measurements of attachment loss and pocket depth were significant predictors of attachment loss and pocket depth at 1 year, respectively. While the RCT nature of the study should ensure that participants were randomly assigned into treatment condition regardless of their baseline measurements, it will still be good practice to include baseline attachment loss and pocket depth into the final model.\nAttachment Loss vs Pocket Depth Change Scores\nAttachment loss and pocket depth change are highly related to each other, but this should not impact the analysis. PI’s will need to be consulted to interpret the clinical significance of findings, and to help fully understand the implications of any possible differences that may arise between attachment loss and pocket depth change in the analysis.\nBack to top of tabset"
  },
  {
    "objectID": "Project_1_Regression/Project_1_Regression_R/Project_1_Regression.html#Model_1",
    "href": "Project_1_Regression/Project_1_Regression_R/Project_1_Regression.html#Model_1",
    "title": "Advanced Data Analysis - Project 1",
    "section": "Running the Model",
    "text": "Running the Model\nFor ease of interpretation I will be performing this analysis as two separate linear regressions, one for each outcome variable of either attachment loss or pocket depth change. I will begin with a simple linear regression only including the PEV and either attachment loss change or pocket depth change.\n\nAttachment Loss ChangePocket Depth ChangePost-Hoc Analysis\n\n\nWe start by constructing a model with attachment loss change as the dependent variable, and treatment group as the independent variable.\n\n# Running a SLR with attachment loss change as the DV, treatment group as the IV with no treatment as the reference group\nmodel_attach1 &lt;- lm(attachchange ~ placebo + low +  medium + high, data = data_missing)\nsummary(model_attach1)\n\n\nCall:\nlm(formula = attachchange ~ placebo + low + medium + high, data = data_missing)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.88283 -0.14813  0.05115  0.17174  0.53945 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.22169    0.05590  -3.966 0.000139 ***\nplacebo      0.13462    0.07906   1.703 0.091771 .  \nlow          0.20388    0.08092   2.520 0.013365 *  \nmedium       0.21514    0.08197   2.625 0.010063 *  \nhigh         0.05690    0.08728   0.652 0.515950    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2681 on 98 degrees of freedom\nMultiple R-squared:  0.09368,   Adjusted R-squared:  0.05669 \nF-statistic: 2.532 on 4 and 98 DF,  p-value: 0.0451\n\n# Apply Bonferroni correction\np_values &lt;- summary(model_attach1)$coefficients[,4] # This line selects the fourth column of the resulting coefficients table from summary(model_attach), which is the p-values\np_adjusted &lt;- p.adjust(p_values, method = \"bonferroni\")\n\n# Compare adjusted p-values to unadjusted p-values\np_comparison &lt;- cbind(p_values, p_adjusted)\np_comparison\n\n                p_values   p_adjusted\n(Intercept) 0.0001392148 0.0006960742\nplacebo     0.0917709125 0.4588545624\nlow         0.0133650749 0.0668253747\nmedium      0.0100625287 0.0503126435\nhigh        0.5159498143 1.0000000000\n\n# Get confidence intervals\nconf_intervals &lt;- confint(model_attach1)\nconf_intervals\n\n                  2.5 %     97.5 %\n(Intercept) -0.33262557 -0.1107577\nplacebo     -0.02226569  0.2915028\nlow          0.04330175  0.3644540\nmedium       0.05247446  0.3777966\nhigh        -0.11629489  0.2300962\n\n\nThe overall model is significant (F(4,98) = 2.53, p = 0.0451). Looking at the individual t-statistics, we see that - following adjustment for multiple pairwise comparisons - the placebo group (p = 0.549), low concentration group (p = 0.0668), and high concentration group (p = 1.000) are not statistically different from the control group. The medium concentration group is approaching significance (p = 0.0503).\nThat was with the no treatment control group as the reference. Let’s see if any of our treatment conditions were different from the placebo group.\n\n# Running a SLR with attachment loss change as the DV, treatment group as the IV with placebo as the reference group\nmodel_attach2 &lt;- lm(attachchange ~ control + low +  medium + high, data = data_missing)\nsummary(model_attach2)\n\n\nCall:\nlm(formula = attachchange ~ control + low + medium + high, data = data_missing)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.88283 -0.14813  0.05115  0.17174  0.53945 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept) -0.08707    0.05590  -1.558   0.1225  \ncontrol     -0.13462    0.07906  -1.703   0.0918 .\nlow          0.06926    0.08092   0.856   0.3941  \nmedium       0.08052    0.08197   0.982   0.3284  \nhigh        -0.07772    0.08728  -0.890   0.3754  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2681 on 98 degrees of freedom\nMultiple R-squared:  0.09368,   Adjusted R-squared:  0.05669 \nF-statistic: 2.532 on 4 and 98 DF,  p-value: 0.0451\n\n# Apply Bonferroni correction\np_values &lt;- summary(model_attach2)$coefficients[,4] # This line selects the fourth column of the resulting coefficients table from summary(model_attach), which is the p-values\np_adjusted &lt;- p.adjust(p_values, method = \"bonferroni\")\n\n# Compare adjusted p-values to unadjusted p-values\np_comparison &lt;- cbind(p_values, p_adjusted)\np_comparison\n\n              p_values p_adjusted\n(Intercept) 0.12254523  0.6127262\ncontrol     0.09177091  0.4588546\nlow         0.39412117  1.0000000\nmedium      0.32836700  1.0000000\nhigh        0.37538435  1.0000000\n\n# Get confidence intervals\nconf_intervals &lt;- confint(model_attach2)\nconf_intervals\n\n                  2.5 %     97.5 %\n(Intercept) -0.19800701 0.02386082\ncontrol     -0.29150281 0.02226569\nlow         -0.09131681 0.22983549\nmedium      -0.08214410 0.24317801\nhigh        -0.25091345 0.09547760\n\n\nAfter applying a bonferroni correction, none of the groups are significantly different from the placebo group (p &gt; .05).\nConclusion\nWhile the overall model was significant, the only groups that were statistically different from the no treatment control were the low and medium concentration gel groups (p &lt; 0.05). However, since this was a placebo-controlled RCT, and none of the treatment groups were significantly different from the placebo group, we can conclude that we fail to reject the null hypothesis that the average attachment loss over 1 year is the same between all groups.\nTables\n\n\nNote: p-values are unadjusted.\nBack to top of tabset\n\n\nNow we will create our model with pocket depth change as the dependent variable and treatment group as the independent variable.\n\n# Running the regression with pocket depth change as the DV, treatment group as the IV with no treatment as the reference group\nmodel_pd &lt;- lm(pdchange ~ placebo + low + medium + high, data = data_missing)\nsummary(model_pd)\n\n\nCall:\nlm(formula = pdchange ~ placebo + low + medium + high, data = data_missing)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.62483 -0.14595 -0.01768  0.16029  0.66130 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.33817    0.05466  -6.187 1.43e-08 ***\nplacebo     -0.01152    0.07730  -0.149   0.8818    \nlow          0.13200    0.07912   1.668   0.0984 .  \nmedium       0.13562    0.08015   1.692   0.0938 .  \nhigh        -0.04413    0.08534  -0.517   0.6063    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2621 on 98 degrees of freedom\nMultiple R-squared:  0.07806,   Adjusted R-squared:  0.04043 \nF-statistic: 2.074 on 4 and 98 DF,  p-value: 0.08994\n\n\nThe overall model is not statistically significant (F(4,98)= 2.074, p = 0.0899). Based on this model, it appears that the gel treatment has no effect on pocket depth change after 1 year.\nConclusion\nNo groups were significantly different from each other in pocket depth change after 1 year (F(4,98)= 2.074, p = 0.0899), and we fail to reject the null hypothesis that the average pocket depth change over 1 year is the same between all groups.\nTables\n\nNote: p-values are unadjusted\nBack to top of tabset\n\n\nWe did not find a main effect based on treatment group. However I am still interested if including any of the covariates changes the results.\n\n# Run a SLR with attachment loss change as the DV and gender as a covariate\nmodel2_attach &lt;- lm(attachchange ~ placebo + low + medium + high + gender, data = data_missing) \nsummary(model2_attach)\n\n\nCall:\nlm(formula = attachchange ~ placebo + low + medium + high + gender, \n    data = data_missing)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.86656 -0.17533  0.02334  0.16395  0.57717 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept) -0.07460    0.10988  -0.679   0.4988  \nplacebo      0.12330    0.07883   1.564   0.1210  \nlow          0.19310    0.08064   2.395   0.0186 *\nmedium       0.21118    0.08143   2.593   0.0110 *\nhigh         0.06704    0.08690   0.771   0.4423  \ngender      -0.08675    0.05593  -1.551   0.1242  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2662 on 97 degrees of freedom\nMultiple R-squared:  0.1156,    Adjusted R-squared:  0.07003 \nF-statistic: 2.536 on 5 and 97 DF,  p-value: 0.03346\n\n# Apply Bonferroni correction\np_values &lt;- summary(model2_attach)$coefficients[,4] # This line selects the fourth column of the resulting coefficients table from summary(model_attach), which is the p-values\np_adjusted &lt;- p.adjust(p_values, method = \"bonferroni\")\n\n# Compare adjusted p-values to unadjusted p-values\np_comparison &lt;- cbind(p_values, p_adjusted)\np_comparison\n\n              p_values p_adjusted\n(Intercept) 0.49883151 1.00000000\nplacebo     0.12104977 0.72629859\nlow         0.01856222 0.11137333\nmedium      0.01097151 0.06582903\nhigh        0.44234244 1.00000000\ngender      0.12415264 0.74491582\n\n# Get confidence intervals\nconf_intervals &lt;- confint(model2_attach)\nconf_intervals\n\n                  2.5 %     97.5 %\n(Intercept) -0.29268916 0.14349242\nplacebo     -0.03315881 0.27976619\nlow          0.03304940 0.35315426\nmedium       0.04956814 0.37278247\nhigh        -0.10544029 0.23951403\ngender      -0.19775102 0.02425639\n\n\nA model including gender does not seem to help anything. What about with all covariates (just for fun)?\n\n# Run a SLR with attachment loss change as the DV and gender as a covariate\nmodel3_attach &lt;- lm(attachchange ~ placebo + low + medium + high + gender + race + age + smoker + sites + attachbase + pdbase , data = data_missing) \nsummary(model3_attach)\n\n\nCall:\nlm(formula = attachchange ~ placebo + low + medium + high + gender + \n    race + age + smoker + sites + attachbase + pdbase, data = data_missing)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.60415 -0.16360  0.02131  0.16645  0.57481 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.160630   0.480855   0.334 0.739127    \nplacebo      0.017898   0.079113   0.226 0.821541    \nlow          0.151477   0.077598   1.952 0.054074 .  \nmedium       0.172196   0.079066   2.178 0.032060 *  \nhigh         0.045948   0.081336   0.565 0.573557    \ngender      -0.050360   0.054953  -0.916 0.361924    \nrace         0.032325   0.026871   1.203 0.232181    \nage         -0.002472   0.002709  -0.912 0.364067    \nsmoker       0.069384   0.055052   1.260 0.210844    \nsites       -0.001501   0.002539  -0.591 0.555885    \nattachbase  -0.165154   0.043233  -3.820 0.000246 ***\npdbase       0.094444   0.072226   1.308 0.194370    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2456 on 89 degrees of freedom\n  (2 observations deleted due to missingness)\nMultiple R-squared:  0.2908,    Adjusted R-squared:  0.2032 \nF-statistic: 3.318 on 11 and 89 DF,  p-value: 0.0007435\n\n# Apply Bonferroni correction\np_values &lt;- summary(model3_attach)$coefficients[,4] # This line selects the fourth column of the resulting coefficients table from summary(model_attach), which is the p-values\np_adjusted &lt;- p.adjust(p_values, method = \"bonferroni\")\n\n# Compare adjusted p-values to unadjusted p-values\np_comparison &lt;- cbind(p_values, p_adjusted)\np_comparison\n\n               p_values  p_adjusted\n(Intercept) 0.739127379 1.000000000\nplacebo     0.821540779 1.000000000\nlow         0.054074263 0.648891153\nmedium      0.032060232 0.384722790\nhigh        0.573556835 1.000000000\ngender      0.361924208 1.000000000\nrace        0.232180658 1.000000000\nage         0.364067210 1.000000000\nsmoker      0.210844428 1.000000000\nsites       0.555885079 1.000000000\nattachbase  0.000246493 0.002957916\npdbase      0.194369803 1.000000000\n\n# Get confidence intervals\nconf_intervals &lt;- confint(model3_attach)\nconf_intervals\n\n                   2.5 %       97.5 %\n(Intercept) -0.794818756  1.116078333\nplacebo     -0.139298371  0.175094025\nlow         -0.002709329  0.305662938\nmedium       0.015093048  0.329298305\nhigh        -0.115665991  0.207561289\ngender      -0.159551369  0.058830891\nrace        -0.021067104  0.085716311\nage         -0.007855313  0.002911679\nsmoker      -0.040003508  0.178771331\nsites       -0.006546011  0.003543891\nattachbase  -0.251056725 -0.079251471\npdbase      -0.049067286  0.237955593\n\n\nNope. Interestingly the best predictor of attachment loss at 1 year is attachment loss at baseline. The results for pocket depth are likely the same.\nI also want to assess if controlling for baseline attachment loss or pocket depth change affects things, since those were strong predictors of each measurement at 1 year (still need to add those SLRs).\n\nmodel4_attach &lt;- lm(attachchange ~ placebo + low + medium + high + attachbase, data = data_missing)\nsummary(model4_attach)\n\n\nCall:\nlm(formula = attachchange ~ placebo + low + medium + high + attachbase, \n    data = data_missing)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.52469 -0.16400  0.02084  0.15231  0.73422 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.10707    0.09304   1.151   0.2527    \nplacebo      0.04202    0.07616   0.552   0.5824    \nlow          0.14605    0.07592   1.924   0.0573 .  \nmedium       0.17586    0.07622   2.307   0.0232 *  \nhigh         0.02665    0.08087   0.330   0.7425    \nattachbase  -0.12902    0.03039  -4.246 4.99e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2475 on 97 degrees of freedom\nMultiple R-squared:  0.2357,    Adjusted R-squared:  0.1963 \nF-statistic: 5.984 on 5 and 97 DF,  p-value: 7.236e-05\n\n# Apply Bonferroni correction\np_values &lt;- summary(model4_attach)$coefficients[,4] # This line selects the fourth column of the resulting coefficients table from summary(model_attach), which is the p-values\np_adjusted &lt;- p.adjust(p_values, method = \"bonferroni\")\n\n# Compare adjusted p-values to unadjusted p-values\np_comparison &lt;- cbind(p_values, p_adjusted)\np_comparison\n\n                p_values   p_adjusted\n(Intercept) 2.526878e-01 1.0000000000\nplacebo     5.824313e-01 1.0000000000\nlow         5.731590e-02 0.3438953943\nmedium      2.317045e-02 0.1390227138\nhigh        7.424826e-01 1.0000000000\nattachbase  4.989769e-05 0.0002993861\n\n\nAfter controlling for treatment group, the only significant predictor of attachment loss change is baseline attachment loss scores (padj = 0.001)\nThese models were just for exploration, fun, and practice. The final models selected are the SLR’s with treatment group as the IV and either attachment loss change or pocket depth change as the DV. Back to top of tabset"
  },
  {
    "objectID": "Project_1_Regression/Project_1_Regression_R/Project_1_Regression.html#Assumptions",
    "href": "Project_1_Regression/Project_1_Regression_R/Project_1_Regression.html#Assumptions",
    "title": "Advanced Data Analysis - Project 1",
    "section": "Evaluating Assumptions",
    "text": "Evaluating Assumptions\nIn order to evaluate the assumptions of our models, we will first gather the residuals of the model predicting attachment loss change score and the model predicting pocket depth change score.\n\n# Calculate the jackknife residuals of model_attach\njackknife_residuals_attach &lt;- rstudent(model_attach1)\n\n# Calculate the jackknife residuals of model_pd\njackknife_residuals_pd &lt;- rstudent(model_pd)\n\nNow that we have our residuals, we can take a closer look at the assumptions.\n\nLinearityIndependenceNormalityEqual Variances (Homoscedasticity)Residuals Centered Around ZeroSummary\n\n\nSince the IV is categorical, we do not need to assess linearity.\nBack to top of tabset\n\n\nIndependence can be assessed in part by the study design and how the data was collected. Based on the information provided by the PI, I will assume subjects are independent from each other (e.g. not siblings).\nAdditionally, we can examine a scatter plot of the model’s residuals against any time point variable (such as ID). Let’s do that.\n\n# Create a scatterplot of jackknife residuals vs ID to assess independence for pocket depth change\nggplot(data_missing, aes(x = id, y = jackknife_residuals_attach)) + \n  geom_point() +\n  geom_hline(yintercept = 0) +\n  labs(title = \"Scatterplot of Jackknife Residuals vs ID for Attachment Loss Change\",\n       x = \"ID\",\n       y = \"Jackknife Residuals\")\n\n\n\n\n\n\n\n# Create a scatterplot of jackknife residuals vs ID to assess independence for pocket depth change\nggplot(data_missing, aes(x = id, y = jackknife_residuals_pd)) + \n  geom_point() +\n  geom_hline(yintercept = 0) +\n  labs(title = \"Scatterplot of Residuals vs ID for Pocket Depth Change\",\n       x = \"ID\",\n       y = \"Jackknife Residuals\")\n\n\n\n\n\n\n\n\nThe pattern appears random, suggesting independence.\nNote: The gap between ID’s 170 and 200 looks odd, but is an artifact from how the experimenters assigned ID, with females starting at 101, and males starting at 201.\nBack to top of tabset\n\n\nHere we will asses that, for any fixed value of X, Y has a normal distribution. We will do this using Q-Q plots and histograms of the residuals.\n\n# Make the Q-Q plots using the jackknife residuals for attachment loss change\nqqnorm(jackknife_residuals_attach, main = \"Q-Q plots of Jackknife Residuals for model_attach\")\nqqline(jackknife_residuals_attach, col = \"black\")\n\n\n\n\n\n\n\n# Create histogram of jackknife residuals for model_attach\nhist(jackknife_residuals_attach, main = \"Histogram of Jackknife Residuals\",\n     xlab = \"Jackknife Residuals\",\n     col = \"lightblue\",\n     border = \"black\")\n\n\n\n\n\n\n\n# Make the Q-Q plots using the jackknife residuals for pocket depth change\nqqnorm(jackknife_residuals_pd, main = \"Q-Q plots of Jackknife Residuals for model_pd\")\nqqline(jackknife_residuals_pd, col = \"black\")\n\n\n\n\n\n\n\n# Create histogram of jackknife residuals for model_pd\nhist(jackknife_residuals_pd, main = \"Histogram of Jackknife Residuals\",\n     xlab = \"Jackknife Residuals\",\n     col = \"lightblue\",\n     border = \"black\")\n\n\n\n\n\n\n\n\nThe histogram for attachment loss change is a little left-tailed. This could be from an outlier. Comparatively, the Q-Q plot and histogram of the residuals for pocket depth change are normally distributed.\nWe can also include the Shapiro-Wilk test of normality, which provides a p-value and allows us to numerically establish that the assumption of normality is met. If the p-value is &lt; 0.05 you conclude that the assumption of normality is not met.\n\nshapiro.test(jackknife_residuals_attach)\n\n\n    Shapiro-Wilk normality test\n\ndata:  jackknife_residuals_attach\nW = 0.9604, p-value = 0.003601\n\nshapiro.test(jackknife_residuals_pd)\n\n\n    Shapiro-Wilk normality test\n\ndata:  jackknife_residuals_pd\nW = 0.99059, p-value = 0.6933\n\n\nThe assumption of normality is violated for the attachment loss change model (p = 0.0036), but not for the pocket depth change model (p = 0.6933). Looking at the histogram of the residuals for attachment loss change, this is likely due to an outlier.\nSummary\nFor attachment loss change, we have a slight violation of normality, which could be due to the presence of an outlier. However, regressions are robust to violations of assumptions and this may not actually be an issue. Outliers in the model will be assessed using jackknife residuals to confirm that these points do not have an excessive amount of influence on the model.\nBased on the Q-Q plots and histograms of the residuals for pocket depth change, we can conclude that we satisfy the assumption of normality.\nBack to top of tabset\n\n\nUsing the scale-location plot will allow us to evaluate the constant variance assumption. This will allow us to see whether the variability of the residuals is roughly constant between each group (Source).\nTo assess homoscedasticity we examine the residual scatterplots by treatment group. The warning sign to look for here is if the variance differs greatly across groups.\n\n# Make the residual scatterplot using the jackknife residuals for model_attach\nggplot(data_missing, aes(x = trtgroup, y = jackknife_residuals_attach)) +\n  geom_point() + \n  labs(title = \"Jackknife Residuals vs Treatment Condition for Attachment Loss Change\",\n       x = \"Treatment Condition\",\n       y = \"JackKnife Residuals\")\n\n\n\n\n\n\n\n# Make the residual scatterplot using the jackknife residuals for model_pd\nggplot(data_missing, aes(x = trtgroup, y = jackknife_residuals_pd)) +\n  geom_point() + \n  labs(title = \"Jackknife Residuals vs Treatment Condition for Pocket Depth Change\",\n       x = \"Treatment Condition\",\n       y = \"JackKnife Residuals\")\n\n\n\n\n\n\n\n\nIt appears that our variances in all groups are equal!\nAdditionally, while it is not recommended to perform a statistical test to assess for equality of variances (because formal tests of equality of variance are not very powerful), we can still do this using Bartlett’s test.\n\nbartlett.test(attachchange ~ trtgroup, data = data_missing)\n\n\n    Bartlett test of homogeneity of variances\n\ndata:  attachchange by trtgroup\nBartlett's K-squared = 2.5462, df = 4, p-value = 0.6364\n\n\nThe null hypothesis of the Bartlett test is that the variances are equal. Thus failing to reject the null (p &gt; 0.05) indicates that the data are consistent with the equal variance assumption.\nSo we meet the assumption of equality of variances, looking good!\nBack to top of tabset\n\n\nTo start, we can simply check that the mean of our residuals is close to 0.\n\n#### For attachment loss change score\n# Generate the mean of the jackknife residuals for attachment loss change. Should be close to 0.\nmean(jackknife_residuals_attach)\n\n[1] -0.004132486\n\n\nWe are looking good for attachment loss change score. What about for pocket depth change?\n\n#### For pocket depth change score\n# Generate the mean of the jackknife residuals for attachment loss change. Should be close to 0.\nmean(jackknife_residuals_pd)\n\n[1] -6.695748e-06\n\n\nA more sophisticated approach is to plot the fitted values vs jackknife residuals. We can then compare the trend between groups to see if they are random. The fitted line should gravitate around 0 with no obvious trends.\n\n# Generate vectors containing the fitted values vs jackknife residuals so we can plot them \nfitted_values_pd &lt;- fitted(model_pd)\n\nggplot(data_missing, aes(x = fitted_values_pd, y = jackknife_residuals_pd)) + \n  geom_point() +\n  geom_smooth(method = \"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n  labs(title = \"Jackknife Residuals vs Fitted Values for Attachment Loss Change\",\n       x = \"Fitted Values\",\n       y = \"Jackknife Residuals\")\n\n$x\n[1] \"Fitted Values\"\n\n$y\n[1] \"Jackknife Residuals\"\n\n$title\n[1] \"Jackknife Residuals vs Fitted Values for Attachment Loss Change\"\n\nattr(,\"class\")\n[1] \"labels\"\n\n\nThe pattern looks random and we can conclude we meet this assumption.\nBack to top of tabset\n\n\nWe meet the assumptions of independence, equal variances, and errors centered around zero required for this analysis. There is a slight violation of normality for the attachment loss change model, but this could be due to an outlier."
  },
  {
    "objectID": "Project_1_Regression/Project_1_Regression_R/Project_1_Regression.html#Multiple_imputation",
    "href": "Project_1_Regression/Project_1_Regression_R/Project_1_Regression.html#Multiple_imputation",
    "title": "Advanced Data Analysis - Project 1",
    "section": "Multiple Imputation",
    "text": "Multiple Imputation\n\nBackgroundCoding ExamplePerforming the Multiple Imputation\n\n\nNote: This information is taken from Biostats II (BIOS 6612) slides, week 13 lecture 20, and from Flexible Imputation of Missing Data 2nd edition.\nMissing Data\nTo review, there are several assumptions for missing data. The first is MCAR (Missing Completely at Random). This is when the probability of having a missing value is the same for everyone, and is rarely ever the case with real data.\nThe second is MAR (missing at random). This is when the probability of having a missing value is the same within groups defined by the observed data. That is, if we know that second variable by which the data is NOT MCAR, in addition to the measurements, we can assume MCAR within that second variable. For instance, if we know the missing data is not equal between the sexes, then we can assume MCAR within groups defined by sex. MAR is often observed in situations such as males missing more data than females, or older participants missing more data than younger ones, etc. MAR is more general and more realistic than MCAR. Modern missing data assumptions typically start from the MAR assumption.\nMNAR (Missing Not At Random) is the final category of missingness, and is when the the probability of having a missing value varies for reasons that are unknown to you. For example, in public opinion research, those with weaker opinions may respond less than those with stronger opinions, and you may not know this ahead of time. MNAR is the most complex and if you have it, you have your work cut out for you. In that situation, you can find more data about the causes for the missingness, or run lots and lots of sensitivity analyses.\nAnd as they say, the best way to handle missing data is not to have it.\nMultiple Imputation\nIn the current experiment, we know that data is not MCAR (males are missing more data than females), and thus cannot simply throw out subjects with missing values. If the data are not MCAR, listwise deletion (deleting any subject with a missing value) can severely bias estimates of means, regression coefficients and correlations.\nMultiple Imputation is a commonly used method to handle missing data when data is not MCAR, but is MAR. It is a process by which you use observed data to “predict” missing data, then use those “imputed” values in further analyses. Multiple imputation builds upon and pools together “single” imputation approaches.\nFor example, there exists very simple ways of imputing data, such as plugging in the average or median values in place of missing values. This is not sophisticated and kind of sketchy.\nThen there exists regression imputation, where for each variable you fit a regression model of it to the observed data, and then use that model to predict missing values, and use those predictions in place of the missing values. However, this injects bias into the estimate for the correlation between X and Y (since the values fall perfectly in line with the hypothesis that X and Y have a non zero correlation. (and are in a perfect line)).\n\nThen there is stochastic regression imputation, which is like regression imputation but adds noise back into the imputations based on the variance of their residuals. This helps account for variance in the results due to missing data.\n\nMultiple imputation works by fitting multiple stochastic imputation models (can be 100’s or 1000’s), analyzing each dataset separately to produce estimates of interest, and finally pooling together these statistics of interest. If done properly, the pooled statistics are unbiased under MAR, and the SE’s will be correct!\n\nMultiple Imputation is “simple, elegant and powerful. It is simple because it fills the holes in the data with plausible values. It is elegant because the uncertainty about the unknown data is coded in the data itself. And it is powerful because it can solve “other” problems that are actually missing data problems in disguise.” - Stef van Buuren\nBack to top of tabset\n\n\nFirst we will begin with an example of multiple imputation using the built in airquality dataset in R.\n\n# Examining data set and missingness\nhead(airquality)\n\n  Ozone Solar.R Wind Temp Month Day\n1    41     190  7.4   67     5   1\n2    36     118  8.0   72     5   2\n3    12     149 12.6   74     5   3\n4    18     313 11.5   62     5   4\n5    NA      NA 14.3   56     5   5\n6    28      NA 14.9   66     5   6\n\ncolSums(is.na(airquality))\n\n  Ozone Solar.R    Wind    Temp   Month     Day \n     37       7       0       0       0       0 \n\nvis_miss(airquality)\n\n\n\n\n\n\n\n\nWe’re missing 37 values in the Ozone column (24% of values!). We will have to address this missing data somehow.\nMean Imputation\nWe could quickly fill in missing data with mean imputation…\n\n# Here's how you do mean imputation in mice\nimp &lt;- mice(airquality, method = \"mean\", m = 1, maxit = 1)\n\n\n iter imp variable\n  1   1  Ozone  Solar.R\n\n\nBut mean imputation will “underestimate the variance, disturb the relations between variables, bias almost any estimate other than the mean and bias the estimate of the mean when data are not MCAR.” You can use mean imputation as a quick fix when there’s a handful of missing values, but it should be avoided in general.\nRegression Imputation\nWe could alternatively do regression imputation, where we fit a model and then use that model to predict the missing values. In regression imputation, you are essentially using the observed data to predict the missing data.\n\n# Performing regression imputation using the mice package\ndata_example &lt;- airquality[, c(\"Ozone\", \"Solar.R\")]\nimp &lt;- mice(data_example, method = \"norm.predict\", seed = 1,\n           m = 1, print = FALSE)\n\n# Plotting missing vs observed values\nxyplot(imp, Ozone ~ Solar.R,\n       main = \"Missing vs Observed values for Ozone ~ Solar.R\",\n       auto.key = list(corner = c(0,1),\n                  points = TRUE, \n                  text = c(\"Observed\", \"Missing\"), col = c(\"steelblue\", \"red3\")))\n\n\n\n\n\n\n\n\nYou predict the values of the missing datapoints with the regression equation resulting from your model. That is, the imputed values correspond to the most likely values under that model. However, the imputed values (red) vary less than the observed values (blue). So each value is the best under the model, but it is very unlikely that the real values would have had this distribution.\nRegression imputation yields unbiased estimates of the means and of the regression weights of the model under MCAR. It also does so under MAR, provided that the factors that influence missinginess are part of the model. However, correlations are biased to be greater/higher.\n“Regression imputation, as well as its modern incarnations in machine learning is probably the most dangerous of all methods described here. We may be led to believe that we’re to do a good job by preserving the relations between the variables. In reality however, regression imputation artificially strengthens the relations in the data. Correlations are biased upwards. Variability is underestimated. Imputations are too good to be true. Regression imputation is a recipe for false positive and spurious relations.”\nStochastic regression imputation\nStochastic regression imputation is a refinement of regression imputation that attempts to address correlation bias by adding noise back into the predictions of the missing values.\n\n# Impute Ozone from Solar.R by stochastic regression imputation using the mice package.\ndata_example &lt;- airquality[, c(\"Ozone\", \"Solar.R\")]\n\n# Perform stochastic regression imputation\nimp &lt;- mice(data_example, method = \"norm.nob\", m = 1, maxit = 1, seed = 1, print = FALSE)\n\n# Plotting missing vs observed values\nxyplot(imp, Ozone ~ Solar.R,\n       main = \"Missing vs Observed values for Ozone ~ Solar.R\",\n       auto.key = list(corner = c(0,1),\n                  points = TRUE, \n                  text = c(\"Observed\", \"Missing\"), col = c(\"steelblue\", \"red3\")))\n\n\n\n\n\n\n\n\n“The method = norm.nob argument requests a plain, non-Bayesian, stochastic regression method. This method first estimates the intercept, slope and residual variance under the linear model, then calculates the predicted value for each missing value, and adds a random draw from the residual to the prediction”.\nThis can create issues though, such as we have one imputed value that is negative! Which might not be plausible (such as in this case, there is no such thing as a negative ozone level).\nA more convenient solution is multiple imputation\nMultiple Imputation\nMultiple imputation creates m &gt; 1 complete data sets. The m results are then pooled into a final point estimate plus standard error. So each data set is identical in observed values, but differs in imputed values.\nWe then estimate the parameters of interest from each dataset. This is done by applying the analytic method you would have used if the dataset was complete in the first place (here, a regression). The results of the model on each dataset will differ because the data is different. These differences are caused by the uncertainty of what value to impute.\nThe last step is that these parameter estimates are then pooled together into a single value, and its variance estimated. The variance is assessed by combining the “within-imputation variance with the extra variance caused by the missing data (between-imputation variance)”. So under the appropriate conditions the pooled estimates are unbiased. MI solves the problem of ‘too small’ SEs of other imputation methods we just covered.\nNow let’s perform the multiple imputation on the airquality data set.\n\n# Perform multiple imputation on the airquality data set\nimp &lt;- mice(airquality, seed = 1, m = 20, print = FALSE) # This line imputes the missing data 20 times\n\n# Fit a linear regression \nmodel_imp &lt;- with(imp, lm(Ozone ~ Wind + Temp + Solar.R)) # You have to run the regression with \"with(imp, lm(etc))\n                  \nsummary(model_imp) # This runs a regression on each imputed dataset (so 20 different regressions)\n\n# A tibble: 80 × 6\n   term        estimate std.error statistic  p.value  nobs\n   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;int&gt;\n 1 (Intercept) -60.7      17.9        -3.40 8.78e- 4   153\n 2 Wind         -2.95      0.514      -5.75 4.96e- 8   153\n 3 Temp          1.53      0.197       7.74 1.42e-12   153\n 4 Solar.R       0.0671    0.0184      3.64 3.70e- 4   153\n 5 (Intercept) -56.8      18.5        -3.07 2.56e- 3   153\n 6 Wind         -3.33      0.532      -6.25 4.04e- 9   153\n 7 Temp          1.56      0.205       7.58 3.49e-12   153\n 8 Solar.R       0.0554    0.0191      2.89 4.37e- 3   153\n 9 (Intercept) -74.0      18.7        -3.95 1.19e- 4   153\n10 Wind         -2.69      0.540      -4.99 1.67e- 6   153\n# ℹ 70 more rows\n\nsummary(pool(model_imp)) # This pools together the parameters of all 20 regressions (statistic is Wald's test) into a single model.\n\n         term     estimate   std.error statistic       df      p.value\n1 (Intercept) -65.87829658 23.09377412 -2.852643 69.97033 5.696702e-03\n2        Wind  -3.01897171  0.66252377 -4.556775 70.51194 2.125022e-05\n3        Temp   1.63483547  0.25107557  6.511328 75.99913 7.203792e-09\n4     Solar.R   0.05861581  0.02267832  2.584662 90.10797 1.135441e-02\n\n\n\n# mids workflow using pipes\nest3 &lt;- airquality %&gt;%\n  mice(seed = 1, print = FALSE, m = 20) %&gt;%\n  with(lm(formula = Ozone ~ Wind + Temp + Solar.R)) %&gt;%\n  pool()\nsummary(est3)\n\n         term     estimate   std.error statistic       df      p.value\n1 (Intercept) -65.87829658 23.09377412 -2.852643 69.97033 5.696702e-03\n2        Wind  -3.01897171  0.66252377 -4.556775 70.51194 2.125022e-05\n3        Temp   1.63483547  0.25107557  6.511328 75.99913 7.203792e-09\n4     Solar.R   0.05861581  0.02267832  2.584662 90.10797 1.135441e-02\n\n\n\n# Run a regression on the imputed airquality data set. \nimp &lt;- mice(airquality, seed = 1, print = FALSE)\n\nfit &lt;- with(imp, lm(Ozone ~ Solar.R))\n\n# Print out the estimates for the first and second data set\ncoef(fit$analyses[[1]])\n\n(Intercept)     Solar.R \n 22.2963201   0.1057437 \n\ncoef(fit$analyses[[2]])\n\n(Intercept)     Solar.R \n 20.7891622   0.1140081 \n\n\nNotice that the paremter estimates differ because of the uncertainty created by the missing data.\nApplying the standard pooling rules is done with\n\nest &lt;- pool(fit)\nsummary(est)\n\n         term   estimate  std.error statistic       df      p.value\n1 (Intercept) 21.5321395 5.67991951  3.790923 136.3544 0.0002245626\n2     Solar.R  0.1091546 0.02846503  3.834693 102.7169 0.0002170873\n\n\nAny R expression produced by expression() can be used on the multiply imputed data…\n\n# Extract residuals and fitted values\n\n# This gets you ALL the residuals for ALL the models/dataset\nimp_residuals &lt;- sapply(model_imp[[4]], residuals)\n\n# Same thing but for the fitted\nimp_fitted &lt;- sapply(model_imp[[4]],fitted)\n\n# Don't know where to go from here. Apparently you can take the average of all of these to get the average residuals, but it's not published anywhere.\n\nWe can compare the results of our multiple imputation model with the listwise deletion model to see how different they came out.\n\nmodel_non_imp &lt;- lm(Ozone ~ Wind + Temp + Solar.R, data = airquality, na.action = na.omit)\nsummary(model_non_imp)\n\n\nCall:\nlm(formula = Ozone ~ Wind + Temp + Solar.R, data = airquality, \n    na.action = na.omit)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-40.485 -14.219  -3.551  10.097  95.619 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -64.34208   23.05472  -2.791  0.00623 ** \nWind         -3.33359    0.65441  -5.094 1.52e-06 ***\nTemp          1.65209    0.25353   6.516 2.42e-09 ***\nSolar.R       0.05982    0.02319   2.580  0.01124 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 21.18 on 107 degrees of freedom\n  (42 observations deleted due to missingness)\nMultiple R-squared:  0.6059,    Adjusted R-squared:  0.5948 \nF-statistic: 54.83 on 3 and 107 DF,  p-value: &lt; 2.2e-16\n\nsummary(pool(model_imp))\n\n         term     estimate   std.error statistic       df      p.value\n1 (Intercept) -65.87829658 23.09377412 -2.852643 69.97033 5.696702e-03\n2        Wind  -3.01897171  0.66252377 -4.556775 70.51194 2.125022e-05\n3        Temp   1.63483547  0.25107557  6.511328 75.99913 7.203792e-09\n4     Solar.R   0.05861581  0.02267832  2.584662 90.10797 1.135441e-02\n\n\nOur regression using MI and the complete case (i.e. listwise deletion) data set are comparable!\nNote: what we are NOT doing in multiple imputation is taking an average of the imputed data and running a model on that as if its a single, complete data set. “Researchers are often tempted to average the multiply imputed data, and analyze the averaged data as if it were complete. This method yields incorrect standard errors, confidence intervals and p-values, and thus should not be used if any form of statistical testing or uncertainty analysis is to be done on the imputed data. The reason is that the procedure ignores the between-imputation variability, and hence shares all the drawbacks of single imputation”.\nNote: It’s recommended to impute then transform, because if you create variables based on other variables, there are relationships between those variables that MI doesn’t account for (it might create combinations of variables that are unrealistic)\nYou can do this as\n\n# Example of impute then transform method (i.e. how to add variables to an imputed dataset)\ndata_ex &lt;- boys[, c(\"age\", \"hgt\", \"wgt\", \"hc\", \"reg\")]\nimp &lt;- mice(data_ex, print = FALSE, seed = 1)\n \n# put the data in long format\nlong &lt;- mice::complete(imp, \"long\", include = TRUE)\n\nlong$new_var &lt;- with(long, 100 * wgt / hgt)\n\nimp.itt &lt;- as.mids(long)\n\nBack to top of tabset\n\n\nNow that we have reviewed the background and gone through coding examples of multiple imputation, we are ready to perform it for the dataset for Project 1.\n\n# Pool the parameters of interest for a regression on the imputed values for attach change control as reference\nmodel_imp &lt;- data %&gt;%\n  mice(seed = 1, print = FALSE, m = 20) %&gt;%\n  with(lm(attachchange ~ placebo + low + medium + high)) %&gt;%\n  pool()\n\nWarning: Number of logged events: 1158\n\n\nNone of the p-values for the pooled regressions on the MI data are significant (p &gt; 0.05).\nHow do those parameter estimates compare to the complete case analysis model?\n\n# Compare coefficients to complete case analysis for attachment loss with control as reference category\nMI &lt;- summary(model_imp)\nMI\n\n         term   estimate  std.error  statistic       df     p.value\n1 (Intercept) -0.1853493 0.06920536 -2.6782512 65.79196 0.009337056\n2     placebo  0.1158204 0.09076258  1.2760813 91.37003 0.205161055\n3         low  0.1608502 0.10027097  1.6041549 59.48514 0.113976747\n4      medium  0.1612700 0.11393844  1.4154130 37.61151 0.165174849\n5        high  0.0930271 0.15435067  0.6026997 18.41109 0.554060413\n\ncomplete_case &lt;- summary(model_attach1)\ncomplete_case$coefficients\n\n               Estimate Std. Error    t value     Pr(&gt;|t|)\n(Intercept) -0.22169165 0.05590110 -3.9657832 0.0001392148\nplacebo      0.13461856 0.07905610  1.7028232 0.0917709125\nlow          0.20387790 0.08091650  2.5196086 0.0133650749\nmedium       0.21513551 0.08196711  2.6246567 0.0100625287\nhigh         0.05690063 0.08727557  0.6519652 0.5159498143\n\n\nThey are drastically different, although still not significant.\nWe can also plot the imputed values for a sample of m dataset (not too many or it’s hard to see!) in order to visually assess that our imputed values are comparabe to the observed values.\n\n# Plot imputed values for m = 5 data set\nimpy &lt;- mice(data, seed = 1, print = FALSE, m = 5)\n\nWarning: Number of logged events: 285\n\nmodel_impy &lt;- with(imp, attachchange ~ trtgroup)\n\n# This is cool, here's how I can plot the imputed values for attach change \nxyplot(impy, attachchange ~ trtgroup,\n       main = \"Example of Imputed Values for m = 5 Dataset\",\n       auto.key = list(space = \"right\",\n        points = TRUE, \n        text = c(\"Observed\", \"Missing\"), col = c(\"blue4\", \"red3\")))\n\n\n\n\n\n\n\n\nWhat about for the same model but with placebo as the reference group?\n\n# Pool the parameters of interest for a regression on the imputed values for attach change placebo as reference\nmodel_imp &lt;- data %&gt;%\n  mice(seed = 1, print = FALSE, m = 20) %&gt;%\n  with(lm(attachchange ~ control + low + medium + high)) %&gt;%\n  pool()\n\nWarning: Number of logged events: 1158\n\n# Compare coefficients for complete case analysis with attachment with placebo as reference category\nsummary(model_imp)\n\n         term    estimate  std.error  statistic       df   p.value\n1 (Intercept) -0.06952890 0.06697153 -1.0381859 75.81298 0.3024834\n2     control -0.11582043 0.09076258 -1.2760813 91.37003 0.2051611\n3         low  0.04502973 0.11027780  0.4083299 41.79227 0.6851161\n4      medium  0.04544952 0.11870540  0.3828766 33.28878 0.7042453\n5        high -0.02279333 0.16350701 -0.1394028 16.62753 0.8908064\n\nsummary(model_attach2)$coefficients\n\n               Estimate Std. Error    t value   Pr(&gt;|t|)\n(Intercept) -0.08707309 0.05590110 -1.5576275 0.12254523\ncontrol     -0.13461856 0.07905610 -1.7028232 0.09177091\nlow          0.06925934 0.08091650  0.8559360 0.39412117\nmedium       0.08051695 0.08196711  0.9823081 0.32836700\nhigh        -0.07771793 0.08727557 -0.8904889 0.37538435\n\n\nThe estimates are more comparable, although still pretty different. Still not significant with both methods.\nAnd finally for pocket depth change.\n\n# Pool the parameters of interest for a regression on the imputed values for pocket depth  change placebo as reference\nmodel_imp &lt;- data %&gt;%\n  mice(seed = 1, print = FALSE, m = 20) %&gt;%\n  with(lm(pdchange ~ placebo + low + medium + high)) %&gt;%\n  pool()\n\nWarning: Number of logged events: 1158\n\n# Compare coefficients with complete case analysis for pocket depth change\nMI &lt;- summary(model_imp)\nMI\n\n         term    estimate  std.error  statistic       df      p.value\n1 (Intercept) -0.33597389 0.06144440 -5.4679330 66.87925 7.296278e-07\n2     placebo -0.00982096 0.08061544 -0.1218248 92.74234 9.033013e-01\n3         low  0.11415425 0.07995226  1.4277802 96.05307 1.565984e-01\n4      medium  0.11260463 0.08900896  1.2650932 60.47912 2.106907e-01\n5        high -0.02704497 0.10936047 -0.2473011 30.38938 8.063384e-01\n\n\nHere the models are a LOT more comparable, although still not significant. It seems we can conclude that missing values impact the attachment loss measurements more than pocket depth change.\nLet’s finish by plotting the imputed values for pocket depth change\n\n# Plot imputed values for m = 5 data set\nimpy &lt;- mice(data, seed = 1, print = FALSE, m = 5)\n\nWarning: Number of logged events: 285\n\nmodel_impy &lt;- with(imp, pdchange ~ trtgroup)\n\n# This is cool, here's how I can plot the imputed values for attach change \nxyplot(impy, pdchange ~ trtgroup,\n       main = \"Example of Imputed Values for m = 5 Dataset\",\n       auto.key = list(space = \"right\",\n        points = TRUE, \n        text = c(\"Observed\", \"Missing\"), col = c(\"blue4\", \"red3\")))\n\n\n\n\n\n\n\n\nLooks pretty good.\nBack to top of tabset"
  },
  {
    "objectID": "Project_1_Regression/Project_1_Regression_R/Project_1_Regression.html#Jackknife",
    "href": "Project_1_Regression/Project_1_Regression_R/Project_1_Regression.html#Jackknife",
    "title": "Advanced Data Analysis - Project 1",
    "section": "Identify Outliers using Jackknife Residuals",
    "text": "Identify Outliers using Jackknife Residuals\nIn this section we will explore how our analysis would have changed if we identified outliers using the jackknife residuals.\n\nBackgroundIdentifying OutliersRerunning the Model with Outliers Removed\n\n\n*Note: This information is from BIOS 6602 Week 6 Lecture 10\nJackknife residuals are a type of residual used in regressions that allows us to detect outliers that have a significant impact on the regression coefficients of the model.\nJackknife residuals are calculated by systematically leaving out one observation (i) at a time from a dataset, fitting the regression model to the remaining dataset, and predicting the left out observation. The residual for each observation is then computed based on that prediction.\nThis process follows three step:\n\nFit the regression model to the data, excluding observation (i)\nUse the model to predict the left-out observation (i)\nCalculate the jackknife residual as the difference between the actual value and that fitted value\n\nJackknife residuals are similar to standardized residuals. Standardized residuals are standardized by the standard error of the regression, and are primarily used for identifying outliers. Jackknife residuals on the other hand allow you to assess the leverage of individuals data points, to see how much they actually influence the model (Source)\n\nJackknife residuals follow exactly a t(n-p-2) distribution, where approximately 5% of residuals are expected to exceed 1.96 in absolute value. Jackknife residuals make suspicious values more obvious compared to other residuals.\nDefinitions vary, but we generally consider a residual to be an outlier if the jackknife residual is +- 3.\nHowever, a potential outlier value may not actually have that dramatic of an impact on the model (which is what we are concerned that outliers will do). That is why we use jackknife residuals to investigate leverage and influence.\n\nExtreme X values can have high leverage.\nExtreme X values can have high influence\nA high-leverage point becomes an influential point if its Y value doesn’t follow the pattern of the rest of the data (i.e. is too low or too high)\n\n\nAn observation is influential if removing it substantially changes the estimate of the coefficients for that model. We use five measurements to assess influence: Jackknife residuals, Leverage, Cook’s Distance (Cook’s D), DFFITS, and DFBETAS.\n\nLeverage: Measures how far a measurement deviates from the mean. You want to examine values greater than 2(p+1)/n\nCook’s D: Measures the influence of an observation on regression predictions. You want to examine observations with d_i &gt; 1.0\nDFFITS: Measures the influence of an observation on regression predictions (related to Cook’s D). You want to examine observations outside the range of +-2(sqrt(p+10n)). If h_i is near zero, then that observation has little effect.\nDDFBETAS: Measure the influence of an observation on *individual* coefficient estimates. You want to examine estimates outside the range of +- 2/sqrt(n). A large DFBETA for variable k indicates that the i-th observation has a sizeable impact on the k-th regression coefficient.\n\nNote: p = the number of variables in the model\n\nBack to top of tabset\n\n\nThis website was used for coding information for this section, alongside the notes from BIOS 6602\nNow that we have covered the background for jackknife residuals, we can apply that knowledge to assess for outliers in our dataset for this project!\n\nAttachment LossPocket Depth\n\n\n\nSet upJackknife ResidualsLeverageCook’s DDFFITSDFBETASAdditional PlotsFinal Look and Conclusion\n\n\nRecall that we computed the jackknife residuals earlier.\n\n# Calculate the jackknife residuals of model_attach\njackknife_residuals_attach &lt;- rstudent(model_attach1)\n\n# Calculate the jackknife residuals of model_pd\njackknife_residuals_pd &lt;- rstudent(model_pd)\n\nLet’s generate a table so we can handily compare: ID, jackknife residual, leverage (diagonal hat), DFFITS, and DFBETAs for each participant.\n\n# Get leverage values (hat values)\nhat_values_attach &lt;- hatvalues(model_attach1)\n\n# Get Cook's D values\ncooks_d_attach &lt;- cooks.distance(model_attach1)\n\n# Get the DFFITS values\ndffits_attach &lt;- dffits(model_attach1)\n\n# Get the DFBETAS\ndfbetas_attach &lt;- dfbetas(model_attach1)\n\n# Make a table with ID and all diagnostic values\ndiagnostics_attach &lt;- data.frame(id = data_missing$id, jackknife_residuals = jackknife_residuals_attach, leverage = hat_values_attach, cooks_D = cooks_d_attach, dffits = dffits_attach, dfbetas = dfbetas_attach)\n\nkable(head(diagnostics_attach), caption = \"Diagnostics for Attachment Loss Change Model\", format = \"html\") %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\"))\n\n\nDiagnostics for Attachment Loss Change Model\n\n\nid\njackknife_residuals\nleverage\ncooks_D\ndffits\ndfbetas..Intercept.\ndfbetas.placebo\ndfbetas.low\ndfbetas.medium\ndfbetas.high\n\n\n\n\n101\n0.5800959\n0.0500000\n0.0035664\n0.1330831\n0.0000000\n0.0000000\n0.0000000\n0.0973313\n0.0000000\n\n\n103\n1.5996714\n0.0434783\n0.0228989\n0.3410511\n0.3410511\n-0.2411595\n-0.2356149\n-0.2325949\n-0.2184475\n\n\n104\n1.4044559\n0.0476190\n0.0195311\n0.3140459\n0.0000000\n0.0000000\n0.2270548\n0.0000000\n0.0000000\n\n\n105\n-0.8928811\n0.0434783\n0.0072626\n-0.1903629\n0.0000000\n-0.1346069\n0.0000000\n0.0000000\n0.0000000\n\n\n106\n-1.6995151\n0.0500000\n0.0298289\n-0.3898955\n0.0000000\n0.0000000\n0.0000000\n-0.2851530\n0.0000000\n\n\n107\n-2.7107885\n0.0476190\n0.0690131\n-0.6061507\n0.0000000\n0.0000000\n-0.4382463\n0.0000000\n0.0000000\n\n\n\n\n\n\n\nBack to top of tabset\n\n\nWe can start by making a plot of the jackknife residuals to assess if there are outliers &lt; -3 or &gt; 3 SDs).\n\n# Plot jackknife outiers and label any values that are &gt; 3 or &lt; -3.\nggplot(data_missing, aes(x = id, y = jackknife_residuals_attach)) + geom_point() + \n  geom_hline(yintercept = c(-3,0,3)) + ylim(-4,4) + \n  labs(y = \"Jackknife Residuals\",\n       title = \"Jackknife Residuals vs ID\") + \n  geom_text(aes(label = ifelse(jackknife_residuals_attach &gt; 3 | jackknife_residuals_attach &lt; -3, id, '')), \n            vjust = -1)\n\n\n\n\n\n\n\n\nParticipant 168 is a potential outlier based on the residual value.\nLet’s look at this participant more closely.\n\n# Examine participant 168 to assess what values could make them an outlier\ndata_missing[data_missing$id == 168,]\n\n    id trtgroup gender race      age smoker sites attachbase attach1year\n58 168        5      2    5 54.20397      0   168   5.089286    4.041667\n     pdbase  pd1year attachchange   pdchange placebo control low medium high\n58 3.410714 2.904762    -1.047619 -0.5059524       0       0   0      0    1\n   trt trt3groups\n58   1          3\n\n# Sort our dataset by descending order of attachment at baseline to see if participant 168 is the highest\n\nkable(head(data_missing[order(-data_missing$attachbase),]), caption = \"Diagnostics for Attachment Loss Change Model\", format = \"html\") %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\"))\n\n\nDiagnostics for Attachment Loss Change Model\n\n\n\nid\ntrtgroup\ngender\nrace\nage\nsmoker\nsites\nattachbase\nattach1year\npdbase\npd1year\nattachchange\npdchange\nplacebo\ncontrol\nlow\nmedium\nhigh\ntrt\ntrt3groups\n\n\n\n\n58\n168\n5\n2\n5\n54.20397\n0\n168\n5.089286\n4.041667\n3.410714\n2.904762\n-1.0476190\n-0.5059524\n0\n0\n0\n0\n1\n1\n3\n\n\n3\n104\n3\n2\n5\n55.17864\n1\n138\n4.956522\n5.304348\n5.217391\n4.891304\n0.3478261\n-0.3260870\n0\n0\n1\n0\n0\n1\n3\n\n\n40\n144\n2\n2\n2\n49.07598\n0\n126\n4.388889\n3.488000\n3.666667\n3.230159\n-0.9008889\n-0.4365079\n0\n1\n0\n0\n0\n0\n2\n\n\n102\n269\n3\n1\n5\n64.90075\n0\n162\n4.080247\n3.685185\n3.370370\n3.265432\n-0.3950617\n-0.1049383\n0\n0\n1\n0\n0\n1\n3\n\n\n18\n121\n4\n2\n5\n54.49692\n1\n138\n4.014493\n3.825000\n3.608696\n3.783333\n-0.1894928\n0.1746377\n0\n0\n0\n1\n0\n1\n3\n\n\n17\n120\n3\n2\n5\n41.83710\n1\n150\n3.886667\n3.920000\n4.200000\n3.880000\n0.0333333\n-0.3200000\n0\n0\n1\n0\n0\n1\n3\n\n\n\n\n\n\n\nWe have confirmed that participant 168 has the largest values for attachment loss at baseline (5.09). If that’s the case however, participant 3 is not far behind them (4.96) (who incidentally has the highest baseline pocket depth measurement), followed by participant 40 a large amount lower (4.39).\nWe can also (apparently) run a test statistic to test if we have an outlier using the ‘car’ package\n\noutlierTest(model_attach1)\n\nNo Studentized residuals with Bonferroni p &lt; 0.05\nLargest |rstudent|:\n    rstudent unadjusted p-value Bonferroni p\n58 -3.602896         0.00049863     0.051359\n\n\nThis is a test of a hypothesis that we do not have an outlier. We reject that hypothesis (p &lt; 0.05) so we have an outlier (I think).\nBack to top of tabset\n\n\nLeverage is a measure of geometric distance of an observation’s predictor point from the center point of the predictor space. In other words, leverage is a measurement of how far that observation deviates from the mean of that variable. High leverage observations have the potential to be very influential, but they are not necessarily influential. It’s possible for a high leverage point to not be influential, but very difficult for a low leverage point to be influential.\nLeverage is calculated as\n&lt;img src=“Media/leverage.png” width=“90%&gt;\nIf X _i is close to Xbar, then h_i is small (i.e. the i-th point has low leverage).\n\n# Caculate cutoff for leverage\ncutoff_leverage &lt;- 2*((4+1)/103)\ncutoff_leverage\n\n[1] 0.09708738\n\n# Make leverage plot in ggplot. Set a horizontal line at the cutoff point, and label ID's for participants that pass that line\nggplot(diagnostics_attach, aes(x = id, y = leverage)) +\n  geom_point() + \n  geom_hline(yintercept = cutoff_leverage) +\n  labs(title = \"Leverage Plot with Cutoff Line\",\n       x = \"ID\",\n       y = \"Hat Values\") + \n  geom_text(aes(label = ifelse(leverage &gt; cutoff_leverage, id, \"\")), vjust = -0.5)\n\n\n\n\n\n\n\n\nNone of our data points pass the cutoff point for leverage/hat values. I think this may be because the IV is categorical which changes the pattern and interpretation. Will check with a professor.\nBack to top of tabset\n\n\nCook’s D combines information about the residuals and leverage into a single value. A higher Cook’s D value signifies that the data point woud greatly change the regression coefficients, and is therefore influential and may impact the model’s accuracy.\nApparently we actually want to look at a cutoff of Cook’s D as 4/(n-p-1). Let’s calculate that and store it.\n\n# Calculate cutoff for Cook's D\ncutoff_d &lt;- 4/((nrow(data_missing) - length(model_attach1$coefficients) - 1))\ncutoff_d\n\n[1] 0.04123711\n\n# Make Cook's D plot in ggplot. Set a horizontal line at the cutoff point, and label ID's for participants that pass that line\nggplot(diagnostics_attach, aes(x = id, y = cooks_D)) +\n  geom_point() + \n  geom_hline(yintercept = cutoff_d) +\n  labs(title = \"Cook's D with Cutoff Line\",\n       x = \"ID\",\n       y = \"Cook's D\") + \n  geom_text(aes(label = ifelse(cooks_D &gt; cutoff_d, id, \"\")), vjust = -0.5)\n\n\n\n\n\n\n\n\nWe see that participants 107, 144, 146 and 168 are past the cutoff for Cook’s D. As when looking at the residuals plot, participant 168 is drastically different compared to the other potential outliers.\nBack to top of tabset\n\n\nNow we will examine DFFITS (Difference in fits). DFFITS is a measure used to identify influential data points in a regression analysis. It quantifes how much the predicted values (Y) of a model change when that particular observation is left out from the analysis. So in this case it is the difference in attachment loss change if we take out observation i from the analysis.\nLet’s make a plot where we label the values that are past the cutoff.\n\n# Calculate the cutoff for DFFITS\ncutoff_dffits &lt;- 2 * sqrt((4+1)/103)\ncutoff_dffits\n\n[1] 0.4406526\n\n# Make DFFITS plot in ggplot. Set a horizontal line at the cutoff points, and label ID's for participants that pass those lines.\nggplot(diagnostics_attach, aes(x = id, y = dffits)) + \n  geom_point() +\n  geom_hline(yintercept = c(cutoff_dffits, -cutoff_dffits)) + \n  geom_text(aes(label = ifelse(dffits &lt; -cutoff_dffits | dffits &gt; cutoff_dffits, id, \"\")), vjust = -0.5) +\n  labs(title = \"DFFITS by ID\",\n       x = \"ID\",\n       y = \"DFFITS\")\n\n\n\n\n\n\n\n\nAgain we are flagging participants 107, 113, 144, 168. The new addition is 113, who is right on the cutoff line. Again participant 168 is the most egregious, and the other potential outliers are a lot closer to the cutoff.\nBack to top of tabset\n\n\nNow we can look at the DFBETAs (Difference in Betas). DFBETAS quantify how much the beta coefficients for each variable in the model change when you exclude observation i from the analysis.\n\n# Calculate the cutoff for DFBETAS\ncutoff_dfbetas &lt;- 2/sqrt(103)\ncutoff_dfbetas\n\n[1] 0.1970659\n\n# Make DFBETAS plot in ggplot. Set a horizontal line at the cutoff points, and label ID's for participants that pass those lines.\n\n# For placebo group\nggplot(diagnostics_attach, aes(x = id, y = dfbetas.placebo)) + \n  geom_point() +\n  geom_hline(yintercept = c(cutoff_dfbetas, -cutoff_dfbetas)) + \n  geom_text(aes(label = ifelse(dfbetas.placebo &lt; -cutoff_dfbetas | dfbetas.placebo &gt; cutoff_dfbetas, id, \"\")), vjust = -0.5) +\n  labs(title = \"DFBETAS by ID for Placebo\",\n       x = \"ID\",\n       y = \"DFBETA\")\n\n\n\n\n\n\n\n# For low dose group\nggplot(diagnostics_attach, aes(x = id, y = dfbetas.low)) + \n  geom_point() +\n  geom_hline(yintercept = c(cutoff_dfbetas, -cutoff_dfbetas)) + \n  geom_text(aes(label = ifelse(dfbetas.low &lt; -cutoff_dfbetas | dfbetas.low &gt; cutoff_dfbetas, id, \"\")), vjust = -0.5) +\n  labs(title = \"DFBETAS by ID for Low Dose\",\n       x = \"ID\",\n       y = \"DFBETA\")\n\n\n\n\n\n\n\n# For medium dose group\nggplot(diagnostics_attach, aes(x = id, y = dfbetas.medium)) + \n  geom_point() +\n  geom_hline(yintercept = c(cutoff_dfbetas, -cutoff_dfbetas)) + \n  geom_text(aes(label = ifelse(dfbetas.medium &lt; -cutoff_dfbetas | dfbetas.medium &gt; cutoff_dfbetas, id, \"\")), vjust = -0.5) +\n  labs(title = \"DFBETAS by ID for Medium Dose\",\n       x = \"ID\",\n       y = \"DFBETA\")\n\n\n\n\n\n\n\n# For high does group\nggplot(diagnostics_attach, aes(x = id, y = dfbetas.high)) + \n  geom_point() +\n  geom_hline(yintercept = c(cutoff_dfbetas, -cutoff_dfbetas)) + \n  geom_text(aes(label = ifelse(dfbetas.high &lt; -cutoff_dfbetas | dfbetas.high &gt; cutoff_dfbetas, id, \"\")), vjust = -0.5) +\n  labs(title = \"DFBETAS by ID for High Dose\",\n       x = \"ID\",\n       y = \"DFBETA\")\n\n\n\n\n\n\n\n\nThat’s a lot of points that are past the cutoff! I think for these plots it might matter less if we’re pass that point. I think the idea here is we get a fine tune look at, for instance, how participant 168 drastically changes the beta for high, compared to how much any other point changes the betas. Other possible values of concern are 144 and 146. All the other data points are still roughly around the cutoff of .20, but 168 is around .7!\nBack to top of tabset\n\n\nThe car package in R has some handy plots that we can use to help us with diagnosing outliers.\nThe first is the influence plot, which essentially combines the jackknife residuals plot with the Cook’s D plot.\n\n# Influence plot\ninfluencePlot(model_attach1, main = \"Influence Plot\",\n              sub = \"Circle size is proportional to Cook's Distance\")\n\n\n\n\n\n\n\n\n      StudRes        Hat       CookD\n6  -2.7107885 0.04761905 0.069013121\n8  -0.5238664 0.06250000 0.003686439\n12  1.4073671 0.06250000 0.026147438\n58 -3.6028957 0.06250000 0.154223694\n\n\nUsing this plot, we can see that obseration 58 (ID 168) has the largest residual value and Cook’s D value by a large margin!\nAdditionally, a single line of code provides us with some useful diagnostic plots.\n\n# infIndexPlot gives us a series of plots that we need to investigate influence points\ninfIndexPlot(model_attach1)\n\n\n\n\n\n\n\n\nThe first plot is the Cook’s D plot, the second plot is the studentized residuals plot, and the leverage/hat values plot, all of which we plotted before. New is the third plot which is the Bonferroni P-value plot.\nBased on these plots we can see pretty readily that participant 168 is a true outlier. That is, they are a point that has high leverage AND influence in this model.\nBack to top of tabset\n\n\nWe saw that participant 168 is the most egregious offender, and participants 107, 144, 146, and MAYBE 113 are flagging past our different cutoff points.\nLet’s take one last look at the table of these participants\n\n# Pretty print diagnostics table of potential outliers\nkable(diagnostics_attach[diagnostics_attach$id %in% c(107,113,144,146,168),], caption = \"Correlation Matrix\", format = \"html\") %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\"))\n\n\nCorrelation Matrix\n\n\n\nid\njackknife_residuals\nleverage\ncooks_D\ndffits\ndfbetas..Intercept.\ndfbetas.placebo\ndfbetas.low\ndfbetas.medium\ndfbetas.high\n\n\n\n\n6\n107\n-2.710789\n0.0476190\n0.0690131\n-0.6061507\n0.0000000\n0.0000000\n-0.4382463\n0.0000000\n0.0000000\n\n\n11\n113\n2.092587\n0.0434783\n0.0384816\n0.4461411\n0.0000000\n0.3154694\n0.0000000\n0.0000000\n0.0000000\n\n\n40\n144\n-2.670168\n0.0434783\n0.0610008\n-0.5692818\n-0.5692818\n0.4025431\n0.3932880\n0.3882470\n0.3646322\n\n\n42\n146\n-2.291912\n0.0434783\n0.0457672\n-0.4886373\n-0.4886373\n0.3455188\n0.3375748\n0.3332479\n0.3129784\n\n\n58\n168\n-3.602896\n0.0625000\n0.1542237\n-0.9302637\n0.0000000\n0.0000000\n0.0000000\n0.0000000\n-0.7143938\n\n\n\n\n\n\n\nAcross the board we can see that participant 168 has worse values for almost everything, in particular the jackknife residual, Cook’s D, and DFFITS values. I can confidently conclude that we should remove participant 168 as an outlier, and feel justified in keeping participants 107, 113, 144, and 146 based on the closeness to the rest of the data points on the previous plots.\n\n\n\n\n\n\nSet UpJackknife ResidualsLeverageCook’s DDFFITSDFBETASAdditional PlotsFinal Look and Conclusion\n\n\nLet’s generate a table so we can handily compare: ID, jackknife residual, leverage (diagonal hat), DFFITS, and DFBETAs for each participant.\n\n# Get leverage values (hat values)\nhat_values_pd &lt;- hatvalues(model_pd)\n\n# Get Cook's D values\ncooks_d_pd &lt;- cooks.distance(model_pd)\n\n# Get the DFFITS values\ndffits_pd &lt;- dffits(model_pd)\n\n# Get the DFBETAS\ndfbetas_pd &lt;- dfbetas(model_pd)\n\n# Make a table with ID and all diagnostic values\ndiagnostics_pd &lt;- data.frame(id = data_missing$id, jackknife_residuals = jackknife_residuals_pd, leverage = hat_values_pd, cooks_D = cooks_d_pd, dffits = dffits_pd, dfbetas = dfbetas_pd)\n\nkable(head(diagnostics_pd), caption = \"Diagnostics for Attachment Loss Change Model\", format = \"html\") %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\"))\n\n\nDiagnostics for Attachment Loss Change Model\n\n\nid\njackknife_residuals\nleverage\ncooks_D\ndffits\ndfbetas..Intercept.\ndfbetas.placebo\ndfbetas.low\ndfbetas.medium\ndfbetas.high\n\n\n\n\n101\n1.4284091\n0.0500000\n0.0212518\n0.3276995\n0.0000000\n0.0000000\n0.0000000\n0.2396655\n0.0000000\n\n\n103\n1.3517762\n0.0434783\n0.0164727\n0.2881997\n0.2881997\n-0.2037879\n-0.1991025\n-0.1965505\n-0.1845955\n\n\n104\n-0.4668576\n0.0476190\n0.0021971\n-0.1043925\n0.0000000\n0.0000000\n-0.0754757\n0.0000000\n0.0000000\n\n\n105\n-0.4451251\n0.0434783\n0.0018161\n-0.0949010\n0.0000000\n-0.0671051\n0.0000000\n0.0000000\n0.0000000\n\n\n106\n-2.5107398\n0.0500000\n0.0629491\n-0.5760032\n0.0000000\n0.0000000\n0.0000000\n-0.4212642\n0.0000000\n\n\n107\n-1.7949702\n0.0476190\n0.0315049\n-0.4013675\n0.0000000\n0.0000000\n-0.2901882\n0.0000000\n0.0000000\n\n\n\n\n\n\n\nBack to top of tabset\n\n\nWe can start by making a plot of the jackknife residuals to assess if there are outliers &lt; -3 or &gt; 3 SDs).\n\n# Plot jackknife outiers and label any values that are &gt; 3 or &lt; -3.\nggplot(data_missing, aes(x = id, y = jackknife_residuals_pd)) + geom_point() + \n  geom_hline(yintercept = c(-3,0,3)) + ylim(-4,4) + \n  labs(y = \"Jackknife Residuals\",\n       title = \"Jackknife Residuals vs ID\") + \n  geom_text(aes(label = ifelse(jackknife_residuals_pd &gt; 3 | jackknife_residuals_pd &lt; -3, id, '')), \n            vjust = -1)\n\n\n\n\n\n\n\n\nWe have no jackknife residuals +- 3. That’s a good sign!\nLet’s sort our dataset and see who has the highest pocket depth at baseline.\n\n# Sort our dataset by descending order of pocket depth at baseline to see who is the highest\nkable(head(data_missing[order(-data_missing$pdbase),]), caption = \"Diagnostics for Attachment Loss Change Model\", format = \"html\") %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\"))\n\n\nDiagnostics for Attachment Loss Change Model\n\n\n\nid\ntrtgroup\ngender\nrace\nage\nsmoker\nsites\nattachbase\nattach1year\npdbase\npd1year\nattachchange\npdchange\nplacebo\ncontrol\nlow\nmedium\nhigh\ntrt\ntrt3groups\n\n\n\n\n3\n104\n3\n2\n5\n55.17864\n1\n138\n4.956522\n5.304348\n5.217391\n4.891304\n0.3478261\n-0.3260870\n0\n0\n1\n0\n0\n1\n3\n\n\n20\n124\n2\n2\n5\n41.01574\n1\n162\n2.901235\n2.808642\n4.771605\n4.067901\n-0.0925926\n-0.7037037\n0\n1\n0\n0\n0\n0\n2\n\n\n17\n120\n3\n2\n5\n41.83710\n1\n150\n3.886667\n3.920000\n4.200000\n3.880000\n0.0333333\n-0.3200000\n0\n0\n1\n0\n0\n1\n3\n\n\n77\n234\n1\n2\n5\n51.12115\n0\n144\n2.756944\n2.527778\n4.083333\n3.631944\n-0.2291667\n-0.4513889\n1\n0\n0\n0\n0\n0\n1\n\n\n5\n106\n4\n2\n5\n42.14921\n1\n168\n2.369048\n1.922619\n3.910714\n3.083333\n-0.4464286\n-0.8273810\n0\n0\n0\n1\n0\n1\n3\n\n\n6\n107\n3\n2\n5\n37.15811\n1\n156\n3.544872\n2.839744\n3.897436\n3.237179\n-0.7051282\n-0.6602564\n0\n0\n1\n0\n0\n1\n3\n\n\n\n\n\n\n\nParticipant 104 has the highest baseline pocket depth. Interestingly it is not 168, who was our outlier for attachment loss.\nLet’s run that test we did earlier to see if there’s an outlier in the pocket depth model.\n\noutlierTest(model_pd)\n\nNo Studentized residuals with Bonferroni p &lt; 0.05\nLargest |rstudent|:\n   rstudent unadjusted p-value Bonferroni p\n82 2.664155          0.0090393      0.93105\n\n\nThis is a test of a hypothesis that we do not have an outlier. We fail to reject the null and thus have more evidence that we do not have outliers for the pocket depth model.\nBack to top of tabset\n\n\nLet’s plot leverage for our model on pocket depth change.\n\n# Caculate cutoff for leverage\ncutoff_leverage &lt;- 2*((4+1)/103)\ncutoff_leverage\n\n[1] 0.09708738\n\n# Make leverage plot in ggplot. Set a horizontal line at the cutoff point, and label ID's for participants that pass that line\nggplot(diagnostics_pd, aes(x = id, y = leverage)) +\n  geom_point() + \n  geom_hline(yintercept = cutoff_leverage) +\n  labs(title = \"Leverage Plot with Cutoff Line\",\n       x = \"ID\",\n       y = \"Hat Values\") + \n  geom_text(aes(label = ifelse(leverage &gt; cutoff_leverage, id, \"\")), vjust = -0.5)\n\n\n\n\n\n\n\n\nAgain we are good on leverage.\nBack to top of tabset\n\n\nLet’s make our plot for Cook’s D for pocket depth change.\n\n# Calculate cutoff for Cook's D\ncutoff_d &lt;- 4/((nrow(data_missing) - length(model_attach1$coefficients) - 1))\ncutoff_d\n\n[1] 0.04123711\n\n# Make Cook's D plot in ggplot. Set a horizontal line at the cutoff point, and label ID's for participants that pass that line\nggplot(diagnostics_pd, aes(x = id, y = cooks_D)) +\n  geom_point() + \n  geom_hline(yintercept = cutoff_d) +\n  labs(title = \"Cook's D with Cutoff Line\",\n       x = \"ID\",\n       y = \"Cook's D\") + \n  geom_text(aes(label = ifelse(cooks_D &gt; cutoff_d, id, \"\")), vjust = -0.5)\n\n\n\n\n\n\n\n\nWe see that participants 106, 118, and 239 have Cook’s D values past the cutoff. Interestingly participant 104 who had the highest baseline pocket depth did not flag here.\nBack to top of tabset\n\n\nLet’s make the DFFITS plot for pocket depth change.\n\n# Calculate the cutoff for DFFITS\ncutoff_dffits &lt;- 2 * sqrt((4+1)/103)\ncutoff_dffits\n\n[1] 0.4406526\n\n# Make DFFITS plot in ggplot. Set a horizontal line at the cutoff points, and label ID's for participants that pass those lines.\nggplot(diagnostics_pd, aes(x = id, y = dffits)) + \n  geom_point() +\n  geom_hline(yintercept = c(cutoff_dffits, -cutoff_dffits)) + \n  geom_text(aes(label = ifelse(dffits &lt; -cutoff_dffits | dffits &gt; cutoff_dffits, id, \"\")), vjust = -0.5) +\n  labs(title = \"DFFITS by ID\",\n       x = \"ID\",\n       y = \"DFFITS\")\n\n\n\n\n\n\n\n\nParticipants 106, 118, and 239 are flagging here again. In addition we have 137 and 233, which are near enough to the cutoff that we can ignore them.\nBack to top of tabset\n\n\nNow we can look at the DFBETAS plot for pocket depth change.\n\n# Calculate the cutoff for DFBETAS\ncutoff_dfbetas &lt;- 2/sqrt(103)\ncutoff_dfbetas\n\n[1] 0.1970659\n\n# Make DFBETAS plot in ggplot. Set a horizontal line at the cutoff points, and label ID's for participants that pass those lines.\n\n# For placebo group\nggplot(diagnostics_pd, aes(x = id, y = dfbetas.placebo)) + \n  geom_point() +\n  geom_hline(yintercept = c(cutoff_dfbetas, -cutoff_dfbetas)) + \n  geom_text(aes(label = ifelse(dfbetas.placebo &lt; -cutoff_dfbetas | dfbetas.placebo &gt; cutoff_dfbetas, id, \"\")), vjust = -0.5) +\n  labs(title = \"DFBETAS by ID for Placebo\",\n       x = \"ID\",\n       y = \"DFBETA\")\n\n\n\n\n\n\n\n# For low dose group\nggplot(diagnostics_pd, aes(x = id, y = dfbetas.low)) + \n  geom_point() +\n  geom_hline(yintercept = c(cutoff_dfbetas, -cutoff_dfbetas)) + \n  geom_text(aes(label = ifelse(dfbetas.low &lt; -cutoff_dfbetas | dfbetas.low &gt; cutoff_dfbetas, id, \"\")), vjust = -0.5) +\n  labs(title = \"DFBETAS by ID for Low Dose\",\n       x = \"ID\",\n       y = \"DFBETA\")\n\n\n\n\n\n\n\n# For medium dose group\nggplot(diagnostics_pd, aes(x = id, y = dfbetas.medium)) + \n  geom_point() +\n  geom_hline(yintercept = c(cutoff_dfbetas, -cutoff_dfbetas)) + \n  geom_text(aes(label = ifelse(dfbetas.medium &lt; -cutoff_dfbetas | dfbetas.medium &gt; cutoff_dfbetas, id, \"\")), vjust = -0.5) +\n  labs(title = \"DFBETAS by ID for Medium Dose\",\n       x = \"ID\",\n       y = \"DFBETA\")\n\n\n\n\n\n\n\n# For high does group\nggplot(diagnostics_pd, aes(x = id, y = dfbetas.high)) + \n  geom_point() +\n  geom_hline(yintercept = c(cutoff_dfbetas, -cutoff_dfbetas)) + \n  geom_text(aes(label = ifelse(dfbetas.high &lt; -cutoff_dfbetas | dfbetas.high &gt; cutoff_dfbetas, id, \"\")), vjust = -0.5) +\n  labs(title = \"DFBETAS by ID for High Dose\",\n       x = \"ID\",\n       y = \"DFBETA\")\n\n\n\n\n\n\n\n\nAgain we have a lot of points that are past the cutoff here. None as far off as 168 in the attachment loss model, 239 is the only one that is concerning.\nBack to top of tabset\n\n\nThe car package in R has some handy plots that we can use to help us with diagnosing outliers.\nThe first is the influence plot, which essentially combines the jackknife residuals plot with the Cook’s D plot.\n\n# Influence plot\ninfluencePlot(model_pd, main = \"Influence Plot\",\n              sub = \"Circle size is proportional to Cook's Distance\")\n\n\n\n\n\n\n\n\n      StudRes        Hat       CookD\n5  -2.5107398 0.05000000 0.062949101\n8  -0.8158788 0.06250000 0.008905825\n12  0.4184503 0.06250000 0.002354494\n82  2.6641548 0.04761905 0.066819586\n\n\nHere we can see that none of our residuals are +- 3, but we do have some concerns with large Cook’s distance, particularly with observation 82 (ID = 239).\nAdditionally, a single line of code provides us with some useful diagnostic plots.\n\n# infIndexPlot gives us a series of plots that we need to investigate influence points\ninfIndexPlot(model_pd)\n\n\n\n\n\n\n\n\nBased on these plots, it is arguable that participant 239 is an outlier for pocket depth. Though they do not have an extreme jackknife residual, the large Cook’s distance suggests that this data point has substantial infuence on the model’s parameters.\nHowever, a closer look reveals that the influence and leverage values for the pocket depth model are all comparable to the potential outliers in the attachmnet loss model we chose to keep (Cook’s D ~0.06). Specifically, while it appears that participant 239 here has a large Cook’s D (0.068) compared to the rest of the values, it is nowhere near as high as participant 168 was in the attachment loss model (0.15)!\nBack to top of tabset\n\n\nParticipants 106 and 239 were the only potential concerns here. However, as noted, their Cook’s D is comparable to the values we kept in the attachment loss model, and are &lt; 1/2 of what the Cook’s D was for participant 168 who we plan to remove as an outlier in the attachment loss model!\n\n# Pretty print diagnostics table of potential outliers\nkable(diagnostics_pd[diagnostics_attach$id %in% c(106,239),], caption = \"Correlation Matrix\", format = \"html\") %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\"))\n\n\nCorrelation Matrix\n\n\n\nid\njackknife_residuals\nleverage\ncooks_D\ndffits\ndfbetas..Intercept.\ndfbetas.placebo\ndfbetas.low\ndfbetas.medium\ndfbetas.high\n\n\n\n\n5\n106\n-2.510740\n0.050000\n0.0629491\n-0.5760032\n0\n0\n0.0000000\n-0.4212642\n0\n\n\n82\n239\n2.664155\n0.047619\n0.0668196\n0.5957231\n0\n0\n0.4307071\n0.0000000\n0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Remove participant 168 from the dataset\ndata_missing_outlier &lt;- data_missing[data_missing$id != 168,]\n\n# Run model 1 attachment loss\nmodel_attach1_outlier &lt;- lm(attachchange ~ placebo + low + medium + high, data = data_missing_outlier)\nsummary(model_attach1_outlier)\n\n\nCall:\nlm(formula = attachchange ~ placebo + low + medium + high, data = data_missing_outlier)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.68731 -0.16279  0.04936  0.16823  0.53945 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.22169    0.05277  -4.201  5.9e-05 ***\nplacebo      0.13462    0.07463   1.804  0.07435 .  \nlow          0.20388    0.07638   2.669  0.00891 ** \nmedium       0.21514    0.07737   2.780  0.00652 ** \nhigh         0.11576    0.08399   1.378  0.17130    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2531 on 97 degrees of freedom\nMultiple R-squared:  0.09493,   Adjusted R-squared:  0.05761 \nF-statistic: 2.543 on 4 and 97 DF,  p-value: 0.04442\n\n# Run model 2 attachmnent loss\nmodel_attach2_outlier &lt;- lm(attachchange ~ control + low + medium + high, data = data_missing_outlier)\nsummary(model_attach2_outlier)\n\n\nCall:\nlm(formula = attachchange ~ control + low + medium + high, data = data_missing_outlier)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.68731 -0.16279  0.04936  0.16823  0.53945 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept) -0.08707    0.05277  -1.650   0.1022  \ncontrol     -0.13462    0.07463  -1.804   0.0743 .\nlow          0.06926    0.07638   0.907   0.3668  \nmedium       0.08052    0.07737   1.041   0.3006  \nhigh        -0.01886    0.08399  -0.225   0.8228  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2531 on 97 degrees of freedom\nMultiple R-squared:  0.09493,   Adjusted R-squared:  0.05761 \nF-statistic: 2.543 on 4 and 97 DF,  p-value: 0.04442\n\n# Run the pocket depth model\nmodel_pd_outlier &lt;- lm(pdchange ~ control + low + medium + high, data = data_missing_outlier)\nsummary(model_pd_outlier)\n\n\nCall:\nlm(formula = pdchange ~ control + low + medium + high, data = data_missing_outlier)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-0.6248 -0.1513 -0.0129  0.1616  0.6613 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.34969    0.05488  -6.372 6.26e-09 ***\ncontrol      0.01152    0.07761   0.148   0.8823    \nlow          0.14352    0.07943   1.807   0.0739 .  \nmedium       0.14714    0.08046   1.829   0.0705 .  \nhigh        -0.02437    0.08734  -0.279   0.7809    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2632 on 97 degrees of freedom\nMultiple R-squared:  0.07456,   Adjusted R-squared:  0.0364 \nF-statistic: 1.954 on 4 and 97 DF,  p-value: 0.1077\n\n\nThere is no difference in our models after removing outlier 168."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Sean Vieau",
    "section": "",
    "text": "Graduate student in the Masters in Applied Biostatistics program at the Colorado School of Public Health. I have compiled this portfolio to showcase my most significant projects and skills.\nIn my spare time I am an avid dancer, jiu jitsu practitioner, and drummer!"
  },
  {
    "objectID": "Project_1_Regression/Project_1_Regression_SAS/Project_1_Regression_SAS.html",
    "href": "Project_1_Regression/Project_1_Regression_SAS/Project_1_Regression_SAS.html",
    "title": "This is markdown",
    "section": "",
    "text": "# This is code\nprint(\"Hello world\")"
  },
  {
    "objectID": "Project_2/Project_2_R/Code/Project2.html",
    "href": "Project_2/Project_2_R/Code/Project2.html",
    "title": "Advanced Data Analysis - Project 2",
    "section": "",
    "text": "The aim of the current study is to assess how treatment response differs for HIV+ patients 2 years after initiating Highly Active Antiretroviral Therapy (HAART) based on hard drug usage (such as heroin or cocaine). This study is of particular scientific interest because it is unclear whether the use of hard drugs inhibits the immune system in humans; treatment strategies may differ based on these results. The researchers are interested in comparing subjects who never used hard drugs to current hard drug users (those that use hard drugs at year 2) or previous hard drug users (those who used drugs at year 0 or 1). Outcomes of interest are: viral load (HIV copies in a mL of blood), CD4+ T cell count (a measure of immunologic health), and aggregate physical and quality of life scores from the SF-36.\nThe clinical hypothesis is that, if hard drugs inhibit the immune system in humans, subjects who currently or previously used hard drugs will have higher viral load and lower CD4+ T cell counts than those who never used hard drugs. Additionally, the researchers are interested in knowing if potential differences between the drug use groups can be explained by differences in adherence to the treatment regimen. The researchers are agnostic on how quality of life changes after treatment, since side effects of the treatment are significant.\nThe project description provided by the PI is available below:"
  },
  {
    "objectID": "Project_2/Project_2_R/Code/Project2.html#labeling-categorical-variables",
    "href": "Project_2/Project_2_R/Code/Project2.html#labeling-categorical-variables",
    "title": "Advanced Data Analysis - Project 2",
    "section": "Labeling Categorical Variables",
    "text": "Labeling Categorical Variables\nLet’s factor and label our categorical variables so they are appropriately represented (and not doubles, which will yield incorrect results in models)\n\n# Converting all appropriate variables from doubles to categorical variables\n\ndata$HASHV &lt;- factor(data$HASHV,\n                     levels = c(1, 2),\n                     labels = c(\"No\", \"Yes\"))\n\ndata$HASHF &lt;- factor(data$HASHF,\n                     levels = c(0, 1, 2, 3, 4),\n                     labels = c(\"Never\", \"Daily\", \"Weekly\", \"Monthly\", \"Less Often\"))\n\ndata$income &lt;- factor(data$income,\n                      levels = c(1, 2, 3, 4, 5, 6, 7, 9),\n                      labels = c(\"Less than $10,000\", \"$10,000-$19,999\", \"$20,000-$29,999\", \"$30,000-$39,999\", \"$40,000-$49,999\", \"$50,000-$59,999\", \"$60,000 or more\", \"Do not wish to answer\"))\n\ndata$HBP &lt;- factor(data$HBP,\n                   levels = c(1, 2, 3, 4, 9, -1),\n                   labels = c(\"No\", \"Yes\", \"No, based on data trajectory\", \"Yes, based on data trajectory\", \"Insufficient data, may include reported treatment without diagnosis\", \"Improbable Value\"))\n\ndata$DIAB &lt;- factor(data$DIAB,\n                    levels = c(1, 2, 3, 4, 9),\n                    labels = c(\"No\", \"Yes\", \"No, based on data trajectory\", \"Yes, based on data trajectory\", \"Insufficient data\"))\n                      \ndata$LIV34 &lt;- factor(data$LIV34,\n                     levels = c(1, 2, 9),\n                     labels = c(\"No\", \"Yes\", \"Insufficient Data\"))\n\ndata$KID &lt;- factor(data$KID,\n                   levels = c(1, 2, 3, 4, 9),\n                   labels = c(\"No\", \"Yes\", \"No, based on data trajectory\", \"Yes, based on data trajectory\", \"Insufficient data\"))\n\ndata$FRP &lt;- factor(data$FRP,\n                   levels = c(1,2,9),\n                   labels = c(\"No\", \"Yes\", \"Insufficient Data\"))\n\ndata$FP &lt;- factor(data$FP,\n                  levels = c(1,2,9),\n                  labels = c(\"No\", \"Yes\", \"Insufficient Data\"))\n\ndata$DYSLIP &lt;- factor(data$DYSLIP,\n                      levels = c(1, 2, 3, 4, 9),\n                      labels = c(\"No\", \"Yes\", \"No, based on data trajectory\", \"Yes, based on data trajectory\", \"Insufficient data\"))\n\ndata$SMOKE &lt;- factor(data$SMOKE,\n                     levels = c(1, 2, 3),\n                     labels = c(\"Never Smoked\", \"Former Smoker\", \"Current Smoker\"))\n\ndata$DKGRP &lt;- factor(data$DKGRP,\n                     levels = c(0, 1, 2, 3),\n                     labels = c(\"None\", \"1-3 drinks/week\", \"4-13 drinks/week\", \"&gt;13 drinks/week\"))\n\ndata$HEROPIATE &lt;- factor(data$HEROPIATE,\n                         levels = c(1, 2, -9),\n                         labels = c(\"No\", \"Yes\", \"Not Specified\"))\n\ndata$IDU &lt;- factor(data$IDU,\n                   levels = c(1, 2),\n                   labels = c(\"No\", \"Yes\"))\n\ndata$ADH &lt;- factor(data$ADH,\n                   levels = c(1, 2, 3, 4),\n                   labels = c(\"100%\", \"95-99%\", \"75-94%\", \"&lt;75%\"))\n\ndata$RACE &lt;- factor(data$RACE,\n                    levels = c(1, 2, 3, 4, 5, 6, 7),\n                    labels = c(\"White, non-Hispanic\", \"White, Hispanic\", \"Black, non-Hispanic \", \"Black, Hispanic\",  \"American Indian or Alaskan Native\", \"Asian or Pacific Islander\", \"Other Hispanic\"))\n\ndata$EDUCBAS &lt;- factor(data$EDUCBAS,\n                       levels = c(1, 2, 3, 4, 5, 6, 7),\n                       labels = c(\"8th grade or less \", \"9,10, or 11th grade\", \"12th grade\", \"At least one year college but no degree\", \"Four years college / got degree \", \"Some graduate work\", \"Post-graduate degree\"))\n\ndata$hard_drugs &lt;- factor(data$hard_drugs,\n                          levels = c(0, 1),\n                          labels = c(\"No\", \"Yes\"))\n\nLet’s take another look to check that those variables are no longer doubles.\n\n# Examine data\nglimpse(data)\n\nRows: 3,632\nColumns: 33\n$ newid      &lt;dbl&gt; 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4,…\n$ AGG_MENT   &lt;dbl&gt; 44.90710, 58.20754, 59.65136, 56.80657, 46.34190, 48.71791,…\n$ AGG_PHYS   &lt;dbl&gt; 52.52557, 41.29347, 48.54453, 46.73991, 27.92331, 38.03807,…\n$ HASHV      &lt;fct&gt; No, No, No, No, No, Yes, No, Yes, No, Yes, No, No, Yes, No,…\n$ HASHF      &lt;fct&gt; NA, Less Often, Never, Never, Never, Never, Never, Never, N…\n$ income     &lt;fct&gt; \"$30,000-$39,999\", \"$30,000-$39,999\", \"$30,000-$39,999\", \"$…\n$ BMI        &lt;dbl&gt; 24.71756, 26.06801, 27.16421, 25.71786, 26.66936, 25.96576,…\n$ HBP        &lt;fct&gt; \"No\", \"No\", \"No\", \"No\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", …\n$ DIAB       &lt;fct&gt; \"No\", \"No\", \"No\", \"No\", \"Insufficient data\", \"Insufficient …\n$ LIV34      &lt;fct&gt; No, No, No, No, No, No, No, No, No, No, No, No, No, No, Yes…\n$ KID        &lt;fct&gt; \"No\", \"No\", \"No\", \"No\", \"Insufficient data\", \"Insufficient …\n$ FRP        &lt;fct&gt; No, No, No, No, Yes, No, No, No, No, No, No, No, No, No, No…\n$ FP         &lt;fct&gt; No, No, No, No, Insufficient Data, Insufficient Data, No, N…\n$ TCHOL      &lt;dbl&gt; 133, 131, 180, 171, 125, NA, 134, 105, 141, 153, 170, 170, …\n$ TRIG       &lt;dbl&gt; 176, 107, 233, 139, NA, NA, NA, 104, 162, 127, NA, NA, 82, …\n$ LDL        &lt;dbl&gt; 62, 66, 86, 96, NA, NA, NA, 44, 73, 84, NA, NA, 127, NA, NA…\n$ DYSLIP     &lt;fct&gt; \"Yes\", \"No\", \"Yes\", \"No\", \"Yes\", \"Insufficient data\", \"Yes,…\n$ CESD       &lt;dbl&gt; 14, 2, 1, 18, 20, 18, 18, 21, 23, 17, 18, 22, 23, 14, 1, 1,…\n$ SMOKE      &lt;fct&gt; Current Smoker, Current Smoker, Current Smoker, Current Smo…\n$ DKGRP      &lt;fct&gt; None, &gt;13 drinks/week, None, 1-3 drinks/week, None, &gt;13 dri…\n$ HEROPIATE  &lt;fct&gt; No, No, No, No, No, No, No, No, No, No, No, No, No, No, No,…\n$ IDU        &lt;fct&gt; Yes, No, No, No, Yes, No, No, No, No, No, Yes, Yes, Yes, Ye…\n$ LEU3N      &lt;dbl&gt; 104.1659, 262.0061, 345.4010, 292.3271, 257.8278, 459.4562,…\n$ VLOAD      &lt;dbl&gt; 1.020130e+05, 2.700000e+01, 6.000000e+01, 9.000000e+00, 8.1…\n$ ADH        &lt;fct&gt; NA, 95-99%, 100%, 100%, NA, 100%, 100%, 100%, 100%, 100%, N…\n$ RACE       &lt;fct&gt; \"White, non-Hispanic\", \"White, non-Hispanic\", \"White, non-H…\n$ EDUCBAS    &lt;fct&gt; \"At least one year college but no degree\", \"At least one ye…\n$ hivpos     &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ age        &lt;dbl&gt; 52, 53, 54, 55, 54, 55, 56, 60, 61, 62, 47, 48, 49, 51, 52,…\n$ ART        &lt;dbl&gt; 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1,…\n$ everART    &lt;dbl&gt; 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1,…\n$ years      &lt;dbl&gt; 0, 1, 2, 3, 0, 1, 2, 6, 7, 8, 0, 1, 2, 4, 5, 6, 7, 8, 0, 1,…\n$ hard_drugs &lt;fct&gt; Yes, No, No, No, Yes, No, No, No, No, No, Yes, Yes, Yes, Ye…\n\n\nLooks good."
  },
  {
    "objectID": "Project_2/Project_2_R/Code/Project2.html#filtering-data-set",
    "href": "Project_2/Project_2_R/Code/Project2.html#filtering-data-set",
    "title": "Advanced Data Analysis - Project 2",
    "section": "Filtering Data Set",
    "text": "Filtering Data Set\nNow let’s take a look at the header to get a good feeling for our data.\n\n# Pretty print data header\nkable(head(data), format = \"html\", full_width = TRUE) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\"))\n\n\n\n\nnewid\nAGG_MENT\nAGG_PHYS\nHASHV\nHASHF\nincome\nBMI\nHBP\nDIAB\nLIV34\nKID\nFRP\nFP\nTCHOL\nTRIG\nLDL\nDYSLIP\nCESD\nSMOKE\nDKGRP\nHEROPIATE\nIDU\nLEU3N\nVLOAD\nADH\nRACE\nEDUCBAS\nhivpos\nage\nART\neverART\nyears\nhard_drugs\n\n\n\n\n1\n44.90710\n52.52557\nNo\nNA\n$30,000-$39,999\n24.71756\nNo\nNo\nNo\nNo\nNo\nNo\n133\n176\n62\nYes\n14\nCurrent Smoker\nNone\nNo\nYes\n104.1659\n102013\nNA\nWhite, non-Hispanic\nAt least one year college but no degree\n1\n52\n0\n0\n0\nYes\n\n\n1\n58.20754\n41.29346\nNo\nLess Often\n$30,000-$39,999\n26.06801\nNo\nNo\nNo\nNo\nNo\nNo\n131\n107\n66\nNo\n2\nCurrent Smoker\n&gt;13 drinks/week\nNo\nNo\n262.0061\n27\n95-99%\nWhite, non-Hispanic\nAt least one year college but no degree\n1\n53\n1\n1\n1\nNo\n\n\n1\n59.65136\n48.54453\nNo\nNever\n$30,000-$39,999\n27.16421\nNo\nNo\nNo\nNo\nNo\nNo\n180\n233\n86\nYes\n1\nCurrent Smoker\nNone\nNo\nNo\n345.4010\n60\n100%\nWhite, non-Hispanic\nAt least one year college but no degree\n1\n54\n1\n1\n2\nNo\n\n\n1\n56.80657\n46.73991\nNo\nNever\n$40,000-$49,999\n25.71786\nNo\nNo\nNo\nNo\nNo\nNo\n171\n139\n96\nNo\n18\nCurrent Smoker\n1-3 drinks/week\nNo\nNo\n292.3271\n9\n100%\nWhite, non-Hispanic\nAt least one year college but no degree\n1\n55\n1\n1\n3\nNo\n\n\n2\n46.34190\n27.92331\nNo\nNever\n$10,000-$19,999\n26.66936\nYes\nInsufficient data\nNo\nInsufficient data\nYes\nInsufficient Data\n125\nNA\nNA\nYes\n20\nCurrent Smoker\nNone\nNo\nYes\n257.8278\n8121\nNA\nBlack, non-Hispanic\n9,10, or 11th grade\n1\n54\n0\n0\n0\nYes\n\n\n2\n48.71791\n38.03807\nYes\nNever\nLess than $10,000\n25.96576\nYes\nInsufficient data\nNo\nInsufficient data\nNo\nInsufficient Data\nNA\nNA\nNA\nInsufficient data\n18\nCurrent Smoker\n&gt;13 drinks/week\nNo\nNo\n459.4562\n21\n100%\nBlack, non-Hispanic\n9,10, or 11th grade\n1\n55\n1\n1\n1\nNo\n\n\n\n\n\n\n\nHmm, we have 8 years worth of data points, but the experimenters are only interested in the first 2 years.\nOut of curiosity, let’s look at how many participants they had each year.\n\n# Visualize patient drop off over 8 years of study\nbarplot(table(data$years))\n\n\n\n\n\n\n\n# Check number of patients in each year\nkable(table(data$years), format = \"html\", full_width = FALSE) %&gt;%\n  kable_styling(bootstrap_options = c(\"condensed\", \"hover\", \"striped\"))\n\n\n\n\nVar1\nFreq\n\n\n\n\n0\n550\n\n\n1\n550\n\n\n2\n550\n\n\n3\n414\n\n\n4\n381\n\n\n5\n338\n\n\n6\n325\n\n\n7\n272\n\n\n8\n252\n\n\n\n\n\n\n\nThis is interesting, we don’t seem to have as drastic a drop off as I expected. The researchers managed to retain all participants for the first 2 years, and 50% by the end of the 8-year study.\nLet’s filter to only include values from the first 2 years, as this is the timeframe the researchers are interested in.\n\n# Filter long form data set to be include only first 2 years\ndata_2 &lt;- data[data$years &lt;= 2,]\n\n# Check how many visits we have in the filtered data set.\ndim(data_2)\n\n[1] 1650   33\n\n\nWe went from 3632 visits in the 8 year data set to 1650 in the 2 year filtered data set.\n\n# Double check if any patients dropped out within the first 2 years\nany(is.na(data_2$years))\n\n[1] FALSE\n\n\nLuckily, no patients dropped out within the first 2 years of the study!"
  },
  {
    "objectID": "Project_2/Project_2_R/Code/Project2.html#transpose-to-wideform",
    "href": "Project_2/Project_2_R/Code/Project2.html#transpose-to-wideform",
    "title": "Advanced Data Analysis - Project 2",
    "section": "Transpose to Wideform",
    "text": "Transpose to Wideform\nWe can also see that the provided data set is in longform. Let’s convert that to wideform.\n\n# Create new wideform data set for first 2 years of study              \ndata_wide_2 &lt;- pivot_wider(data_2, id_cols = newid, names_from = years, values_from = -c(newid, years))\n\nAnd take a look at the header to check that was done correctly.\n\n# Pretty print header of wideform data\nkable(head(data_wide_2), format = \"html\", full_width = TRUE) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\"))\n\n\n\n\nnewid\nAGG_MENT_0\nAGG_MENT_1\nAGG_MENT_2\nAGG_PHYS_0\nAGG_PHYS_1\nAGG_PHYS_2\nHASHV_0\nHASHV_1\nHASHV_2\nHASHF_0\nHASHF_1\nHASHF_2\nincome_0\nincome_1\nincome_2\nBMI_0\nBMI_1\nBMI_2\nHBP_0\nHBP_1\nHBP_2\nDIAB_0\nDIAB_1\nDIAB_2\nLIV34_0\nLIV34_1\nLIV34_2\nKID_0\nKID_1\nKID_2\nFRP_0\nFRP_1\nFRP_2\nFP_0\nFP_1\nFP_2\nTCHOL_0\nTCHOL_1\nTCHOL_2\nTRIG_0\nTRIG_1\nTRIG_2\nLDL_0\nLDL_1\nLDL_2\nDYSLIP_0\nDYSLIP_1\nDYSLIP_2\nCESD_0\nCESD_1\nCESD_2\nSMOKE_0\nSMOKE_1\nSMOKE_2\nDKGRP_0\nDKGRP_1\nDKGRP_2\nHEROPIATE_0\nHEROPIATE_1\nHEROPIATE_2\nIDU_0\nIDU_1\nIDU_2\nLEU3N_0\nLEU3N_1\nLEU3N_2\nVLOAD_0\nVLOAD_1\nVLOAD_2\nADH_0\nADH_1\nADH_2\nRACE_0\nRACE_1\nRACE_2\nEDUCBAS_0\nEDUCBAS_1\nEDUCBAS_2\nhivpos_0\nhivpos_1\nhivpos_2\nage_0\nage_1\nage_2\nART_0\nART_1\nART_2\neverART_0\neverART_1\neverART_2\nhard_drugs_0\nhard_drugs_1\nhard_drugs_2\n\n\n\n\n1\n44.90710\n58.20754\n59.65136\n52.52557\n41.29346\n48.54453\nNo\nNo\nNo\nNA\nLess Often\nNever\n$30,000-$39,999\n$30,000-$39,999\n$30,000-$39,999\n24.71756\n26.06801\n27.16421\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\n133\n131\n180\n176\n107\n233\n62\n66\n86\nYes\nNo\nYes\n14\n2\n1\nCurrent Smoker\nCurrent Smoker\nCurrent Smoker\nNone\n&gt;13 drinks/week\nNone\nNo\nNo\nNo\nYes\nNo\nNo\n104.1659\n262.0061\n345.4010\n102013.000\n27.00000\n60.00000\nNA\n95-99%\n100%\nWhite, non-Hispanic\nWhite, non-Hispanic\nWhite, non-Hispanic\nAt least one year college but no degree\nAt least one year college but no degree\nAt least one year college but no degree\n1\n1\n1\n52\n53\n54\n0\n1\n1\n0\n1\n1\nYes\nNo\nNo\n\n\n2\n46.34190\n48.71791\n45.41483\n27.92331\n38.03807\n37.32204\nNo\nYes\nNo\nNever\nNever\nNever\n$10,000-$19,999\nLess than $10,000\n$10,000-$19,999\n26.66936\n25.96576\n26.96037\nYes\nYes\nYes\nInsufficient data\nInsufficient data\nInsufficient data\nNo\nNo\nNo\nInsufficient data\nInsufficient data\nInsufficient data\nYes\nNo\nNo\nInsufficient Data\nInsufficient Data\nNo\n125\nNA\n134\nNA\nNA\nNA\nNA\nNA\nNA\nYes\nInsufficient data\nYes, based on data trajectory\n20\n18\n18\nCurrent Smoker\nCurrent Smoker\nCurrent Smoker\nNone\n&gt;13 drinks/week\n4-13 drinks/week\nNo\nNo\nNo\nYes\nNo\nNo\n257.8278\n459.4562\n263.0693\n8121.000\n21.00000\n48.00000\nNA\n100%\n100%\nBlack, non-Hispanic\nBlack, non-Hispanic\nBlack, non-Hispanic\n9,10, or 11th grade\n9,10, or 11th grade\n9,10, or 11th grade\n1\n1\n1\n54\n55\n56\n0\n1\n1\n0\n1\n1\nYes\nNo\nNo\n\n\n3\n40.22337\n44.42011\n41.70079\n60.06970\n62.71705\n58.51450\nNo\nNo\nYes\nLess Often\nLess Often\nLess Often\n$50,000-$59,999\n$50,000-$59,999\n$50,000-$59,999\n28.59085\n28.35320\n28.18510\nInsufficient data, may include reported treatment without diagnosis\nInsufficient data, may include reported treatment without diagnosis\nNo\nInsufficient data\nInsufficient data\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\n170\n170\n180\nNA\nNA\n82\nNA\nNA\n127\nYes\nYes\nYes\n18\n22\n23\nFormer Smoker\nFormer Smoker\nFormer Smoker\nNone\nNone\nNone\nNo\nNo\nNo\nYes\nYes\nYes\n563.1223\n488.9100\n405.1816\n4001.556\n2020.00000\n27.50917\nNA\n100%\n100%\nWhite, non-Hispanic\nWhite, non-Hispanic\nWhite, non-Hispanic\nPost-graduate degree\nPost-graduate degree\nPost-graduate degree\n1\n1\n1\n47\n48\n49\n0\n1\n1\n0\n1\n1\nYes\nYes\nYes\n\n\n4\n42.90638\n31.15971\n52.68223\n50.78850\n44.62883\n51.50533\nYes\nNo\nNo\nLess Often\nWeekly\nNever\nLess than $10,000\nLess than $10,000\nNA\n20.36451\n18.21865\n20.28485\nNo\nNo\nNo\nNo\nNo, based on data trajectory\nNo\nNo\nNo\nNo\nInsufficient data\nInsufficient data\nInsufficient data\nNo\nYes\nNo\nNo\nYes\nNo\n214\n197\n251\n97\nNA\n260\n147\nNA\n152\nYes\nYes, based on data trajectory\nYes\n14\n25\n13\nCurrent Smoker\nCurrent Smoker\nCurrent Smoker\n1-3 drinks/week\n4-13 drinks/week\nNone\nYes\nYes\nNo\nNo\nNo\nNo\n110.4218\n159.6297\n179.6409\n740.000\n26.64732\n27.00000\nNA\n75-94%\n95-99%\nWhite, non-Hispanic\nWhite, non-Hispanic\nWhite, non-Hispanic\nFour years college / got degree\nFour years college / got degree\nFour years college / got degree\n1\n1\n1\n44\n45\n46\n0\n1\n1\n0\n1\n1\nYes\nYes\nNo\n\n\n5\n56.42904\n56.21993\n66.50629\n43.75671\n30.47055\n18.82350\nNo\nNA\nYes\nMonthly\nMonthly\nLess Often\n$50,000-$59,999\n$10,000-$19,999\nDo not wish to answer\n22.26986\n24.97865\n20.80193\nNo\nNo\nNo\nInsufficient data\nInsufficient data\nInsufficient data\nNo\nNo\nInsufficient Data\nInsufficient data\nInsufficient data\nInsufficient data\nNo\nNo\nYes\nInsufficient Data\nInsufficient Data\nYes\n196\n204\nNA\n162\n192\nNA\n135\nNA\nNA\nYes\nInsufficient data\nYes, based on data trajectory\n1\n0\n1\nCurrent Smoker\nCurrent Smoker\nCurrent Smoker\nNone\n1-3 drinks/week\n1-3 drinks/week\nNot Specified\nNA\nNo\nYes\nYes\nYes\n252.6634\n92.6634\n59.6219\n62727.039\n30389.00000\n419.50000\nNA\n100%\n100%\nWhite, non-Hispanic\nWhite, non-Hispanic\nWhite, non-Hispanic\nPost-graduate degree\nPost-graduate degree\nPost-graduate degree\n1\n1\n1\n53\n54\n55\n0\n1\n1\n0\n1\n1\nYes\nYes\nYes\n\n\n6\n59.74437\n53.84956\n50.26010\n56.86261\n57.91396\n55.95668\nNo\nNo\nNo\nNA\nNA\nLess Often\n$30,000-$39,999\n$30,000-$39,999\nNA\n23.22166\n23.75318\n22.41001\nNo\nNo\nNo\nInsufficient data\nInsufficient data\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\n216\n216\n151\nNA\nNA\n125\nNA\nNA\n81\nInsufficient data\nInsufficient data\nNo\n3\n3\n4\nNever Smoked\nNever Smoked\nNever Smoked\n1-3 drinks/week\n1-3 drinks/week\n1-3 drinks/week\nNo\nNo\nNo\nNo\nNo\nYes\n634.1246\n745.6517\n893.4328\n15745.000\n7870.00000\n53.50000\nNA\n95-99%\n95-99%\nWhite, non-Hispanic\nWhite, non-Hispanic\nWhite, non-Hispanic\nSome graduate work\nSome graduate work\nSome graduate work\n1\n1\n1\n36\n37\n38\n0\n1\n1\n0\n1\n1\nNo\nNo\nYes\n\n\n\n\n\n\n\nGood. now we have a long and wide form of the data set for the first two years of the study.\nFinally, let’s just clean that wide data set up a bit to drop repeat measures of variables that are constant over time (race, education at baseline, HIV serostatus, everART)\n\n# Clean up the wide data set a bit by deleting multiple observations across time for constant variables such as race\ndata_wide_2 &lt;- data_wide_2 %&gt;% select(-RACE_1, -RACE_2, -EDUCBAS_1, -EDUCBAS_2, -hivpos_1, - hivpos_2, -everART_1, -everART_2)\n\nNow that our data sets are adequately prepared, we can move on to performing our data checks to ensure fidelity of the data set."
  },
  {
    "objectID": "Project_2/Project_2_R/Code/Project2.html#missingness",
    "href": "Project_2/Project_2_R/Code/Project2.html#missingness",
    "title": "Advanced Data Analysis - Project 2",
    "section": "Missingness",
    "text": "Missingness\nFirst we begin by examining missingness in our data set\n\n# Check missingness for long form data\ngg_miss_var(data_2)\n\n\n\n\n\n\n\n\nThis shows that we are missing the most values for LDL, TRIG, ADH, TCHOL, and income.\nA closer examination reveals…\n\n# Visualize missing values for longform data\nvis_miss(data_2)\n\n\n\n\n\n\n\n\n53% of LDL, 53% of TRIG, 33% of ADH, 32% of TCHOL, and 24% of income values are missing.\nLDL andTRIG have egregious amounts of missing data (&gt; 50%). TCHOL and income are in a range where we may be able to save them with MI or a linear mixed model that allows for missing data. We will have to see.\nADH is missing 33% of values. That could be problematic as that’s a key variable the researchers are interested in.\nBut there’s an odd, systematic pattern there… what if we look at the wide form of the data?\n\n# Visualize missing values for wideform data\nvis_miss(data_wide_2)\n\n\n\n\n\n\n\n\nAh, 1/3 of the values for ADH are missing because there are 3 time points and you can’t have baseline adherence to a protocol you just started (i.e. ADH_0).\nThere’s a small blip there that looks like someone DOES have a value for ADH_0, I wonder what that’s about…\n\n# For some reason participant 426 has an adherence of 1 at baseline\nadh_at_baseline &lt;- data_wide_2 %&gt;%\n  filter(ADH_0 == \"100%\") %&gt;% \n  select(newid, ADH_0)\n\n# Pretty print\nkable(adh_at_baseline, format = \"html\", full_width = FALSE)\n\n\n\n\nnewid\nADH_0\n\n\n\n\n426\n100%\n\n\n\n\n\n\n\nApparently if you’re participant 426, you can have 100% adherence to a protocol you’ve just started (clearly a typo).\nConclusion is we can still use ADH as a variable! We just have to use adherence at years 1 or 2."
  },
  {
    "objectID": "Project_2/Project_2_R/Code/Project2.html#data-cleaning",
    "href": "Project_2/Project_2_R/Code/Project2.html#data-cleaning",
    "title": "Advanced Data Analysis - Project 2",
    "section": "Data Cleaning",
    "text": "Data Cleaning\nWe just examined missingness as a preliminary check. However there is more work to be done.\nThe dataset we received has variables that were coded inconsistently. For instance, some variables are coded so that missing values are represented by a blank, and some variables (like CESD) are coded so that missing values are represented by -1. In other cases, such as with BMI, improbable values are coded as 999.\nWe can see this if we examine the mins and maxes for each numerical variable.\n\n# Code from ChatGPT\n# This function summarizes the mins and maxes of numeric variables\nsummarize_column &lt;- function(column) {\n  if (is.numeric(column)) {\n    return(data.frame(\n      Type = \"Numeric\",\n      Min = min(column, na.rm = TRUE),\n      Max = max(column, na.rm = TRUE)\n    ))\n  }\n}\n\n# Apply the function to each column and bind the results into a single data frame\nsummary_df &lt;- map_dfr(data_2, summarize_column, .id = \"Column\") %&gt;%\n  mutate(across(everything(), ~ format(., scientific = FALSE))) # Eliminates scientific notation\n\n# Pretty print the mins and maxes of longform data_2\nkable(summary_df, format = \"html\") %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\"))\n\n\n\n\nColumn\nType\nMin\nMax\n\n\n\n\nnewid\nNumeric\n1.000000\n550.00000\n\n\nAGG_MENT\nNumeric\n7.229315\n73.31224\n\n\nAGG_PHYS\nNumeric\n9.116614\n70.43724\n\n\nBMI\nNumeric\n-1.000000\n1000.11465\n\n\nTCHOL\nNumeric\n57.000000\n613.00000\n\n\nTRIG\nNumeric\n28.000000\n2237.00000\n\n\nLDL\nNumeric\n5.000000\n704.00000\n\n\nCESD\nNumeric\n-1.000000\n53.00000\n\n\nLEU3N\nNumeric\n10.859882\n1727.92558\n\n\nVLOAD\nNumeric\n0.246597\n190695039.60000\n\n\nhivpos\nNumeric\n1.000000\n1.00000\n\n\nage\nNumeric\n20.000000\n75.00000\n\n\nART\nNumeric\n0.000000\n1.00000\n\n\neverART\nNumeric\n0.000000\n1.00000\n\n\nyears\nNumeric\n0.000000\n2.00000\n\n\n\n\n\n\n\nIn effect, our data is not correctly showing all missing values. Let’s clean all that up, variable by variable.\n\nCleaning Dependent Variables\nFirst we will begin by examining and cleaning our 4 primary outcomes of interest.\nThe first two are laboratory measures.\n\nViral load (VLOAD): The number of HIV copies in a mL of blood\nCD4+ T cell count (LEU3N): A measure of immunologic health.\n\nIn untreated HIV infection, viral load increases over time and CD4+ T cell counts decline as the immune system is attacked by the virus. Once treatment is initiated, we expect viral load to decrease rapidly and CD4 counts to recover.\nOur last two measures are quality of life measures from the SF-36.\n\nAggregate physical quality of life score (AGG_PHYS)\nAggregate mental quality of life score (AGG_MENT)\n\nThese scores range from 0 to 100, with higher scores indicating better quality of life. The researchers are not sure what happens to quality of life after initiating treatment. While in theory subjects’ improving health should result in increased quality of life, the side effects of these treatments are significant. If subjects experience declines in quality of life after initiating treatment, we would be concerned that they would stop treatment.\n\nViral LoadCD4+ T Cell CountAggregate Mental QOL ScoreAggregate Physical QOL Score\n\n\nStandardized viral load\n\n0 = 0 copies/ml\n999,999,999 = 999,999,999 copies/ml\nBlank = Missing\n\nOur min max function earlier showed the max VLOAD was 190695039.60. I wonder if this is real or a data error?\n\n# create histogram of viral load\nhist(data_2$VLOAD)\n\n\n\n\n\n\n\n# Create qqplots of viral load\nqqnorm(data_2$VLOAD)\nqqline(data_2$VLOAD)\n\n\n\n\n\n\n\n\nYeah, looks like there are about 3 data points throwing off our qqplot from being normal.\nLet’s investigate.\n\n# Sort data set by descending viral load\nsorted_data &lt;- data_2[order(-data_2$VLOAD),] %&gt;%\n  select(newid, VLOAD, years)\n\n# Pretty print resulting table\nkable(head(sorted_data), format = \"html\") %&gt;%\n  kable_styling(bootstrap_options = c(\"condensed\", \"striped\", \"hover\"))\n\n\n\n\nnewid\nVLOAD\nyears\n\n\n\n\n224\n190695040\n0\n\n\n78\n143419477\n0\n\n\n437\n83835646\n0\n\n\n196\n5227511\n0\n\n\n142\n2520009\n0\n\n\n19\n2515706\n0\n\n\n\n\n\n\n\nSo the highest VLOAD value is 75x the 5th highest, and the 4th highest is 2x 5th highest. All these values are from different patients at the baseline.\nBased on the data dictionary provided, these values fall below the specified range of 999,999,999 copies/ml. That the PI’s specified this range could mean these are real data points. Maybe immediately after when someone is first exposed to HIV the viral load is incredibly high, and these 4 or so patients fell in that time period?\nI will first check if removing them makes our data normally distributed.\nWe will then add them back into the data set and keep them in mind. Checking with the jackknife residuals after we run our model will tell us if they are high leverage points.\n\n# Create boxplot to assess for outliers\noutlier_vload &lt;- boxplot(data_2$VLOAD, main = \"Boxplot for VLOAD\")$out\ntext(x = rep(1.2, length(outlier_vload)),\n     y = outlier_vload, labels = outlier_vload, col = 'red', cex = 0.8)\n\n\n\n\n\n\n\n\nIndeed the boxplot shows these values really mess with our data.\nThese top 4 patients based on VLOAD are 224, 78, 437, and 196. Patient 196 has double the VLOAD of the next highest person, which means this could be an outlier or real data, but let’s remove them just to see.\n\n# Remove 4 highest viral load visits\ndata_vload_removed &lt;- data\ndata_vload_removed$VLOAD[data_vload_removed$newid %in% c(224, 78, 437, 196)] &lt;- NA\n\n# Create plots to assess for normality\nhist(data_vload_removed$VLOAD)\n\n\n\n\n\n\n\nqqnorm(data_vload_removed$VLOAD)\n\n\n\n\n\n\n\n\nOh, that makes more sense. Those might not have been outliers, we just need to log transform viral load. Right, viral load is often used as a real world example of a biological measurement that is logarithmic…\nLet’s do that log transform, and just pretend we remembered that from the start.\n\n# Log transform viral load in the long form data set\ndata_2$VLOAD_log &lt;- log(data_2$VLOAD)\n\n# Create a histogram of log transformed viral load\nhist(data_2$VLOAD_log)\n\n\n\n\n\n\n\n# Create qqplot of log transformed viral load\nqqnorm(data_2$VLOAD_log)\nqqline(data_2$VLOAD_log)\n\n\n\n\n\n\n\n# Create boxplot of log transformed viral load to assess for potential outliers\noutlier_vload &lt;- boxplot(data_2$VLOAD_log, main = \"Boxplot for VLOAD\")\n\n\n\n\n\n\n\n\nThat looks much better!\nI’d say that’s roughly normally distributed, maybe a bit right tailed but likely still acceptable.\nLooks like this took care of those values that were showing up as outliers before, and we have no outliers for this variable.\nVLOAD has now been cleaned!\nBack to top of tabset\n\n\nA measure of immunologic health.\nNumber of CD4 positive cells (helpers)\n\n0 - 9999 cells\nBlank = Missing\n\n\n# Create histogram of CD4 T Cells\nhist(data_2$LEU3N, breaks = 24)\n\n\n\n\n\n\n\n# Create qqplot of CD4 T Cells\nqqnorm(data_2$LEU3N)\nqqline(data_2$LEU3N)\n\n\n\n\n\n\n\n# Sort data set by descending CD4 T Cell\nsorted_data &lt;- data_2[order(-data_2$LEU3N),] %&gt;%\n  select(newid, LEU3N, years)\n\n# Pretty print resulting table\nkable(head(sorted_data), format = \"html\") %&gt;%\n  kable_styling(bootstrap_options = c(\"condensed\", \"striped\", \"hover\"))\n\n\n\n\nnewid\nLEU3N\nyears\n\n\n\n\n63\n1727.926\n2\n\n\n255\n1560.152\n2\n\n\n355\n1505.613\n2\n\n\n291\n1395.958\n2\n\n\n7\n1395.432\n2\n\n\n137\n1382.626\n2\n\n\n\n\n\n\n\nThese values all look believable and like there was no errors during data collection or entering. It is unclear whether this variable is right tailed because of outliers, or if it needs to be transformed.\nLet’s look at potential outliers.\n\n# Create boxplot of CD4 T cell count to assess for outliers\noutlier_leu3n &lt;- boxplot(data_2$LEU3N, main = \"Boxplot for Leu3n\")$out\ntext(x = rep(1.2, length(outlier_leu3n)),\n     y = outlier_leu3n, labels = outlier_leu3n, col = 'red', cex = 0.8)\n\n\n\n\n\n\n\n\nYeah, the boxplot is showing a lot of outliers.\nLet’s try a log transform.\n\n# Log transform CD4 T Cell count\ndata_2$LEU3N_log &lt;- log(data_2$LEU3N)\n\n# Create histogram of log transformed CD4 T cell count\nhist(data_2$LEU3N_log)\n\n\n\n\n\n\n\n# Create qqplot of log transformed CD4 T cell count\nqqnorm(data_2$LEU3N_log)\nqqline(data_2$LEU3N_log)\n\n\n\n\n\n\n\n\nThat… didn’t work. Maybe let’s try standardization.\n\n# Perform a standardization transformation of CD4 T Cell Count\ndata_2$LEU3N_standard &lt;- scale(data_2$LEU3N)\n\n# Create histogram of standardized CD4 T Cell count\nhist(data_2$LEU3N_standard)\n\n\n\n\n\n\n\n# Create qqplot of standardized CD4 T Cell count.\nqqnorm(data_2$LEU3N_standard)\nqqline(data_2$LEU3N_standard)\n\n\n\n\n\n\n\n\nHmm, that didn’t do the trick.\nAt this point I asked my professor in passing and he recommended the bestNormalize package (which he happened to write) to help in selecting the best transformation for a variable.\nLet’s take a shot at it.\n\n# Use bestNormalize R package to select the best transformation for CD4 T Cell count\nBNObject &lt;- bestNormalize(data_2$LEU3N)\nBNObject\n\nBest Normalizing transformation with 1606 Observations\n Estimated Normality Statistics (Pearson P / df, lower =&gt; more normal):\n - arcsinh(x): 2.9315\n - Box-Cox: 1.1307\n - Center+scale: 1.6827\n - Double Reversed Log_b(x+a): 3.0188\n - Log_b(x+a): 2.9358\n - orderNorm (ORQ): 1.0576\n - sqrt(x + a): 1.1914\n - Yeo-Johnson: 1.1353\nEstimation method: Out-of-sample via CV with 10 folds and 5 repeats\n \nBased off these, bestNormalize chose:\norderNorm Transformation with 1606 nonmissing obs and ties\n - 1607 unique values \n - Original quantiles:\n      0%      25%      50%      75%     100% \n  10.860  285.847  438.078  611.673 1727.926 \n\n\nThe bestNormalize function selects the best transformation according to the Pearson P statistic (divided by its degrees of freedom), as calculated by the nortest package. There are a variety of normality tests out there, but the benefit of the Pearson P / df is that it is a relatively interpretable goodness of fit test, and the ratio P / df can be compared between transformations as an absolute measure of the departure from normality (if the data follows close to a normal distribution, this ratio will be close to 1).\nHere we can see that orderNorm (1.14), Yeo-Johnson (1.26), and Box-Cox (1.26) all perform relatively similar to each other. Let’s see what those plots look like if I do those transformations.\n\n# Peform ordernNorm transformation of CD4 T Cell count\ndata_2$LEU3N_orderNorm &lt;- orderNorm(data_2$LEU3N)$x.t\n\nWarning in orderNorm(data_2$LEU3N): Ties in data, Normal distribution not guaranteed\n\n# Peform Box-Cox transformation of CD4 T Cell count\ndata_2$LEU3N_boxcox &lt;- boxcox(data_2$LEU3N)$x.t\n\n# Peform Yeo-Johnson transformation of CD4 T Cell count\ndata_2$LEU3N_yeojohnson &lt;- yeojohnson(data_2$LEU3N)$x.t\n\n# Plot all histograms using MASS\npar(mfrow = c(3,1))\nMASS::truehist(data_2$LEU3N_orderNorm, main = \"OrderNorm transformation\", nbins = 24)\nMASS::truehist(data_2$LEU3N_boxcox, main = \"Box Cox transformation\", nbins = 24)\nMASS::truehist(data_2$LEU3N_yeojohnson, main = \"Yeo-Johnson transformation\", nbins = 24)\n\n\n\n\n\n\n\n\n\n# This function visualizes the estimated normality statistics obtained for each fold and repeat of cross-validation via boxplots. It allows you to compare transformation methods\nboxplot(log10(BNObject$oos_preds), yaxt = 'n')\naxis(2, at=log10(c(.1,.5, 1, 2, 5, 10)), labels=c(.1,.5, 1, 2, 5, 10))\n\n\n\n\n\n\n\n\nI will select Box-Cox as those two names are more familiar to me so I trust it more per the availability heuristic (and because orderNorm looks TOO good to be true).\nIt looks like we are good on LEU3N and can move forward!\n\n# Get mean and SD for LEU3N\nmean_value &lt;- mean(data_2$LEU3N, na.rm = TRUE)\nsd_value &lt;- sd(data_2$LEU3N, na.rm = TRUE)\n\n# Copy data set so we are not changing our main data set.\nfiltered_data &lt;- data_2\n\n# Shows we have 10 values &gt;= 3SDs from the mean.\nfiltered_data %&gt;%\n  select(LEU3N) %&gt;%\n  filter(LEU3N &gt;= mean_value + 3*sd_value)\n\n# A tibble: 10 × 1\n   LEU3N\n   &lt;dbl&gt;\n 1 1395.\n 2 1728.\n 3 1323.\n 4 1319.\n 5 1383.\n 6 1560.\n 7 1277.\n 8 1396.\n 9 1297.\n10 1506.\n\n# Filter our values greater than 3SD from the mean.\nfiltered_data$LEU3N[filtered_data$LEU3N &gt;= (mean_value + (3 * sd_value)) | filtered_data$LEU3N &lt;= (mean_value - (3 * sd_value))] &lt;- NA\n\n# View the hist again now that we have filtered outleirs\nhist(filtered_data$LEU3N, breaks = 24)\n\n\n\n\n\n\n\n# Create boxplot of CD4 T cell count to assess for outliers\noutlier_leu3n &lt;- boxplot(filtered_data$LEU3N, main = \"Boxplot for Leu3n\")$out\ntext(x = rep(1.2, length(outlier_leu3n)),\n     y = outlier_leu3n, labels = outlier_leu3n, col = 'red', cex = 0.8)\n\n\n\n\n\n\n\n\nOptions: We can try running the analysis with our transformed LEU3N values, or run it without transformation and see how many of those high values are real outliers with high leverage and influence.\nMore information on Box-Cox Transformation here\nBack to top of tabset\n\n\nThe values for AGG_MENT in our data set range from 7.229315 to 73.31224, which is believable and leads us to conclude there were no data entry errors here.\nLet’s examine normality.\n\n# Create a histogram for aggregate mental QOL score\nhist(data_2$AGG_MENT)\n\n\n\n\n\n\n\n# Create qqplot for aggregate mental QOL score\nqqnorm(data_2$AGG_MENT)\nqqline(data_2$AGG_MENT)\n\n\n\n\n\n\n\n# Sort by descending to examine highest values\nsorted_data_2 &lt;- data_2[order(data_2$AGG_MENT),] %&gt;%\n  select(newid, AGG_MENT, years)\n\n# Pretty print resulting table\nkable(head(sorted_data_2), format = \"html\") %&gt;%\n  kable_styling(bootstrap_options = c(\"hover\", \"striped\", \"condensed\"))\n\n\n\n\nnewid\nAGG_MENT\nyears\n\n\n\n\n263\n7.229315\n0\n\n\n368\n9.851272\n0\n\n\n368\n10.510143\n2\n\n\n499\n10.891071\n0\n\n\n306\n10.954260\n0\n\n\n309\n11.206351\n2\n\n\n\n\n\n\n# Create boxplot of aggregate mental QOL to assess for outliers\noutlier_AGG_MENT &lt;- boxplot(data_2$AGG_MENT, main = \"Boxplot for Aggregate Mental QOL\")$out\ntext(x = rep(1.2, length(outlier_AGG_MENT)),\n     y = outlier_AGG_MENT, labels = outlier_AGG_MENT, col = 'red', cex = 0.8)\n\n\n\n\n\n\n\n\nIt appears that AGG_MENT only has 2 potential outliers, and is also not normally distributed, it is left-tailed. Let’s address that.\n\n# Use bestNormalize function to test which transformation performs the best\nBNobject &lt;- bestNormalize(data_2$AGG_MENT)\nBNobject\n\nBest Normalizing transformation with 1640 Observations\n Estimated Normality Statistics (Pearson P / df, lower =&gt; more normal):\n - arcsinh(x): 9.528\n - Box-Cox: 3.2786\n - Center+scale: 5.1983\n - Double Reversed Log_b(x+a): 2.3484\n - Exp(x): 169.1326\n - Log_b(x+a): 9.5298\n - orderNorm (ORQ): 1.1035\n - sqrt(x + a): 6.9749\n - Yeo-Johnson: 3.1637\nEstimation method: Out-of-sample via CV with 10 folds and 5 repeats\n \nBased off these, bestNormalize chose:\norderNorm Transformation with 1640 nonmissing obs and ties\n - 1641 unique values \n - Original quantiles:\n    0%    25%    50%    75%   100% \n 7.229 37.863 50.386 56.401 73.312 \n\n\nThe orderNorm transformation beats out the other transformations by a mile. Let’s perform that.\n\n# Perform orderNorm transformation of aggregate mental QOL score\ndata_2$AGG_MENT_orderNorm &lt;- orderNorm(data_2$AGG_MENT)$x.t\n\nWarning in orderNorm(data_2$AGG_MENT): Ties in data, Normal distribution not guaranteed\n\nMASS::truehist(data_2$AGG_MENT_orderNorm, main = \"OrderNorm transformation\", nbins = 24)\n\n\n\n\n\n\n\n\nThat appears to be what we have to do but I have some misgivings with orderNorm transforming everything…\nCome back to this\nBack to top of tabset\n\n\nAGG_PHYS has a min of 9.12 and a max of 73.57. These are within the specified range of 0 - 100, and it appears there were no data error entries.\n\n# Creat histogram of aggregate physical QOL score\nhist(data_2$AGG_PHYS)\n\n\n\n\n\n\n\n# Create qqplots of aggregate physical QOL score\nqqnorm(data_2$AGG_PHYS)\nqqline(data_2$AGG_PHYS)\n\n\n\n\n\n\n\n\nAGG_PHYS is not normally distributed, it is left-tailed.\nLet’s test which type of transformation might suit it.\n\n# Use bestNormalize function to test which transformation performs the best\nBNobject &lt;- bestNormalize(data_2$AGG_PHYS)\nBNobject\n\nBest Normalizing transformation with 1640 Observations\n Estimated Normality Statistics (Pearson P / df, lower =&gt; more normal):\n - arcsinh(x): 6.6531\n - Box-Cox: 2.4949\n - Center+scale: 3.7989\n - Double Reversed Log_b(x+a): 1.8185\n - Exp(x): 174.1359\n - Log_b(x+a): 6.6555\n - orderNorm (ORQ): 1.1149\n - sqrt(x + a): 4.9699\n - Yeo-Johnson: 2.0515\nEstimation method: Out-of-sample via CV with 10 folds and 5 repeats\n \nBased off these, bestNormalize chose:\norderNorm Transformation with 1640 nonmissing obs and ties\n - 1641 unique values \n - Original quantiles:\n    0%    25%    50%    75%   100% \n 9.117 44.240 52.791 56.389 70.437 \n\n\nAgain orderNorm performs the best.\n\n# Perform orderNorm transformation of aggregate mental QOL score\ndata_2$AGG_PHYS_orderNorm &lt;- orderNorm(data_2$AGG_PHYS)$x.t\n\nWarning in orderNorm(data_2$AGG_PHYS): Ties in data, Normal distribution not guaranteed\n\nMASS::truehist(data_2$AGG_PHYS_orderNorm, main = \"OrderNorm transformation\", nbins = 24)\n\n\n\n\n\n\n\n\nOrderNorm transforming all our DVs might make interpration difficult…\nCome back to this.\nBack to top of tabset\n\n\n\n\n\nCleaning Covariates\nNow let’s perform data quality checks on our covariates.\n\nHash UseHash FrequencyIncomeBMIHigh Blood PressureDiabetesLiver DiseaseKidney DiseaseFrailty Related PhenotypeFrailty PhenotypeTotal CholesterolTriglyceridesLow Density LipoproteinDyslipidemiaDepressionSmoking StatusDrinking GroupHeroin or Opiate UseIntravenous Drug UseAdherenceRaceEducation at BaselineHIV SerostatusAgeAntiretroviral TherapyeverARTHard Drug UseSummary\n\n\nHash/marijuana use since last visit\n\n1 = no\n2 = yes\nblank = missing\n\n\n# Create a barplot for hash use\nbarplot(table(data_2$HASHV))\n\n\n\n\n\n\n\n\nMissing data is correctly handled for this variable.\nWe have more visits where participants used hash since the last visit than visits where participants did not use hash.\nBack to top of tabset\n\n\nFrequency hash/marijuana was used since last visit.\n\n0 = Never\n1 = Daily\n2 = Weekly\n3 = Monthly\n4 = Less Often\nBlank = Missing\n\n\n# Create barplot for hash frequency\nbarplot(table(data_2$HASHF))\n\n\n\n\n\n\n\n\nThis variable is coded correctly. Most participants answered they have never used Hash.\nBack to top of tabset\n\n\nIncome\n\n1 = Less than $10,000\n2 = $10,000 - $19,999\n3 = $20,000 - $29,999\n4 = $30,000 - $39,999\n5 = $40,000 - $49,999\n6 = $50,000 - $59,999\n7 = $60,000 or more\n9 = Do not wish to answer\n\nThe min and max for income are 1 - 9, which matches that data dictionary.\n\n# Create barplot for income\nbarplot(table(data_2$income))\n\n\n\n\n\n\n\n# Get values for each income level\nkable(table(data_2$income), format = \"html\") %&gt;%\n  kable_styling(bootstrap_options = c(\"condensed\", \"hover\", \"striped\"))\n\n\n\n\nVar1\nFreq\n\n\n\n\nLess than $10,000\n285\n\n\n$10,000-$19,999\n164\n\n\n$20,000-$29,999\n175\n\n\n$30,000-$39,999\n172\n\n\n$40,000-$49,999\n123\n\n\n$50,000-$59,999\n202\n\n\n$60,000 or more\n89\n\n\nDo not wish to answer\n38\n\n\n\n\n\n\n\nWe have to convert those values of ‘Do not wish to answer’ to be NA.\n\n# Converting scores of 9 (do not wish to answer) to be NA\ndata$income[data_2$income == 9] &lt;- NA\nbarplot(table(data_2$income))\n\n\n\n\n\n\n\n\nLooks good, we just converted 38 participants from do not wish to answer, to count as missing.\nBack to top of tabset\n\n\nBody Mass Index\nWe have a min of -1 and a max of 1000.\n\n-1: Improbable values\n999: Insufficient data (why it shows up with decimals and is not exactly 999, who knows).\n\n\n# Create histogram of BMI\nhist(data_2$BMI)\n\n\n\n\n\n\n\n\nLet’s convert those values of -1 and &gt;= 998 into missing values.\n\n# Convert missing and improbably values to NA\ndata_2$BMI[data_2$BMI &lt; 0 | data_2$BMI &gt;= 998] &lt;- NA\n\nAnd check out the histogram again and the qqplot.\n\n# Create histogram of BMI\nhist(data_2$BMI, breaks = 20)\n\n\n\n\n\n\n\n# Create qqplot of BMI\nqqnorm(data_2$BMI)\nqqline(data_2$BMI)\n\n\n\n\n\n\n\nsummarize_column(data_2$BMI)\n\n     Type      Min    Max\n1 Numeric 15.94907 52.832\n\n\nLooks better. Now we have a BMI range of 15.94 - 52.83.\nThe histogram and qqplots show BMI is slightly right skewed, with more morbidly obese patients than underweight. Is this close enough to normal to ignore, if we take out outliers?\nThe patient with a BMI of 52.83 may be an outlier based on the qqplot.\n\n# Investigating highest BMI value to see if its an outlier\nhighest_bmi &lt;- data[data$newid == 206,]\n\n# Plot BMI for each year for this patient\nplot(highest_bmi$years, highest_bmi$BMI)\n\n\n\n\n\n\n\n\nInterestingly, participant 206 got heavier over the first year, then dropped weight in the proceeding years. Either that or that second year entry point was an error and was meant to be 42.83\n\n# Testing to see if these plots look normal after taking the participant with BMI of 52.83 out\ndata_2$BMI[data_2$newid == 206 & data_2$years == 1] &lt;- NA\nhist(data_2$BMI, breaks = 20)\n\n\n\n\n\n\n\nqqnorm(data_2$BMI)\nqqline(data_2$BMI)\n\n\n\n\n\n\n\n# Add back in that value we removed\ndata_2$BMI[data_2$newid == 206 & data_2$years == 1] &lt;- 52.832\n\nAfter removing that highest BMI value, the histogram is still right tailed.\nWhat do the boxplots look like?\n\noutlier_bmi &lt;- boxplot(data_2$BMI, main = \"Boxplot for BMI\")$out\ntext(x = rep(1.2, length(outlier_bmi)),\n     y = outlier_bmi, labels = outlier_bmi, col = 'red', cex = 0.8)\n\n\n\n\n\n\n\n\nThat’s a lot of potential outliers for BMI. If we really want to use this variable we may have to remove these values to keep BMI normally distributed.\n\n# Run simple correlation matrix to see if BMI correlated with any outcomes of interest\n\n# Correlation between VMI and VLOAD log\ncor_test_result &lt;- cor.test(data_2$BMI, data_2$VLOAD_log)\ncor_test_result\n\n\n    Pearson's product-moment correlation\n\ndata:  data_2$BMI and data_2$VLOAD_log\nt = -5.8077, df = 1499, p-value = 7.721e-09\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.1974585 -0.0984866\nsample estimates:\n      cor \n-0.148344 \n\n# Correlation between BMI and CD4+ T Cell count boxcox\ncor_test_result &lt;- cor.test(data_2$BMI, data_2$LEU3N_boxcox)\ncor_test_result\n\n\n    Pearson's product-moment correlation\n\ndata:  data_2$BMI and data_2$LEU3N_boxcox\nt = 7.5505, df = 1503, p-value = 7.484e-14\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.1420089 0.2393834\nsample estimates:\n      cor \n0.1911664 \n\n# Correlation between BMI and mental QOL \ncor_test_result &lt;- cor.test(data_2$BMI, data_2$AGG_MENT)\ncor_test_result\n\n\n    Pearson's product-moment correlation\n\ndata:  data_2$BMI and data_2$AGG_MENT\nt = 2.2657, df = 1504, p-value = 0.02361\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.007834005 0.108516221\nsample estimates:\n       cor \n0.05832342 \n\n# Correlation between BMI and physical QOL.\ncor_test_result &lt;- cor.test(data_2$BMI, data_2$AGG_PHYS)\ncor_test_result\n\n\n    Pearson's product-moment correlation\n\ndata:  data_2$BMI and data_2$AGG_PHYS\nt = 4.4256, df = 1504, p-value = 1.031e-05\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.06323068 0.16296024\nsample estimates:\n     cor \n0.113381 \n\n\nBMI is correlated with all our outcome variables, so we should make sure it’s cleaned.\nIt looks like we have a lot of outliers for BMI we can clean out from our data set to make sure it has a normal distribution.\n\n# Compute mean and sd values for BMI\nmean_value &lt;- mean(data_2$BMI, na.rm = TRUE)\nsd_value &lt;- sd(data_2$BMI, na.rm = TRUE)\n\n# Use plotly to label outliers\nfig &lt;- plot_ly(data = data_2, type = 'box') \nfig &lt;- fig %&gt;% add_boxplot(y = ~BMI, name = \"Suspected Outlier\", \n                           boxpoints = 'suspectedoutliers',\n                           marker = list(color = 'rgb(8,81,156)',\n                                         outliercolor = 'rgba(219, 64, 82, 0.6)',\n                                         line = list(outliercolor = 'rgba(219, 64, 82, 1.0)',\n                                                     outlierwidth = 2)),\n                           line = list(color = 'rgb(8,81,156)'),\n                           text = ~paste(\"ID:\", newid),\n                           hoverinfo = \"text\")\nfig\n\nWarning: Ignoring 135 observations\n\n\n\n\n\n# Identify outliers\ndata_2$BMI_outlier &lt;- (abs(data_2$BMI - mean_value) &gt; 3* sd_value)\n\n# See how many outliers we have\nfiltered_data &lt;- data_2 %&gt;%\n  select(newid, BMI, BMI_outlier) %&gt;%\n  filter(BMI_outlier == \"TRUE\")\ndim(filtered_data)\n\n[1] 22  3\n\n# Pretty Print\nkable(head(filtered_data), format = \"html\") %&gt;%\n  kable_styling(bootstrap_options = c(\"condensed\", \"hover\", \"striped\"))\n\n\n\n\nnewid\nBMI\nBMI_outlier\n\n\n\n\n56\n38.89617\nTRUE\n\n\n107\n39.58391\nTRUE\n\n\n131\n40.38171\nTRUE\n\n\n131\n39.29654\nTRUE\n\n\n137\n40.19738\nTRUE\n\n\n137\n39.25950\nTRUE\n\n\n\n\n\n\n\nWe have 22 patients that had a BMI +- 3 SD from the mean. Let’s clear them from the data set.\n\n# Filter our values greater than 3SD from the mean.\ndata_2$BMI[(abs(data_2$BMI - mean_value) &gt; 3* sd_value)] &lt;- NA\n\n# Create histogram of BMI now that we have removed outleirs\nhist(data_2$BMI)\n\n\n\n\n\n\n\n# Create qqplot of BMI now that we have removed outliers\nqqnorm(data_2$BMI)\nqqline(data_2$BMI)\n\n\n\n\n\n\n\n# Plot outliers one more time\nfig &lt;- plot_ly(data = data_2, type = 'box') \nfig &lt;- fig %&gt;% add_boxplot(y = ~BMI, name = \"Suspected Outlier\", \n                           boxpoints = 'suspectedoutliers',\n                           marker = list(color = 'rgb(8,81,156)',\n                                         outliercolor = 'rgba(219, 64, 82, 0.6)',\n                                         line = list(outliercolor = 'rgba(219, 64, 82, 1.0)',\n                                                     outlierwidth = 2)),\n                           line = list(color = 'rgb(8,81,156)'),\n                           text = ~paste(\"ID:\", newid),\n                           hoverinfo = \"text\")\nfig\n\nWarning: Ignoring 157 observations\n\n\n\n\n\n\nThose all look a lot better! We can now conclude that BMI is normally distributed and can be used as a covariate in our models.\nBack to top of tabset\n\n\nHigh Blood Pressure (SBP &gt;= 140 or DBP &gt;= 90 or (diagnosed with hypertension and use of medication)\n\n1 = No\n2 = Yes\n3 = No, based on data trajectory\n4 = Yes, based on data trajectory\n9 = Insufficient data, may include reported treatment without diagnosis\n-1 = improbable value\n\nWe will have to exclude values of 9 or -1.\n\n# Create barplot of high blood pressure\nbarplot(table(data_2$HBP))\n\n\n\n\n\n\n\n\n\n# Get values for high blood pressure category\nkable(table(data_2$HBP), format = \"html\") %&gt;%\n  kable_styling(bootstrap_options = c(\"condensed\", \"hover\", \"striped\"))\n\n\n\n\nVar1\nFreq\n\n\n\n\nNo\n1109\n\n\nYes\n366\n\n\nNo, based on data trajectory\n34\n\n\nYes, based on data trajectory\n4\n\n\nInsufficient data, may include reported treatment without diagnosis\n137\n\n\nImprobable Value\n0\n\n\n\n\n\n\n\nThere are 137 participants with insufficient data. Let’s purge them from the data set.\n\n# Convert values of insufficient data to NA for high blood pressure\ndata_2$HBP[data_2$HBP == \"Insufficient data, may include reported treatment without diagnosis\"] &lt;- NA\n\n# Drop empty levels\ndata_2$HBP &lt;- droplevels(data_2$HBP) \n\n# Create barplot of high blood pressure \nbarplot(table(data_2$HBP))\n\n\n\n\n\n\n\n# Pretty print table\nkable(table(data_2$HBP), format = \"html\") %&gt;%\n  kable_styling(bootstrap_options = c(\"condensed\", \"hover\", \"striped\"))\n\n\n\n\nVar1\nFreq\n\n\n\n\nNo\n1109\n\n\nYes\n366\n\n\nNo, based on data trajectory\n34\n\n\nYes, based on data trajectory\n4\n\n\n\n\n\n\n\nLooks better.\nOnly 34 visits where participants had no based on trajectory, and 4 that had yes based on trajectory.\nWe will have to decide to either exclude these or merge them into the no or yes groups, respectively. We can do that after we run our correlation matrix to see if there’s any relationship here worth pursuing.\nBack to top of tabset\n\n\nDiabetes (GLUC 2 &gt;= 126 or (diagnosed with diabetes and use of medication))\n\n1 = No\n2 = Yes\n3 = No, based on data trajectory\n4 = Yes, based on data trajectory\n9 = Insufficient data\n\n\n# Create barplot of diabetes\nbarplot(table(data_2$DIAB))\n\n\n\n\n\n\n\n# Pretty print table\nkable(table(data_2$DIAB), format = \"html\") %&gt;%\n  kable_styling(bootstrap_options = c(\"condensed\", \"hover\", \"striped\"))\n\n\n\n\nVar1\nFreq\n\n\n\n\nNo\n628\n\n\nYes\n65\n\n\nNo, based on data trajectory\n76\n\n\nYes, based on data trajectory\n0\n\n\nInsufficient data\n881\n\n\n\n\n\n\n\nThere are 881 visits with patient who had insuffiicent data to make a diabetes diagnosis!\nLet’s change those values to NA.\n\n# Convert values of insufficient data to NA for diabetes\ndata_2$DIAB[data_2$DIAB == \"Insufficient data\"] &lt;- NA\n\n# Drop empty levels\ndata_2$DIAB &lt;- droplevels(data_2$DIAB)\n\n# Create a barplot for diabetes\nbarplot(table(data_2$DIAB))\n\n\n\n\n\n\n\n# Pretty print table\nkable(table(data_2$DIAB), format = \"html\") %&gt;%\n  kable_styling(bootstrap_options = c(\"condensed\", \"hover\", \"striped\"))\n\n\n\n\nVar1\nFreq\n\n\n\n\nNo\n628\n\n\nYes\n65\n\n\nNo, based on data trajectory\n76\n\n\n\n\n\n\n\nGreat, HBP is now cleaned.\nNotably, there were no visits with a yes, based on trajectory.\nBack to top of tabset\n\n\nLiver disease stage 3/4 (SGPT or SGOP &gt; 150), preliminary algorithm\n\n1 = No\n2 = Yes\n9 = Insufficient data\n\n\n# Create barplot of liver disease stage\nbarplot(table(data_2$LIV34))\n\n\n\n\n\n\n\n# Pretty print table\nkable(table(data_2$LIV34), format = \"html\") %&gt;%\n  kable_styling(bootstrap_options = c(\"condensed\", \"hover\", \"striped\"))\n\n\n\n\nVar1\nFreq\n\n\n\n\nNo\n1092\n\n\nYes\n51\n\n\nInsufficient Data\n507\n\n\n\n\n\n\n\nThere are 507 patients with insufficient data for a liver disease diagnosis.\nLet’s convert those values to NA to reflect this.\n\n# Convert values of insufficient data to NA for liver disease stage\ndata_2$LIV34[data_2$LIV34 == \"Insufficient Data\"] &lt;- NA\n\n# Drop empty levels\ndata_2$LIV34 &lt;- droplevels(data_2$LIV34)\n\n# Create barplot of cleaned liver stage disease\nbarplot(table(data_2$LIV34))\n\n\n\n\n\n\n\n# Pretty print table\nkable(table(data_2$LIV34), format = \"html\") %&gt;%\n  kable_styling(bootstrap_options = c(\"condensed\", \"hover\", \"striped\"))\n\n\n\n\nVar1\nFreq\n\n\n\n\nNo\n1092\n\n\nYes\n51\n\n\n\n\n\n\n\nLooks good, LIV34 is now cleaned!\nBack to top of tabset\n\n\nKidney disease (EGFR &lt; 60 or UPRCR &gt;= 200)\n\n1 = No\n2 = Yes\n3 = No, based on data trajectory\n4 = Yes, based on data trajectory\n- 9 = Insufficient data\n\n\n# Create barplots of kidney disease\nbarplot(table(data_2$KID))\n\n\n\n\n\n\n\n# Pretty print table\nkable(table(data_2$KID), format = \"html\") %&gt;%\n  kable_styling(bootstrap_options = c(\"condensed\", \"hover\", \"striped\"))\n\n\n\n\nVar1\nFreq\n\n\n\n\nNo\n476\n\n\nYes\n94\n\n\nNo, based on data trajectory\n12\n\n\nYes, based on data trajectory\n0\n\n\nInsufficient data\n1068\n\n\n\n\n\n\n\nThere are 1068 visits where there was insufficient data for a diagnosis.\nLet’s convert those to NA values.\n\n# Convert values of insufficient data to NA for kidney disease\ndata_2$KID[data_2$KID == \"Insufficient data\"] &lt;- NA\n\n# Drop empty levels\ndata_2$KID &lt;- droplevels(data_2$KID)\n\n# Create barplot of kidney disease\nbarplot(table(data_2$KID))\n\n\n\n\n\n\n\n\nLooks good, KID is now cleaned!\nBack to top of tabset\n\n\nFrailty Related Phenotype (3 out of 4 conditions = YES; WTLOS, PHDWA, HLTWB, HLTVA\n\n1 = No\n2 = Yes\n9 = Insufficient data\n\n\n# Create barplot of frailty related phenotype\nbarplot(table(data_2$FRP))\n\n\n\n\n\n\n\n# Pretty print table\nkable(table(data_2$FRP), format = \"html\") %&gt;%\n  kable_styling(bootstrap_options = c(\"condensed\", \"hover\", \"striped\"))\n\n\n\n\nVar1\nFreq\n\n\n\n\nNo\n1555\n\n\nYes\n92\n\n\nInsufficient Data\n3\n\n\n\n\n\n\n\nOnly 3 patients with insufficient data.\nLet’s convert them to NA.\n\n# Convert values of insufficient data to NA for frailty related phenotype\ndata_2$FRP[data_2$FRP == \"Insufficient Data\"] &lt;- NA\n\n# Drop empty levels\ndata_2$FRP &lt;- droplevels(data_2$FRP)\n\n# Create barplot of frailty related phenotype\nbarplot(table(data_2$FRP))\n\n\n\n\n\n\n\n\nLooks good, FRP is now cleaned.\nBack to top of tabset\n\n\nFrailty Phenotype (3 out of 5 conditions = YES: WTLOS, PHWDA, HLTVA, SLOW, WEAK)\n\n1 = No\n2 = Yes\n9 = Insufficient Data\n\n\n# Create barplot of frailty phenotype\nbarplot(table(data_2$FP))\n\n\n\n\n\n\n\n# Pretty print table\nkable(table(data_2$FP), format = \"html\") %&gt;%\n  kable_styling(bootstrap_options = c(\"condensed\", \"hover\", \"striped\"))\n\n\n\n\nVar1\nFreq\n\n\n\n\nNo\n1195\n\n\nYes\n98\n\n\nInsufficient Data\n357\n\n\n\n\n\n\n\n357 visits with insufficient data. Let’s convert to NA.\n\n# Convert values of insufficient data to NA for frailty phenotype\ndata_2$FP[data_2$FP == \"Insufficient Data\"] &lt;- NA\n\n# Drop empty levels\ndata_2$FP &lt;- droplevels(data_2$FP)\n\n# Create barplot of frailty phenotype\nbarplot(table(data_2$FP))\n\n\n\n\n\n\n\n\nLooks good, FP is now cleaned!\nBack to top of tabset\n\n\nTotal cholesterol mg/dL\n\n# Create histogram for total cholesterol\nhist(data_2$TCHOL)\n\n\n\n\n\n\n\n# Create qqplot for total cholesterol\nqqnorm(data_2$TCHOL)\nqqline(data_2$TCHOL)\n\n\n\n\n\n\n\n\nThe histogram and qq plot show what may be outliers for total cholesterol at the higher range. How many values are potential outliers?\n\n# Create boxplot to assess for outliers for total cholesterol\noutlier_tchol &lt;- boxplot(data_2$TCHOL, main = \"Boxplot for Total Cholesterol\")$out\ntext(x = rep(1.2, length(outlier_tchol)),\n     y = outlier_tchol, labels = outlier_tchol, col = 'red', cex = 0.8)\n\n\n\n\n\n\n\n# Sort by descending total cholesterol\nsorted_data &lt;- data_2[order(-data_2$TCHOL),] %&gt;%\n  select(newid, TCHOL, years)\n\n# Pretty print table\nkable(head(sorted_data), format = \"html\") %&gt;%\n  kable_styling(bootstrap_options = c(\"condensed\", \"hover\", \"striped\"))\n\n\n\n\nnewid\nTCHOL\nyears\n\n\n\n\n199\n613\n2\n\n\n89\n375\n2\n\n\n428\n362\n2\n\n\n433\n350\n2\n\n\n418\n348\n1\n\n\n203\n337\n1\n\n\n\n\n\n\n\nThe highest cholesterol value is ~2x higher than the next highest value. Let’s see what happens if we remove it.\n\n# Delete highest total cholesterol value\ndata_2$TCHOL[data_2$TCHOL == 613] &lt;- NA \n\n# Create histogram of total cholesterol\nhist(data_2$TCHOL)\n\n\n\n\n\n\n\n# Create qqplot of total cholesterol\nqqnorm(data_2$TCHOL)\nqqline(data_2$TCHOL)\n\n\n\n\n\n\n\n\nLooks better but still slightly right skewed. This variable had ~30% missing values, so we may end up not using it.\nBack to top of tabset\n\n\nTriglycerides, mg/dL\n\n# Create histogram of triglycerides\nhist(data_2$TRIG)\n\n\n\n\n\n\n\n# Create qqplot of triglycerides\nqqnorm(data_2$TRIG)\nqqline(data_2$TRIG)\n\n\n\n\n\n\n\n\nVERY skewed! Based on the qqplots, it looks like we would have to perform a log transform on TRIG if we wanted to use it. However we have nearly 50% missing values for this variable, so we should drop it as a covariate.\nBack to top of tabset\n\n\nLow Density Lipoprotein (fasting) mg/dL\n\n# Create a histogram for LDL\nhist(data_2$LDL)\n\n\n\n\n\n\n\n\nLooks like we may have an erroneous value at the highest range there.\n\n# Sort by descending total cholesterol\nsorted_data &lt;- data_2[order(-data_2$LDL),] %&gt;%\n  select(newid, LDL, years)\n\n# Pretty print table\nkable(head(sorted_data), format = \"html\") %&gt;%\n  kable_styling(bootstrap_options = c(\"condensed\", \"hover\", \"striped\"))\n\n\n\n\nnewid\nLDL\nyears\n\n\n\n\n19\n704\n0\n\n\n413\n704\n0\n\n\n275\n247\n2\n\n\n275\n217\n1\n\n\n242\n212\n2\n\n\n203\n204\n2\n\n\n\n\n\n\n\nPatients 19 and 413 have the same value of 704 at baseline. Clearly an error with the measurement process.\nLDL has close to 50% missing values and we will not be using it in our model, so I will move on. But good to know we can’t just blindly trust all the values to be correct!\nBack to top of tabset\n\n\nDyslipidemia at visit. fasting TC &gt;=200 mg/dl or &gt;=130 mg/dl or HDL &lt; 40 mg/dl or triglycerides &gt;=150 mg/dl or use of lipid lowering medications (HICHOLRX) with self report or clinical diagnosis in the past.\n\n1 = No\n2 = Yes\n3 = No, based on data trajectory\n4 = Yes from data trajectory\n9 = Insufficient data\n\n\n# Create barplot of dyslipidemia\nbarplot(table(data_2$DYSLIP))\n\n\n\n\n\n\n\n# Pretty print table\nkable(table(data_2$DYSLIP), format = \"html\") %&gt;%\n  kable_styling(bootstrap_options = c(\"condensed\", \"hover\", \"striped\"))\n\n\n\n\nVar1\nFreq\n\n\n\n\nNo\n235\n\n\nYes\n630\n\n\nNo, based on data trajectory\n10\n\n\nYes, based on data trajectory\n57\n\n\nInsufficient data\n718\n\n\n\n\n\n\n\nThere are 718 visits with insufficient data for a dyslipidemia diagnosis.\nLet’s convert those to NAs to reflect this.\n\n# Convert values of insufficient data to NA for dyslipidemia\ndata_2$DYSLIP[data_2$DYSLIP == \"Insufficient data\"] &lt;- NA\n\n# Drop empty levels\ndata_2$DYSLIP &lt;- droplevels(data_2$DYSLIP)\n\n# Create barplot of dyslipidemia\nbarplot(table(data_2$DYSLIP))\n\n\n\n\n\n\n\n\nLooks good, DYSLIP is now cleaned!\nBack to top of tabset\n\n\nCenter for Epidemiological Studies Depression Scale ( &gt;= 16 is depressed).\n\n0 - 60\n-1 = missing\n\n\n# Create histogram for depression score\nhist(data_2$CESD)\n\n\n\n\n\n\n\n\nLet’s correctly reflect those -1’s as NA’s\n\n# Remove depression scores that were coded as missing\ndata_2$CESD[data_2$CESD == -1] &lt;- NA\n\n# Create histogram of depression score\nhist(data_2$CESD)\n\n\n\n\n\n\n\n# Create qqplot for depression score\nqqnorm(data_2$CESD)\nqqline(data_2$CESD)\n\n\n\n\n\n\n\n\nLooks good. CESD is now cleaned!\nIt IS right skewed though. Will have to handle that if we want to use it.\nA square root transformation may be worth it?\nBack to top of tabset\n\n# Try a sqrt transformation\ndata_2$CESD_sqrt &lt;- sqrt(data_2$CESD)\n\n# Create histogram of sqrt depression score\nhist(data_2$CESD_sqrt)\n\n\n\n\n\n\n\n# Create qqplot of sqrt depressions score\nqqnorm(data_2$CESD_sqrt)\nqqline(data_2$CESD_sqrt)\n\n\n\n\n\n\n\n\n\n\nSmoking status\n\n1 = Never smoked\n2 = Former smoker\n3 = Current smoker\nBlank = missing\n\n\n# Create barplot of smoking status\nbarplot(table(data_2$SMOKE))\n\n\n\n\n\n\n\n\nLooks good, nothing to do here.\nBack to top of tabset\n\n\nAlcohol use since last visit\n\n0 = None\n1 = 1 to 3 drinks/week\n2 = 4 to 13 drinks/week\n3 = More than 13 drinks/week\nBlank = Missing\n\n\n# Create barplot of drinking group\nbarplot(table(data_2$DKGRP))\n\n\n\n\n\n\n\n\nLooks good, nothing to do here.\nBack to top of tabset\n\n\nTook heroin or other opiates since last visit?\n\n1 = No\n2 = Yes\n-9 = Not specified in form\nBlank = Missing\n\n\n# Create barplot of heroin or opiate use\nbarplot(table(data_2$HEROPIATE))\n\n\n\n\n\n\n\n# Pretty print table\nkable(table(data_2$HEROPIATE), format = \"html\") %&gt;%\n  kable_styling(bootstrap_options = c(\"condensed\", \"hover\", \"striped\"))\n\n\n\n\nVar1\nFreq\n\n\n\n\nNo\n1552\n\n\nYes\n66\n\n\nNot Specified\n20\n\n\n\n\n\n\n\nOnly 20 visits where participants did not specify drinking frequency on their form.\nLet’s correct those to be NA.\n\n# Convert values of insufficient data to NA for heroin or opiate use\ndata_2$HEROPIATE[data_2$HEROPIATE == \"Not Specified\"] &lt;- NA\n\n# Drop empty levels\ndata_2$HEROPIATE &lt;- droplevels(data_2$HEROPIATE)\n\n# Create barplot of heroin or opiate use\nbarplot(table(data_2$HEROPIATE))\n\n\n\n\n\n\n\n\nLooks good. HEROPIATE is now cleaned!\nBack to top of tabset\n\n\nTook/used drugs with a needle since last visit?\n\n1 = No\n2 = Yes\nBlank = Missing\n\n\n# Create barplot of intravenous drug use\nbarplot(table(data_2$IDU))\n\n\n\n\n\n\n\n\nLooks good. Nothing to do here\nBack to top of tabset\n\n\nAdherence to meds taken since last visit\n\n1 = 100%\n2 = 95-99%\n3 = 75-94%\n4 &lt;75%\nBlank = Missing\n\n\n# Create bar plot of adherence\nbarplot(table(data_2$ADH))\n\n\n\n\n\n\n\n# Pretty print table\nkable(table(data_2$ADH), format = \"html\") %&gt;%\n  kable_styling(bootstrap_options = c(\"condensed\", \"hover\", \"striped\"))\n\n\n\n\nVar1\nFreq\n\n\n\n\n100%\n504\n\n\n95-99%\n474\n\n\n75-94%\n102\n\n\n&lt;75%\n21\n\n\n\n\n\n\n\nVERY interesting. I was thinking that 100% vs 95-99% adherence was an arbitrary difference to choose to divide groups on, and was actually planning to merge the two. However, this shows why the experimenters likely made that decision: both groups have close to the same amount of observations (~500)&gt; That’s really good to know.\nWe could still play with the idea of simplifying this into two groups: &gt;= 95% and &lt; 95%. We will revisit that in the model selection.\nBack to top of tabset\n\n\nRace\n\n1 = White, non-Hispanic\n2 = White, Hispanic\n3 = Black, non-Hispanic\n4 = Black, Hispanic\n5 = American Indian or Alaskan Native\n6 = Asian or Pacific Islander\n7 = Other 8 = Other Hispanic (created for 2001-03 new recruits)\nBlank = Missing\n\n\n# Create barplot of race\nbarplot(table(data_2$RACE))\n\n\n\n\n\n\n\n# Pretty print table\nkable(table(data_2$RACE), format = \"html\") %&gt;%\n  kable_styling(bootstrap_options = c(\"condensed\", \"hover\", \"striped\"))\n\n\n\n\nVar1\nFreq\n\n\n\n\nWhite, non-Hispanic\n1020\n\n\nWhite, Hispanic\n57\n\n\nBlack, non-Hispanic\n474\n\n\nBlack, Hispanic\n9\n\n\nAmerican Indian or Alaskan Native\n0\n\n\nAsian or Pacific Islander\n0\n\n\nOther Hispanic\n18\n\n\n\n\n\n\n\nThis all looks coded properly. As is a common thing I am seeing, we have a predominant proportion of participants who are white, non-Hispanic. The data set might be large enough that we can use race as a covariate.\nIt might be worth dummy coding as white vs non white and see if there are any differences. That’s not the main focus of this project though so I will leave that to if I have extra time at the end.\nBack to top of tabset\n\n\nBaseline or earliest reported education (highest grade or level)\n\n1 = 8th grade or less\n2 = 9,10, or 11th grade\n3 = 12th grade\n4 = At least one year college but no degree\n5 = Four years college / got degree\n6 = Some graduate work\n7 = Post-graduate degree\nBlank = Missing\n\n\n# Create barplot of education at baseline\nbarplot(table(data_2$EDUCBAS))\n\n\n\n\n\n\n\n# Pretty print table\nkable(table(data_2$EDUCBAS), format = \"html\") %&gt;%\n  kable_styling(bootstrap_options = c(\"condensed\", \"hover\", \"striped\"))\n\n\n\n\nVar1\nFreq\n\n\n\n\n8th grade or less\n12\n\n\n9,10, or 11th grade\n138\n\n\n12th grade\n225\n\n\nAt least one year college but no degree\n585\n\n\nFour years college / got degree\n324\n\n\nSome graduate work\n129\n\n\nPost-graduate degree\n237\n\n\n\n\n\n\n\nThis all checks out. And it looks like there are enough participants in each group (except for 8th grade or less) to run analyses with this variable. It will be interesting to see what relationships arise, as I expect there to be a strong association between education and HIV exposure.\nBack to top of tabset\n\n\nHIV Serostatus\n\n0 = Negative\n1 = Positive\n\n\n# Checking that all patients are HIV pos\nany(is.na(data_2$hivpos))\n\n[1] FALSE\n\n\nAll patients in this data set are HIV+\nBack to top of tabset\n\n\nAge at visit\n\n# Create histogram for age\nhist(data_2$age)\n\n\n\n\n\n\n\n# Create qqplot for age\nqqnorm(data_2$age)\nqqline(data_2$age)\n\n\n\n\n\n\n\n\nNice and normally distributed, how we like it.\nBack to top of tabset\n\n\nTake ART at visit\n\n0 = NO\n1 = YES\n\n\n# Create barplot of antiretroviral therapy\nbarplot(table(data_2$ART))\n\n\n\n\n\n\n\n# Pretty print table\nkable(table(data_2$ART), format = \"html\") %&gt;%\n  kable_styling(bootstrap_options = c(\"condensed\", \"hover\", \"striped\"))\n\n\n\n\nVar1\nFreq\n\n\n\n\n0\n550\n\n\n1\n1100\n\n\n\n\n\n\n\nI’m not too sure how useful this variable will be. It just means there were some visits where patients were not given ART, I suppose. But most visits had participants receiving ART.\nBack to top of tabset\n\n\nEver took ART.\n\n0 = NO\n1 = YES\n\n\n# Create barplot of everART\nbarplot(table(data_2$everART))\n\n\n\n\n\n\n\n# Pretty print table\nkable(table(data_2$everART), format = \"html\") %&gt;%\n  kable_styling(bootstrap_options = c(\"condensed\", \"hover\", \"striped\"))\n\n\n\n\nVar1\nFreq\n\n\n\n\n0\n550\n\n\n1\n1100\n\n\n\n\n\n\n\nThis has the exact same split as ART. Which makes me think they are exactly the same values for each participant\n\n# Check if everART and ART are identical\nall(data_2$everART == data_2$ART)\n\n[1] TRUE\n\n\nYup, this is either an accidental duplicate of ART, or there is no distinction of significance between the two. What exactly does “Ever took ART” (the explanation provided by the data dictionary) mean? Was this taken at baseline?\nEither way looks like we’re not using this variable.\nBack to top of tabset\n\n\nHard drug use (either injection drugs or illicit heroin/opiate use) since last visit\n\n0 = No\n1 = Yes\nBlank = Missing\n\n\n# Create barplot of hard drug use category\nbarplot(table(data_2$hard_drugs))\n\n\n\n\n\n\n\n# Pretty print table\nkable(table(data_2$hard_drugs), format = \"html\") %&gt;%\n  kable_styling(bootstrap_options = c(\"condensed\", \"hover\", \"striped\"))\n\n\n\n\nVar1\nFreq\n\n\n\n\nNo\n1452\n\n\nYes\n198\n\n\n\n\n\n\n\nThere were 198 visits where participants had used hard_drugs since the last visit.\nThis variable looks good. We will just have to do some dummy coding to create the categories the researchers were interested in.\nBack to top of tabset\n\n\nFor the CONSORT diagram, we just removed:\n38 visits where patients did not report income.\nX BMI values that were missing and y values that were improbable.\n137 visits with insufficient data for a HBP diagnosis\n881 visits with insufficient data for a DIAB diagnosis.\n507 visits with insufficient data for a LIV34 diagnosis\n1068 visits with insuffcient data for a kidney disease diagnosis\n3 visits with insuffiient data for FRP diagnosis\n357 visits with insufficient data for FP diagnosis\n718 visits with insufficent data for dyslipidemia diagnosis\nX visits with missing values for CESD\n20 visits where heroin or opiate use was not specified\nCOME BACK TO THIS BECAUSE i THINK I NEED TO COME BACK AND DELETE OUTLIERS.\nBack to top of tabset"
  },
  {
    "objectID": "Project_2/Project_2_R/Code/Project2.html#missingness-redux",
    "href": "Project_2/Project_2_R/Code/Project2.html#missingness-redux",
    "title": "Advanced Data Analysis - Project 2",
    "section": "Missingness Redux",
    "text": "Missingness Redux\nWe first examined missingness before performing data cleaning just to get a sense of the data set.\nLet’s compare what our missingness looked like pre- and post-data cleaning.\n\n# Visualize missingness for pre-cleaned data\ngg_miss_var(data)\n\n\n\n\n\n\n\n# Visualize missingness for post-cleaned data\ngg_miss_var(data_2)\n\n\n\n\n\n\n\n\nThe order for missingness has changed, now with KID at the top, followed by DIAB, LDL, TRIG, and DYSLIP.\nTCHOL, LIV34, and income are further behind, with levels of missingness that may be salvageable (~30%).\n\n# Visualize missingness for pre-cleaned dataset\nvis_miss(data)\n\n\n\n\n\n\n\n# Visualize missingness for post-cleaned dataset\nvis_miss(data_2)\n\n\n\n\n\n\n\n\nAnd for good measure let’s now examine missingness in the wide form data set.\n\n# Create new wideform data set for first 2 years of study              \ndata_wide_2 &lt;- pivot_wider(data_2, id_cols = newid, names_from = years, values_from = -c(newid, years))\n\n# Visualize missing values in the wideform data set\nvis_miss(data_wide_2)\n\n\n\n\n\n\n\n\nThere are no real trends that become apparent when looking at this plot.\nTo summarize, it appears that diagnoses that were determined by algorithm often had insufficient data to make a diagnosis, so perhaps this is an issue with those algorithms. Additionally, lab measurements of LDL and TRIG seem to have been too onerous for participants to have gotten. Maybe they opted out of those tests, or maybe the tests were only ordered under certain circumstances.\nThese would be valuable questions to bring forth to the PI. But for now it appear as if we won’t be able to use these variables.\nVariables with a small amount of missingness that we can ignore and let them drop from the analysis when we run the model (i.e. &lt; 5%)\nAGG_MENT, AGG_PHYS, HASH_V, HASHF, FRP, SMOKE, DKGRP, HEROPIATE, IDU, LEU3N, VLOAD, ADH, EDUCBASE, AGE, ART, years, hard-drugs\nVariables with enough missingness we have to address (i.e. 5-20%)\nbaseline cases (~8%) BMI, HBP\nVariables with &gt;20% that are edge cases, may have to drop, maybe can salvage.\nTCHOL\nincome\nLIV34\nVariables with so much missing data we have to drop those variables &gt;40%\n\nLDL\nTRIG\nDIAB\nKID\nDYSLIP"
  },
  {
    "objectID": "Project_2/Project_2_R/Code/Project2.html#change-scores",
    "href": "Project_2/Project_2_R/Code/Project2.html#change-scores",
    "title": "Advanced Data Analysis - Project 2",
    "section": "Change Scores",
    "text": "Change Scores\n\n# Create change scores for outcome variables\ndata_wide_2$VLOAD_log_CHANGE &lt;- data_wide_2$VLOAD_log_2 - data_wide_2$VLOAD_log_0\ndata_wide_2$LEU3N_CHANGE &lt;- data_wide_2$LEU3N_2 - data_wide_2$LEU3N_0\ndata_wide_2$AGG_MENT_CHANGE &lt;- data_wide_2$AGG_MENT_2 - data_wide_2$AGG_MENT_0\ndata_wide_2$AGG_MENT_PHYS &lt;- data_wide_2$AGG_PHYS_2 - data_wide_2$AGG_PHYS_0"
  },
  {
    "objectID": "Project_2/Project_2_R/Code/Project2.html#hard-drug-use-groups",
    "href": "Project_2/Project_2_R/Code/Project2.html#hard-drug-use-groups",
    "title": "Advanced Data Analysis - Project 2",
    "section": "Hard Drug Use Groups",
    "text": "Hard Drug Use Groups\nWe need to make dummy codes for our hard drug use groups.\n\n# Making dummy codes for hard drug use groups\n\n# Create dummy code for current drug users (those who used at year 2)\ndata_wide_2$current_drug &lt;- ifelse(data_wide_2$hard_drugs_2 == \"Yes\", 1, 0) \n\n# Create dummy code for previous drug users (those who used at years 0 or 1, but not 2)\ndata_wide_2$previous_drug &lt;- ifelse((data_wide_2$hard_drugs_1 == \"Yes\" | data_wide_2$hard_drugs_0 == \"Yes\") & data_wide_2$hard_drugs_2 == \"No\", 1, 0)\n\n# Create dummy code for never drug users (did not use at years 0, 1, or 2)\ndata_wide_2$never_drug &lt;- ifelse(data_wide_2$hard_drugs_1 == \"No\" & data_wide_2$hard_drugs_0 == \"No\" & data_wide_2$hard_drugs_2 == \"No\", 1, 0)\n\n# Create a single variable that contains drug use group\ndata_wide_2$hard_drugs_grp &lt;- ifelse(data_wide_2$never_drug == 1, 0, \n                                 ifelse(data_wide_2$previous_drug == 1, 1,\n                                        ifelse(data_wide_2$current_drug == 1, 2, NA)))\n\n# Let's factor this new variable and label it too\ndata_wide_2$hard_drugs_grp &lt;- factor(data_wide_2$hard_drugs_grp,\n                                     levels = c(0,1,2),\n                                     labels = c(\"Never User\", \"Previous User\", \"Current User\"))\n\n# Double check our criteria worked as intended\ndata_wide_2 %&gt;%\n  select(newid, hard_drugs_0, hard_drugs_1, hard_drugs_2, hard_drugs_grp)\n\n# A tibble: 550 × 5\n   newid hard_drugs_0 hard_drugs_1 hard_drugs_2 hard_drugs_grp\n   &lt;dbl&gt; &lt;fct&gt;        &lt;fct&gt;        &lt;fct&gt;        &lt;fct&gt;         \n 1     1 Yes          No           No           Previous User \n 2     2 Yes          No           No           Previous User \n 3     3 Yes          Yes          Yes          Current User  \n 4     4 Yes          Yes          No           Previous User \n 5     5 Yes          Yes          Yes          Current User  \n 6     6 No           No           Yes          Current User  \n 7     7 No           No           Yes          Current User  \n 8     8 Yes          No           No           Previous User \n 9     9 Yes          Yes          Yes          Current User  \n10    10 Yes          No           No           Previous User \n# ℹ 540 more rows"
  }
]