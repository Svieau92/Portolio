[
  {
    "objectID": "testestetst.html",
    "href": "testestetst.html",
    "title": "Untitled",
    "section": "",
    "text": "library(configSAS)\nconfigSAS::set_sas_engine()\n\nUsing SAS Config named: oda\nSAS Connection established. Subprocess id is 9952\n\nsas = knitr::opts_chunk$get(\"sas\")\n\n\nproc candisc data=sashelp.iris out=outcan distance anova;\n   class Species;\n   var SepalLength SepalWidth PetalLength PetalWidth;\nrun;\n\nlstlog\n\n\n&lt;!DOCTYPE html&gt;\n\n\n\n\n\nSAS Output\n\n\n\n\n\n\n\nThe SAS System\n\n\n\n\nThe CANDISC Procedure\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTotal Sample Size\n\n\n150\n\n\nDF Total\n\n\n149\n\n\n\n\nVariables\n\n\n4\n\n\nDF Within Classes\n\n\n147\n\n\n\n\nClasses\n\n\n3\n\n\nDF Between Classes\n\n\n2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNumber of Observations Read\n\n\n150\n\n\n\n\nNumber of Observations Used\n\n\n150\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nClass Level Information\n\n\n\n\nSpecies\n\n\nVariableName\n\n\nFrequency\n\n\nWeight\n\n\nProportion\n\n\n\n\n\n\nSetosa\n\n\nSetosa\n\n\n50\n\n\n50.0000\n\n\n0.333333\n\n\n\n\nVersicolor\n\n\nVersicolor\n\n\n50\n\n\n50.0000\n\n\n0.333333\n\n\n\n\nVirginica\n\n\nVirginica\n\n\n50\n\n\n50.0000\n\n\n0.333333\n\n\n\n\n\n\n\n\nThe SAS System\n\n\n\n\nThe CANDISC Procedure\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSquared Distance to Species\n\n\n\n\nFrom Species\n\n\nSetosa\n\n\nVersicolor\n\n\nVirginica\n\n\n\n\n\n\nSetosa\n\n\n0\n\n\n89.86419\n\n\n179.38471\n\n\n\n\nVersicolor\n\n\n89.86419\n\n\n0\n\n\n17.20107\n\n\n\n\nVirginica\n\n\n179.38471\n\n\n17.20107\n\n\n0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nF Statistics, NDF=4, DDF=144 for Squared Distance to Species\n\n\n\n\nFrom Species\n\n\nSetosa\n\n\nVersicolor\n\n\nVirginica\n\n\n\n\n\n\nSetosa\n\n\n0\n\n\n550.18889\n\n\n1098\n\n\n\n\nVersicolor\n\n\n550.18889\n\n\n0\n\n\n105.31265\n\n\n\n\nVirginica\n\n\n1098\n\n\n105.31265\n\n\n0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProb &gt; Mahalanobis Distance for Squared Distance to Species\n\n\n\n\nFrom Species\n\n\nSetosa\n\n\nVersicolor\n\n\nVirginica\n\n\n\n\n\n\nSetosa\n\n\n1.0000\n\n\n&lt;.0001\n\n\n&lt;.0001\n\n\n\n\nVersicolor\n\n\n&lt;.0001\n\n\n1.0000\n\n\n&lt;.0001\n\n\n\n\nVirginica\n\n\n&lt;.0001\n\n\n&lt;.0001\n\n\n1.0000\n\n\n\n\n\n\n\n\n\nThe SAS System\n\n\n\n\nThe CANDISC Procedure\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUnivariate Test Statistics\n\n\n\n\nF Statistics, Num DF=2, Den DF=147\n\n\n\n\nVariable\n\n\nLabel\n\n\nTotalStandardDeviation\n\n\nPooledStandardDeviation\n\n\nBetweenStandardDeviation\n\n\nR-Square\n\n\nR-Square/ (1-RSq)\n\n\nF Value\n\n\nPr &gt; F\n\n\n\n\n\n\nSepalLength\n\n\nSepal Length (mm)\n\n\n8.2807\n\n\n5.1479\n\n\n7.9506\n\n\n0.6187\n\n\n1.6226\n\n\n119.26\n\n\n&lt;.0001\n\n\n\n\nSepalWidth\n\n\nSepal Width (mm)\n\n\n4.3587\n\n\n3.3969\n\n\n3.3682\n\n\n0.4008\n\n\n0.6688\n\n\n49.16\n\n\n&lt;.0001\n\n\n\n\nPetalLength\n\n\nPetal Length (mm)\n\n\n17.6530\n\n\n4.3033\n\n\n20.9070\n\n\n0.9414\n\n\n16.0566\n\n\n1180.16\n\n\n&lt;.0001\n\n\n\n\nPetalWidth\n\n\nPetal Width (mm)\n\n\n7.6224\n\n\n2.0465\n\n\n8.9673\n\n\n0.9289\n\n\n13.0613\n\n\n960.01\n\n\n&lt;.0001\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAverage R-Square\n\n\n\n\n\n\nUnweighted\n\n\n0.7224358\n\n\n\n\nWeighted by Variance\n\n\n0.8689444\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMultivariate Statistics and F Approximations\n\n\n\n\nS=2 M=0.5 N=71\n\n\n\n\nStatistic\n\n\nValue\n\n\nF Value\n\n\nNum DF\n\n\nDen DF\n\n\nPr &gt; F\n\n\n\n\n\n\nNOTE: F Statistic for Roy's Greatest Root is an upper bound.\n\n\n\n\nNOTE: F Statistic for Wilks' Lambda is exact.\n\n\n\n\n\n\nWilks' Lambda\n\n\n0.02343863\n\n\n199.15\n\n\n8\n\n\n288\n\n\n&lt;.0001\n\n\n\n\nPillai's Trace\n\n\n1.19189883\n\n\n53.47\n\n\n8\n\n\n290\n\n\n&lt;.0001\n\n\n\n\nHotelling-Lawley Trace\n\n\n32.47732024\n\n\n582.20\n\n\n8\n\n\n203.4\n\n\n&lt;.0001\n\n\n\n\nRoy's Greatest Root\n\n\n32.19192920\n\n\n1166.96\n\n\n4\n\n\n145\n\n\n&lt;.0001\n\n\n\n\n\n\n\n\nThe SAS System\n\n\n\n\nThe CANDISC Procedure\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\nCanonicalCorrelation\n\n\nAdjustedCanonicalCorrelation\n\n\nApproximateStandardError\n\n\nSquaredCanonicalCorrelation\n\n\nEigenvalues of Inv(E)*H= CanRsq/(1-CanRsq)\n\n\nTest of H0: The canonical correlations in the current row and all that follow are zero\n\n\n\n\n \n\n\nEigenvalue\n\n\nDifference\n\n\nProportion\n\n\nCumulative\n\n\nLikelihoodRatio\n\n\nApproximateF Value\n\n\nNum DF\n\n\nDen DF\n\n\nPr &gt; F\n\n\n\n\n\n\n1\n\n\n0.984821\n\n\n0.984508\n\n\n0.002468\n\n\n0.969872\n\n\n32.1919\n\n\n31.9065\n\n\n0.9912\n\n\n0.9912\n\n\n0.02343863\n\n\n199.15\n\n\n8\n\n\n288\n\n\n&lt;.0001\n\n\n\n\n2\n\n\n0.471197\n\n\n0.461445\n\n\n0.063734\n\n\n0.222027\n\n\n0.2854\n\n\n \n\n\n0.0088\n\n\n1.0000\n\n\n0.77797337\n\n\n13.79\n\n\n3\n\n\n145\n\n\n&lt;.0001\n\n\n\n\n\n\n\n\nThe SAS System\n\n\n\n\nThe CANDISC Procedure\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTotal Canonical Structure\n\n\n\n\nVariable\n\n\nLabel\n\n\nCan1\n\n\nCan2\n\n\n\n\n\n\nSepalLength\n\n\nSepal Length (mm)\n\n\n0.791888\n\n\n0.217593\n\n\n\n\nSepalWidth\n\n\nSepal Width (mm)\n\n\n-0.530759\n\n\n0.757989\n\n\n\n\nPetalLength\n\n\nPetal Length (mm)\n\n\n0.984951\n\n\n0.046037\n\n\n\n\nPetalWidth\n\n\nPetal Width (mm)\n\n\n0.972812\n\n\n0.222902\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBetween Canonical Structure\n\n\n\n\nVariable\n\n\nLabel\n\n\nCan1\n\n\nCan2\n\n\n\n\n\n\nSepalLength\n\n\nSepal Length (mm)\n\n\n0.991468\n\n\n0.130348\n\n\n\n\nSepalWidth\n\n\nSepal Width (mm)\n\n\n-0.825658\n\n\n0.564171\n\n\n\n\nPetalLength\n\n\nPetal Length (mm)\n\n\n0.999750\n\n\n0.022358\n\n\n\n\nPetalWidth\n\n\nPetal Width (mm)\n\n\n0.994044\n\n\n0.108977\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPooled Within Canonical Structure\n\n\n\n\nVariable\n\n\nLabel\n\n\nCan1\n\n\nCan2\n\n\n\n\n\n\nSepalLength\n\n\nSepal Length (mm)\n\n\n0.222596\n\n\n0.310812\n\n\n\n\nSepalWidth\n\n\nSepal Width (mm)\n\n\n-0.119012\n\n\n0.863681\n\n\n\n\nPetalLength\n\n\nPetal Length (mm)\n\n\n0.706065\n\n\n0.167701\n\n\n\n\nPetalWidth\n\n\nPetal Width (mm)\n\n\n0.633178\n\n\n0.737242\n\n\n\n\n\n\n\n\n\nThe SAS System\n\n\n\n\nThe CANDISC Procedure\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTotal-Sample Standardized Canonical Coefficients\n\n\n\n\nVariable\n\n\nLabel\n\n\nCan1\n\n\nCan2\n\n\n\n\n\n\nSepalLength\n\n\nSepal Length (mm)\n\n\n-0.686779533\n\n\n0.019958173\n\n\n\n\nSepalWidth\n\n\nSepal Width (mm)\n\n\n-0.668825075\n\n\n0.943441829\n\n\n\n\nPetalLength\n\n\nPetal Length (mm)\n\n\n3.885795047\n\n\n-1.645118866\n\n\n\n\nPetalWidth\n\n\nPetal Width (mm)\n\n\n2.142238715\n\n\n2.164135931\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPooled Within-Class Standardized Canonical Coefficients\n\n\n\n\nVariable\n\n\nLabel\n\n\nCan1\n\n\nCan2\n\n\n\n\n\n\nSepalLength\n\n\nSepal Length (mm)\n\n\n-.4269548486\n\n\n0.0124075316\n\n\n\n\nSepalWidth\n\n\nSepal Width (mm)\n\n\n-.5212416758\n\n\n0.7352613085\n\n\n\n\nPetalLength\n\n\nPetal Length (mm)\n\n\n0.9472572487\n\n\n-.4010378190\n\n\n\n\nPetalWidth\n\n\nPetal Width (mm)\n\n\n0.5751607719\n\n\n0.5810398645\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRaw Canonical Coefficients\n\n\n\n\nVariable\n\n\nLabel\n\n\nCan1\n\n\nCan2\n\n\n\n\n\n\nSepalLength\n\n\nSepal Length (mm)\n\n\n-.0829377642\n\n\n0.0024102149\n\n\n\n\nSepalWidth\n\n\nSepal Width (mm)\n\n\n-.1534473068\n\n\n0.2164521235\n\n\n\n\nPetalLength\n\n\nPetal Length (mm)\n\n\n0.2201211656\n\n\n-.0931921210\n\n\n\n\nPetalWidth\n\n\nPetal Width (mm)\n\n\n0.2810460309\n\n\n0.2839187853\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nClass Means on Canonical Variables\n\n\n\n\nSpecies\n\n\nCan1\n\n\nCan2\n\n\n\n\n\n\nSetosa\n\n\n-7.607599927\n\n\n0.215133017\n\n\n\n\nVersicolor\n\n\n1.825049490\n\n\n-0.727899622\n\n\n\n\nVirginica\n\n\n5.782550437\n\n\n0.512766605\n\n\n\n\n\n\n\n\n\n\n\n\u00145                                                          The SAS System                      Sunday, November  3, 2024 06:55:00 AM\n\n24         ods listing close;ods html5 (id=saspy_internal) file=_tomods1 options(bitmap_mode='inline') device=svg style=HTMLBlue;\n24       ! ods graphics on / outputfmt=png;\n25         \n26         proc candisc data=sashelp.iris out=outcan distance anova;\n27            class Species;\n28            var SepalLength SepalWidth PetalLength PetalWidth;\n29         run;\n30         \n31         \n32         ods html5 (id=saspy_internal) close;ods listing;\n33         \n\u00146                                                          The SAS System                      Sunday, November  3, 2024 06:55:00 AM\n\n34"
  },
  {
    "objectID": "Resume/Publications_and_Presentations.html",
    "href": "Resume/Publications_and_Presentations.html",
    "title": "Publications and Presentations",
    "section": "",
    "text": "Publications\nSpeed, F., Saladrigas, C., Teel, A., Vieau, S., Bright, V., Gopinath, J., Welle, C., Restrepo, D., Gibson, E. (2024). High-Speed in Vivo Calcium Recording Using Structured Illumination with Self-Supervised Denoising.” Optics Continuum3 (11): 2044. https://doi.org/10.1364/optcon.532996.\n\n\nProfessional Presentations\nSpeed, F., Kumar, V., Saladrigas, C., Vieau, S., Bright, V., Restrepo, D., Welle, C., Gopinath, J., Kymissis, I., Gibson, E. (2023). Illumination methods for voltage imaging with a microLED array. Poster presented at the Society for Neuroscience\nPavlova, M. , Castillo-Flores, J., Vieau, S., Velumurgan, B., McGrath, P. S., Bush, K., Hopkin, A., Bruckner, A., Kogut, I., Roop, D., Bilousova, G. (2022). A novel preclinical model for translating an induced pluripotent stem cell therapy for the treatment of skin diseases. A poster presented at the International Society for Stem Cell Research 2022.\nSpeed, F., Vieau, S., Parra, A., Kumar, V., Suckow, R., Supekar, D., Peng, X., Sias, A., Hansen, S., Martinez, G., Peet, G., Bright, V., Hughes, E., Gopinath, J., Kymissis, J., Restrepo, D., Welle, C., Gibson, E. (2022). Development of the 3DFAST-GEVI Optical Neural Interface. Poster presented at the Neuroscience Society Retreat.\nVieau, S., Badanes, L. (2019). Acceptance, Reappraisal, and Inhibition: Potential Buffers Against Heightened Cortisol Reactivity Associated with Early Life Stress. A poster presented at the 31st Association for Psychological Science Annual Convention, Washington, D.C.\nVieau, S., Mahalingam, R. (2018). Using CRISPR-Cas9 to deactivate latent varicella zoster virus. A poster presented at the 2018 Impact and Innovation Showcase, Denver, Colorado.\n\n\nUndergraduate Presentations\n(*faculty advisor) (**regional award winner of outstanding empirical research, RMPA)\nAlvarez, L., Maxwell, I. A., Slagle, D. R., Howell, K. S., Hickman, H. A., & Vieau, S. (2016). Neuroticism and mindfulness as predictors of depression and mindfulness as a moderator for depression and neuroticism. A paper symposium presented at the annual conference of the Rocky Mountain Psychological Association, Denver, Colorado.  \nHowell, K., Snyders, J., Vieau, S., Davis, K., Martinez, T., Malek, J., & Gale, B. (2016). Musical preferences as predictors of creativity. A paper symposium presented at the annual Undergraduate Research Conference: A Symposium of Scholarly Work and Creative Projects, Denver, Colorado.\nLee, A., Vieau, S., & Stem, D. (2018). Meditation: Effects on working memory in trauma and non-trauma individuals. A paper symposium presented at the annual conference of the Rocky Mountain Psychological Association, Denver, Colorado.\nMaxwell, I. A., Vieau, S., Haider, A., Malek, J., & Wiggins, A. (2016). The role of gender, stress, and cognitive function. A paper symposium presented at the annual conference of the Rocky Mountain Psychological Association, Denver, Colorado.\nMortensen, C. R*., Cialdini, R. B., Jaeger, C. M., Jacobson, R. P., Maxwell, I. A., Vieau, S., & Neel, R. (2016). Preventing backfire effects with trend information when using social norms for water conservation. A poster presented at the annual conference of the Rocky Mountain Psychological Association, Denver, Colorado.\nRichmond, A. S*., Gale, B., Murphy, P., & Vieau, S. (2016). The validity and reliability of the learner-centered syllabus scale. A poster presented at the annual conference of the Rocky Mountain Psychological Association, Denver, Colorado.\nSchneider, D. G., Stem, D., Vieau, S., & Garofalo, A. M. (2018). Recommended physical activity and mental health. A paper symposium presented at the annual conference of the Rocky Mountain Psychological Association, Denver, Colorado.\nSchneider, D. G., Vieau, S., Kusick, M., & Hanson, T. (2017). 5-minute mindfulness meditation provides relief from state anxiety for experienced and novice meditators. a paper symposium presented at the annual conference of the Rocky Mountain Psychological Association, Salt Lake City, Utah.\nSnyders, J., Vieau, S., Hanson, T., & Rowland, A. (2017). Mindfulness meditation: Threshold of an adverse effect. A paper symposium presented at the annual conference of the Rocky Mountain Psychological Association, Salt Lake City, Utah.\nStem, D., Lee, A., Eccles, R. D., McDonald, E., Vieau, S., & Schneider, D. (2018). The effects of anxiety prime, state anxiety, and trauma. A paper symposium presented at the annual conference of the Rocky Mountain Psychological Association, Denver, Colorado.\nStem, D., Ricciardelli, L., & Vieau, S. (2018). Life satisfaction, meaning in life, and quality of life as predictors of depression in college students. A poster presented at the annual conference of the Rocky Mountain Psychological Association, Denver, Colorado.\n**Vieau, S., Maxwell, I. A., Moore, M., Stem, D., & Davis, K. N. (2017). The effect of synchrony and mindfulness on prosocial attitudes. a paper symposium presented at the annual conference of the Rocky Mountain Psychological Association, Salt Lake City, Utah.\nVieau, S., Noel, S. M., & Kusick, M. (2017). The experience of synchrony leads to harmonization of velocity. A paper symposium presented at the annual conference of the Rocky Mountain Psychological Association, Salt Lake City, Utah.\n**White, M., Pletcher, J., Vieau, S., Garofalo, A. M., Eccles, R. D., & Stem, D. (2018). Does mindfulness moderate sexual assault and PTSD symptoms?. A paper symposium presented at the annual conference of the Rocky Mountain Psychological Association, Denver, Colorado."
  },
  {
    "objectID": "Project_2/Project_2_SAS/Project_2_MLR_SAS.html",
    "href": "Project_2/Project_2_SAS/Project_2_MLR_SAS.html",
    "title": "Project 2 - MLR with Confounding and Interaction (SAS)",
    "section": "",
    "text": "The aim of the current study is to assess how treatment response differs for HIV+ patients 2 years after initiating Highly Active Antiretroviral Therapy (HAART) based on hard drug usage (such as heroin or cocaine). This study is of particular scientific interest because it is unclear whether the use of hard drugs inhibits the immune system in humans; treatment strategies may differ based on these results. The researchers are interested in comparing subjects who never used hard drugs to current hard drug users (those that use hard drugs at year 2) or previous hard drug users (those who used drugs at year 0 or 1). Outcomes of interest are: viral load (HIV copies in a mL of blood), CD4+ T cell count (a measure of immunologic health), and aggregate physical and quality of life scores from the SF-36.\nThe clinical hypothesis is that, if hard drugs inhibit the immune system in humans, subjects who currently or previously used hard drugs will have higher viral load and lower CD4+ T cell counts than those who never used hard drugs. Additionally, the researchers are interested in knowing if potential differences between the drug use groups can be explained by differences in adherence to the treatment regimen. The researchers are agnostic on how quality of life changes after treatment, since side effects of the treatment are significant.\nThe project description provided by the PI is available below:"
  },
  {
    "objectID": "Project_2/Project_2_SAS/Project_2_MLR_SAS.html#connect-to-sas",
    "href": "Project_2/Project_2_SAS/Project_2_MLR_SAS.html#connect-to-sas",
    "title": "Project 2 - MLR with Confounding and Interaction (SAS)",
    "section": "Connect to SAS",
    "text": "Connect to SAS\nThis document was created in RStudio with Quarto.\nTo code in SAS we must first connect RStudio to the SAS server.\n\n# This code connects to SAS On Demand\nlibrary(configSAS)\nconfigSAS::set_sas_engine()\n\nUsing SAS Config named: oda\nSAS Connection established. Subprocess id is 35804\n\n#  This code allows you to run ```{sas}``` chunks\nsas = knitr::opts_chunk$get(\"sas\")\n\n\nCreate Library\nWe begin by making our library for the project.\n\n* Create library;\n%LET CourseRoot = /home/u63376223/sasuser.v94/Advanced Data Analysis;\nLIBNAME Proj2 \"&CourseRoot/Project 2 MLR\";\n\n\n\nImport Data Set\n\n* Import the dataset;\nPROC IMPORT\n    DATAFILE = \"&CourseRoot/Project 2 MLR/hiv_dataset.csv\"\n    OUT = Proj2.raw_data\n    REPLACE;\n    RUN;\n\n\n\nInspect Data\n\n* Check contents of data set;\nODS SELECT Variables;\nPROC CONTENTS DATA = Proj2.raw_data;\n  RUN;\n\nlstlog\n\n\n&lt;!DOCTYPE html&gt;\n\n\n\n\n\nSAS Output\n\n\n\n\n\n\n\nThe SAS System\n\n\n\n\nThe CONTENTS Procedure\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAlphabetic List of Variables and Attributes\n\n\n\n\n#\n\n\nVariable\n\n\nType\n\n\nLen\n\n\nFormat\n\n\nInformat\n\n\n\n\n\n\n25\n\n\nADH\n\n\nChar\n\n\n2\n\n\n$2.\n\n\n$2.\n\n\n\n\n2\n\n\nAGG_MENT\n\n\nNum\n\n\n8\n\n\nBEST12.\n\n\nBEST32.\n\n\n\n\n3\n\n\nAGG_PHYS\n\n\nNum\n\n\n8\n\n\nBEST12.\n\n\nBEST32.\n\n\n\n\n30\n\n\nART\n\n\nNum\n\n\n8\n\n\nBEST12.\n\n\nBEST32.\n\n\n\n\n7\n\n\nBMI\n\n\nNum\n\n\n8\n\n\nBEST12.\n\n\nBEST32.\n\n\n\n\n18\n\n\nCESD\n\n\nNum\n\n\n8\n\n\nBEST12.\n\n\nBEST32.\n\n\n\n\n9\n\n\nDIAB\n\n\nNum\n\n\n8\n\n\nBEST12.\n\n\nBEST32.\n\n\n\n\n20\n\n\nDKGRP\n\n\nNum\n\n\n8\n\n\nBEST12.\n\n\nBEST32.\n\n\n\n\n17\n\n\nDYSLIP\n\n\nNum\n\n\n8\n\n\nBEST12.\n\n\nBEST32.\n\n\n\n\n27\n\n\nEDUCBAS\n\n\nNum\n\n\n8\n\n\nBEST12.\n\n\nBEST32.\n\n\n\n\n13\n\n\nFP\n\n\nNum\n\n\n8\n\n\nBEST12.\n\n\nBEST32.\n\n\n\n\n12\n\n\nFRP\n\n\nNum\n\n\n8\n\n\nBEST12.\n\n\nBEST32.\n\n\n\n\n5\n\n\nHASHF\n\n\nChar\n\n\n2\n\n\n$2.\n\n\n$2.\n\n\n\n\n4\n\n\nHASHV\n\n\nNum\n\n\n8\n\n\nBEST12.\n\n\nBEST32.\n\n\n\n\n8\n\n\nHBP\n\n\nNum\n\n\n8\n\n\nBEST12.\n\n\nBEST32.\n\n\n\n\n21\n\n\nHEROPIATE\n\n\nNum\n\n\n8\n\n\nBEST12.\n\n\nBEST32.\n\n\n\n\n22\n\n\nIDU\n\n\nNum\n\n\n8\n\n\nBEST12.\n\n\nBEST32.\n\n\n\n\n11\n\n\nKID\n\n\nNum\n\n\n8\n\n\nBEST12.\n\n\nBEST32.\n\n\n\n\n16\n\n\nLDL\n\n\nChar\n\n\n3\n\n\n$3.\n\n\n$3.\n\n\n\n\n23\n\n\nLEU3N\n\n\nNum\n\n\n8\n\n\nBEST12.\n\n\nBEST32.\n\n\n\n\n10\n\n\nLIV34\n\n\nNum\n\n\n8\n\n\nBEST12.\n\n\nBEST32.\n\n\n\n\n26\n\n\nRACE\n\n\nNum\n\n\n8\n\n\nBEST12.\n\n\nBEST32.\n\n\n\n\n19\n\n\nSMOKE\n\n\nNum\n\n\n8\n\n\nBEST12.\n\n\nBEST32.\n\n\n\n\n14\n\n\nTCHOL\n\n\nChar\n\n\n3\n\n\n$3.\n\n\n$3.\n\n\n\n\n15\n\n\nTRIG\n\n\nChar\n\n\n3\n\n\n$3.\n\n\n$3.\n\n\n\n\n24\n\n\nVLOAD\n\n\nNum\n\n\n8\n\n\nBEST12.\n\n\nBEST32.\n\n\n\n\n29\n\n\nage\n\n\nNum\n\n\n8\n\n\nBEST12.\n\n\nBEST32.\n\n\n\n\n31\n\n\neverART\n\n\nNum\n\n\n8\n\n\nBEST12.\n\n\nBEST32.\n\n\n\n\n33\n\n\nhard_drugs\n\n\nNum\n\n\n8\n\n\nBEST12.\n\n\nBEST32.\n\n\n\n\n28\n\n\nhivpos\n\n\nNum\n\n\n8\n\n\nBEST12.\n\n\nBEST32.\n\n\n\n\n6\n\n\nincome\n\n\nChar\n\n\n2\n\n\n$2.\n\n\n$2.\n\n\n\n\n1\n\n\nnewid\n\n\nNum\n\n\n8\n\n\nBEST12.\n\n\nBEST32.\n\n\n\n\n32\n\n\nyears\n\n\nNum\n\n\n8\n\n\nBEST12.\n\n\nBEST32.\n\n\n\n\n\n\n\n\n\n\n\n\u00149                                                          The SAS System                      Sunday, November  3, 2024 06:51:00 AM\n\n165        ods listing close;ods html5 (id=saspy_internal) file=_tomods1 options(bitmap_mode='inline') device=svg style=HTMLBlue;\n165      ! ods graphics on / outputfmt=png;\n166        \n167        * Check contents of data set;\n168        ODS SELECT Variables;\n169        PROC CONTENTS DATA = Proj2.raw_data;\n170          RUN;\n171        \n172        \n173        ods html5 (id=saspy_internal) close;ods listing;\n174        \n\u001410                                                         The SAS System                      Sunday, November  3, 2024 06:51:00 AM\n\n175        \n\n\n\n\n\n\nFixing Data Types\nFor some reason we randomly have some of those variables as characters when they should be numeric. Let’s fix that.\n\n* Fix character vars that should actually be numeric;\nDATA Proj2.raw_data;\n    SET Proj2.raw_data;\n      income_num = INPUT(income, BEST32.);\n      HASHF_num = INPUT(HASHF, BEST32.);\n      ADH_num = INPUT(ADH, BEST32.);\n    FORMAT income_num HASHF_num ADH_num BEST32.;\n    DROP income HASHF ADH;\n    RENAME income_num = income HASHF_num = HASHF ADH_num = ADH;\nRUN;\n\n* Check contents of data set;\nODS SELECT Variables;\nPROC CONTENTS DATA = Proj2.raw_data;\n  RUN; \n\nlstlog\n\n\n&lt;!DOCTYPE html&gt;\n\n\n\n\n\nSAS Output\n\n\n\n\n\n\n\nThe SAS System\n\n\n\n\nThe CONTENTS Procedure\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAlphabetic List of Variables and Attributes\n\n\n\n\n#\n\n\nVariable\n\n\nType\n\n\nLen\n\n\nFormat\n\n\nInformat\n\n\n\n\n\n\n33\n\n\nADH\n\n\nNum\n\n\n8\n\n\nBEST32.\n\n\n \n\n\n\n\n2\n\n\nAGG_MENT\n\n\nNum\n\n\n8\n\n\nBEST12.\n\n\nBEST32.\n\n\n\n\n3\n\n\nAGG_PHYS\n\n\nNum\n\n\n8\n\n\nBEST12.\n\n\nBEST32.\n\n\n\n\n27\n\n\nART\n\n\nNum\n\n\n8\n\n\nBEST12.\n\n\nBEST32.\n\n\n\n\n5\n\n\nBMI\n\n\nNum\n\n\n8\n\n\nBEST12.\n\n\nBEST32.\n\n\n\n\n16\n\n\nCESD\n\n\nNum\n\n\n8\n\n\nBEST12.\n\n\nBEST32.\n\n\n\n\n7\n\n\nDIAB\n\n\nNum\n\n\n8\n\n\nBEST12.\n\n\nBEST32.\n\n\n\n\n18\n\n\nDKGRP\n\n\nNum\n\n\n8\n\n\nBEST12.\n\n\nBEST32.\n\n\n\n\n15\n\n\nDYSLIP\n\n\nNum\n\n\n8\n\n\nBEST12.\n\n\nBEST32.\n\n\n\n\n24\n\n\nEDUCBAS\n\n\nNum\n\n\n8\n\n\nBEST12.\n\n\nBEST32.\n\n\n\n\n11\n\n\nFP\n\n\nNum\n\n\n8\n\n\nBEST12.\n\n\nBEST32.\n\n\n\n\n10\n\n\nFRP\n\n\nNum\n\n\n8\n\n\nBEST12.\n\n\nBEST32.\n\n\n\n\n32\n\n\nHASHF\n\n\nNum\n\n\n8\n\n\nBEST32.\n\n\n \n\n\n\n\n4\n\n\nHASHV\n\n\nNum\n\n\n8\n\n\nBEST12.\n\n\nBEST32.\n\n\n\n\n6\n\n\nHBP\n\n\nNum\n\n\n8\n\n\nBEST12.\n\n\nBEST32.\n\n\n\n\n19\n\n\nHEROPIATE\n\n\nNum\n\n\n8\n\n\nBEST12.\n\n\nBEST32.\n\n\n\n\n20\n\n\nIDU\n\n\nNum\n\n\n8\n\n\nBEST12.\n\n\nBEST32.\n\n\n\n\n9\n\n\nKID\n\n\nNum\n\n\n8\n\n\nBEST12.\n\n\nBEST32.\n\n\n\n\n14\n\n\nLDL\n\n\nChar\n\n\n3\n\n\n$3.\n\n\n$3.\n\n\n\n\n21\n\n\nLEU3N\n\n\nNum\n\n\n8\n\n\nBEST12.\n\n\nBEST32.\n\n\n\n\n8\n\n\nLIV34\n\n\nNum\n\n\n8\n\n\nBEST12.\n\n\nBEST32.\n\n\n\n\n23\n\n\nRACE\n\n\nNum\n\n\n8\n\n\nBEST12.\n\n\nBEST32.\n\n\n\n\n17\n\n\nSMOKE\n\n\nNum\n\n\n8\n\n\nBEST12.\n\n\nBEST32.\n\n\n\n\n12\n\n\nTCHOL\n\n\nChar\n\n\n3\n\n\n$3.\n\n\n$3.\n\n\n\n\n13\n\n\nTRIG\n\n\nChar\n\n\n3\n\n\n$3.\n\n\n$3.\n\n\n\n\n22\n\n\nVLOAD\n\n\nNum\n\n\n8\n\n\nBEST12.\n\n\nBEST32.\n\n\n\n\n26\n\n\nage\n\n\nNum\n\n\n8\n\n\nBEST12.\n\n\nBEST32.\n\n\n\n\n28\n\n\neverART\n\n\nNum\n\n\n8\n\n\nBEST12.\n\n\nBEST32.\n\n\n\n\n30\n\n\nhard_drugs\n\n\nNum\n\n\n8\n\n\nBEST12.\n\n\nBEST32.\n\n\n\n\n25\n\n\nhivpos\n\n\nNum\n\n\n8\n\n\nBEST12.\n\n\nBEST32.\n\n\n\n\n31\n\n\nincome\n\n\nNum\n\n\n8\n\n\nBEST32.\n\n\n \n\n\n\n\n1\n\n\nnewid\n\n\nNum\n\n\n8\n\n\nBEST12.\n\n\nBEST32.\n\n\n\n\n29\n\n\nyears\n\n\nNum\n\n\n8\n\n\nBEST12.\n\n\nBEST32.\n\n\n\n\n\n\n\n\n\n\n\n\u001411                                                         The SAS System                      Sunday, November  3, 2024 06:51:00 AM\n\n178        ods listing close;ods html5 (id=saspy_internal) file=_tomods1 options(bitmap_mode='inline') device=svg style=HTMLBlue;\n178      ! ods graphics on / outputfmt=png;\n179        \n180        * Fix character vars that should actually be numeric;\n181        DATA Proj2.raw_data;\n182            SET Proj2.raw_data;\n183              income_num = INPUT(income, BEST32.);\n184              HASHF_num = INPUT(HASHF, BEST32.);\n185              ADH_num = INPUT(ADH, BEST32.);\n186            FORMAT income_num HASHF_num ADH_num BEST32.;\n187            DROP income HASHF ADH;\n188            RENAME income_num = income HASHF_num = HASHF ADH_num = ADH;\n189        RUN;\nnewid=1 AGG_MENT=44.90709633 AGG_PHYS=52.52556921 HASHV=1 HASHF=NA income=4 BMI=24.71755909 HBP=1 DIAB=1 LIV34=1 KID=1 FRP=1 FP=1\nTCHOL=133 TRIG=176 LDL=62 DYSLIP=2 CESD=14 SMOKE=3 DKGRP=0 HEROPIATE=1 IDU=2 LEU3N=104.1659454 VLOAD=102013 ADH=NA RACE=1 EDUCBAS=4\nhivpos=1 age=52 ART=0 everART=0 years=0 hard_drugs=1 income_num=4 HASHF_num=. ADH_num=. _ERROR_=1 _N_=1\nnewid=2 AGG_MENT=46.34189863 AGG_PHYS=27.92331491 HASHV=1 HASHF=0 income=2 BMI=26.66936015 HBP=2 DIAB=9 LIV34=1 KID=9 FRP=2 FP=9\nTCHOL=125 TRIG=NA LDL=NA DYSLIP=2 CESD=20 SMOKE=3 DKGRP=0 HEROPIATE=1 IDU=2 LEU3N=257.8277832 VLOAD=8121 ADH=NA RACE=3 EDUCBAS=2\nhivpos=1 age=54 ART=0 everART=0 years=0 hard_drugs=1 income_num=2 HASHF_num=0 ADH_num=. _ERROR_=1 _N_=5\nnewid=3 AGG_MENT=40.22336832 AGG_PHYS=60.06969568 HASHV=1 HASHF=4 income=6 BMI=28.59084821 HBP=9 DIAB=9 LIV34=1 KID=1 FRP=1 FP=1\nTCHOL=170 TRIG=NA LDL=NA DYSLIP=2 CESD=18 SMOKE=2 DKGRP=0 HEROPIATE=1 IDU=2 LEU3N=563.1222984 VLOAD=4001.556158 ADH=NA RACE=1\nEDUCBAS=7 hivpos=1 age=47 ART=0 everART=0 years=0 hard_drugs=1 income_num=6 HASHF_num=4 ADH_num=. _ERROR_=1 _N_=11\nnewid=3 AGG_MENT=50.03522655 AGG_PHYS=51.93552598 HASHV=1 HASHF=0 income=NA BMI=26.76923019 HBP=1 DIAB=9 LIV34=1 KID=2 FRP=1 FP=1\nTCHOL=180 TRIG=NA LDL=NA DYSLIP=9 CESD=14 SMOKE=2 DKGRP=1 HEROPIATE=1 IDU=2 LEU3N=366.3389365 VLOAD=4.219372675 ADH=2 RACE=1\nEDUCBAS=7 hivpos=1 age=51 ART=1 everART=1 years=4 hard_drugs=1 income_num=. HASHF_num=0 ADH_num=2 _ERROR_=1 _N_=14\nnewid=3 AGG_MENT=58.09463531 AGG_PHYS=57.69027017 HASHV=2 HASHF=0 income=NA BMI=30.03092866 HBP=1 DIAB=9 LIV34=1 KID=2 FRP=1 FP=1\nTCHOL=165 TRIG=NA LDL=NA DYSLIP=2 CESD=0 SMOKE=2 DKGRP=2 HEROPIATE=1 IDU=1 LEU3N=317.7432274 VLOAD=3.726178748 ADH=1 RACE=1\nEDUCBAS=7 hivpos=1 age=55 ART=1 everART=1 years=8 hard_drugs=0 income_num=. HASHF_num=0 ADH_num=1 _ERROR_=1 _N_=18\nnewid=4 AGG_MENT=42.9063758 AGG_PHYS=50.7885018 HASHV=2 HASHF=4 income=1 BMI=20.36451325 HBP=1 DIAB=1 LIV34=1 KID=9 FRP=1 FP=1\nTCHOL=214 TRIG=97 LDL=147 DYSLIP=2 CESD=14 SMOKE=3 DKGRP=1 HEROPIATE=2 IDU=1 LEU3N=110.4217933 VLOAD=740 ADH=NA RACE=1 EDUCBAS=5\nhivpos=1 age=44 ART=0 everART=0 years=0 hard_drugs=1 income_num=1 HASHF_num=4 ADH_num=. _ERROR_=1 _N_=19\nnewid=4 AGG_MENT=52.68222906 AGG_PHYS=51.50532947 HASHV=1 HASHF=0 income=NA BMI=20.28485484 HBP=1 DIAB=1 LIV34=1 KID=9 FRP=1 FP=1\nTCHOL=251 TRIG=260 LDL=152 DYSLIP=2 CESD=13 SMOKE=3 DKGRP=0 HEROPIATE=1 IDU=1 LEU3N=179.6409126 VLOAD=27 ADH=2 RACE=1 EDUCBAS=5\nhivpos=1 age=46 ART=1 everART=1 years=2 hard_drugs=0 income_num=. HASHF_num=0 ADH_num=2 _ERROR_=1 _N_=21\nnewid=5 AGG_MENT=56.42904322 AGG_PHYS=43.75671037 HASHV=1 HASHF=3 income=6 BMI=22.2698601 HBP=1 DIAB=9 LIV34=1 KID=9 FRP=1 FP=9\nTCHOL=196 TRIG=162 LDL=135 DYSLIP=2 CESD=1 SMOKE=3 DKGRP=0 HEROPIATE=-9 IDU=2 LEU3N=252.6633917 VLOAD=62727.03871 ADH=NA RACE=1\nEDUCBAS=7 hivpos=1 age=53 ART=0 everART=0 years=0 hard_drugs=1 income_num=6 HASHF_num=3 ADH_num=. _ERROR_=1 _N_=28\nnewid=5 AGG_MENT=62.45484821 AGG_PHYS=37.13247764 HASHV=2 HASHF=0 income=NA BMI=24.73481164 HBP=2 DIAB=1 LIV34=1 KID=9 FRP=1 FP=9\nTCHOL=209 TRIG=212 LDL=140 DYSLIP=2 CESD=1 SMOKE=3 DKGRP=0 HEROPIATE=1 IDU=2 LEU3N=362.780339 VLOAD=49 ADH=2 RACE=1 EDUCBAS=7\nhivpos=1 age=60 ART=1 everART=1 years=7 hard_drugs=1 income_num=. HASHF_num=0 ADH_num=2 _ERROR_=1 _N_=35\nnewid=6 AGG_MENT=59.74437147 AGG_PHYS=56.86260913 HASHV=1 HASHF=NA income=4 BMI=23.22165848 HBP=1 DIAB=9 LIV34=1 KID=1 FRP=1 FP=1\nTCHOL=216 TRIG=NA LDL=NA DYSLIP=9 CESD=3 SMOKE=1 DKGRP=1 HEROPIATE=1 IDU=1 LEU3N=634.1246084 VLOAD=15745 ADH=NA RACE=1 EDUCBAS=6\nhivpos=1 age=36 ART=0 everART=0 years=0 hard_drugs=0 income_num=4 HASHF_num=. ADH_num=. _ERROR_=1 _N_=37\nnewid=6 AGG_MENT=53.84956108 AGG_PHYS=57.91396015 HASHV=1 HASHF=NA income=4 BMI=23.7531848 HBP=1 DIAB=9 LIV34=1 KID=1 FRP=1 FP=1\nTCHOL=216 TRIG=NA LDL=NA DYSLIP=9 CESD=3 SMOKE=1 DKGRP=1 HEROPIATE=1 IDU=1 LEU3N=745.6517188 VLOAD=7870 ADH=2 RACE=1 EDUCBAS=6\nhivpos=1 age=37 ART=1 everART=1 years=1 hard_drugs=0 income_num=4 HASHF_num=. ADH_num=2 _ERROR_=1 _N_=38\nnewid=6 AGG_MENT=50.26009928 AGG_PHYS=55.95667802 HASHV=1 HASHF=4 income=NA BMI=22.41000894 HBP=1 DIAB=1 LIV34=1 KID=1 FRP=1 FP=1\nTCHOL=151 TRIG=125 LDL=81 DYSLIP=1 CESD=4 SMOKE=1 DKGRP=1 HEROPIATE=1 IDU=2 LEU3N=893.4327659 VLOAD=53.5 ADH=2 RACE=1 EDUCBAS=6\nhivpos=1 age=38 ART=1 everART=1 years=2 hard_drugs=1 income_num=. HASHF_num=4 ADH_num=2 _ERROR_=1 _N_=39\nnewid=7 AGG_MENT=48.90463502 AGG_PHYS=52.78791763 HASHV=1 HASHF=1 income=2 BMI=24.71687978 HBP=9 DIAB=9 LIV34=1 KID=1 FRP=1 FP=1\nTCHOL=NA TRIG=NA LDL=NA DYSLIP=9 CESD=9 SMOKE=2 DKGRP=1 HEROPIATE=1 IDU=1 LEU3N=917.9889891 VLOAD=35171 ADH=NA RACE=7 EDUCBAS=5\nhivpos=1 age=47 ART=0 everART=0 years=0 hard_drugs=0 income_num=2 HASHF_num=1 ADH_num=. _ERROR_=1 _N_=40\nnewid=8 AGG_MENT=49.47258363 AGG_PHYS=55.23964561 HASHV=1 HASHF=2 income=4 BMI=28.85804285 HBP=9 DIAB=9 LIV34=1 KID=1 FRP=1 FP=1\nTCHOL=NA TRIG=NA LDL=NA DYSLIP=9 CESD=10 SMOKE=3 DKGRP=3 HEROPIATE=. IDU=2 LEU3N=552.9199764 VLOAD=326013 ADH=NA RACE=1 EDUCBAS=4\nhivpos=1 age=30 ART=0 everART=0 years=0 hard_drugs=1 income_num=4 HASHF_num=2 ADH_num=. _ERROR_=1 _N_=43\nnewid=8 AGG_MENT=30.53194073 AGG_PHYS=64.30965386 HASHV=1 HASHF=1 income=NA BMI=29.21564539 HBP=1 DIAB=1 LIV34=1 KID=1 FRP=1 FP=1\nTCHOL=171 TRIG=74 LDL=104 DYSLIP=1 CESD=10 SMOKE=2 DKGRP=0 HEROPIATE=1 IDU=1 LEU3N=749.3410624 VLOAD=11 ADH=2 RACE=1 EDUCBAS=4\nhivpos=1 age=31 ART=1 everART=1 years=1 hard_drugs=0 income_num=. HASHF_num=1 ADH_num=2 _ERROR_=1 _N_=44\nnewid=9 AGG_MENT=45.53638994 AGG_PHYS=47.73831907 HASHV=2 HASHF=0 income=6 BMI=19.91076684 HBP=1 DIAB=1 LIV34=1 KID=1 FRP=1 FP=1\nTCHOL=167 TRIG=71 LDL=113 DYSLIP=1 CESD=8 SMOKE=2 DKGRP=1 HEROPIATE=1 IDU=2 LEU3N=459.5461372 VLOAD=13827.21219 ADH=NA RACE=1\nEDUCBAS=7 hivpos=1 age=47 ART=0 everART=0 years=0 hard_drugs=1 income_num=6 HASHF_num=0 ADH_num=. _ERROR_=1 _N_=46\nnewid=9 AGG_MENT=21.70321405 AGG_PHYS=59.12957759 HASHV=2 HASHF=0 income=NA BMI=22.35143274 HBP=2 DIAB=1 LIV34=1 KID=1 FRP=1 FP=1\nTCHOL=184 TRIG=85 LDL=114 DYSLIP=1 CESD=21 SMOKE=2 DKGRP=2 HEROPIATE=1 IDU=2 LEU3N=359.2919589 VLOAD=1.493193928 ADH=1 RACE=1\nEDUCBAS=7 hivpos=1 age=51 ART=1 everART=1 years=4 hard_drugs=1 income_num=. HASHF_num=0 ADH_num=1 _ERROR_=1 _N_=50\nnewid=10 AGG_MENT=39.68873012 AGG_PHYS=48.18627251 HASHV=2 HASHF=2 income=1 BMI=20.59908544 HBP=1 DIAB=9 LIV34=1 KID=9 FRP=1 FP=9\nTCHOL=NA TRIG=NA LDL=NA DYSLIP=9 CESD=21 SMOKE=3 DKGRP=1 HEROPIATE=1 IDU=2 LEU3N=186.7824358 VLOAD=684562 ADH=NA RACE=3 EDUCBAS=2\nhivpos=1 age=40 ART=0 everART=0 years=0 hard_drugs=1 income_num=1 HASHF_num=2 ADH_num=. _ERROR_=1 _N_=52\nnewid=11 AGG_MENT=20.6560246 AGG_PHYS=60.319661 HASHV=1 HASHF=0 income=4 BMI=18.55779905 HBP=2 DIAB=9 LIV34=1 KID=9 FRP=1 FP=1\nTCHOL=98 TRIG=NA LDL=NA DYSLIP=2 CESD=18 SMOKE=3 DKGRP=0 HEROPIATE=2 IDU=1 LEU3N=342.9786179 VLOAD=5605 ADH=NA RACE=3 EDUCBAS=2\nhivpos=1 age=32 ART=0 everART=0 years=0 hard_drugs=1 income_num=4 HASHF_num=0 ADH_num=. _ERROR_=1 _N_=56\nWARNING: Limit set by ERRORS= option reached.  Further errors of this type will not be printed.\nnewid=11 AGG_MENT=25.70015642 AGG_PHYS=51.92545397 HASHV=1 HASHF=0 income=NA BMI=19.82680928 HBP=2 DIAB=9 LIV34=1 KID=9 FRP=1 FP=1\nTCHOL=98 TRIG=NA LDL=NA DYSLIP=2 CESD=23 SMOKE=3 DKGRP=0 HEROPIATE=2 IDU=1 LEU3N=342.6178646 VLOAD=6566 ADH=3 RACE=3 EDUCBAS=2\nhivpos=1 age=33 ART=1 everART=1 years=1 hard_drugs=1 income_num=. HASHF_num=0 ADH_num=3 _ERROR_=1 _N_=57\n190        \n191        * Check contents of data set;\n192        ODS SELECT Variables;\n193        PROC CONTENTS DATA = Proj2.raw_data;\n194          RUN;\n195        \n196        \n197        ods html5 (id=saspy_internal) close;ods listing;\n198        \n\u001412                                                         The SAS System                      Sunday, November  3, 2024 06:51:00 AM\n\n199        \n\n\n\n\nNow everything appears properly imported.\n\n\nHandle NA Values\nWe have missing values in this data set as ‘NA’. SAS will not be able to handle those. We need to convert them to missing values in SAS format (“” for characters and . for numeric);\n\n/* Code from ChatGPT */\n/* Convert 'NA' to missing values and ensure numeric format for multiple variables */\nDATA Proj2.data;\n    SET Proj2.raw_data;\n\n    /* Define arrays for character variables */\n    array char_vars[*] $ _CHAR_;\n\n    /* Loop through character variables to handle 'NA' */\n    do i = 1 to dim(char_vars);\n        if char_vars[i] = 'NA' then char_vars[i] = ' ';\n    end;\n\n    /* Define arrays for numeric variables */\n    array num_vars[*] _NUM_;\n\n    /* Loop through numeric variables to ensure proper format */\n    do i = 1 to dim(num_vars);\n        /* Assuming 'NA' for numeric variables is represented as . (missing value) */\n        num_vars[i] = input(put(num_vars[i], best32.), best32.);\n    end;\nRUN;\n\n\n\nFormat Variables\nWe will then create proper formats of the categorical variables.\n\n* Create format;\nPROC FORMAT;\n    VALUE HASHVfmt\n        1 = 'No'\n        2 = 'Yes';\n    VALUE HASHFfmt\n        1 = \"Never\"\n        2 = \"Daily\"\n        3 = \"Weekly\"\n        4 = \"Monthly\"\n        5 = \"Less Often\";\n    VALUE incomefmt\n        1 = \"Less than $10,000\"\n        2 = \"$10,000-$19,999\"\n        3 = \"$20,000-$29,999\"\n        4 = \"$30,000-$39,999\"\n        5 = \"$40,000-$49,999\"\n        6 = \"$50,000-$59,999\"\n        7 = \"$60,000 or more\"\n        9 = \"Do not wish to answer\";\n    VALUE HBPfmt\n        1 = \"No\"\n        2 = \"Yes\"\n        3 = \"No, based on data trajectory\"\n        4 = \"Yes, based on data trajectory\"\n        9 = \"Insufficient data, may include reported treatment without diagnosis\"\n       -1 = \"Improbable Value\";\n    VALUE DIABfmt\n        1 = \"No\"\n        2 = \"Yes\"\n        3 = \"No, based on data trajectory\"\n        4 = \"Yes, based on data trajectory\"\n        9 = \"Insufficient data\";\n    VALUE LIV34fmt\n        1 = \"No\"\n        2 = \"Yes\"\n        9 = \"Insufficient Data\";\n    VALUE KIDfmt\n        1 = \"No\"\n        2 = \"Yes\"\n        3 = \"No, based on data trajectory\"\n        4 = \"Yes, based on data trajectory\"\n        9 = \"Insufficient data\";\n    VALUE FRPfmt\n        1 = \"No\"\n        2 = \"Yes\"\n        9 = \"Insufficient Data\";\n    VALUE FPfmt\n        1 = \"No\"\n        2 = \"Yes\"\n        9 = \"Insufficient Data\";\n    VALUE DYSLIPfmt\n        1 = \"No\"\n        2 = \"Yes\"\n        3 = \"No, based on data trajectory\"\n        4 = \"Yes, based on data trajectory\"\n        9 = \"Insufficient data\";\n    VALUE SMOKEfmt\n        1 = \"Never Smoked\"\n        2 = \"Former Smoker\"\n        3 = \"Current Smoker\";\n    VALUE DKGRPfmt\n        0 = \"None\"\n        1 = \"1-3 drinks/week\"\n        2 = \"4-13 drinks/week\"\n        3 = \"&gt;13 drinks/week\";\n    VALUE HEROPIATEfmt\n        1 = \"No\"\n        2 = \"Yes\"\n       -9 = \"Not Specified\";\n    VALUE IDUfmt\n        1 = \"No\"\n        2 = \"Yes\";\n    VALUE ADHfmt\n        1 = \"100%\"\n        2 = \"95-99%\"\n        3 = \"75-94%\"\n        4 = \"&lt;75%\";\n    VALUE RACEfmt\n        1 = \"White, non-Hispanic\"\n        2 = \"White, Hispanic\"\n        3 = \"Black, non-Hispanic\"\n        4 = \"Black, Hispanic\"\n        5 = \"American Indian or Alaskan Native\"\n        6 = \"Asian or Pacific Islander\"\n        7 = \"Other Hispanic\";\n    VALUE EDUCBASfmt\n        1 = \"8th grade or less\"\n        2 = \"9,10, or 11th grade\"\n        3 = \"12th grade\"\n        4 = \"At least one year college but no degree\"\n        5 = \"Four years college or got degree\"\n        6 = \"Some graduate work\"\n        7 = \"Post-graduate degree\";\n    VALUE hard_drugsfmt\n        0 = \"No\"\n        1 = \"Yes\";\nRUN;\n\n/* Apply formats to your dataset */\nDATA Proj2.data;\n    SET Proj2.raw_data;\n    FORMAT HASHV HASHVfmt. HASHF HASHFfmt. income2 incomefmt. HBP HBPfmt. DIAB DIABfmt. LIV34 LIV34fmt. KID KIDfmt. FRP FRPfmt. FP FPfmt. DYSLIP DYSLIPfmt. \n           SMOKE SMOKEfmt. DKGRP DKGRPfmt. HEROPIATE HEROPIATEfmt. IDU IDUfmt. ADH ADHfmt. RACE RACEfmt. \n           EDUCBAS EDUCBASfmt. hard_drugs hard_drugsfmt.;\nRUN;\n\n\n\n\nCreate Labels\nLet’s also change the labels of all variable names so they are capitalized and make our output look more professional.\n\n* Change labels to capitalize variable names;\nDATA Proj2.data;\n    SET Proj2.raw_data;\n    LABEL \n        newid = \"ID\"\n        AGG_MENT = \"Aggregate Mental QOL Score\"\n        AGG_PHYS = \"Aggregate Physical QOL Score\"\n        HASHF = \"Hash/Marijuana Use Since Last Visit\"\n        HASHV = \"Frequency of Hash/Marijuana Use\"\n        income = \"Income\"\n        HBP = \"High Blood Pressure\"\n        DIAB = \"Diabetes\"\n        LIV34 = \"Liver Disease Stage 3/4\"\n        KID = \"Kidney Disease\"\n        FRP = \"Frailty Related Phenotype\"\n        FP = \"Frailty Phenotype\"\n        BMI = \"BMI\"\n        TCHOL = \"Total Cholesterol\"\n        TRIG = \"Triglycerides\"\n        LDL = \"LDL\"\n        DYSLIP = \"Dyslipidemia\"\n        SMOKE = \"Smoking Status\"\n        CESD = \"CESD Depression Score\"\n        DKGRP = \"Drinking Group\"\n        HEROPIATE = \"Heroin or Opiate Use Since Last Visit\"\n        IDU = \"Intravenous Drug Usage Since Last Visit\"\n        LEU3N = \"CD4+ T Cell Count\"\n        VLOAD = \"Viral Load"
  },
  {
    "objectID": "Project_2/Project_2_SAS/Project_2_MLR_SAS.html#transpose-to-wideform",
    "href": "Project_2/Project_2_SAS/Project_2_MLR_SAS.html#transpose-to-wideform",
    "title": "Project 2 - MLR with Confounding and Interaction (SAS)",
    "section": "Transpose to Wideform",
    "text": "Transpose to Wideform\nWe can also see that the provided data set is in longform. Let’s convert that to wideform.\n\nPROC SORT DATA = Proj2.data2;\n  BY years newid;\n  RUN;\n\n* Transpose the data out of wide format into long format;\nPROC TRANSPOSE DATA=Proj2.data2 OUT=Proj2.data_wide_2 PREFIX=Year;\n    BY years;\n    ID newid;\n    VAR _ALL_;\nRUN;\n\nPROC PRINT DATA = Proj2.data_wide_2 (OBS = 100);\n      RUN;\n\n\nlstlog\n\n\n&lt;!DOCTYPE html&gt;\n\n\n\n\n\nSAS Output\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObs\n\n\nyears\n\n\nNAME\n\n\nLABEL\n\n\nYear1\n\n\nYear2\n\n\nYear3\n\n\nYear4\n\n\nYear5\n\n\nYear6\n\n\nYear7\n\n\nYear8\n\n\nYear9\n\n\nYear10\n\n\nYear11\n\n\nYear12\n\n\nYear13\n\n\nYear14\n\n\nYear15\n\n\nYear16\n\n\nYear17\n\n\nYear18\n\n\nYear19\n\n\nYear20\n\n\nYear21\n\n\nYear22\n\n\nYear23\n\n\nYear24\n\n\nYear25\n\n\nYear26\n\n\nYear27\n\n\nYear28\n\n\nYear29\n\n\nYear30\n\n\nYear31\n\n\nYear32\n\n\nYear33\n\n\nYear34\n\n\nYear35\n\n\nYear36\n\n\nYear37\n\n\nYear38\n\n\nYear39\n\n\nYear40\n\n\nYear41\n\n\nYear42\n\n\nYear43\n\n\nYear44\n\n\nYear45\n\n\nYear46\n\n\nYear47\n\n\nYear48\n\n\nYear49\n\n\nYear50\n\n\nYear51\n\n\nYear52\n\n\nYear53\n\n\nYear54\n\n\nYear55\n\n\nYear56\n\n\nYear57\n\n\nYear58\n\n\nYear59\n\n\nYear60\n\n\nYear61\n\n\nYear62\n\n\nYear63\n\n\nYear64\n\n\nYear65\n\n\nYear66\n\n\nYear67\n\n\nYear68\n\n\nYear69\n\n\nYear70\n\n\nYear71\n\n\nYear72\n\n\nYear73\n\n\nYear74\n\n\nYear75\n\n\nYear76\n\n\nYear77\n\n\nYear78\n\n\nYear79\n\n\nYear80\n\n\nYear81\n\n\nYear82\n\n\nYear83\n\n\nYear84\n\n\nYear85\n\n\nYear86\n\n\nYear87\n\n\nYear88\n\n\nYear89\n\n\nYear90\n\n\nYear91\n\n\nYear92\n\n\nYear93\n\n\nYear94\n\n\nYear95\n\n\nYear96\n\n\nYear97\n\n\nYear98\n\n\nYear99\n\n\nYear100\n\n\nYear101\n\n\nYear102\n\n\nYear103\n\n\nYear104\n\n\nYear105\n\n\nYear106\n\n\nYear107\n\n\nYear108\n\n\nYear109\n\n\nYear110\n\n\nYear111\n\n\nYear112\n\n\nYear113\n\n\nYear114\n\n\nYear115\n\n\nYear116\n\n\nYear117\n\n\nYear118\n\n\nYear119\n\n\nYear120\n\n\nYear121\n\n\nYear122\n\n\nYear123\n\n\nYear124\n\n\nYear125\n\n\nYear126\n\n\nYear127\n\n\nYear128\n\n\nYear129\n\n\nYear130\n\n\nYear131\n\n\nYear132\n\n\nYear133\n\n\nYear134\n\n\nYear135\n\n\nYear136\n\n\nYear137\n\n\nYear138\n\n\nYear139\n\n\nYear140\n\n\nYear141\n\n\nYear142\n\n\nYear143\n\n\nYear144\n\n\nYear145\n\n\nYear146\n\n\nYear147\n\n\nYear148\n\n\nYear149\n\n\nYear150\n\n\nYear151\n\n\nYear152\n\n\nYear153\n\n\nYear154\n\n\nYear155\n\n\nYear156\n\n\nYear157\n\n\nYear158\n\n\nYear159\n\n\nYear160\n\n\nYear161\n\n\nYear162\n\n\nYear163\n\n\nYear164\n\n\nYear165\n\n\nYear166\n\n\nYear167\n\n\nYear168\n\n\nYear169\n\n\nYear170\n\n\nYear171\n\n\nYear172\n\n\nYear173\n\n\nYear174\n\n\nYear175\n\n\nYear176\n\n\nYear177\n\n\nYear178\n\n\nYear179\n\n\nYear180\n\n\nYear181\n\n\nYear182\n\n\nYear183\n\n\nYear184\n\n\nYear185\n\n\nYear186\n\n\nYear187\n\n\nYear188\n\n\nYear189\n\n\nYear190\n\n\nYear191\n\n\nYear192\n\n\nYear193\n\n\nYear194\n\n\nYear195\n\n\nYear196\n\n\nYear197\n\n\nYear198\n\n\nYear199\n\n\nYear200\n\n\nYear201\n\n\nYear202\n\n\nYear203\n\n\nYear204\n\n\nYear205\n\n\nYear206\n\n\nYear207\n\n\nYear208\n\n\nYear209\n\n\nYear210\n\n\nYear211\n\n\nYear212\n\n\nYear213\n\n\nYear214\n\n\nYear215\n\n\nYear216\n\n\nYear217\n\n\nYear218\n\n\nYear219\n\n\nYear220\n\n\nYear221\n\n\nYear222\n\n\nYear223\n\n\nYear224\n\n\nYear225\n\n\nYear226\n\n\nYear227\n\n\nYear228\n\n\nYear229\n\n\nYear230\n\n\nYear231\n\n\nYear232\n\n\nYear233\n\n\nYear234\n\n\nYear235\n\n\nYear236\n\n\nYear237\n\n\nYear238\n\n\nYear239\n\n\nYear240\n\n\nYear241\n\n\nYear242\n\n\nYear243\n\n\nYear244\n\n\nYear245\n\n\nYear246\n\n\nYear247\n\n\nYear248\n\n\nYear249\n\n\nYear250\n\n\nYear251\n\n\nYear252\n\n\nYear253\n\n\nYear254\n\n\nYear255\n\n\nYear256\n\n\nYear257\n\n\nYear258\n\n\nYear259\n\n\nYear260\n\n\nYear261\n\n\nYear262\n\n\nYear263\n\n\nYear264\n\n\nYear265\n\n\nYear266\n\n\nYear267\n\n\nYear268\n\n\nYear269\n\n\nYear270\n\n\nYear271\n\n\nYear272\n\n\nYear273\n\n\nYear274\n\n\nYear275\n\n\nYear276\n\n\nYear277\n\n\nYear278\n\n\nYear279\n\n\nYear280\n\n\nYear281\n\n\nYear282\n\n\nYear283\n\n\nYear284\n\n\nYear285\n\n\nYear286\n\n\nYear287\n\n\nYear288\n\n\nYear289\n\n\nYear290\n\n\nYear291\n\n\nYear292\n\n\nYear293\n\n\nYear294\n\n\nYear295\n\n\nYear296\n\n\nYear297\n\n\nYear298\n\n\nYear299\n\n\nYear300\n\n\nYear301\n\n\nYear302\n\n\nYear303\n\n\nYear304\n\n\nYear305\n\n\nYear306\n\n\nYear307\n\n\nYear308\n\n\nYear309\n\n\nYear310\n\n\nYear311\n\n\nYear312\n\n\nYear313\n\n\nYear314\n\n\nYear315\n\n\nYear316\n\n\nYear317\n\n\nYear318\n\n\nYear319\n\n\nYear320\n\n\nYear321\n\n\nYear322\n\n\nYear323\n\n\nYear324\n\n\nYear325\n\n\nYear326\n\n\nYear327\n\n\nYear328\n\n\nYear329\n\n\nYear330\n\n\nYear331\n\n\nYear332\n\n\nYear333\n\n\nYear334\n\n\nYear335\n\n\nYear336\n\n\nYear337\n\n\nYear338\n\n\nYear339\n\n\nYear340\n\n\nYear341\n\n\nYear342\n\n\nYear343\n\n\nYear344\n\n\nYear345\n\n\nYear346\n\n\nYear347\n\n\nYear348\n\n\nYear349\n\n\nYear350\n\n\nYear351\n\n\nYear352\n\n\nYear353\n\n\nYear354\n\n\nYear355\n\n\nYear356\n\n\nYear357\n\n\nYear358\n\n\nYear359\n\n\nYear360\n\n\nYear361\n\n\nYear362\n\n\nYear363\n\n\nYear364\n\n\nYear365\n\n\nYear366\n\n\nYear367\n\n\nYear368\n\n\nYear369\n\n\nYear370\n\n\nYear371\n\n\nYear372\n\n\nYear373\n\n\nYear374\n\n\nYear375\n\n\nYear376\n\n\nYear377\n\n\nYear378\n\n\nYear379\n\n\nYear380\n\n\nYear381\n\n\nYear382\n\n\nYear383\n\n\nYear384\n\n\nYear385\n\n\nYear386\n\n\nYear387\n\n\nYear388\n\n\nYear389\n\n\nYear390\n\n\nYear391\n\n\nYear392\n\n\nYear393\n\n\nYear394\n\n\nYear395\n\n\nYear396\n\n\nYear397\n\n\nYear398\n\n\nYear399\n\n\nYear400\n\n\nYear401\n\n\nYear402\n\n\nYear403\n\n\nYear404\n\n\nYear405\n\n\nYear406\n\n\nYear407\n\n\nYear408\n\n\nYear409\n\n\nYear410\n\n\nYear411\n\n\nYear412\n\n\nYear413\n\n\nYear414\n\n\nYear415\n\n\nYear416\n\n\nYear417\n\n\nYear418\n\n\nYear419\n\n\nYear420\n\n\nYear421\n\n\nYear422\n\n\nYear423\n\n\nYear424\n\n\nYear425\n\n\nYear426\n\n\nYear427\n\n\nYear428\n\n\nYear429\n\n\nYear430\n\n\nYear431\n\n\nYear432\n\n\nYear433\n\n\nYear434\n\n\nYear435\n\n\nYear436\n\n\nYear437\n\n\nYear438\n\n\nYear439\n\n\nYear440\n\n\nYear441\n\n\nYear442\n\n\nYear443\n\n\nYear444\n\n\nYear445\n\n\nYear446\n\n\nYear447\n\n\nYear448\n\n\nYear449\n\n\nYear450\n\n\nYear451\n\n\nYear452\n\n\nYear453\n\n\nYear454\n\n\nYear455\n\n\nYear456\n\n\nYear457\n\n\nYear458\n\n\nYear459\n\n\nYear460\n\n\nYear461\n\n\nYear462\n\n\nYear463\n\n\nYear464\n\n\nYear465\n\n\nYear466\n\n\nYear467\n\n\nYear468\n\n\nYear469\n\n\nYear470\n\n\nYear471\n\n\nYear472\n\n\nYear473\n\n\nYear474\n\n\nYear475\n\n\nYear476\n\n\nYear477\n\n\nYear478\n\n\nYear479\n\n\nYear480\n\n\nYear481\n\n\nYear482\n\n\nYear483\n\n\nYear484\n\n\nYear485\n\n\nYear486\n\n\nYear487\n\n\nYear488\n\n\nYear489\n\n\nYear490\n\n\nYear491\n\n\nYear492\n\n\nYear493\n\n\nYear494\n\n\nYear495\n\n\nYear496\n\n\nYear497\n\n\nYear498\n\n\nYear499\n\n\nYear500\n\n\nYear501\n\n\nYear502\n\n\nYear503\n\n\nYear504\n\n\nYear505\n\n\nYear506\n\n\nYear507\n\n\nYear508\n\n\nYear509\n\n\nYear510\n\n\nYear511\n\n\nYear512\n\n\nYear513\n\n\nYear514\n\n\nYear515\n\n\nYear516\n\n\nYear517\n\n\nYear518\n\n\nYear519\n\n\nYear520\n\n\nYear521\n\n\nYear522\n\n\nYear523\n\n\nYear524\n\n\nYear525\n\n\nYear526\n\n\nYear527\n\n\nYear528\n\n\nYear529\n\n\nYear530\n\n\nYear531\n\n\nYear532\n\n\nYear533\n\n\nYear534\n\n\nYear535\n\n\nYear536\n\n\nYear537\n\n\nYear538\n\n\nYear539\n\n\nYear540\n\n\nYear541\n\n\nYear542\n\n\nYear543\n\n\nYear544\n\n\nYear545\n\n\nYear546\n\n\nYear547\n\n\nYear548\n\n\nYear549\n\n\nYear550\n\n\n\n\n\n\n1\n\n\n0\n\n\nnewid\n\n\nID\n\n\n1\n\n\n2\n\n\n3\n\n\n4\n\n\n5\n\n\n6\n\n\n7\n\n\n8\n\n\n9\n\n\n10\n\n\n11\n\n\n12\n\n\n13\n\n\n14\n\n\n15\n\n\n16\n\n\n17\n\n\n18\n\n\n19\n\n\n20\n\n\n21\n\n\n22\n\n\n23\n\n\n24\n\n\n25\n\n\n26\n\n\n27\n\n\n28\n\n\n29\n\n\n30\n\n\n31\n\n\n32\n\n\n33\n\n\n34\n\n\n35\n\n\n36\n\n\n37\n\n\n38\n\n\n39\n\n\n40\n\n\n41\n\n\n42\n\n\n43\n\n\n44\n\n\n45\n\n\n46\n\n\n47\n\n\n48\n\n\n49\n\n\n50\n\n\n51\n\n\n52\n\n\n53\n\n\n54\n\n\n55\n\n\n56\n\n\n57\n\n\n58\n\n\n59\n\n\n60\n\n\n61\n\n\n62\n\n\n63\n\n\n64\n\n\n65\n\n\n66\n\n\n67\n\n\n68\n\n\n69\n\n\n70\n\n\n71\n\n\n72\n\n\n73\n\n\n74\n\n\n75\n\n\n76\n\n\n77\n\n\n78\n\n\n79\n\n\n80\n\n\n81\n\n\n82\n\n\n83\n\n\n84\n\n\n85\n\n\n86\n\n\n87\n\n\n88\n\n\n89\n\n\n90\n\n\n91\n\n\n92\n\n\n93\n\n\n94\n\n\n95\n\n\n96\n\n\n97\n\n\n98\n\n\n99\n\n\n100\n\n\n101\n\n\n102\n\n\n103\n\n\n104\n\n\n105\n\n\n106\n\n\n107\n\n\n108\n\n\n109\n\n\n110\n\n\n111\n\n\n112\n\n\n113\n\n\n114\n\n\n115\n\n\n116\n\n\n117\n\n\n118\n\n\n119\n\n\n120\n\n\n121\n\n\n122\n\n\n123\n\n\n124\n\n\n125\n\n\n126\n\n\n127\n\n\n128\n\n\n129\n\n\n130\n\n\n131\n\n\n132\n\n\n133\n\n\n134\n\n\n135\n\n\n136\n\n\n137\n\n\n138\n\n\n139\n\n\n140\n\n\n141\n\n\n142\n\n\n143\n\n\n144\n\n\n145\n\n\n146\n\n\n147\n\n\n148\n\n\n149\n\n\n150\n\n\n151\n\n\n152\n\n\n153\n\n\n154\n\n\n155\n\n\n156\n\n\n157\n\n\n158\n\n\n159\n\n\n160\n\n\n161\n\n\n162\n\n\n163\n\n\n164\n\n\n165\n\n\n166\n\n\n167\n\n\n168\n\n\n169\n\n\n170\n\n\n171\n\n\n172\n\n\n173\n\n\n174\n\n\n175\n\n\n176\n\n\n177\n\n\n178\n\n\n179\n\n\n180\n\n\n181\n\n\n182\n\n\n183\n\n\n184\n\n\n185\n\n\n186\n\n\n187\n\n\n188\n\n\n189\n\n\n190\n\n\n191\n\n\n192\n\n\n193\n\n\n194\n\n\n195\n\n\n196\n\n\n197\n\n\n198\n\n\n199\n\n\n200\n\n\n201\n\n\n202\n\n\n203\n\n\n204\n\n\n205\n\n\n206\n\n\n207\n\n\n208\n\n\n209\n\n\n210\n\n\n211\n\n\n212\n\n\n213\n\n\n214\n\n\n215\n\n\n216\n\n\n217\n\n\n218\n\n\n219\n\n\n220\n\n\n221\n\n\n222\n\n\n223\n\n\n224\n\n\n225\n\n\n226\n\n\n227\n\n\n228\n\n\n229\n\n\n230\n\n\n231\n\n\n232\n\n\n233\n\n\n234\n\n\n235\n\n\n236\n\n\n237\n\n\n238\n\n\n239\n\n\n240\n\n\n241\n\n\n242\n\n\n243\n\n\n244\n\n\n245\n\n\n246\n\n\n247\n\n\n248\n\n\n249\n\n\n250\n\n\n251\n\n\n252\n\n\n253\n\n\n254\n\n\n255\n\n\n256\n\n\n257\n\n\n258\n\n\n259\n\n\n260\n\n\n261\n\n\n262\n\n\n263\n\n\n264\n\n\n265\n\n\n266\n\n\n267\n\n\n268\n\n\n269\n\n\n270\n\n\n271\n\n\n272\n\n\n273\n\n\n274\n\n\n275\n\n\n276\n\n\n277\n\n\n278\n\n\n279\n\n\n280\n\n\n281\n\n\n282\n\n\n283\n\n\n284\n\n\n285\n\n\n286\n\n\n287\n\n\n288\n\n\n289\n\n\n290\n\n\n291\n\n\n292\n\n\n293\n\n\n294\n\n\n295\n\n\n296\n\n\n297\n\n\n298\n\n\n299\n\n\n300\n\n\n301\n\n\n302\n\n\n303\n\n\n304\n\n\n305\n\n\n306\n\n\n307\n\n\n308\n\n\n309\n\n\n310\n\n\n311\n\n\n312\n\n\n313\n\n\n314\n\n\n315\n\n\n316\n\n\n317\n\n\n318\n\n\n319\n\n\n320\n\n\n321\n\n\n322\n\n\n323\n\n\n324\n\n\n325\n\n\n326\n\n\n327\n\n\n328\n\n\n329\n\n\n330\n\n\n331\n\n\n332\n\n\n333\n\n\n334\n\n\n335\n\n\n336\n\n\n337\n\n\n338\n\n\n339\n\n\n340\n\n\n341\n\n\n342\n\n\n343\n\n\n344\n\n\n345\n\n\n346\n\n\n347\n\n\n348\n\n\n349\n\n\n350\n\n\n351\n\n\n352\n\n\n353\n\n\n354\n\n\n355\n\n\n356\n\n\n357\n\n\n358\n\n\n359\n\n\n360\n\n\n361\n\n\n362\n\n\n363\n\n\n364\n\n\n365\n\n\n366\n\n\n367\n\n\n368\n\n\n369\n\n\n370\n\n\n371\n\n\n372\n\n\n373\n\n\n374\n\n\n375\n\n\n376\n\n\n377\n\n\n378\n\n\n379\n\n\n380\n\n\n381\n\n\n382\n\n\n383\n\n\n384\n\n\n385\n\n\n386\n\n\n387\n\n\n388\n\n\n389\n\n\n390\n\n\n391\n\n\n392\n\n\n393\n\n\n394\n\n\n395\n\n\n396\n\n\n397\n\n\n398\n\n\n399\n\n\n400\n\n\n401\n\n\n402\n\n\n403\n\n\n404\n\n\n405\n\n\n406\n\n\n407\n\n\n408\n\n\n409\n\n\n410\n\n\n411\n\n\n412\n\n\n413\n\n\n414\n\n\n415\n\n\n416\n\n\n417\n\n\n418\n\n\n419\n\n\n420\n\n\n421\n\n\n422\n\n\n423\n\n\n424\n\n\n425\n\n\n426\n\n\n427\n\n\n428\n\n\n429\n\n\n430\n\n\n431\n\n\n432\n\n\n433\n\n\n434\n\n\n435\n\n\n436\n\n\n437\n\n\n438\n\n\n439\n\n\n440\n\n\n441\n\n\n442\n\n\n443\n\n\n444\n\n\n445\n\n\n446\n\n\n447\n\n\n448\n\n\n449\n\n\n450\n\n\n451\n\n\n452\n\n\n453\n\n\n454\n\n\n455\n\n\n456\n\n\n457\n\n\n458\n\n\n459\n\n\n460\n\n\n461\n\n\n462\n\n\n463\n\n\n464\n\n\n465\n\n\n466\n\n\n467\n\n\n468\n\n\n469\n\n\n470\n\n\n471\n\n\n472\n\n\n473\n\n\n474\n\n\n475\n\n\n476\n\n\n477\n\n\n478\n\n\n479\n\n\n480\n\n\n481\n\n\n482\n\n\n483\n\n\n484\n\n\n485\n\n\n486\n\n\n487\n\n\n488\n\n\n489\n\n\n490\n\n\n491\n\n\n492\n\n\n493\n\n\n494\n\n\n495\n\n\n496\n\n\n497\n\n\n498\n\n\n499\n\n\n500\n\n\n501\n\n\n502\n\n\n503\n\n\n504\n\n\n505\n\n\n506\n\n\n507\n\n\n508\n\n\n509\n\n\n510\n\n\n511\n\n\n512\n\n\n513\n\n\n514\n\n\n515\n\n\n516\n\n\n517\n\n\n518\n\n\n519\n\n\n520\n\n\n521\n\n\n522\n\n\n523\n\n\n524\n\n\n525\n\n\n526\n\n\n527\n\n\n528\n\n\n529\n\n\n530\n\n\n531\n\n\n532\n\n\n533\n\n\n534\n\n\n535\n\n\n536\n\n\n537\n\n\n538\n\n\n539\n\n\n540\n\n\n541\n\n\n542\n\n\n543\n\n\n544\n\n\n545\n\n\n546\n\n\n547\n\n\n548\n\n\n549\n\n\n550\n\n\n\n\n2\n\n\n0\n\n\nAGG_MENT\n\n\nAggregate Mental QOL Score\n\n\n44.90709633\n\n\n46.34189863\n\n\n40.22336832\n\n\n42.9063758\n\n\n56.42904322\n\n\n59.74437147\n\n\n48.90463502\n\n\n49.47258363\n\n\n45.53638994\n\n\n39.68873012\n\n\n20.6560246\n\n\n54.81455227\n\n\n28.85739466\n\n\n25.40110088\n\n\n56.64555861\n\n\n55.27925182\n\n\n60.20028926\n\n\n30.50618267\n\n\n29.23291201\n\n\n41.74346997\n\n\n14.49701143\n\n\n47.8433612\n\n\n44.64371009\n\n\n36.95605011\n\n\n57.26127545\n\n\n55.90154007\n\n\n52.99936528\n\n\n44.93578751\n\n\n37.33909628\n\n\n55.94105437\n\n\n28.84225258\n\n\n24.17841567\n\n\n54.24091072\n\n\n59.35758878\n\n\n31.00778993\n\n\n42.83212235\n\n\n15.00132338\n\n\n47.20816568\n\n\n45.7161055\n\n\n37.44118744\n\n\n54.82161077\n\n\n59.39219818\n\n\n50.17125134\n\n\n47.09464045\n\n\n37.97032038\n\n\n52.56847057\n\n\n26.84991868\n\n\n21.91253399\n\n\n54.03847322\n\n\n57.25770517\n\n\n31.15344484\n\n\n44.80386099\n\n\n18.66870753\n\n\n55.00502309\n\n\n38.26060147\n\n\n62.80381008\n\n\n38.01967341\n\n\n56.17657141\n\n\n28.97197014\n\n\n49.54090479\n\n\n25.45330063\n\n\n55.84071432\n\n\n37.03022579\n\n\n34.37708536\n\n\n64.30509389\n\n\n15.2880245\n\n\n58.49072424\n\n\n44.35952084\n\n\n42.61476497\n\n\n54.03562831\n\n\n35.98028527\n\n\n38.80471651\n\n\n28.42764966\n\n\n55.32227489\n\n\n38.20395397\n\n\n56.36964827\n\n\n59.70107642\n\n\n52.71623202\n\n\n63.3067771\n\n\n69.52835756\n\n\n49.79566633\n\n\n58.11358381\n\n\n57.1817594\n\n\n27.80425236\n\n\n53.12321655\n\n\n41.09764295\n\n\n45.2492205\n\n\n48.33506624\n\n\n57.16114056\n\n\n46.66973569\n\n\n55.48622664\n\n\n52.37870119\n\n\n50.81374888\n\n\n55.99294902\n\n\n63.18503661\n\n\n55.57782867\n\n\n35.60046662\n\n\n54.83371945\n\n\n49.71030097\n\n\n43.56618984\n\n\n53.09591717\n\n\n15.43850568\n\n\n60.0828673\n\n\n59.08471666\n\n\n55.1031116\n\n\n49.73132042\n\n\n69.77405692\n\n\n46.65313341\n\n\n58.43656013\n\n\n48.53335137\n\n\n51.05421319\n\n\n50.29151968\n\n\n56.21526775\n\n\n54.6833279\n\n\n55.95530033\n\n\n57.85799428\n\n\n43.59565414\n\n\n52.59851681\n\n\n56.52779089\n\n\n61.25032036\n\n\n37.50065667\n\n\n57.53513046\n\n\n39.66384925\n\n\n58.25530746\n\n\n33.64235974\n\n\n43.4280117\n\n\n27.32524528\n\n\n47.12020489\n\n\n46.39069815\n\n\n49.79721814\n\n\n48.96635438\n\n\n48.50359486\n\n\n56.73786618\n\n\n59.79455506\n\n\n62.21599146\n\n\n40.04382798\n\n\n59.00321653\n\n\n61.19501695\n\n\n55.80922872\n\n\n44.56991563\n\n\n54.35963795\n\n\n57.41441958\n\n\n60.48729778\n\n\n55.59802127\n\n\n34.89982338\n\n\n59.62723295\n\n\n43.04315207\n\n\n54.18809677\n\n\n44.56102099\n\n\n27.27257208\n\n\n55.81475564\n\n\n31.7195194\n\n\n54.37276684\n\n\n39.26651082\n\n\n39.8007732\n\n\n44.14576523\n\n\n51.62315051\n\n\n49.92568755\n\n\n55.20663015\n\n\n33.27065278\n\n\n32.7167337\n\n\n51.72464941\n\n\n59.32639404\n\n\n33.950261\n\n\n37.31971151\n\n\n52.33484425\n\n\n54.49566295\n\n\n57.45889675\n\n\n49.35961699\n\n\n39.39246488\n\n\n56.99847474\n\n\n43.75318257\n\n\n34.17037223\n\n\n56.98610838\n\n\n66.03931682\n\n\n60.86107338\n\n\n36.86443878\n\n\n18.47108363\n\n\n42.62791575\n\n\n54.92504294\n\n\n43.11796574\n\n\n55.32150129\n\n\n29.66852869\n\n\n40.6053607\n\n\n59.72792875\n\n\n56.30255861\n\n\n32.27389502\n\n\n56.19741641\n\n\n49.63019438\n\n\n33.12589815\n\n\n47.1240047\n\n\n62.47464536\n\n\n46.33301788\n\n\n45.7294271\n\n\n41.39780451\n\n\n58.64009194\n\n\n32.64157771\n\n\n49.29067208\n\n\n36.52275773\n\n\n56.86850701\n\n\n48.44789102\n\n\n53.71268071\n\n\n54.1942751\n\n\n57.84423412\n\n\n44.2112939\n\n\n20.10357006\n\n\n38.18370849\n\n\n29.56054888\n\n\n32.41398737\n\n\n48.61459863\n\n\n62.50975942\n\n\n57.56784011\n\n\n40.45812779\n\n\n56.94086225\n\n\n60.35002596\n\n\n43.69742769\n\n\n45.67373516\n\n\n46.01490917\n\n\n55.27986456\n\n\n21.23946433\n\n\n50.53627504\n\n\n27.11062091\n\n\n53.74048836\n\n\n61.26740097\n\n\n50.21125783\n\n\n38.41881194\n\n\n43.62290519\n\n\n41.02598411\n\n\n52.6267915\n\n\n41.67451285\n\n\n42.86417001\n\n\n59.49656776\n\n\n50.46274247\n\n\n58.35408499\n\n\n25.21954144\n\n\n49.06311002\n\n\n42.4924651\n\n\n57.91463872\n\n\n51.78245006\n\n\n57.40255669\n\n\n38.70464807\n\n\n62.87811014\n\n\n56.1293704\n\n\n43.66730306\n\n\n52.13741071\n\n\n63.54840666\n\n\n17.48689106\n\n\n22.60492862\n\n\n60.5695263\n\n\n58.11522668\n\n\n55.48855263\n\n\n34.45389937\n\n\n14.82695721\n\n\n55.28007549\n\n\n58.07315286\n\n\n37.72829508\n\n\n53.04221156\n\n\n17.72460227\n\n\n25.81095391\n\n\n56.95205229\n\n\n23.23027012\n\n\n57.15296032\n\n\n7.229314639\n\n\n51.3883965\n\n\n56.57349111\n\n\n51.80912285\n\n\n60.20710221\n\n\n47.9626426\n\n\n59.82328646\n\n\n57.83976873\n\n\n45.59302134\n\n\n56.36516596\n\n\n50.9662026\n\n\n52.57435648\n\n\n57.60013143\n\n\n45.96283078\n\n\n40.83709095\n\n\n54.54252145\n\n\n54.77686949\n\n\n.\n\n\n24.59522317\n\n\n28.60959111\n\n\n54.85442979\n\n\n21.10264284\n\n\n60.0434481\n\n\n40.65410157\n\n\n44.97638151\n\n\n15.86449563\n\n\n58.46842882\n\n\n50.98092458\n\n\n49.16951616\n\n\n52.68832187\n\n\n56.19768587\n\n\n48.45504708\n\n\n14.47467652\n\n\n27.99832123\n\n\n55.9296789\n\n\n57.72581246\n\n\n46.59953803\n\n\n39.51842032\n\n\n52.27656032\n\n\n27.35106593\n\n\n53.58340822\n\n\n56.07849377\n\n\n55.14669532\n\n\n10.95426032\n\n\n58.66419103\n\n\n39.15312478\n\n\n13.23025243\n\n\n51.56500084\n\n\n56.15329027\n\n\n52.82428953\n\n\n19.70408237\n\n\n56.35254038\n\n\n59.15998194\n\n\n56.72567927\n\n\n52.65943172\n\n\n19.48507658\n\n\n22.47015153\n\n\n49.23922915\n\n\n60.97370972\n\n\n46.62687815\n\n\n54.06137106\n\n\n59.12210042\n\n\n59.64360339\n\n\n19.7533652\n\n\n61.09561566\n\n\n58.74308544\n\n\n25.76872347\n\n\n21.69023124\n\n\n31.06370086\n\n\n34.88280882\n\n\n59.01470626\n\n\n43.32784213\n\n\n43.28019422\n\n\n56.79820478\n\n\n53.47592958\n\n\n34.22779345\n\n\n23.08299715\n\n\n54.4060014\n\n\n44.14936634\n\n\n45.44901541\n\n\n23.07008977\n\n\n43.85748107\n\n\n35.02143355\n\n\n16.06107669\n\n\n41.67046054\n\n\n36.24201761\n\n\n59.62165979\n\n\n60.0663615\n\n\n29.85164517\n\n\n24.10671858\n\n\n35.63749528\n\n\n57.46971071\n\n\n51.53840712\n\n\n57.84048423\n\n\n48.12909897\n\n\n45.55938906\n\n\n59.07906037\n\n\n56.52029693\n\n\n47.59968778\n\n\n59.23337887\n\n\n54.52261305\n\n\n28.86494475\n\n\n56.043878\n\n\n48.42036472\n\n\n22.98411857\n\n\n9.851271916\n\n\n51.03974099\n\n\n58.10419716\n\n\n27.87862233\n\n\n51.97951567\n\n\n37.91572028\n\n\n56.41118561\n\n\n44.0628002\n\n\n57.66195005\n\n\n57.45842049\n\n\n41.41341609\n\n\n52.66383926\n\n\n36.4358552\n\n\n53.87665341\n\n\n55.36381018\n\n\n12.34274815\n\n\n53.807147\n\n\n49.35243071\n\n\n41.45996643\n\n\n25.1993662\n\n\n59.09951629\n\n\n28.02204697\n\n\n51.64397542\n\n\n16.49700159\n\n\n16.24194452\n\n\n47.16912215\n\n\n57.6932579\n\n\n55.15503435\n\n\n46.53138718\n\n\n55.6887795\n\n\n32.01474686\n\n\n58.69994584\n\n\n20.51778086\n\n\n22.29185165\n\n\n59.59675491\n\n\n11.77395338\n\n\n57.3371592\n\n\n58.14959468\n\n\n29.8050355\n\n\n49.24398259\n\n\n52.28619235\n\n\n45.19809053\n\n\n60.89033517\n\n\n60.00082102\n\n\n36.6419966\n\n\n29.18777756\n\n\n42.74035705\n\n\n45.40368121\n\n\n52.21119991\n\n\n61.33139365\n\n\n40.45196938\n\n\n57.17312904\n\n\n47.9529171\n\n\n53.1064198\n\n\n55.14060025\n\n\n38.00839149\n\n\n11.53918455\n\n\n43.2151011\n\n\n52.56922145\n\n\n25.82506706\n\n\n35.97855121\n\n\n37.1757479\n\n\n59.85038522\n\n\n57.27995019\n\n\n48.00448089\n\n\n57.6544045\n\n\n41.05843742\n\n\n32.1642343\n\n\n28.58129307\n\n\n53.07675901\n\n\n56.75465677\n\n\n21.79447488\n\n\n54.19609124\n\n\n52.56933808\n\n\n57.73775476\n\n\n56.29213233\n\n\n62.84540428\n\n\n48.61545346\n\n\n31.9850838\n\n\n39.78128776\n\n\n48.97337315\n\n\n34.16222134\n\n\n38.49142164\n\n\n50.6629814\n\n\n57.10534403\n\n\n41.26145375\n\n\n57.73923143\n\n\n18.2474954\n\n\n15.28742875\n\n\n58.29456284\n\n\n22.58945074\n\n\n59.24018918\n\n\n51.2448025\n\n\n60.5568868\n\n\n29.18635615\n\n\n53.78867684\n\n\n59.74750756\n\n\n37.26971282\n\n\n34.99989358\n\n\n14.52634758\n\n\n52.65099913\n\n\n57.2050549\n\n\n50.07881744\n\n\n55.14843209\n\n\n47.21096988\n\n\n46.93728799\n\n\n36.75984859\n\n\n65.41425146\n\n\n31.04270352\n\n\n61.10205012\n\n\n45.25820082\n\n\n38.69200261\n\n\n58.86425279\n\n\n48.86050958\n\n\n57.2197457\n\n\n47.37693953\n\n\n26.58864794\n\n\n58.23018946\n\n\n50.01948816\n\n\n45.07959611\n\n\n54.91741659\n\n\n37.66727657\n\n\n18.56651377\n\n\n26.31842275\n\n\n55.2386958\n\n\n45.56042825\n\n\n35.21151678\n\n\n29.15473996\n\n\n24.60830646\n\n\n28.91478685\n\n\n54.18473359\n\n\n10.89107096\n\n\n55.72050264\n\n\n59.34009564\n\n\n31.94939773\n\n\n55.13165259\n\n\n42.55142039\n\n\n38.67971903\n\n\n19.65026861\n\n\n16.16910102\n\n\n37.71609185\n\n\n35.34259759\n\n\n13.90309442\n\n\n51.09088919\n\n\n55.30108203\n\n\n49.67028269\n\n\n54.74771109\n\n\n46.67806644\n\n\n46.67028725\n\n\n35.54908453\n\n\n65.3754143\n\n\n33.50022607\n\n\n61.93168637\n\n\n46.54057458\n\n\n37.40457179\n\n\n61.01520861\n\n\n51.10776382\n\n\n55.52026988\n\n\n47.51880687\n\n\n28.6761316\n\n\n59.10628598\n\n\n49.74518799\n\n\n46.66879072\n\n\n56.02847977\n\n\n37.82457675\n\n\n18.24435926\n\n\n24.61715035\n\n\n53.99004375\n\n\n44.78887286\n\n\n36.00873127\n\n\n26.77681\n\n\n22.94389753\n\n\n30.83025148\n\n\n55.6050719\n\n\n14.56969282\n\n\n54.59474565\n\n\n57.87176163\n\n\n31.30130198\n\n\n56.89496004\n\n\n44.58682615\n\n\n38.6926798\n\n\n19.73639923\n\n\n17.77842135\n\n\n\n\n3\n\n\n0\n\n\nAGG_PHYS\n\n\nAggregate Physical QOL Score\n\n\n52.52556921\n\n\n27.92331491\n\n\n60.06969568\n\n\n50.7885018\n\n\n43.75671037\n\n\n56.86260913\n\n\n52.78791763\n\n\n55.23964561\n\n\n47.73831907\n\n\n48.18627251\n\n\n60.319661\n\n\n45.43126292\n\n\n53.70117805\n\n\n43.38554764\n\n\n51.80922475\n\n\n54.40453498\n\n\n47.778456\n\n\n57.52488289\n\n\n29.67280338\n\n\n57.74035835\n\n\n65.48612341\n\n\n51.10825454\n\n\n27.9915838\n\n\n63.8495274\n\n\n40.38870398\n\n\n60.0360685\n\n\n53.90043722\n\n\n45.73484483\n\n\n45.65506839\n\n\n45.42662874\n\n\n53.23901179\n\n\n41.63265954\n\n\n54.11517873\n\n\n44.67259002\n\n\n57.6208115\n\n\n56.73240484\n\n\n66.92566909\n\n\n48.63680802\n\n\n32.38797336\n\n\n60.94873774\n\n\n40.64490122\n\n\n57.63632968\n\n\n52.3820502\n\n\n47.82576317\n\n\n46.24615127\n\n\n43.53796025\n\n\n53.54671752\n\n\n43.00381751\n\n\n53.08641193\n\n\n46.02727717\n\n\n56.34342759\n\n\n54.09221317\n\n\n69.70656702\n\n\n59.15231892\n\n\n38.07057284\n\n\n56.89571744\n\n\n34.12944488\n\n\n54.71944508\n\n\n53.20333552\n\n\n54.00088936\n\n\n53.33958298\n\n\n54.45469716\n\n\n61.93622617\n\n\n42.78922318\n\n\n43.13387693\n\n\n59.84270531\n\n\n59.14633671\n\n\n62.7138085\n\n\n55.45899793\n\n\n59.78439088\n\n\n44.68413692\n\n\n48.21627745\n\n\n57.12655967\n\n\n58.38684446\n\n\n61.00620487\n\n\n53.3412434\n\n\n51.50620809\n\n\n60.45582408\n\n\n42.57927661\n\n\n24.54257229\n\n\n53.78416978\n\n\n55.35549931\n\n\n51.38713226\n\n\n37.52420464\n\n\n52.04275694\n\n\n54.80514572\n\n\n49.30583373\n\n\n56.7374146\n\n\n51.78613831\n\n\n45.23988699\n\n\n54.98186152\n\n\n42.21095973\n\n\n55.13405838\n\n\n49.28968222\n\n\n52.28814103\n\n\n57.40393685\n\n\n59.15532293\n\n\n53.13744109\n\n\n51.13011474\n\n\n60.14854155\n\n\n56.1836515\n\n\n65.6209325\n\n\n57.97912101\n\n\n53.87280375\n\n\n54.4599066\n\n\n54.15150285\n\n\n33.39634562\n\n\n44.57307807\n\n\n51.94043261\n\n\n37.11039816\n\n\n59.32653214\n\n\n57.54304139\n\n\n53.52077993\n\n\n54.09217809\n\n\n58.40426847\n\n\n56.10666541\n\n\n36.25011232\n\n\n57.77055154\n\n\n49.96454589\n\n\n53.84450087\n\n\n25.84027717\n\n\n55.78216233\n\n\n49.25027618\n\n\n51.4798918\n\n\n55.95574981\n\n\n45.90478294\n\n\n48.15291001\n\n\n58.70809319\n\n\n51.65288937\n\n\n54.00772741\n\n\n48.20272024\n\n\n56.82505903\n\n\n58.30461485\n\n\n51.24532276\n\n\n48.98011585\n\n\n45.61758065\n\n\n55.34140573\n\n\n57.65568272\n\n\n45.91700556\n\n\n55.60889985\n\n\n56.58797567\n\n\n39.68852288\n\n\n53.62241057\n\n\n50.17677799\n\n\n68.40339956\n\n\n54.23704724\n\n\n43.95213213\n\n\n55.63713503\n\n\n57.25134445\n\n\n65.36187349\n\n\n55.97567799\n\n\n64.72220752\n\n\n48.71384796\n\n\n55.73399189\n\n\n58.19304078\n\n\n55.79675049\n\n\n56.61993693\n\n\n41.76002283\n\n\n59.33564088\n\n\n36.03900559\n\n\n63.97502184\n\n\n57.12338385\n\n\n35.35257304\n\n\n51.38576459\n\n\n41.78164152\n\n\n46.68700711\n\n\n55.1674739\n\n\n43.37825719\n\n\n55.56928504\n\n\n61.1106371\n\n\n46.59052075\n\n\n21.19306493\n\n\n33.75048662\n\n\n52.25451928\n\n\n28.0745021\n\n\n56.89132999\n\n\n50.56670668\n\n\n31.78703565\n\n\n57.56034496\n\n\n49.89447463\n\n\n54.82051464\n\n\n55.44863345\n\n\n60.0339518\n\n\n51.99517094\n\n\n45.92739731\n\n\n53.27609449\n\n\n63.51900343\n\n\n56.25796417\n\n\n54.39536079\n\n\n43.94396426\n\n\n48.66782076\n\n\n57.61805453\n\n\n59.12262446\n\n\n31.05722009\n\n\n56.90750295\n\n\n59.80852118\n\n\n41.49762221\n\n\n50.60306545\n\n\n51.26386783\n\n\n59.10066484\n\n\n46.81605922\n\n\n53.80645364\n\n\n39.04400149\n\n\n59.5551066\n\n\n51.14501132\n\n\n36.66685749\n\n\n56.58799153\n\n\n52.6301647\n\n\n39.95625314\n\n\n37.02889827\n\n\n54.42828732\n\n\n54.22231809\n\n\n59.64817626\n\n\n55.51028082\n\n\n52.93590063\n\n\n51.84957416\n\n\n42.47773511\n\n\n33.57382038\n\n\n55.81547979\n\n\n47.75510891\n\n\n40.70533719\n\n\n59.17050501\n\n\n49.80318134\n\n\n30.58237733\n\n\n38.42080101\n\n\n52.03473439\n\n\n51.30409382\n\n\n60.71748409\n\n\n55.58104636\n\n\n56.61716969\n\n\n37.72078192\n\n\n53.66735158\n\n\n53.5202245\n\n\n53.09521482\n\n\n59.8394278\n\n\n42.17708717\n\n\n50.56327277\n\n\n56.537662\n\n\n53.93021788\n\n\n56.6268142\n\n\n56.30502779\n\n\n46.29542946\n\n\n50.39555298\n\n\n40.72972709\n\n\n54.63392229\n\n\n42.32797557\n\n\n66.23138482\n\n\n60.60188419\n\n\n56.41467301\n\n\n48.43515511\n\n\n55.66038838\n\n\n60.33527141\n\n\n61.28673928\n\n\n53.30906588\n\n\n55.29792116\n\n\n37.09388395\n\n\n55.5497104\n\n\n44.08671849\n\n\n60.01535532\n\n\n55.7498728\n\n\n43.39874245\n\n\n55.28895115\n\n\n56.22095441\n\n\n59.21904995\n\n\n53.5829052\n\n\n53.85844385\n\n\n53.69066759\n\n\n54.27342184\n\n\n53.50914646\n\n\n52.35513876\n\n\n58.50707156\n\n\n41.62738307\n\n\n46.95334472\n\n\n60.36627541\n\n\n53.48908558\n\n\n49.01514602\n\n\n56.23421865\n\n\n55.60786361\n\n\n52.90126355\n\n\n.\n\n\n48.10439306\n\n\n35.07070636\n\n\n50.49258864\n\n\n62.73794097\n\n\n57.43956148\n\n\n53.9445345\n\n\n55.27734652\n\n\n62.59491538\n\n\n52.78866338\n\n\n56.95177101\n\n\n53.65598526\n\n\n57.9508684\n\n\n52.41423485\n\n\n55.0445815\n\n\n41.04473207\n\n\n48.58421342\n\n\n51.45225587\n\n\n57.92809579\n\n\n46.15133535\n\n\n60.58884554\n\n\n54.4166108\n\n\n34.05089419\n\n\n39.1998311\n\n\n59.34238337\n\n\n45.7559771\n\n\n66.98683268\n\n\n47.80548038\n\n\n46.67211098\n\n\n38.12506858\n\n\n19.7469282\n\n\n49.35724807\n\n\n57.24346859\n\n\n40.32752989\n\n\n58.81502698\n\n\n48.96450049\n\n\n50.54293284\n\n\n49.24279366\n\n\n40.69046925\n\n\n60.72912705\n\n\n52.29887358\n\n\n54.63495985\n\n\n51.65404496\n\n\n57.64868175\n\n\n54.12367033\n\n\n37.8589322\n\n\n22.36902585\n\n\n26.24576558\n\n\n54.77306603\n\n\n51.38330558\n\n\n47.68642036\n\n\n59.78245869\n\n\n50.49030107\n\n\n58.04155942\n\n\n55.48776496\n\n\n56.74144941\n\n\n52.02890262\n\n\n44.73375612\n\n\n57.55937478\n\n\n58.82704257\n\n\n23.00227798\n\n\n60.61278737\n\n\n58.17302876\n\n\n53.22598295\n\n\n53.5756432\n\n\n50.27357887\n\n\n45.27484258\n\n\n51.57916229\n\n\n27.22411031\n\n\n53.88919749\n\n\n59.55742483\n\n\n53.81716395\n\n\n42.82430998\n\n\n63.01495167\n\n\n52.86842448\n\n\n43.51265922\n\n\n52.12551442\n\n\n52.93873415\n\n\n49.98358868\n\n\n51.98894713\n\n\n31.33445263\n\n\n20.38863946\n\n\n57.06589642\n\n\n57.92555355\n\n\n62.05782306\n\n\n54.05028372\n\n\n50.47981663\n\n\n59.26092932\n\n\n66.41075639\n\n\n51.75600046\n\n\n54.49390434\n\n\n62.88423423\n\n\n56.70548717\n\n\n61.49125467\n\n\n59.37633192\n\n\n54.42860514\n\n\n58.32567729\n\n\n55.79893502\n\n\n58.42241283\n\n\n57.11709611\n\n\n47.20351545\n\n\n48.11833173\n\n\n59.0753383\n\n\n66.55368873\n\n\n44.75832371\n\n\n52.47124215\n\n\n57.61343062\n\n\n29.93089516\n\n\n57.18603429\n\n\n62.70425898\n\n\n57.86727645\n\n\n33.95328345\n\n\n69.02608905\n\n\n59.8546857\n\n\n25.51010161\n\n\n52.30493028\n\n\n59.43407068\n\n\n55.12183746\n\n\n33.26968497\n\n\n47.96717902\n\n\n64.7251045\n\n\n47.85278289\n\n\n45.98165414\n\n\n59.46818164\n\n\n41.966291\n\n\n56.65329802\n\n\n56.160549\n\n\n55.3256471\n\n\n39.31430271\n\n\n50.76158639\n\n\n54.0283901\n\n\n49.52739959\n\n\n54.84585399\n\n\n31.38103189\n\n\n21.56723871\n\n\n49.23331688\n\n\n55.95807149\n\n\n47.14047627\n\n\n56.18640591\n\n\n55.87741037\n\n\n56.32807053\n\n\n53.78981096\n\n\n54.07154446\n\n\n44.38161878\n\n\n67.0880503\n\n\n56.2788019\n\n\n38.99348413\n\n\n41.59534395\n\n\n37.99816783\n\n\n42.054125\n\n\n59.89443396\n\n\n55.77572367\n\n\n19.16064531\n\n\n54.10442612\n\n\n56.10158555\n\n\n36.76530004\n\n\n44.3199829\n\n\n41.23482191\n\n\n55.7306508\n\n\n61.8836914\n\n\n49.86015718\n\n\n56.25003073\n\n\n49.44596504\n\n\n58.47114947\n\n\n58.03065891\n\n\n52.81659166\n\n\n44.25407038\n\n\n47.95881325\n\n\n60.29218496\n\n\n60.39566653\n\n\n52.62121452\n\n\n38.61177706\n\n\n57.61859772\n\n\n28.89853537\n\n\n53.15002608\n\n\n33.58718332\n\n\n65.13676597\n\n\n57.74287227\n\n\n58.99396849\n\n\n53.71220726\n\n\n55.00935042\n\n\n47.10002146\n\n\n41.21718614\n\n\n43.38967569\n\n\n58.11277341\n\n\n37.60365504\n\n\n41.49790274\n\n\n59.69123611\n\n\n45.70014157\n\n\n48.15883946\n\n\n51.21983945\n\n\n56.38589957\n\n\n49.66760983\n\n\n55.69288776\n\n\n38.38354063\n\n\n29.182103\n\n\n44.97778348\n\n\n56.56029297\n\n\n28.83575955\n\n\n62.94944045\n\n\n52.59529923\n\n\n42.38097038\n\n\n41.34878696\n\n\n47.6209803\n\n\n35.8901402\n\n\n59.53247936\n\n\n54.66523749\n\n\n45.9442124\n\n\n45.59171159\n\n\n46.63836632\n\n\n39.41634052\n\n\n51.83224567\n\n\n43.82460999\n\n\n56.53749777\n\n\n52.18642336\n\n\n54.42064225\n\n\n40.87374772\n\n\n61.20712432\n\n\n49.23381217\n\n\n67.84366436\n\n\n54.76290824\n\n\n44.93816059\n\n\n57.37481342\n\n\n55.30659175\n\n\n57.61185314\n\n\n52.10312837\n\n\n34.71879059\n\n\n67.27056611\n\n\n37.61056169\n\n\n40.6513606\n\n\n58.89410397\n\n\n43.09377041\n\n\n49.62306918\n\n\n51.63290661\n\n\n53.80934645\n\n\n49.92953784\n\n\n57.12286101\n\n\n39.38202829\n\n\n28.78816222\n\n\n44.80818019\n\n\n56.60617566\n\n\n30.44103092\n\n\n61.0220448\n\n\n50.99413284\n\n\n43.16926349\n\n\n41.21025518\n\n\n48.43680695\n\n\n35.25326656\n\n\n57.81311034\n\n\n53.65001087\n\n\n48.09074311\n\n\n46.11700099\n\n\n46.10732681\n\n\n41.30256151\n\n\n51.98162794\n\n\n42.97297765\n\n\n57.66157827\n\n\n51.75947377\n\n\n53.63632576\n\n\n41.86296071\n\n\n61.89409919\n\n\n48.42701776\n\n\n66.19701569\n\n\n55.22545556\n\n\n44.87357115\n\n\n56.11184383\n\n\n55.91401705\n\n\n56.83995542\n\n\n50.93693303\n\n\n33.68442806\n\n\n67.98936219\n\n\n\n\n4\n\n\n0\n\n\nHASHV\n\n\nFrequency of Hash/Marijuana Use\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n2\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n2\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n2\n\n\n1\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n2\n\n\n2\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n1\n\n\n2\n\n\n2\n\n\n2\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n2\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n\n\n5\n\n\n0\n\n\nBMI\n\n\nBMI\n\n\n24.71755909\n\n\n26.66936015\n\n\n28.59084821\n\n\n20.36451325\n\n\n22.2698601\n\n\n23.22165848\n\n\n24.71687978\n\n\n28.85804285\n\n\n19.91076684\n\n\n20.59908544\n\n\n18.55779905\n\n\n20.41588386\n\n\n22.59478671\n\n\n24.76099529\n\n\n23.50498072\n\n\n28.49781375\n\n\n22.40198052\n\n\n25.57984025\n\n\n30.28356289\n\n\n.\n\n\n29.38139815\n\n\n26.46393147\n\n\n28.18114908\n\n\n29.87478411\n\n\n22.41865037\n\n\n22.95171446\n\n\n30.83560493\n\n\n21.28501568\n\n\n21.26730089\n\n\n18.08698609\n\n\n23.95586878\n\n\n23.50436634\n\n\n27.30225751\n\n\n23.01403184\n\n\n23.52290082\n\n\n.\n\n\n29.93570737\n\n\n22.38699688\n\n\n28.10827882\n\n\n28.06668984\n\n\n22.78103647\n\n\n23.35994894\n\n\n28.82353861\n\n\n20.66837323\n\n\n19.57768426\n\n\n21.11509934\n\n\n21.72714364\n\n\n22.56549683\n\n\n28.2262161\n\n\n24.18474563\n\n\n25.97318324\n\n\n.\n\n\n29.6420227\n\n\n25.22083056\n\n\n22.56627048\n\n\n35.63301161\n\n\n26.12902076\n\n\n32.3503827\n\n\n34.25844443\n\n\n24.86527348\n\n\n22.38535218\n\n\n28.31800032\n\n\n24.37559702\n\n\n30.85484314\n\n\n24.77003426\n\n\n18.58561938\n\n\n29.15438597\n\n\n22.95517755\n\n\n31.63990141\n\n\n24.16777204\n\n\n19.15583397\n\n\n24.68472047\n\n\n23.82667665\n\n\n25.39119013\n\n\n27.42841944\n\n\n25.60215889\n\n\n23.95181972\n\n\n22.66867874\n\n\n29.40173576\n\n\n29.52729025\n\n\n33.15458521\n\n\n26.32205808\n\n\n21.42230343\n\n\n26.61577251\n\n\n27.97225231\n\n\n20.97697573\n\n\n30.58827691\n\n\n16.49533026\n\n\n24.74638597\n\n\n23.42457484\n\n\n22.87218979\n\n\n24.0948245\n\n\n27.91399185\n\n\n18.90645938\n\n\n23.7976645\n\n\n35.71839291\n\n\n25.15915564\n\n\n.\n\n\n23.17761984\n\n\n22.36873272\n\n\n27.17900197\n\n\n22.47903429\n\n\n25.07122984\n\n\n20.30380546\n\n\n32.73480724\n\n\n35.4251915\n\n\n35.49820897\n\n\n27.45211252\n\n\n25.46359593\n\n\n27.98967617\n\n\n22.37653412\n\n\n23.74135788\n\n\n25.30431967\n\n\n26.42397771\n\n\n25.77774977\n\n\n23.84926951\n\n\n21.53702251\n\n\n24.36431947\n\n\n24.44639575\n\n\n28.00329248\n\n\n23.36554298\n\n\n21.88186932\n\n\n24.14137008\n\n\n25.82638438\n\n\n22.19005552\n\n\n21.57987116\n\n\n18.82735382\n\n\n20.1029076\n\n\n23.97946439\n\n\n24.6818319\n\n\n40.38171049\n\n\n28.95411003\n\n\n28.18986798\n\n\n21.85539897\n\n\n27.62865593\n\n\n23.59116852\n\n\n40.19737932\n\n\n25.37766325\n\n\n20.36829577\n\n\n27.07913629\n\n\n18.36093126\n\n\n19.51273759\n\n\n26.10182282\n\n\n23.60873929\n\n\n.\n\n\n27.54451151\n\n\n20.21602969\n\n\n23.14054788\n\n\n19.16627691\n\n\n33.61986678\n\n\n20.86479973\n\n\n24.53881873\n\n\n34.22888667\n\n\n23.68084007\n\n\n24.2125348\n\n\n25.65908357\n\n\n35.88133498\n\n\n38.43674804\n\n\n28.10798986\n\n\n18.97630937\n\n\n21.48903344\n\n\n22.95456341\n\n\n22.93735174\n\n\n30.438229\n\n\n23.77690753\n\n\n19.61476713\n\n\n25.54230425\n\n\n28.50098475\n\n\n32.50179878\n\n\n21.70971924\n\n\n25.44043505\n\n\n17.66341311\n\n\n18.8580838\n\n\n25.84317924\n\n\n29.38705517\n\n\n18.31754195\n\n\n24.4219366\n\n\n37.58512107\n\n\n24.9246685\n\n\n23.42483286\n\n\n25.55884588\n\n\n18.88270687\n\n\n23.06424482\n\n\n22.80598533\n\n\n29.5027734\n\n\n21.20695653\n\n\n20.73982381\n\n\n23.34598785\n\n\n28.33548351\n\n\n40.83100861\n\n\n26.018265\n\n\n28.19027415\n\n\n24.31253848\n\n\n26.72543366\n\n\n21.66121082\n\n\n21.89628814\n\n\n24.69690104\n\n\n25.94472123\n\n\n29.25018104\n\n\n24.32759472\n\n\n32.07433024\n\n\n25.57123624\n\n\n28.32349769\n\n\n30.92268897\n\n\n25.93892394\n\n\n39.86299293\n\n\n29.02027654\n\n\n20.75599528\n\n\n20.64574135\n\n\n24.38456131\n\n\n23.35417394\n\n\n24.25496366\n\n\n28.89143144\n\n\n23.65186733\n\n\n27.69578896\n\n\n26.01372175\n\n\n22.14339947\n\n\n25.1523312\n\n\n20.69746212\n\n\n22.7189423\n\n\n25.40077725\n\n\n25.60492358\n\n\n24.72998551\n\n\n22.3928242\n\n\n23.84878776\n\n\n25.85164786\n\n\n19.76774158\n\n\n26.93885028\n\n\n23.23123314\n\n\n22.17517945\n\n\n19.06301309\n\n\n25.05480785\n\n\n28.27142131\n\n\n26.32620528\n\n\n37.37963439\n\n\n23.52901212\n\n\n35.2885235\n\n\n21.44542332\n\n\n22.58480639\n\n\n18.14498106\n\n\n21.63629977\n\n\n30.1131553\n\n\n23.6590749\n\n\n25.3311889\n\n\n30.60311183\n\n\n21.80724292\n\n\n23.15385665\n\n\n24.83487379\n\n\n25.11192494\n\n\n20.02774432\n\n\n22.61652748\n\n\n19.00611701\n\n\n22.25797594\n\n\n.\n\n\n28.10652648\n\n\n22.08394004\n\n\n24.69868898\n\n\n29.62937876\n\n\n28.57861881\n\n\n26.99608102\n\n\n23.77773016\n\n\n23.10167351\n\n\n20.98788099\n\n\n23.58040397\n\n\n31.06014858\n\n\n30.56879388\n\n\n28.85030199\n\n\n38.70893733\n\n\n22.7304256\n\n\n27.36650571\n\n\n21.86069232\n\n\n22.05304516\n\n\n32.13917255\n\n\n26.25423631\n\n\n22.06320565\n\n\n17.5358944\n\n\n18.54026526\n\n\n26.95299322\n\n\n26.57796679\n\n\n24.27905878\n\n\n23.54915165\n\n\n19.366132\n\n\n25.96163334\n\n\n23.7266333\n\n\n22.77067248\n\n\n25.65699853\n\n\n27.00585183\n\n\n25.36897851\n\n\n29.01076547\n\n\n24.60383289\n\n\n24.67557779\n\n\n24.32571536\n\n\n24.51009107\n\n\n29.20910393\n\n\n26.1744865\n\n\n29.48014535\n\n\n30.255786\n\n\n23.60073519\n\n\n20.91575973\n\n\n23.5157256\n\n\n25.25401928\n\n\n23.3147739\n\n\n27.57321534\n\n\n21.95516055\n\n\n22.17556961\n\n\n24.07525364\n\n\n30.60705422\n\n\n21.05077563\n\n\n18.22014879\n\n\n24.14696476\n\n\n25.47521319\n\n\n.\n\n\n23.56448789\n\n\n20.86516626\n\n\n18.59185732\n\n\n29.31917496\n\n\n21.09085804\n\n\n20.10335123\n\n\n18.87867836\n\n\n28.58452626\n\n\n29.73550417\n\n\n28.5251343\n\n\n24.43247651\n\n\n24.91221095\n\n\n24.53104878\n\n\n21.65341643\n\n\n23.91921742\n\n\n21.86984815\n\n\n19.22058965\n\n\n22.58745879\n\n\n23.24736007\n\n\n20.07202448\n\n\n21.71791416\n\n\n21.59778976\n\n\n45.27516288\n\n\n23.01872358\n\n\n19.96501699\n\n\n22.86094876\n\n\n27.94189938\n\n\n23.79049724\n\n\n29.3673979\n\n\n26.39365223\n\n\n32.60624194\n\n\n20.75030251\n\n\n19.50064695\n\n\n24.76069973\n\n\n22.92862969\n\n\n24.48358245\n\n\n.\n\n\n33.40087013\n\n\n21.58431713\n\n\n24.16663442\n\n\n28.46704545\n\n\n34.8789824\n\n\n34.53379363\n\n\n24.0486138\n\n\n26.39189813\n\n\n23.7913047\n\n\n22.80062704\n\n\n27.30905534\n\n\n24.44143042\n\n\n31.46421815\n\n\n25.44574354\n\n\n23.18376957\n\n\n23.54234053\n\n\n25.97729053\n\n\n27.92746348\n\n\n27.68951121\n\n\n26.95672034\n\n\n24.79668012\n\n\n23.39881244\n\n\n23.323804\n\n\n24.44466894\n\n\n35.37199216\n\n\n26.7731843\n\n\n29.53845704\n\n\n28.67389604\n\n\n28.52450554\n\n\n20.15646916\n\n\n18.2394671\n\n\n29.83328885\n\n\n22.56970944\n\n\n32.41562948\n\n\n26.93869492\n\n\n22.02286217\n\n\n26.39483613\n\n\n28.70382437\n\n\n23.87602562\n\n\n22.85031881\n\n\n28.2907718\n\n\n22.247428\n\n\n26.32230948\n\n\n26.80377975\n\n\n27.72696136\n\n\n22.93120338\n\n\n28.60369542\n\n\n28.31685986\n\n\n30.37819609\n\n\n22.56134312\n\n\n21.04230531\n\n\n24.90941192\n\n\n22.18449055\n\n\n34.65119337\n\n\n28.57691139\n\n\n26.46703867\n\n\n24.68743467\n\n\n30.31198795\n\n\n22.72018643\n\n\n27.91795957\n\n\n33.63368335\n\n\n24.5216571\n\n\n36.70450592\n\n\n30.442801\n\n\n20.73059879\n\n\n19.55320173\n\n\n27.74740874\n\n\n27.27466022\n\n\n26.78603823\n\n\n25.38078317\n\n\n26.03265918\n\n\n24.90370745\n\n\n24.38168418\n\n\n27.38400212\n\n\n20.87952875\n\n\n.\n\n\n23.22717608\n\n\n26.76090716\n\n\n26.00852587\n\n\n20.11871958\n\n\n25.52663946\n\n\n28.17891756\n\n\n20.03680683\n\n\n26.78973048\n\n\n24.64588711\n\n\n23.04176554\n\n\n22.70943166\n\n\n23.65750094\n\n\n29.82872882\n\n\n27.16452733\n\n\n25.76387474\n\n\n23.29970095\n\n\n21.39054133\n\n\n18.39350091\n\n\n24.13341908\n\n\n24.28872972\n\n\n20.57527137\n\n\n22.13442136\n\n\n28.4865362\n\n\n18.54354153\n\n\n23.58865137\n\n\n24.9376593\n\n\n23.62698819\n\n\n25.69833642\n\n\n22.43416374\n\n\n27.22044022\n\n\n29.00567486\n\n\n28.32592359\n\n\n27.47916113\n\n\n27.03635988\n\n\n26.45697354\n\n\n30.61042576\n\n\n24.22955463\n\n\n26.94837432\n\n\n24.79622126\n\n\n22.4237857\n\n\n30.44991364\n\n\n18.4252178\n\n\n25.7066164\n\n\n21.83070965\n\n\n24.51932629\n\n\n29.54788949\n\n\n25.85074409\n\n\n25.68625088\n\n\n17.0602828\n\n\n27.85761122\n\n\n40.29766346\n\n\n27.57888716\n\n\n28.37264126\n\n\n29.48344626\n\n\n26.43585076\n\n\n23.97849408\n\n\n23.40025186\n\n\n17.95250088\n\n\n18.55379739\n\n\n23.21424273\n\n\n31.15066982\n\n\n20.33425424\n\n\n20.66213387\n\n\n20.68446355\n\n\n23.28507974\n\n\n19.52479543\n\n\n17.99533101\n\n\n26.30892281\n\n\n20.61568634\n\n\n23.53074544\n\n\n23.27998072\n\n\n21.41022835\n\n\n33.08867127\n\n\n33.80208395\n\n\n28.0545052\n\n\n23.41002255\n\n\n23.6425349\n\n\n22.4414018\n\n\n.\n\n\n24.65412726\n\n\n28.58565543\n\n\n30.1834012\n\n\n23.23885881\n\n\n28.72765522\n\n\n19.62954074\n\n\n25.02714667\n\n\n20.16700359\n\n\n21.39978115\n\n\n29.66648403\n\n\n22.47205207\n\n\n26.10569372\n\n\n18.00307576\n\n\n28.49737298\n\n\n40.97841645\n\n\n27.13312619\n\n\n27.7339902\n\n\n28.01624379\n\n\n26.97149478\n\n\n24.90349326\n\n\n23.99361234\n\n\n19.29845905\n\n\n19.65306045\n\n\n23.15460489\n\n\n29.36856768\n\n\n20.96342225\n\n\n24.0027924\n\n\n19.72121676\n\n\n25.08201211\n\n\n20.03681036\n\n\n21.17168895\n\n\n27.17257131\n\n\n18.3871166\n\n\n22.1272192\n\n\n23.24415566\n\n\n20.27389091\n\n\n30.07934918\n\n\n31.28557421\n\n\n28.00107574\n\n\n23.36116453\n\n\n25.42777078\n\n\n24.25708435\n\n\n.\n\n\n22.87895117\n\n\n24.17362182\n\n\n29.03906816\n\n\n\n\n6\n\n\n0\n\n\nHBP\n\n\nHigh Blood Pressure\n\n\n1\n\n\n2\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n4\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n2\n\n\n9\n\n\n2\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n3\n\n\n2\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n9\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n2\n\n\n9\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n9\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n3\n\n\n1\n\n\n9\n\n\n1\n\n\n2\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n2\n\n\n1\n\n\n9\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n2\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n2\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n3\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n9\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n3\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n2\n\n\n9\n\n\n2\n\n\n9\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n3\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n2\n\n\n9\n\n\n2\n\n\n9\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n\n\n7\n\n\n0\n\n\nDIAB\n\n\nDiabetes\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n3\n\n\n1\n\n\n1\n\n\n2\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n3\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n3\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n2\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n3\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n3\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n3\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n2\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n3\n\n\n9\n\n\n3\n\n\n2\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n2\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n2\n\n\n9\n\n\n1\n\n\n9\n\n\n3\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n3\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n2\n\n\n1\n\n\n1\n\n\n9\n\n\n3\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n3\n\n\n2\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n3\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n3\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n3\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n3\n\n\n3\n\n\n2\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n2\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n3\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n3\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n3\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n2\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n3\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n2\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n3\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n3\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n3\n\n\n9\n\n\n3\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n3\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n2\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n2\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n3\n\n\n9\n\n\n3\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n3\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n2\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n2\n\n\n9\n\n\n9\n\n\n1\n\n\n\n\n8\n\n\n0\n\n\nLIV34\n\n\nLiver Disease Stage 3/4\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n2\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n2\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n2\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n2\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n2\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n2\n\n\n9\n\n\n1\n\n\n2\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n2\n\n\n9\n\n\n1\n\n\n2\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n\n\n9\n\n\n0\n\n\nKID\n\n\nKidney Disease\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n2\n\n\n9\n\n\n9\n\n\n2\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n2\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n2\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n2\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n2\n\n\n1\n\n\n9\n\n\n2\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n2\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n2\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n3\n\n\n3\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n3\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n2\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n2\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n2\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n3\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n3\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n3\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n2\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n2\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n2\n\n\n9\n\n\n1\n\n\n3\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n2\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n2\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n2\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n2\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n2\n\n\n1\n\n\n9\n\n\n2\n\n\n3\n\n\n2\n\n\n9\n\n\n9\n\n\n1\n\n\n2\n\n\n9\n\n\n9\n\n\n1\n\n\n2\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n2\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n2\n\n\n9\n\n\n9\n\n\n1\n\n\n2\n\n\n9\n\n\n9\n\n\n1\n\n\n2\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n2\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n\n\n10\n\n\n0\n\n\nFRP\n\n\nFrailty Related Phenotype\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n\n\n11\n\n\n0\n\n\nFP\n\n\nFrailty Phenotype\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n2\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n2\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n2\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n2\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n2\n\n\n9\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n2\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n2\n\n\n1\n\n\n\n\n12\n\n\n0\n\n\nTCHOL\n\n\nTotal Cholesterol\n\n\n133\n\n\n125\n\n\n170\n\n\n214\n\n\n196\n\n\n216\n\n\nNA\n\n\nNA\n\n\n167\n\n\nNA\n\n\n98\n\n\n98\n\n\n178\n\n\n132\n\n\nNA\n\n\nNA\n\n\n182\n\n\n165\n\n\n196\n\n\n150\n\n\n264\n\n\n133\n\n\n125\n\n\n170\n\n\n196\n\n\n216\n\n\nNA\n\n\n167\n\n\nNA\n\n\n98\n\n\n178\n\n\n132\n\n\nNA\n\n\n182\n\n\n165\n\n\n150\n\n\n264\n\n\n133\n\n\n125\n\n\n170\n\n\n196\n\n\n216\n\n\nNA\n\n\n167\n\n\nNA\n\n\n98\n\n\n178\n\n\n132\n\n\nNA\n\n\n182\n\n\n165\n\n\n150\n\n\n264\n\n\n210\n\n\n177\n\n\nNA\n\n\n203\n\n\n174\n\n\n155\n\n\n224\n\n\nNA\n\n\n166\n\n\n171\n\n\nNA\n\n\n261\n\n\nNA\n\n\n176\n\n\nNA\n\n\n141\n\n\nNA\n\n\nNA\n\n\n178\n\n\n131\n\n\nNA\n\n\n139\n\n\n162\n\n\n201\n\n\n148\n\n\n100\n\n\n167\n\n\n196\n\n\nNA\n\n\n148\n\n\n147\n\n\n186\n\n\nNA\n\n\n213\n\n\n139\n\n\n217\n\n\nNA\n\n\nNA\n\n\n185\n\n\n188\n\n\n134\n\n\n182\n\n\n209\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n127\n\n\n201\n\n\n189\n\n\n147\n\n\n193\n\n\nNA\n\n\n177\n\n\n233\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n113\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n139\n\n\n171\n\n\n146\n\n\n119\n\n\nNA\n\n\nNA\n\n\n194\n\n\n154\n\n\n234\n\n\n179\n\n\n100\n\n\n136\n\n\n188\n\n\n133\n\n\n172\n\n\nNA\n\n\n136\n\n\n224\n\n\n143\n\n\nNA\n\n\n195\n\n\nNA\n\n\n153\n\n\n194\n\n\n189\n\n\nNA\n\n\n133\n\n\nNA\n\n\n192\n\n\nNA\n\n\nNA\n\n\n129\n\n\n215\n\n\n142\n\n\n118\n\n\n176\n\n\n176\n\n\nNA\n\n\nNA\n\n\n113\n\n\n143\n\n\n150\n\n\nNA\n\n\n190\n\n\nNA\n\n\n213\n\n\n138\n\n\nNA\n\n\n139\n\n\n196\n\n\n159\n\n\n140\n\n\n247\n\n\n139\n\n\nNA\n\n\n200\n\n\n212\n\n\nNA\n\n\n200\n\n\nNA\n\n\nNA\n\n\n174\n\n\n222\n\n\n160\n\n\n146\n\n\n195\n\n\n191\n\n\n130\n\n\n170\n\n\n142\n\n\n155\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n228\n\n\nNA\n\n\n112\n\n\n143\n\n\n125\n\n\n172\n\n\n131\n\n\n135\n\n\nNA\n\n\nNA\n\n\n129\n\n\nNA\n\n\nNA\n\n\n220\n\n\nNA\n\n\nNA\n\n\n184\n\n\n141\n\n\n146\n\n\nNA\n\n\n148\n\n\n167\n\n\nNA\n\n\n170\n\n\nNA\n\n\n115\n\n\n129\n\n\n128\n\n\nNA\n\n\n150\n\n\n209\n\n\n169\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n190\n\n\nNA\n\n\n214\n\n\n160\n\n\n193\n\n\nNA\n\n\nNA\n\n\n196\n\n\n200\n\n\n106\n\n\n96\n\n\n216\n\n\n109\n\n\nNA\n\n\nNA\n\n\n156\n\n\n220\n\n\n201\n\n\n142\n\n\n172\n\n\n242\n\n\n147\n\n\n175\n\n\n251\n\n\n146\n\n\n194\n\n\n143\n\n\nNA\n\n\n168\n\n\nNA\n\n\n151\n\n\nNA\n\n\nNA\n\n\n186\n\n\nNA\n\n\n58\n\n\n151\n\n\n158\n\n\n158\n\n\n222\n\n\n149\n\n\n214\n\n\n160\n\n\n214\n\n\nNA\n\n\n150\n\n\n135\n\n\n196\n\n\nNA\n\n\nNA\n\n\n241\n\n\n141\n\n\n141\n\n\nNA\n\n\n117\n\n\nNA\n\n\nNA\n\n\n153\n\n\n166\n\n\n134\n\n\n216\n\n\nNA\n\n\n168\n\n\n165\n\n\nNA\n\n\n209\n\n\nNA\n\n\nNA\n\n\n143\n\n\nNA\n\n\n136\n\n\nNA\n\n\n258\n\n\n150\n\n\n167\n\n\n164\n\n\nNA\n\n\n189\n\n\n139\n\n\nNA\n\n\n214\n\n\n199\n\n\n155\n\n\nNA\n\n\nNA\n\n\n123\n\n\n205\n\n\nNA\n\n\nNA\n\n\n149\n\n\nNA\n\n\n168\n\n\n151\n\n\nNA\n\n\n98\n\n\n176\n\n\nNA\n\n\n191\n\n\n143\n\n\n163\n\n\nNA\n\n\n187\n\n\nNA\n\n\n147\n\n\n125\n\n\n153\n\n\n155\n\n\n151\n\n\n191\n\n\n179\n\n\n185\n\n\nNA\n\n\n98\n\n\n200\n\n\n169\n\n\n188\n\n\nNA\n\n\nNA\n\n\n118\n\n\nNA\n\n\n142\n\n\n109\n\n\nNA\n\n\n184\n\n\nNA\n\n\nNA\n\n\n178\n\n\n132\n\n\nNA\n\n\n192\n\n\n144\n\n\nNA\n\n\n153\n\n\n186\n\n\n171\n\n\n124\n\n\n231\n\n\nNA\n\n\n152\n\n\nNA\n\n\nNA\n\n\n212\n\n\n185\n\n\n201\n\n\n288\n\n\n188\n\n\n131\n\n\n138\n\n\n181\n\n\n132\n\n\n193\n\n\n164\n\n\n161\n\n\n173\n\n\n148\n\n\n110\n\n\n152\n\n\nNA\n\n\nNA\n\n\n218\n\n\n117\n\n\n116\n\n\n98\n\n\n103\n\n\n137\n\n\nNA\n\n\n161\n\n\n154\n\n\nNA\n\n\n127\n\n\n186\n\n\n228\n\n\nNA\n\n\n130\n\n\nNA\n\n\n168\n\n\n173\n\n\n182\n\n\n155\n\n\nNA\n\n\n166\n\n\n165\n\n\n97\n\n\n152\n\n\n167\n\n\n149\n\n\n205\n\n\nNA\n\n\n196\n\n\nNA\n\n\n139\n\n\nNA\n\n\nNA\n\n\n186\n\n\nNA\n\n\nNA\n\n\n143\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n150\n\n\n135\n\n\n179\n\n\n252\n\n\nNA\n\n\nNA\n\n\n201\n\n\nNA\n\n\n214\n\n\n197\n\n\nNA\n\n\nNA\n\n\n226\n\n\n128\n\n\nNA\n\n\nNA\n\n\n148\n\n\n200\n\n\n115\n\n\n210\n\n\n155\n\n\n135\n\n\nNA\n\n\n139\n\n\nNA\n\n\nNA\n\n\n159\n\n\n187\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n264\n\n\nNA\n\n\n159\n\n\n114\n\n\n160\n\n\n179\n\n\nNA\n\n\n119\n\n\n202\n\n\n177\n\n\nNA\n\n\nNA\n\n\n185\n\n\n134\n\n\nNA\n\n\n193\n\n\n133\n\n\n143\n\n\nNA\n\n\nNA\n\n\n228\n\n\n112\n\n\n125\n\n\n170\n\n\n115\n\n\n216\n\n\n196\n\n\n141\n\n\n153\n\n\n216\n\n\nNA\n\n\n167\n\n\n214\n\n\nNA\n\n\nNA\n\n\n125\n\n\n98\n\n\nNA\n\n\n142\n\n\n178\n\n\n132\n\n\nNA\n\n\n152\n\n\nNA\n\n\nNA\n\n\n182\n\n\n165\n\n\nNA\n\n\n150\n\n\nNA\n\n\nNA\n\n\n264\n\n\n177\n\n\nNA\n\n\nNA\n\n\n185\n\n\n134\n\n\nNA\n\n\n193\n\n\n133\n\n\n143\n\n\nNA\n\n\nNA\n\n\n228\n\n\n112\n\n\n125\n\n\n170\n\n\n115\n\n\n216\n\n\n196\n\n\n141\n\n\n153\n\n\n216\n\n\nNA\n\n\n167\n\n\n214\n\n\nNA\n\n\nNA\n\n\n125\n\n\n98\n\n\nNA\n\n\n142\n\n\n178\n\n\n132\n\n\nNA\n\n\n152\n\n\nNA\n\n\nNA\n\n\n182\n\n\n165\n\n\nNA\n\n\n150\n\n\nNA\n\n\nNA\n\n\n264\n\n\n\n\n13\n\n\n0\n\n\nTRIG\n\n\nTriglycerides\n\n\n176\n\n\nNA\n\n\nNA\n\n\n97\n\n\n162\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n71\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n141\n\n\n72\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n48\n\n\n160\n\n\n80\n\n\n167\n\n\n176\n\n\nNA\n\n\nNA\n\n\n162\n\n\nNA\n\n\nNA\n\n\n71\n\n\nNA\n\n\nNA\n\n\n141\n\n\n72\n\n\nNA\n\n\nNA\n\n\n48\n\n\n80\n\n\n167\n\n\n176\n\n\nNA\n\n\nNA\n\n\n162\n\n\nNA\n\n\nNA\n\n\n71\n\n\nNA\n\n\nNA\n\n\n141\n\n\n72\n\n\nNA\n\n\nNA\n\n\n48\n\n\n80\n\n\n167\n\n\n327\n\n\n83\n\n\nNA\n\n\n139\n\n\n186\n\n\n78\n\n\n138\n\n\nNA\n\n\n111\n\n\n98\n\n\nNA\n\n\n569\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n78\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n74\n\n\nNA\n\n\nNA\n\n\n76\n\n\nNA\n\n\n447\n\n\n72\n\n\n122\n\n\n160\n\n\nNA\n\n\n120\n\n\n141\n\n\n178\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n386\n\n\nNA\n\n\nNA\n\n\n123\n\n\n292\n\n\n339\n\n\n84\n\n\n96\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n117\n\n\nNA\n\n\n144\n\n\n213\n\n\nNA\n\n\n269\n\n\n113\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n59\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n107\n\n\n184\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n81\n\n\n154\n\n\n48\n\n\n183\n\n\n38\n\n\n176\n\n\nNA\n\n\nNA\n\n\n718\n\n\n101\n\n\n216\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n181\n\n\nNA\n\n\n118\n\n\nNA\n\n\n136\n\n\nNA\n\n\n121\n\n\nNA\n\n\nNA\n\n\n159\n\n\nNA\n\n\n98\n\n\n152\n\n\n162\n\n\n201\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n114\n\n\nNA\n\n\n58\n\n\nNA\n\n\n81\n\n\nNA\n\n\nNA\n\n\n171\n\n\nNA\n\n\n53\n\n\nNA\n\n\n482\n\n\nNA\n\n\nNA\n\n\n120\n\n\n101\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n41\n\n\n134\n\n\n70\n\n\n83\n\n\n162\n\n\nNA\n\n\n127\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n416\n\n\nNA\n\n\n55\n\n\n89\n\n\nNA\n\n\n153\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n52\n\n\nNA\n\n\nNA\n\n\n210\n\n\nNA\n\n\nNA\n\n\n94\n\n\nNA\n\n\n95\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n104\n\n\nNA\n\n\n120\n\n\nNA\n\n\n68\n\n\n279\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n72\n\n\nNA\n\n\n97\n\n\n204\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n56\n\n\n89\n\n\nNA\n\n\nNA\n\n\n82\n\n\n51\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n74\n\n\n115\n\n\nNA\n\n\n279\n\n\n145\n\n\n66\n\n\nNA\n\n\n101\n\n\nNA\n\n\n74\n\n\nNA\n\n\n36\n\n\nNA\n\n\n76\n\n\nNA\n\n\nNA\n\n\n133\n\n\nNA\n\n\n33\n\n\nNA\n\n\n130\n\n\n135\n\n\n87\n\n\n62\n\n\n119\n\n\n133\n\n\n163\n\n\nNA\n\n\n59\n\n\nNA\n\n\n162\n\n\nNA\n\n\nNA\n\n\n108\n\n\n104\n\n\n49\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n172\n\n\nNA\n\n\n65\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n89\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n286\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n121\n\n\n128\n\n\n71\n\n\nNA\n\n\nNA\n\n\n111\n\n\n286\n\n\nNA\n\n\n126\n\n\nNA\n\n\n264\n\n\nNA\n\n\nNA\n\n\n108\n\n\n100\n\n\nNA\n\n\nNA\n\n\n61\n\n\nNA\n\n\n55\n\n\n97\n\n\nNA\n\n\nNA\n\n\n204\n\n\nNA\n\n\n208\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n438\n\n\nNA\n\n\n89\n\n\n124\n\n\n216\n\n\n226\n\n\n106\n\n\n91\n\n\n150\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n58\n\n\n97\n\n\n110\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n193\n\n\n234\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n141\n\n\n72\n\n\nNA\n\n\n120\n\n\n162\n\n\nNA\n\n\nNA\n\n\n231\n\n\n137\n\n\n131\n\n\nNA\n\n\nNA\n\n\n90\n\n\nNA\n\n\nNA\n\n\n122\n\n\nNA\n\n\n45\n\n\n477\n\n\nNA\n\n\n65\n\n\n106\n\n\n218\n\n\n52\n\n\n264\n\n\n240\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n199\n\n\n385\n\n\nNA\n\n\nNA\n\n\n114\n\n\n75\n\n\n47\n\n\nNA\n\n\nNA\n\n\n104\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n266\n\n\n88\n\n\nNA\n\n\nNA\n\n\n660\n\n\nNA\n\n\n68\n\n\n79\n\n\nNA\n\n\n171\n\n\nNA\n\n\n70\n\n\n48\n\n\n126\n\n\nNA\n\n\nNA\n\n\n63\n\n\nNA\n\n\nNA\n\n\n160\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n473\n\n\nNA\n\n\nNA\n\n\n88\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n80\n\n\n86\n\n\n73\n\n\n248\n\n\nNA\n\n\nNA\n\n\n192\n\n\nNA\n\n\n122\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n88\n\n\nNA\n\n\nNA\n\n\n92\n\n\n92\n\n\n131\n\n\nNA\n\n\n49\n\n\nNA\n\n\nNA\n\n\n69\n\n\nNA\n\n\nNA\n\n\n65\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n167\n\n\nNA\n\n\n100\n\n\n213\n\n\n167\n\n\nNA\n\n\nNA\n\n\n71\n\n\n120\n\n\n83\n\n\nNA\n\n\nNA\n\n\n123\n\n\n339\n\n\nNA\n\n\n213\n\n\n176\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n416\n\n\n55\n\n\nNA\n\n\nNA\n\n\n104\n\n\n82\n\n\n162\n\n\n104\n\n\n172\n\n\nNA\n\n\nNA\n\n\n71\n\n\n126\n\n\nNA\n\n\nNA\n\n\n124\n\n\nNA\n\n\nNA\n\n\n193\n\n\n141\n\n\n72\n\n\nNA\n\n\n385\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n48\n\n\nNA\n\n\n80\n\n\nNA\n\n\nNA\n\n\n167\n\n\n83\n\n\nNA\n\n\nNA\n\n\n123\n\n\n339\n\n\nNA\n\n\n213\n\n\n176\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n416\n\n\n55\n\n\nNA\n\n\nNA\n\n\n104\n\n\n82\n\n\n162\n\n\n104\n\n\n172\n\n\nNA\n\n\nNA\n\n\n71\n\n\n126\n\n\nNA\n\n\nNA\n\n\n124\n\n\nNA\n\n\nNA\n\n\n193\n\n\n141\n\n\n72\n\n\nNA\n\n\n385\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n48\n\n\nNA\n\n\n80\n\n\nNA\n\n\nNA\n\n\n167\n\n\n\n\n14\n\n\n0\n\n\nLDL\n\n\nLDL\n\n\n62\n\n\nNA\n\n\nNA\n\n\n147\n\n\n135\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n113\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n101\n\n\n75\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n105\n\n\n704\n\n\n84\n\n\n169\n\n\n62\n\n\nNA\n\n\nNA\n\n\n135\n\n\nNA\n\n\nNA\n\n\n113\n\n\nNA\n\n\nNA\n\n\n101\n\n\n75\n\n\nNA\n\n\nNA\n\n\n105\n\n\n84\n\n\n169\n\n\n62\n\n\nNA\n\n\nNA\n\n\n135\n\n\nNA\n\n\nNA\n\n\n113\n\n\nNA\n\n\nNA\n\n\n101\n\n\n75\n\n\nNA\n\n\nNA\n\n\n105\n\n\n84\n\n\n169\n\n\n136\n\n\n110\n\n\nNA\n\n\n112\n\n\n106\n\n\n101\n\n\n147\n\n\nNA\n\n\n116\n\n\n98\n\n\nNA\n\n\n128\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n68\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n83\n\n\nNA\n\n\nNA\n\n\n87\n\n\nNA\n\n\n82\n\n\n51\n\n\n105\n\n\n128\n\n\nNA\n\n\n80\n\n\n93\n\n\n141\n\n\nNA\n\n\n124\n\n\nNA\n\n\n103\n\n\nNA\n\n\nNA\n\n\n111\n\n\n94\n\n\n43\n\n\n89\n\n\n154\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n79\n\n\n149\n\n\nNA\n\n\n92\n\n\n117\n\n\nNA\n\n\n85\n\n\n174\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n56\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n87\n\n\n89\n\n\n101\n\n\n69\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n170\n\n\n112\n\n\n60\n\n\n66\n\n\n141\n\n\n62\n\n\nNA\n\n\nNA\n\n\n56\n\n\n135\n\n\n79\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n92\n\n\nNA\n\n\n132\n\n\nNA\n\n\n43\n\n\nNA\n\n\n125\n\n\nNA\n\n\nNA\n\n\n61\n\n\nNA\n\n\n83\n\n\n63\n\n\n104\n\n\n101\n\n\nNA\n\n\nNA\n\n\n73\n\n\nNA\n\n\n99\n\n\nNA\n\n\n150\n\n\nNA\n\n\n136\n\n\n80\n\n\nNA\n\n\n74\n\n\nNA\n\n\n86\n\n\n80\n\n\n129\n\n\nNA\n\n\nNA\n\n\n125\n\n\n149\n\n\nNA\n\n\n138\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n88\n\n\n126\n\n\n144\n\n\n74\n\n\n107\n\n\nNA\n\n\n100\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n42\n\n\n84\n\n\nNA\n\n\n110\n\n\n81\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n68\n\n\nNA\n\n\nNA\n\n\n142\n\n\nNA\n\n\nNA\n\n\n124\n\n\nNA\n\n\n97\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n61\n\n\nNA\n\n\n75\n\n\nNA\n\n\n88\n\n\n117\n\n\n90\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n67\n\n\nNA\n\n\n147\n\n\n93\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n153\n\n\n145\n\n\nNA\n\n\n63\n\n\n163\n\n\n41\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n125\n\n\n89\n\n\n99\n\n\n144\n\n\n74\n\n\n121\n\n\nNA\n\n\n92\n\n\n127\n\n\n84\n\n\nNA\n\n\n106\n\n\nNA\n\n\n97\n\n\nNA\n\n\nNA\n\n\n121\n\n\nNA\n\n\n5\n\n\nNA\n\n\n94\n\n\n90\n\n\n153\n\n\n83\n\n\n143\n\n\n100\n\n\n156\n\n\nNA\n\n\n105\n\n\n62\n\n\n135\n\n\nNA\n\n\nNA\n\n\n164\n\n\n58\n\n\n88\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n79\n\n\nNA\n\n\n73\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n106\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n55\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n90\n\n\n113\n\n\nNA\n\n\nNA\n\n\n125\n\n\n54\n\n\nNA\n\n\n152\n\n\nNA\n\n\n84\n\n\nNA\n\n\nNA\n\n\n68\n\n\n137\n\n\nNA\n\n\nNA\n\n\n86\n\n\nNA\n\n\n103\n\n\n99\n\n\nNA\n\n\nNA\n\n\n106\n\n\nNA\n\n\n116\n\n\nNA\n\n\n76\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n89\n\n\n54\n\n\n68\n\n\n74\n\n\n65\n\n\n126\n\n\n110\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n103\n\n\n101\n\n\n115\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n80\n\n\n33\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n101\n\n\n75\n\n\nNA\n\n\n134\n\n\n80\n\n\nNA\n\n\nNA\n\n\n90\n\n\n105\n\n\n68\n\n\nNA\n\n\nNA\n\n\n91\n\n\nNA\n\n\nNA\n\n\n142\n\n\nNA\n\n\n133\n\n\nNA\n\n\nNA\n\n\n77\n\n\n87\n\n\n100\n\n\n51\n\n\n100\n\n\n78\n\n\nNA\n\n\nNA\n\n\n97\n\n\n38\n\n\n42\n\n\nNA\n\n\nNA\n\n\n112\n\n\n64\n\n\n75\n\n\nNA\n\n\nNA\n\n\n68\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n85\n\n\n133\n\n\nNA\n\n\nNA\n\n\n42\n\n\nNA\n\n\n109\n\n\n116\n\n\nNA\n\n\n90\n\n\nNA\n\n\n95\n\n\n105\n\n\n70\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n704\n\n\nNA\n\n\n70\n\n\nNA\n\n\nNA\n\n\n108\n\n\nNA\n\n\nNA\n\n\n99\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n84\n\n\n81\n\n\n104\n\n\n161\n\n\nNA\n\n\nNA\n\n\n91\n\n\nNA\n\n\n87\n\n\n77\n\n\nNA\n\n\nNA\n\n\n161\n\n\n74\n\n\nNA\n\n\nNA\n\n\n88\n\n\n141\n\n\n60\n\n\nNA\n\n\n104\n\n\nNA\n\n\nNA\n\n\n84\n\n\nNA\n\n\nNA\n\n\n89\n\n\n104\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n169\n\n\nNA\n\n\n93\n\n\n22\n\n\n88\n\n\nNA\n\n\nNA\n\n\n71\n\n\n135\n\n\n110\n\n\nNA\n\n\nNA\n\n\n111\n\n\n43\n\n\nNA\n\n\n117\n\n\n62\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n42\n\n\nNA\n\n\nNA\n\n\n61\n\n\n163\n\n\n135\n\n\n58\n\n\n79\n\n\nNA\n\n\nNA\n\n\n113\n\n\n152\n\n\nNA\n\n\nNA\n\n\n54\n\n\nNA\n\n\nNA\n\n\n80\n\n\n101\n\n\n75\n\n\nNA\n\n\n42\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n105\n\n\nNA\n\n\n84\n\n\nNA\n\n\nNA\n\n\n169\n\n\n110\n\n\nNA\n\n\nNA\n\n\n111\n\n\n43\n\n\nNA\n\n\n117\n\n\n62\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n42\n\n\nNA\n\n\nNA\n\n\n61\n\n\n163\n\n\n135\n\n\n58\n\n\n79\n\n\nNA\n\n\nNA\n\n\n113\n\n\n152\n\n\nNA\n\n\nNA\n\n\n54\n\n\nNA\n\n\nNA\n\n\n80\n\n\n101\n\n\n75\n\n\nNA\n\n\n42\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n105\n\n\nNA\n\n\n84\n\n\nNA\n\n\nNA\n\n\n169\n\n\n\n\n15\n\n\n0\n\n\nDYSLIP\n\n\nDyslipidemia\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n2\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n9\n\n\n2\n\n\n9\n\n\n1\n\n\n2\n\n\n9\n\n\n2\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n2\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n2\n\n\n2\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n9\n\n\n1\n\n\n2\n\n\n2\n\n\n9\n\n\n2\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n2\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n2\n\n\n2\n\n\n9\n\n\n2\n\n\n2\n\n\n9\n\n\n2\n\n\n2\n\n\n9\n\n\n9\n\n\n9\n\n\n2\n\n\n9\n\n\n9\n\n\n9\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n2\n\n\n2\n\n\n2\n\n\n9\n\n\n2\n\n\n2\n\n\n9\n\n\n9\n\n\n2\n\n\n2\n\n\n2\n\n\n9\n\n\n9\n\n\n9\n\n\n2\n\n\n2\n\n\n2\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n2\n\n\n9\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n9\n\n\n9\n\n\n2\n\n\n2\n\n\n2\n\n\n9\n\n\n2\n\n\n9\n\n\n2\n\n\n9\n\n\n9\n\n\n2\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n2\n\n\n9\n\n\n2\n\n\n2\n\n\n9\n\n\n9\n\n\n4\n\n\n9\n\n\n3\n\n\n2\n\n\n2\n\n\n2\n\n\n1\n\n\n2\n\n\n2\n\n\n2\n\n\n9\n\n\n2\n\n\n9\n\n\n9\n\n\n9\n\n\n2\n\n\n9\n\n\n1\n\n\n1\n\n\n2\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n2\n\n\n9\n\n\n9\n\n\n1\n\n\n2\n\n\n2\n\n\n9\n\n\n2\n\n\n2\n\n\n9\n\n\n2\n\n\n9\n\n\n2\n\n\n2\n\n\n2\n\n\n9\n\n\n1\n\n\n9\n\n\n3\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n2\n\n\n2\n\n\n9\n\n\n9\n\n\n9\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n1\n\n\n9\n\n\n3\n\n\n9\n\n\n9\n\n\n2\n\n\n2\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n3\n\n\n2\n\n\n2\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n2\n\n\n9\n\n\n9\n\n\n2\n\n\n9\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n2\n\n\n2\n\n\n9\n\n\n2\n\n\n2\n\n\n2\n\n\n9\n\n\n9\n\n\n2\n\n\n1\n\n\n2\n\n\n9\n\n\n2\n\n\n9\n\n\n9\n\n\n2\n\n\n2\n\n\n1\n\n\n9\n\n\n9\n\n\n2\n\n\n1\n\n\n9\n\n\n2\n\n\n9\n\n\n9\n\n\n2\n\n\n9\n\n\n2\n\n\n9\n\n\n2\n\n\n2\n\n\n1\n\n\n2\n\n\n9\n\n\n1\n\n\n2\n\n\n9\n\n\n2\n\n\n9\n\n\n2\n\n\n9\n\n\n9\n\n\n2\n\n\n2\n\n\n9\n\n\n9\n\n\n2\n\n\n9\n\n\n1\n\n\n2\n\n\n9\n\n\n2\n\n\n2\n\n\n9\n\n\n2\n\n\n2\n\n\n2\n\n\n9\n\n\n2\n\n\n9\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n2\n\n\n9\n\n\n2\n\n\n2\n\n\n9\n\n\n3\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n2\n\n\n2\n\n\n9\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n4\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n9\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n9\n\n\n9\n\n\n9\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n1\n\n\n9\n\n\n2\n\n\n2\n\n\n9\n\n\n2\n\n\n2\n\n\n2\n\n\n9\n\n\n2\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n2\n\n\n9\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n4\n\n\n2\n\n\n2\n\n\n9\n\n\n2\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n2\n\n\n9\n\n\n9\n\n\n2\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n9\n\n\n9\n\n\n2\n\n\n9\n\n\n2\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n2\n\n\n9\n\n\n9\n\n\n2\n\n\n2\n\n\n2\n\n\n9\n\n\n1\n\n\n2\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n2\n\n\n9\n\n\n1\n\n\n2\n\n\n2\n\n\n2\n\n\n9\n\n\n2\n\n\n2\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n2\n\n\n9\n\n\n2\n\n\n2\n\n\n2\n\n\n9\n\n\n4\n\n\n2\n\n\n1\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n1\n\n\n2\n\n\n9\n\n\n9\n\n\n1\n\n\n2\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n2\n\n\n1\n\n\n1\n\n\n9\n\n\n2\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n2\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n2\n\n\n9\n\n\n2\n\n\n2\n\n\n2\n\n\n9\n\n\n4\n\n\n2\n\n\n1\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n1\n\n\n2\n\n\n9\n\n\n9\n\n\n1\n\n\n2\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n2\n\n\n1\n\n\n1\n\n\n9\n\n\n2\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n2\n\n\n\n\n16\n\n\n0\n\n\nCESD\n\n\nCESD Depression Score\n\n\n14\n\n\n20\n\n\n18\n\n\n14\n\n\n1\n\n\n3\n\n\n9\n\n\n10\n\n\n8\n\n\n21\n\n\n18\n\n\n17\n\n\n31\n\n\n30\n\n\n2\n\n\n-1\n\n\n6\n\n\n25\n\n\n27\n\n\n32\n\n\n28\n\n\n14\n\n\n20\n\n\n18\n\n\n1\n\n\n3\n\n\n10\n\n\n8\n\n\n21\n\n\n17\n\n\n31\n\n\n30\n\n\n-1\n\n\n6\n\n\n25\n\n\n32\n\n\n28\n\n\n14\n\n\n20\n\n\n18\n\n\n1\n\n\n3\n\n\n10\n\n\n8\n\n\n21\n\n\n17\n\n\n31\n\n\n30\n\n\n-1\n\n\n6\n\n\n25\n\n\n32\n\n\n28\n\n\n4\n\n\n24\n\n\n0\n\n\n27\n\n\n0\n\n\n43\n\n\n8\n\n\n34\n\n\n6\n\n\n23\n\n\n27\n\n\n4\n\n\n44\n\n\n0\n\n\n13\n\n\n11\n\n\n5\n\n\n18\n\n\n13\n\n\n43\n\n\n1\n\n\n13\n\n\n6\n\n\n2\n\n\n7\n\n\n5\n\n\n-1\n\n\n8\n\n\n3\n\n\n0\n\n\n30\n\n\n7\n\n\n14\n\n\n15\n\n\n25\n\n\n12\n\n\n17\n\n\n8\n\n\n16\n\n\n10\n\n\n5\n\n\n0\n\n\n11\n\n\n17\n\n\n15\n\n\n-1\n\n\n14\n\n\n0\n\n\n29\n\n\n12\n\n\n0\n\n\n1\n\n\n9\n\n\n0\n\n\n20\n\n\n0\n\n\n1\n\n\n4\n\n\n11\n\n\n3\n\n\n4\n\n\n5\n\n\n4\n\n\n12\n\n\n12\n\n\n3\n\n\n0\n\n\n20\n\n\n3\n\n\n16\n\n\n5\n\n\n23\n\n\n20\n\n\n23\n\n\n12\n\n\n14\n\n\n17\n\n\n24\n\n\n9\n\n\n6\n\n\n1\n\n\n2\n\n\n14\n\n\n1\n\n\n14\n\n\n10\n\n\n10\n\n\n6\n\n\n5\n\n\n3\n\n\n0\n\n\n12\n\n\n0\n\n\n10\n\n\n3\n\n\n12\n\n\n41\n\n\n11\n\n\n15\n\n\n-1\n\n\n12\n\n\n17\n\n\n15\n\n\n2\n\n\n23\n\n\n2\n\n\n14\n\n\n23\n\n\n4\n\n\n7\n\n\n31\n\n\n19\n\n\n20\n\n\n1\n\n\n12\n\n\n7\n\n\n14\n\n\n6\n\n\n13\n\n\n26\n\n\n5\n\n\n5\n\n\n2\n\n\n11\n\n\n47\n\n\n6\n\n\n8\n\n\n16\n\n\n4\n\n\n29\n\n\n22\n\n\n9\n\n\n3\n\n\n14\n\n\n2\n\n\n7\n\n\n19\n\n\n8\n\n\n3\n\n\n14\n\n\n20\n\n\n17\n\n\n1\n\n\n13\n\n\n1\n\n\n11\n\n\n1\n\n\n11\n\n\n3\n\n\n8\n\n\n4\n\n\n8\n\n\n48\n\n\n16\n\n\n33\n\n\n20\n\n\n8\n\n\n0\n\n\n1\n\n\n18\n\n\n1\n\n\n11\n\n\n20\n\n\n31\n\n\n27\n\n\n0\n\n\n32\n\n\n11\n\n\n25\n\n\n8\n\n\n9\n\n\n24\n\n\n-1\n\n\n14\n\n\n16\n\n\n3\n\n\n14\n\n\n14\n\n\n10\n\n\n8\n\n\n3\n\n\n26\n\n\n6\n\n\n20\n\n\n4\n\n\n11\n\n\n9\n\n\n16\n\n\n1\n\n\n9\n\n\n10\n\n\n5\n\n\n2\n\n\n30\n\n\n25\n\n\n6\n\n\n0\n\n\n10\n\n\n19\n\n\n36\n\n\n0\n\n\n4\n\n\n8\n\n\n5\n\n\n43\n\n\n35\n\n\n2\n\n\n38\n\n\n6\n\n\n49\n\n\n14\n\n\n5\n\n\n10\n\n\n0\n\n\n12\n\n\n0\n\n\n9\n\n\n7\n\n\n1\n\n\n7\n\n\n1\n\n\n5\n\n\n18\n\n\n33\n\n\n0\n\n\n6\n\n\n5\n\n\n23\n\n\n28\n\n\n9\n\n\n24\n\n\n3\n\n\n4\n\n\n11\n\n\n33\n\n\n0\n\n\n14\n\n\n9\n\n\n10\n\n\n11\n\n\n10\n\n\n44\n\n\n30\n\n\n4\n\n\n6\n\n\n8\n\n\n14\n\n\n7\n\n\n26\n\n\n16\n\n\n0\n\n\n8\n\n\n37\n\n\n11\n\n\n21\n\n\n48\n\n\n10\n\n\n7\n\n\n6\n\n\n36\n\n\n1\n\n\n5\n\n\n9\n\n\n9\n\n\n41\n\n\n18\n\n\n6\n\n\n0\n\n\n-1\n\n\n6\n\n\n3\n\n\n3\n\n\n33\n\n\n3\n\n\n1\n\n\n27\n\n\n41\n\n\n14\n\n\n34\n\n\n0\n\n\n7\n\n\n10\n\n\n2\n\n\n17\n\n\n25\n\n\n28\n\n\n16\n\n\n4\n\n\n14\n\n\n48\n\n\n20\n\n\n12\n\n\n37\n\n\n23\n\n\n12\n\n\n0\n\n\n0\n\n\n31\n\n\n30\n\n\n19\n\n\n3\n\n\n17\n\n\n2\n\n\n7\n\n\n14\n\n\n1\n\n\n6\n\n\n41\n\n\n0\n\n\n2\n\n\n27\n\n\n-1\n\n\n10\n\n\n41\n\n\n39\n\n\n8\n\n\n3\n\n\n9\n\n\n3\n\n\n15\n\n\n-1\n\n\n17\n\n\n4\n\n\n3\n\n\n22\n\n\n4\n\n\n22\n\n\n5\n\n\n10\n\n\n29\n\n\n9\n\n\n11\n\n\n8\n\n\n38\n\n\n0\n\n\n15\n\n\n5\n\n\n51\n\n\n14\n\n\n7\n\n\n10\n\n\n11\n\n\n3\n\n\n-1\n\n\n31\n\n\n0\n\n\n21\n\n\n28\n\n\n6\n\n\n45\n\n\n14\n\n\n1\n\n\n25\n\n\n16\n\n\n2\n\n\n13\n\n\n0\n\n\n8\n\n\n24\n\n\n27\n\n\n21\n\n\n4\n\n\n10\n\n\n0\n\n\n16\n\n\n1\n\n\n9\n\n\n8\n\n\n2\n\n\n13\n\n\n35\n\n\n32\n\n\n4\n\n\n24\n\n\n25\n\n\n30\n\n\n0\n\n\n3\n\n\n25\n\n\n0\n\n\n7\n\n\n33\n\n\n29\n\n\n25\n\n\n2\n\n\n28\n\n\n11\n\n\n11\n\n\n2\n\n\n19\n\n\n3\n\n\n33\n\n\n19\n\n\n25\n\n\n2\n\n\n10\n\n\n7\n\n\n9\n\n\n-1\n\n\n8\n\n\n10\n\n\n39\n\n\n28\n\n\n9\n\n\n33\n\n\n6\n\n\n9\n\n\n3\n\n\n23\n\n\n13\n\n\n2\n\n\n24\n\n\n27\n\n\n44\n\n\n16\n\n\n5\n\n\n-1\n\n\n1\n\n\n14\n\n\n15\n\n\n14\n\n\n5\n\n\n19\n\n\n3\n\n\n20\n\n\n18\n\n\n11\n\n\n6\n\n\n1\n\n\n18\n\n\n28\n\n\n3\n\n\n10\n\n\n8\n\n\n8\n\n\n21\n\n\n36\n\n\n27\n\n\n17\n\n\n14\n\n\n12\n\n\n31\n\n\n30\n\n\n27\n\n\n5\n\n\n29\n\n\n-1\n\n\n6\n\n\n25\n\n\n2\n\n\n32\n\n\n7\n\n\n39\n\n\n28\n\n\n24\n\n\n27\n\n\n44\n\n\n16\n\n\n5\n\n\n-1\n\n\n1\n\n\n14\n\n\n15\n\n\n14\n\n\n5\n\n\n19\n\n\n3\n\n\n20\n\n\n18\n\n\n11\n\n\n6\n\n\n1\n\n\n18\n\n\n28\n\n\n3\n\n\n10\n\n\n8\n\n\n8\n\n\n21\n\n\n36\n\n\n27\n\n\n17\n\n\n14\n\n\n12\n\n\n31\n\n\n30\n\n\n27\n\n\n5\n\n\n29\n\n\n-1\n\n\n6\n\n\n25\n\n\n2\n\n\n32\n\n\n7\n\n\n39\n\n\n28\n\n\n\n\n17\n\n\n0\n\n\nSMOKE\n\n\nSmoking Status\n\n\n3\n\n\n3\n\n\n2\n\n\n3\n\n\n3\n\n\n1\n\n\n2\n\n\n3\n\n\n2\n\n\n3\n\n\n3\n\n\n3\n\n\n2\n\n\n3\n\n\n3\n\n\n2\n\n\n3\n\n\n3\n\n\n3\n\n\n3\n\n\n1\n\n\n3\n\n\n3\n\n\n2\n\n\n3\n\n\n1\n\n\n3\n\n\n2\n\n\n3\n\n\n3\n\n\n2\n\n\n3\n\n\n2\n\n\n3\n\n\n3\n\n\n3\n\n\n1\n\n\n3\n\n\n3\n\n\n2\n\n\n3\n\n\n1\n\n\n3\n\n\n2\n\n\n3\n\n\n3\n\n\n2\n\n\n3\n\n\n2\n\n\n3\n\n\n3\n\n\n3\n\n\n1\n\n\n1\n\n\n3\n\n\n2\n\n\n3\n\n\n2\n\n\n3\n\n\n2\n\n\n2\n\n\n3\n\n\n1\n\n\n1\n\n\n2\n\n\n3\n\n\n2\n\n\n3\n\n\n2\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n2\n\n\n2\n\n\n1\n\n\n2\n\n\n2\n\n\n2\n\n\n3\n\n\n3\n\n\n2\n\n\n2\n\n\n3\n\n\n3\n\n\n3\n\n\n2\n\n\n1\n\n\n3\n\n\n2\n\n\n3\n\n\n1\n\n\n3\n\n\n3\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n3\n\n\n1\n\n\n2\n\n\n2\n\n\n2\n\n\n1\n\n\n2\n\n\n3\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n3\n\n\n1\n\n\n2\n\n\n2\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n3\n\n\n1\n\n\n3\n\n\n2\n\n\n3\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n3\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n3\n\n\n2\n\n\n3\n\n\n3\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n2\n\n\n3\n\n\n1\n\n\n1\n\n\n3\n\n\n3\n\n\n2\n\n\n1\n\n\n2\n\n\n3\n\n\n1\n\n\n3\n\n\n1\n\n\n3\n\n\n1\n\n\n1\n\n\n2\n\n\n3\n\n\n3\n\n\n2\n\n\n2\n\n\n2\n\n\n3\n\n\n2\n\n\n2\n\n\n3\n\n\n2\n\n\n1\n\n\n3\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n3\n\n\n3\n\n\n2\n\n\n2\n\n\n3\n\n\n3\n\n\n3\n\n\n2\n\n\n2\n\n\n2\n\n\n3\n\n\n1\n\n\n3\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n3\n\n\n3\n\n\n3\n\n\n3\n\n\n3\n\n\n3\n\n\n2\n\n\n2\n\n\n3\n\n\n3\n\n\n3\n\n\n1\n\n\n3\n\n\n1\n\n\n2\n\n\n2\n\n\n3\n\n\n3\n\n\n3\n\n\n1\n\n\n1\n\n\n2\n\n\n3\n\n\n2\n\n\n3\n\n\n3\n\n\n3\n\n\n2\n\n\n3\n\n\n2\n\n\n1\n\n\n1\n\n\n3\n\n\n2\n\n\n2\n\n\n1\n\n\n3\n\n\n3\n\n\n2\n\n\n1\n\n\n1\n\n\n3\n\n\n3\n\n\n2\n\n\n1\n\n\n2\n\n\n2\n\n\n3\n\n\n3\n\n\n3\n\n\n3\n\n\n3\n\n\n3\n\n\n3\n\n\n3\n\n\n2\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n3\n\n\n3\n\n\n3\n\n\n3\n\n\n2\n\n\n1\n\n\n3\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n3\n\n\n1\n\n\n2\n\n\n2\n\n\n3\n\n\n3\n\n\n2\n\n\n2\n\n\n3\n\n\n3\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n3\n\n\n1\n\n\n1\n\n\n3\n\n\n2\n\n\n1\n\n\n2\n\n\n3\n\n\n3\n\n\n1\n\n\n3\n\n\n2\n\n\n2\n\n\n2\n\n\n3\n\n\n1\n\n\n3\n\n\n1\n\n\n1\n\n\n3\n\n\n2\n\n\n3\n\n\n3\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n3\n\n\n2\n\n\n2\n\n\n3\n\n\n3\n\n\n2\n\n\n2\n\n\n3\n\n\n3\n\n\n2\n\n\n2\n\n\n1\n\n\n2\n\n\n3\n\n\n1\n\n\n2\n\n\n2\n\n\n2\n\n\n3\n\n\n3\n\n\n1\n\n\n2\n\n\n3\n\n\n2\n\n\n1\n\n\n1\n\n\n3\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n3\n\n\n2\n\n\n3\n\n\n2\n\n\n2\n\n\n3\n\n\n1\n\n\n1\n\n\n3\n\n\n3\n\n\n3\n\n\n2\n\n\n3\n\n\n3\n\n\n3\n\n\n2\n\n\n2\n\n\n3\n\n\n2\n\n\n3\n\n\n1\n\n\n1\n\n\n3\n\n\n2\n\n\n3\n\n\n2\n\n\n3\n\n\n2\n\n\n3\n\n\n2\n\n\n1\n\n\n3\n\n\n1\n\n\n3\n\n\n2\n\n\n2\n\n\n1\n\n\n2\n\n\n3\n\n\n1\n\n\n2\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n3\n\n\n2\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n3\n\n\n3\n\n\n3\n\n\n1\n\n\n3\n\n\n3\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n3\n\n\n2\n\n\n3\n\n\n3\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n3\n\n\n3\n\n\n1\n\n\n2\n\n\n3\n\n\n2\n\n\n3\n\n\n2\n\n\n3\n\n\n2\n\n\n3\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n3\n\n\n3\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n3\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n3\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n2\n\n\n3\n\n\n1\n\n\n3\n\n\n3\n\n\n2\n\n\n3\n\n\n3\n\n\n3\n\n\n1\n\n\n1\n\n\n3\n\n\n1\n\n\n3\n\n\n3\n\n\n3\n\n\n1\n\n\n2\n\n\n3\n\n\n3\n\n\n3\n\n\n3\n\n\n2\n\n\n3\n\n\n3\n\n\n2\n\n\n3\n\n\n3\n\n\n3\n\n\n3\n\n\n1\n\n\n1\n\n\n3\n\n\n2\n\n\n1\n\n\n3\n\n\n1\n\n\n1\n\n\n3\n\n\n2\n\n\n1\n\n\n2\n\n\n3\n\n\n3\n\n\n2\n\n\n1\n\n\n2\n\n\n3\n\n\n3\n\n\n2\n\n\n3\n\n\n1\n\n\n3\n\n\n1\n\n\n3\n\n\n1\n\n\n3\n\n\n3\n\n\n3\n\n\n1\n\n\n2\n\n\n3\n\n\n3\n\n\n3\n\n\n3\n\n\n2\n\n\n3\n\n\n3\n\n\n2\n\n\n3\n\n\n3\n\n\n3\n\n\n3\n\n\n1\n\n\n1\n\n\n3\n\n\n2\n\n\n1\n\n\n3\n\n\n1\n\n\n1\n\n\n3\n\n\n2\n\n\n1\n\n\n2\n\n\n3\n\n\n3\n\n\n2\n\n\n1\n\n\n2\n\n\n3\n\n\n3\n\n\n2\n\n\n3\n\n\n1\n\n\n3\n\n\n1\n\n\n\n\n18\n\n\n0\n\n\nDKGRP\n\n\nDrinking Group\n\n\n0\n\n\n0\n\n\n0\n\n\n1\n\n\n0\n\n\n1\n\n\n1\n\n\n3\n\n\n1\n\n\n1\n\n\n0\n\n\n2\n\n\n1\n\n\n2\n\n\n3\n\n\n2\n\n\n0\n\n\n0\n\n\n1\n\n\n2\n\n\n0\n\n\n0\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n0\n\n\n1\n\n\n2\n\n\n1\n\n\n0\n\n\n3\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n0\n\n\n2\n\n\n0\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n0\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n2\n\n\n3\n\n\n2\n\n\n0\n\n\n0\n\n\n1\n\n\n1\n\n\n0\n\n\n1\n\n\n1\n\n\n0\n\n\n3\n\n\n1\n\n\n0\n\n\n1\n\n\n1\n\n\n3\n\n\n2\n\n\n2\n\n\n1\n\n\n3\n\n\n1\n\n\n0\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n0\n\n\n1\n\n\n0\n\n\n1\n\n\n0\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n3\n\n\n0\n\n\n1\n\n\n3\n\n\n0\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n2\n\n\n0\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n3\n\n\n1\n\n\n1\n\n\n1\n\n\n0\n\n\n1\n\n\n0\n\n\n3\n\n\n2\n\n\n0\n\n\n1\n\n\n0\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n0\n\n\n1\n\n\n1\n\n\n2\n\n\n0\n\n\n2\n\n\n0\n\n\n1\n\n\n2\n\n\n0\n\n\n0\n\n\n2\n\n\n0\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n0\n\n\n2\n\n\n1\n\n\n0\n\n\n2\n\n\n0\n\n\n1\n\n\n0\n\n\n0\n\n\n1\n\n\n0\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n3\n\n\n1\n\n\n1\n\n\n1\n\n\n0\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n0\n\n\n1\n\n\n0\n\n\n0\n\n\n0\n\n\n1\n\n\n1\n\n\n0\n\n\n1\n\n\n0\n\n\n0\n\n\n0\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n2\n\n\n0\n\n\n2\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n0\n\n\n1\n\n\n2\n\n\n1\n\n\n0\n\n\n2\n\n\n3\n\n\n2\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n0\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n0\n\n\n1\n\n\n0\n\n\n0\n\n\n0\n\n\n2\n\n\n0\n\n\n2\n\n\n1\n\n\n1\n\n\n3\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n0\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n0\n\n\n1\n\n\n0\n\n\n1\n\n\n2\n\n\n0\n\n\n2\n\n\n0\n\n\n0\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n0\n\n\n0\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n0\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n3\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n0\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n3\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n0\n\n\n1\n\n\n3\n\n\n1\n\n\n0\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n0\n\n\n1\n\n\n2\n\n\n3\n\n\n0\n\n\n2\n\n\n0\n\n\n0\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n0\n\n\n0\n\n\n2\n\n\n0\n\n\n1\n\n\n1\n\n\n0\n\n\n0\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n0\n\n\n0\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n0\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n2\n\n\n3\n\n\n3\n\n\n3\n\n\n1\n\n\n0\n\n\n1\n\n\n2\n\n\n1\n\n\n0\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n0\n\n\n2\n\n\n0\n\n\n0\n\n\n0\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n0\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n0\n\n\n0\n\n\n1\n\n\n0\n\n\n0\n\n\n0\n\n\n2\n\n\n1\n\n\n1\n\n\n0\n\n\n2\n\n\n0\n\n\n2\n\n\n2\n\n\n2\n\n\n0\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n0\n\n\n3\n\n\n0\n\n\n0\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n0\n\n\n2\n\n\n1\n\n\n2\n\n\n2\n\n\n0\n\n\n0\n\n\n2\n\n\n3\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n3\n\n\n1\n\n\n1\n\n\n3\n\n\n1\n\n\n0\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n3\n\n\n1\n\n\n3\n\n\n1\n\n\n1\n\n\n0\n\n\n2\n\n\n0\n\n\n1\n\n\n0\n\n\n0\n\n\n3\n\n\n0\n\n\n1\n\n\n3\n\n\n1\n\n\n0\n\n\n1\n\n\n0\n\n\n1\n\n\n0\n\n\n0\n\n\n2\n\n\n3\n\n\n0\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n3\n\n\n1\n\n\n2\n\n\n0\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n0\n\n\n0\n\n\n3\n\n\n0\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n3\n\n\n1\n\n\n1\n\n\n0\n\n\n1\n\n\n0\n\n\n0\n\n\n1\n\n\n2\n\n\n3\n\n\n1\n\n\n3\n\n\n3\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n0\n\n\n1\n\n\n2\n\n\n0\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n3\n\n\n0\n\n\n3\n\n\n0\n\n\n2\n\n\n0\n\n\n0\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n\n\n19\n\n\n0\n\n\nHEROPIATE\n\n\nHeroin or Opiate Use Since Last Visit\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n-9\n\n\n1\n\n\n1\n\n\n.\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n-9\n\n\n1\n\n\n.\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n-9\n\n\n1\n\n\n.\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n-9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n-9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n-9\n\n\n1\n\n\n1\n\n\n-9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n-9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n-9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n-9\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n-9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n-9\n\n\n1\n\n\n1\n\n\n1\n\n\n-9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n-9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n.\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n-9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n-9\n\n\n1\n\n\n1\n\n\n1\n\n\n-9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n-9\n\n\n1\n\n\n1\n\n\n1\n\n\n.\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n-9\n\n\n1\n\n\n1\n\n\n1\n\n\n.\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n\n\n20\n\n\n0\n\n\nIDU\n\n\nIntravenous Drug Usage Since Last Visit\n\n\n2\n\n\n2\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n2\n\n\n1\n\n\n2\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n1\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n1\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n\n\n21\n\n\n0\n\n\nLEU3N\n\n\nCD4+ T Cell Count\n\n\n104.1659454\n\n\n257.8277832\n\n\n563.1222984\n\n\n110.4217933\n\n\n252.6633917\n\n\n634.1246084\n\n\n917.9889891\n\n\n552.9199764\n\n\n459.5461372\n\n\n186.7824358\n\n\n342.9786179\n\n\n98.6546825\n\n\n575.4565738\n\n\n648.0937554\n\n\n514.0510529\n\n\n538.88494\n\n\n461.4120531\n\n\n266.2782621\n\n\n11.88947635\n\n\n552.6720921\n\n\n792.5264499\n\n\n106.9620041\n\n\n257.6113134\n\n\n560.3858688\n\n\n249.8670905\n\n\n634.4245812\n\n\n561.410269\n\n\n458.1554244\n\n\n193.0061646\n\n\n101.3710838\n\n\n573.8763136\n\n\n650.2085536\n\n\n537.7477184\n\n\n457.2858677\n\n\n268.3541828\n\n\n555.8495044\n\n\n790.475214\n\n\n104.6129603\n\n\n257.6238176\n\n\n568.4633765\n\n\n248.3503414\n\n\n636.2269465\n\n\n556.3834591\n\n\n450.1585618\n\n\n187.1515342\n\n\n98.86003538\n\n\n572.5898376\n\n\n650.7120577\n\n\n543.7509077\n\n\n459.1291111\n\n\n271.7190223\n\n\n551.2277292\n\n\n785.5858319\n\n\n503.2401578\n\n\n304.7078759\n\n\n247.0162971\n\n\n317.4743793\n\n\n210.560778\n\n\n438.1492324\n\n\n385.7920226\n\n\n434.1716253\n\n\n328.5277709\n\n\n459.3865465\n\n\n192.2571125\n\n\n384.5403013\n\n\n123.8567743\n\n\n339.7484792\n\n\n496.5760538\n\n\n588.4104237\n\n\n363.7227458\n\n\n174.8728539\n\n\n138.8523562\n\n\n354.0124063\n\n\n121.7976101\n\n\n798.3750576\n\n\n417.016158\n\n\n393.8675283\n\n\n211.1778218\n\n\n269.7582915\n\n\n471.0699339\n\n\n432.3029249\n\n\n1018.129252\n\n\n188.1672423\n\n\n359.7636432\n\n\n272.6923386\n\n\n90.90856581\n\n\n421.9037218\n\n\n559.07726\n\n\n628.9271981\n\n\n460.1852117\n\n\n114.8993431\n\n\n551.6146194\n\n\n326.2291128\n\n\n234.8573077\n\n\n226.5296199\n\n\n387.0445748\n\n\n50.45879008\n\n\n.\n\n\n362.3132878\n\n\n160.6122441\n\n\n572.2734085\n\n\n449.5078608\n\n\n658.8276985\n\n\n360.5027464\n\n\n391.6593867\n\n\n227.1737576\n\n\n429.0325784\n\n\n327.211854\n\n\n158.7855091\n\n\n264.3500703\n\n\n728.331093\n\n\n338.9861241\n\n\n629.4380978\n\n\n28.08355425\n\n\n378.1343765\n\n\n422.6585789\n\n\n460.0082817\n\n\n503.1434029\n\n\n409.758768\n\n\n221.9535678\n\n\n438.0163643\n\n\n365.6440264\n\n\n432.6797486\n\n\n417.6126409\n\n\n405.5162868\n\n\n135.4542983\n\n\n161.7339042\n\n\n812.5779448\n\n\n104.992393\n\n\n206.2622638\n\n\n262.0259457\n\n\n436.6312992\n\n\n306.2119467\n\n\n215.7822365\n\n\n263.8231197\n\n\n636.0479927\n\n\n927.378781\n\n\n442.2879458\n\n\n357.1605173\n\n\n292.7133548\n\n\n395.6923424\n\n\n45.13497149\n\n\n21.91586195\n\n\n362.7485393\n\n\n.\n\n\n283.4599915\n\n\n332.7337289\n\n\n250.8857265\n\n\n320.47065\n\n\n270.9264051\n\n\n163.9830607\n\n\n477.7101197\n\n\n249.5265646\n\n\n465.1269443\n\n\n187.5886049\n\n\n383.8438194\n\n\n474.7264133\n\n\n488.7325955\n\n\n31.15578138\n\n\n273.5344375\n\n\n249.7564119\n\n\n372.2004051\n\n\n408.7635568\n\n\n358.8848125\n\n\n1021.383793\n\n\n446.8139809\n\n\n461.2424887\n\n\n288.7132242\n\n\n206.9862613\n\n\n460.3446936\n\n\n371.5601675\n\n\n133.289646\n\n\n34.87407578\n\n\n255.5904343\n\n\n524.0794573\n\n\n352.3117014\n\n\n211.6508186\n\n\n340.24821\n\n\n689.1779095\n\n\n740.6033781\n\n\n192.6237875\n\n\n204.1286985\n\n\n350.3675534\n\n\n108.096863\n\n\n382.1849699\n\n\n662.7493747\n\n\n362.6716279\n\n\n470.7129203\n\n\n185.3538891\n\n\n287.459407\n\n\n745.3663371\n\n\n278.5357691\n\n\n208.1264027\n\n\n257.0970728\n\n\n454.7953056\n\n\n50.44848851\n\n\n371.2341366\n\n\n371.0719382\n\n\n79.56623738\n\n\n553.8698714\n\n\n379.7595081\n\n\n790.9339988\n\n\n432.5204126\n\n\n537.5065894\n\n\n319.9169305\n\n\n356.7793239\n\n\n331.7552387\n\n\n226.160539\n\n\n50.64770115\n\n\n461.7936738\n\n\n161.6972185\n\n\n696.2969461\n\n\n560.4116777\n\n\n111.5683212\n\n\n269.007644\n\n\n373.1559369\n\n\n353.1993466\n\n\n39.95495538\n\n\n319.3415193\n\n\n332.5470757\n\n\n665.912512\n\n\n279.3072886\n\n\n291.4684577\n\n\n185.4501174\n\n\n328.9210123\n\n\n141.7728802\n\n\n110.1018792\n\n\n352.8735317\n\n\n712.3315658\n\n\n347.382142\n\n\n228.9528741\n\n\n314.6421411\n\n\n140.4889108\n\n\n196.1371366\n\n\n601.4406078\n\n\n215.9962197\n\n\n174.2930345\n\n\n444.593297\n\n\n430.172704\n\n\n410.0228015\n\n\n638.1287859\n\n\n207.8270442\n\n\n467.7681055\n\n\n173.0880721\n\n\n542.5759991\n\n\n244.3771389\n\n\n360.9040859\n\n\n646.1276871\n\n\n151.6491549\n\n\n75.51463029\n\n\n359.091557\n\n\n437.0022537\n\n\n661.943192\n\n\n.\n\n\n586.5479831\n\n\n1183.289299\n\n\n150.5382044\n\n\n541.7136276\n\n\n395.6794007\n\n\n426.5068206\n\n\n69.67833747\n\n\n419.7179835\n\n\n509.9824923\n\n\n737.2593448\n\n\n415.5929406\n\n\n433.0501993\n\n\n604.6576988\n\n\n459.0037468\n\n\n303.5308179\n\n\n386.4898062\n\n\n242.7279729\n\n\n250.891691\n\n\n144.3906299\n\n\n812.8706201\n\n\n321.2013349\n\n\n276.5630685\n\n\n402.9457095\n\n\n608.6869907\n\n\n108.1909022\n\n\n619.015877\n\n\n418.6756947\n\n\n458.550645\n\n\n122.2686701\n\n\n399.8257784\n\n\n638.2709405\n\n\n528.9798213\n\n\n308.4892812\n\n\n430.5547048\n\n\n348.4286161\n\n\n840.430623\n\n\n917.7224041\n\n\n434.6992251\n\n\n131.2735273\n\n\n555.127539\n\n\n143.2031189\n\n\n420.949411\n\n\n646.2382762\n\n\n264.3560125\n\n\n458.5244471\n\n\n588.4067277\n\n\n916.5999597\n\n\n545.8643645\n\n\n334.9039793\n\n\n479.4641035\n\n\n508.2338667\n\n\n264.7709693\n\n\n489.9960299\n\n\n186.872008\n\n\n823.7286644\n\n\n563.033977\n\n\n466.6129972\n\n\n.\n\n\n13.23316243\n\n\n165.7466347\n\n\n203.8748292\n\n\n635.6526656\n\n\n39.80971257\n\n\n513.0845209\n\n\n340.8043685\n\n\n657.9175401\n\n\n576.9411259\n\n\n399.69444\n\n\n318.1114062\n\n\n351.3244806\n\n\n440.7642445\n\n\n293.7460201\n\n\n423.9481469\n\n\n226.1662324\n\n\n106.2105261\n\n\n424.5134453\n\n\n361.3412606\n\n\n161.9503081\n\n\n403.0961116\n\n\n245.1445487\n\n\n214.6261685\n\n\n506.6390879\n\n\n98.5576915\n\n\n476.3455178\n\n\n689.8856676\n\n\n250.6420726\n\n\n328.9825589\n\n\n.\n\n\n510.0985868\n\n\n206.3739807\n\n\n545.2169673\n\n\n240.4878535\n\n\n552.6455725\n\n\n528.2421476\n\n\n.\n\n\n340.0658825\n\n\n573.4427057\n\n\n648.3790805\n\n\n309.653215\n\n\n525.3787467\n\n\n1217.453086\n\n\n512.5110091\n\n\n461.9872764\n\n\n1067.459362\n\n\n222.2721458\n\n\n33.37305189\n\n\n266.1873167\n\n\n643.7640313\n\n\n264.3872002\n\n\n328.337069\n\n\n515.2953201\n\n\n478.9129428\n\n\n605.0258812\n\n\n599.6924637\n\n\n333.3605852\n\n\n202.7326778\n\n\n134.8406921\n\n\n243.8284327\n\n\n470.5009597\n\n\n237.0857931\n\n\n608.2439637\n\n\n696.7576591\n\n\n420.9945207\n\n\n228.6146261\n\n\n53.97999355\n\n\n385.290125\n\n\n513.0942949\n\n\n415.0122617\n\n\n462.8833009\n\n\n517.1569527\n\n\n97.3776159\n\n\n174.7611482\n\n\n27.64447263\n\n\n645.2669853\n\n\n522.9129775\n\n\n786.3243289\n\n\n165.5541442\n\n\n733.362634\n\n\n25.06713579\n\n\n541.4672721\n\n\n433.371134\n\n\n356.2304821\n\n\n539.9138488\n\n\n207.6693033\n\n\n496.5282393\n\n\n359.06831\n\n\n452.74198\n\n\n458.3092509\n\n\n396.8531842\n\n\n131.6226927\n\n\n407.8606105\n\n\n267.7830989\n\n\n495.0542493\n\n\n215.4644457\n\n\n341.3303331\n\n\n400.4452824\n\n\n258.0412355\n\n\n370.8646693\n\n\n10.92351602\n\n\n10.85988249\n\n\n81.20994147\n\n\n332.2920465\n\n\n189.6509458\n\n\n316.2611138\n\n\n506.9197181\n\n\n237.4092254\n\n\n303.2551763\n\n\n240.7005127\n\n\n457.1362319\n\n\n381.64115\n\n\n551.5487348\n\n\n495.6838194\n\n\n199.5306127\n\n\n477.7949264\n\n\n157.2530762\n\n\n781.5870804\n\n\n269.7422059\n\n\n17.60601807\n\n\n601.8454683\n\n\n780.6120121\n\n\n317.4688474\n\n\n97.99388371\n\n\n270.1221188\n\n\n78.85511309\n\n\n593.1145835\n\n\n104.4711974\n\n\n518.4578656\n\n\n313.4367869\n\n\n54.21060045\n\n\n321.5475516\n\n\n255.4467946\n\n\n240.7313444\n\n\n264.0652162\n\n\n400.0717223\n\n\n586.1939981\n\n\n295.5915953\n\n\n223.689951\n\n\n562.7870036\n\n\n18.19972614\n\n\n158.7001883\n\n\n363.0059822\n\n\n788.749192\n\n\n358.6772416\n\n\n398.6154451\n\n\n299.0274701\n\n\n739.3844382\n\n\n1027.165791\n\n\n701.1897544\n\n\n144.088111\n\n\n484.5621373\n\n\n304.5626259\n\n\n193.6397015\n\n\n123.3911122\n\n\n552.3229863\n\n\n236.4906135\n\n\n363.034092\n\n\n391.7083426\n\n\n105.6822421\n\n\n376.0777041\n\n\n276.6699125\n\n\n480.3029706\n\n\n285.1400712\n\n\n281.9658259\n\n\n257.2953813\n\n\n559.6587513\n\n\n272.1981752\n\n\n214.8273383\n\n\n247.9397162\n\n\n280.5926934\n\n\n465.6964807\n\n\n634.4127401\n\n\n555.8664947\n\n\n454.7254323\n\n\n508.2190003\n\n\n191.6781837\n\n\n15.37296979\n\n\n106.0011725\n\n\n100.3726763\n\n\n.\n\n\n543.7088042\n\n\n573.31352\n\n\n649.8759625\n\n\n330.6253507\n\n\n512.184053\n\n\n463.8921299\n\n\n539.1535329\n\n\n458.5050139\n\n\n269.6565181\n\n\n239.5737957\n\n\n553.2538395\n\n\n295.5460415\n\n\n361.4290204\n\n\n789.743033\n\n\n304.6180828\n\n\n197.0465305\n\n\n121.274613\n\n\n552.6373572\n\n\n242.5001973\n\n\n362.9155766\n\n\n390.9358983\n\n\n107.98778\n\n\n376.6375655\n\n\n274.1428288\n\n\n422.4353847\n\n\n285.6727512\n\n\n279.3727363\n\n\n259.3969883\n\n\n565.8341054\n\n\n271.0715507\n\n\n212.0750557\n\n\n247.9255882\n\n\n276.914983\n\n\n463.9417127\n\n\n636.7271672\n\n\n554.4262394\n\n\n452.6137096\n\n\n505.906447\n\n\n188.8199338\n\n\n12.43380607\n\n\n107.7064682\n\n\n98.80323968\n\n\n.\n\n\n540.5951882\n\n\n570.8354749\n\n\n650.1895553\n\n\n329.3581049\n\n\n508.2521563\n\n\n461.4426773\n\n\n542.2222479\n\n\n460.2481268\n\n\n271.3079055\n\n\n241.5610838\n\n\n548.7523701\n\n\n293.4466522\n\n\n365.7388393\n\n\n788.1401023\n\n\n\n\n22\n\n\n0\n\n\nVLOAD\n\n\nViral Load;';\n\n\n102013\n\n\n8121\n\n\n4001.556158\n\n\n740\n\n\n62727.03871\n\n\n15745\n\n\n35171\n\n\n326013\n\n\n13827.21219\n\n\n684562\n\n\n5605\n\n\n12919\n\n\n29274\n\n\n293885\n\n\n16807\n\n\n14359\n\n\n27428\n\n\n1894\n\n\n2515706\n\n\n1102\n\n\n16599\n\n\n102006\n\n\n8121\n\n\n3999.583382\n\n\n44526.19366\n\n\n15760\n\n\n326007\n\n\n13827.00643\n\n\n684561\n\n\n12907\n\n\n29253\n\n\n293870\n\n\n14357\n\n\n27433\n\n\n1882\n\n\n1098\n\n\n16623\n\n\n102016\n\n\n8103\n\n\n3999.610607\n\n\n45909.6995\n\n\n15751\n\n\n326007\n\n\n13826.4724\n\n\n684569\n\n\n12914\n\n\n29265\n\n\n293888\n\n\n14361\n\n\n27429\n\n\n1892\n\n\n1104\n\n\n16596\n\n\n3643.223545\n\n\n8634\n\n\n24331\n\n\n60137\n\n\n4579.798814\n\n\n39647\n\n\n56725\n\n\n5668\n\n\n288.0922105\n\n\n29471\n\n\n106422\n\n\n71313\n\n\n307516\n\n\n47678\n\n\n396.0347241\n\n\n30126\n\n\n203.9356892\n\n\n153468\n\n\n7959.410205\n\n\n33716\n\n\n167159\n\n\n1599.427908\n\n\n21073\n\n\n3728.546095\n\n\n143419477.1\n\n\n9061\n\n\n250535\n\n\n191114.6198\n\n\n2638\n\n\n71989\n\n\n73089\n\n\n61995\n\n\n559440\n\n\n83811\n\n\n35943\n\n\n2345.876918\n\n\n348683\n\n\n80047\n\n\n17608\n\n\n107520\n\n\n445008\n\n\n208555\n\n\n36523\n\n\n271457\n\n\n.\n\n\n100681\n\n\n349897\n\n\n17682.4819\n\n\n25565\n\n\n580015\n\n\n24465.87118\n\n\n69247\n\n\n59648\n\n\n105709.4603\n\n\n18442\n\n\n20300.84846\n\n\n57270.66529\n\n\n11023\n\n\n91124\n\n\n12226.03088\n\n\n66708\n\n\n3403.038102\n\n\n71902\n\n\n6386\n\n\n68907\n\n\n286269\n\n\n79920\n\n\n2710\n\n\n1261304\n\n\n172803\n\n\n3530.528733\n\n\n7352.041883\n\n\n4303.610215\n\n\n291481\n\n\n17346\n\n\n102012\n\n\n11240.62941\n\n\n25366.44329\n\n\n9094.871369\n\n\n4148.254127\n\n\n17549\n\n\n184421\n\n\n4960\n\n\n6347.364651\n\n\n2759.420027\n\n\n64723\n\n\n11361.21532\n\n\n23690\n\n\n2520009\n\n\n272838.5799\n\n\n563\n\n\n.\n\n\n17233\n\n\n26278\n\n\n25340\n\n\n10968\n\n\n44193\n\n\n712212\n\n\n4542\n\n\n2308049.831\n\n\n22430.45984\n\n\n151083\n\n\n3813\n\n\n199898\n\n\n143012\n\n\n204564.2648\n\n\n136951\n\n\n12160.92928\n\n\n10774.56115\n\n\n1479177.419\n\n\n18\n\n\n74207\n\n\n96009\n\n\n189500.9881\n\n\n129287\n\n\n75956.79683\n\n\n17435\n\n\n11441.85253\n\n\n125850.0208\n\n\n904620\n\n\n29222.72661\n\n\n432.081472\n\n\n1302977.561\n\n\n21576.49455\n\n\n1404\n\n\n415\n\n\n48002\n\n\n6576\n\n\n60910\n\n\n138450\n\n\n396630\n\n\n71022\n\n\n96050.50384\n\n\n419924\n\n\n5762\n\n\n227122\n\n\n1877124\n\n\n20663\n\n\n5361\n\n\n9\n\n\n8120\n\n\n86153\n\n\n5227511.409\n\n\n245308\n\n\n260045\n\n\n23370.48747\n\n\n81501\n\n\n3507.902456\n\n\n3487\n\n\n9610.376879\n\n\n377969.4809\n\n\n145739\n\n\n27534\n\n\n106203\n\n\n99505\n\n\n.\n\n\n8172\n\n\n29993.58872\n\n\n4324.570957\n\n\n3999.556158\n\n\n85632.27531\n\n\n137024\n\n\n755960\n\n\n32611\n\n\n196447.0178\n\n\n56372\n\n\n21613.4841\n\n\n10232\n\n\n17665.7133\n\n\n13444.46647\n\n\n190695039.6\n\n\n404013\n\n\n150746\n\n\n739\n\n\n23700\n\n\n25923\n\n\n268510\n\n\n844923.8225\n\n\n16758\n\n\n7595.433086\n\n\n26958\n\n\n14376\n\n\n44766\n\n\n14217\n\n\n14234\n\n\n22144\n\n\n75010\n\n\n35925\n\n\n127312\n\n\n10076\n\n\n12187.58059\n\n\n12935\n\n\n34449\n\n\n576\n\n\n821\n\n\n93607\n\n\n44392\n\n\n20109\n\n\n423835.9266\n\n\n188736\n\n\n.\n\n\n5950.138143\n\n\n1178144.931\n\n\n8670.349252\n\n\n2791\n\n\n32590\n\n\n47379\n\n\n432807\n\n\n77928.09296\n\n\n2333\n\n\n4360.82071\n\n\n116620\n\n\n1460\n\n\n46051\n\n\n13135\n\n\n22052\n\n\n130005\n\n\n20293\n\n\n62722.03871\n\n\n18448.41207\n\n\n29500\n\n\n8804.251404\n\n\n14109\n\n\n193801\n\n\n45053\n\n\n3658.512557\n\n\n505.0305822\n\n\n313685\n\n\n210005\n\n\n104014\n\n\n11274.90639\n\n\n15743\n\n\n23386.26967\n\n\n53567\n\n\n91369\n\n\n27350\n\n\n112\n\n\n35169\n\n\n28678\n\n\n211650\n\n\n326010\n\n\n35437\n\n\n216812\n\n\n19551\n\n\n97164\n\n\n13825.21219\n\n\n9865.111542\n\n\n5826.346467\n\n\n35251\n\n\n81205\n\n\n1173.308354\n\n\n2028.999819\n\n\n27\n\n\n6747.632724\n\n\n684558\n\n\n3441\n\n\n12714\n\n\n193003\n\n\n.\n\n\n110410\n\n\n158635\n\n\n68601\n\n\n750\n\n\n2196\n\n\n152947\n\n\n5601\n\n\n345507\n\n\n24397.24127\n\n\n41035.95417\n\n\n301195\n\n\n10156\n\n\n1.726178748\n\n\n73830\n\n\n33001\n\n\n21292.16825\n\n\n820910\n\n\n25521\n\n\n132224.5523\n\n\n14488\n\n\n1746.153102\n\n\n622011\n\n\n538.3211723\n\n\n30351\n\n\n12917\n\n\n447.8200865\n\n\n21399\n\n\n631425\n\n\n15053\n\n\n.\n\n\n16068\n\n\n60929\n\n\n33625.962\n\n\n47453.88676\n\n\n6497\n\n\n11707\n\n\n.\n\n\n158129\n\n\n29272\n\n\n293883\n\n\n21084.04042\n\n\n11706.20448\n\n\n2189\n\n\n16806\n\n\n50209\n\n\n1064\n\n\n15302.19195\n\n\n729742\n\n\n93534\n\n\n49044\n\n\n11602\n\n\n252940\n\n\n161913\n\n\n34252\n\n\n7292.824115\n\n\n11375.27135\n\n\n13513.51362\n\n\n45524\n\n\n33767\n\n\n15106\n\n\n3617\n\n\n7402.101067\n\n\n62077\n\n\n59.92306224\n\n\n14547.74129\n\n\n149303\n\n\n1457300.828\n\n\n297004\n\n\n10019\n\n\n2891.595999\n\n\n3895.738836\n\n\n91338\n\n\n335594\n\n\n17684.94787\n\n\n1232318\n\n\n583903\n\n\n260.8995879\n\n\n19428\n\n\n325015\n\n\n69423\n\n\n95681.59479\n\n\n6413\n\n\n126116\n\n\n11382.17606\n\n\n14357\n\n\n162633\n\n\n47957\n\n\n13287\n\n\n13397\n\n\n27425\n\n\n51634\n\n\n160305\n\n\n9393.685488\n\n\n1893\n\n\n86522\n\n\n136772\n\n\n11258\n\n\n832.7579472\n\n\n81532\n\n\n14297\n\n\n2515704\n\n\n153915\n\n\n585546.694\n\n\n25143.02644\n\n\n33775\n\n\n51659.35138\n\n\n4762.280568\n\n\n43257.29962\n\n\n22520.22114\n\n\n421959\n\n\n129749\n\n\n46999\n\n\n1100\n\n\n5691\n\n\n2199\n\n\n10811.79729\n\n\n319365\n\n\n8182.087264\n\n\n121564\n\n\n146179.9677\n\n\n809\n\n\n754\n\n\n15271.82328\n\n\n382387\n\n\n83835646.4\n\n\n71397\n\n\n29101\n\n\n1561.205379\n\n\n968\n\n\n25559\n\n\n17822\n\n\n74822\n\n\n23100\n\n\n234966\n\n\n122344\n\n\n68832\n\n\n16572.05577\n\n\n255286\n\n\n36901\n\n\n5541.773571\n\n\n5244.893425\n\n\n17291.46723\n\n\n8953.405464\n\n\n16596\n\n\n66249\n\n\n30359\n\n\n23884\n\n\n346795\n\n\n24039\n\n\n32318\n\n\n743011\n\n\n2294.361119\n\n\n8632\n\n\n106403\n\n\n307516\n\n\n17601\n\n\n445020\n\n\n100678\n\n\n69260\n\n\n102004\n\n\n3815\n\n\n136958\n\n\n8817.221977\n\n\n1877107\n\n\n5358\n\n\n8119\n\n\n3997.583382\n\n\n137016\n\n\n44756\n\n\n44525.19366\n\n\n14104\n\n\n210025\n\n\n15759\n\n\n326006\n\n\n13822.00643\n\n\n2029.246416\n\n\n684559\n\n\n110409\n\n\n820905\n\n\n12903\n\n\n.\n\n\n33624.23582\n\n\n29250\n\n\n293867\n\n\n252962\n\n\n10001\n\n\n3898.204806\n\n\n14354\n\n\n27433\n\n\n1881\n\n\n421967\n\n\n1095\n\n\n255268\n\n\n9807.490847\n\n\n16618\n\n\n8619\n\n\n106403\n\n\n307503\n\n\n17620\n\n\n445019\n\n\n100667\n\n\n69264\n\n\n102013\n\n\n3817\n\n\n136937\n\n\n4906.812265\n\n\n1877124\n\n\n5373\n\n\n8102\n\n\n3995.610607\n\n\n137021\n\n\n44771\n\n\n45909.6995\n\n\n14113\n\n\n210023\n\n\n15749\n\n\n326007\n\n\n13824.4724\n\n\n2029.493013\n\n\n684567\n\n\n110413\n\n\n820920\n\n\n12911\n\n\n.\n\n\n33624.72902\n\n\n29261\n\n\n293888\n\n\n252952\n\n\n10016\n\n\n3898.204806\n\n\n14361\n\n\n27425\n\n\n1891\n\n\n421965\n\n\n1101\n\n\n255268\n\n\n15823.65168\n\n\n16596\n\n\n\n\n23\n\n\n0\n\n\nRACE\n\n\n \n\n\n1\n\n\n3\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n7\n\n\n1\n\n\n1\n\n\n3\n\n\n3\n\n\n3\n\n\n8\n\n\n3\n\n\n3\n\n\n1\n\n\n1\n\n\n3\n\n\n3\n\n\n3\n\n\n1\n\n\n1\n\n\n3\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n3\n\n\n3\n\n\n8\n\n\n3\n\n\n1\n\n\n1\n\n\n3\n\n\n3\n\n\n1\n\n\n1\n\n\n3\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n3\n\n\n3\n\n\n8\n\n\n3\n\n\n1\n\n\n1\n\n\n3\n\n\n3\n\n\n1\n\n\n1\n\n\n3\n\n\n1\n\n\n1\n\n\n1\n\n\n3\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n3\n\n\n3\n\n\n1\n\n\n1\n\n\n1\n\n\n3\n\n\n1\n\n\n2\n\n\n7\n\n\n7\n\n\n1\n\n\n1\n\n\n3\n\n\n1\n\n\n1\n\n\n3\n\n\n1\n\n\n1\n\n\n3\n\n\n1\n\n\n1\n\n\n1\n\n\n3\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n3\n\n\n8\n\n\n1\n\n\n3\n\n\n3\n\n\n3\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n3\n\n\n1\n\n\n1\n\n\n3\n\n\n1\n\n\n3\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n3\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n3\n\n\n3\n\n\n1\n\n\n1\n\n\n1\n\n\n3\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n3\n\n\n7\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n3\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n3\n\n\n1\n\n\n1\n\n\n1\n\n\n3\n\n\n1\n\n\n1\n\n\n1\n\n\n7\n\n\n1\n\n\n1\n\n\n3\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n8\n\n\n3\n\n\n3\n\n\n1\n\n\n4\n\n\n1\n\n\n1\n\n\n3\n\n\n1\n\n\n1\n\n\n1\n\n\n3\n\n\n3\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n3\n\n\n3\n\n\n1\n\n\n1\n\n\n1\n\n\n3\n\n\n3\n\n\n3\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n3\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n3\n\n\n3\n\n\n1\n\n\n3\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n3\n\n\n3\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n3\n\n\n1\n\n\n1\n\n\n1\n\n\n3\n\n\n1\n\n\n1\n\n\n1\n\n\n4\n\n\n1\n\n\n1\n\n\n3\n\n\n1\n\n\n3\n\n\n1\n\n\n3\n\n\n3\n\n\n3\n\n\n3\n\n\n3\n\n\n2\n\n\n1\n\n\n3\n\n\n1\n\n\n1\n\n\n3\n\n\n1\n\n\n3\n\n\n3\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n3\n\n\n1\n\n\n1\n\n\n8\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n3\n\n\n1\n\n\n3\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n3\n\n\n1\n\n\n1\n\n\n3\n\n\n3\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n8\n\n\n3\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n3\n\n\n3\n\n\n7\n\n\n1\n\n\n1\n\n\n1\n\n\n3\n\n\n8\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n3\n\n\n1\n\n\n3\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n3\n\n\n1\n\n\n3\n\n\n3\n\n\n2\n\n\n1\n\n\n3\n\n\n1\n\n\n3\n\n\n1\n\n\n1\n\n\n3\n\n\n1\n\n\n3\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n3\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n3\n\n\n2\n\n\n1\n\n\n8\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n3\n\n\n1\n\n\n3\n\n\n1\n\n\n1\n\n\n8\n\n\n3\n\n\n1\n\n\n1\n\n\n3\n\n\n3\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n3\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n3\n\n\n3\n\n\n3\n\n\n3\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n3\n\n\n3\n\n\n1\n\n\n1\n\n\n1\n\n\n3\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n3\n\n\n8\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n3\n\n\n1\n\n\n1\n\n\n1\n\n\n3\n\n\n3\n\n\n1\n\n\n3\n\n\n3\n\n\n1\n\n\n3\n\n\n1\n\n\n3\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n3\n\n\n1\n\n\n8\n\n\n1\n\n\n1\n\n\n1\n\n\n3\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n3\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n3\n\n\n3\n\n\n4\n\n\n8\n\n\n3\n\n\n3\n\n\n1\n\n\n2\n\n\n8\n\n\n1\n\n\n1\n\n\n1\n\n\n8\n\n\n1\n\n\n1\n\n\n3\n\n\n8\n\n\n1\n\n\n1\n\n\n1\n\n\n3\n\n\n1\n\n\n3\n\n\n3\n\n\n1\n\n\n8\n\n\n3\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n3\n\n\n1\n\n\n3\n\n\n3\n\n\n1\n\n\n3\n\n\n3\n\n\n1\n\n\n3\n\n\n8\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n3\n\n\n3\n\n\n1\n\n\n3\n\n\n1\n\n\n1\n\n\n8\n\n\n3\n\n\n1\n\n\n3\n\n\n1\n\n\n1\n\n\n1\n\n\n3\n\n\n1\n\n\n3\n\n\n2\n\n\n8\n\n\n1\n\n\n3\n\n\n3\n\n\n1\n\n\n8\n\n\n3\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n3\n\n\n1\n\n\n3\n\n\n3\n\n\n1\n\n\n3\n\n\n3\n\n\n1\n\n\n3\n\n\n8\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n3\n\n\n3\n\n\n1\n\n\n3\n\n\n1\n\n\n1\n\n\n8\n\n\n3\n\n\n1\n\n\n3\n\n\n1\n\n\n1\n\n\n1\n\n\n3\n\n\n1\n\n\n3\n\n\n2\n\n\n8\n\n\n1\n\n\n\n\n24\n\n\n0\n\n\nEDUCBAS\n\n\n \n\n\n4\n\n\n2\n\n\n7\n\n\n5\n\n\n7\n\n\n6\n\n\n5\n\n\n4\n\n\n7\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n4\n\n\n4\n\n\n6\n\n\n4\n\n\n3\n\n\n4\n\n\n4\n\n\n4\n\n\n4\n\n\n2\n\n\n7\n\n\n7\n\n\n6\n\n\n4\n\n\n7\n\n\n2\n\n\n2\n\n\n2\n\n\n4\n\n\n6\n\n\n4\n\n\n3\n\n\n4\n\n\n4\n\n\n4\n\n\n2\n\n\n7\n\n\n7\n\n\n6\n\n\n4\n\n\n7\n\n\n2\n\n\n2\n\n\n2\n\n\n4\n\n\n6\n\n\n4\n\n\n3\n\n\n4\n\n\n4\n\n\n6\n\n\n3\n\n\n4\n\n\n4\n\n\n5\n\n\n5\n\n\n4\n\n\n7\n\n\n5\n\n\n6\n\n\n4\n\n\n5\n\n\n4\n\n\n4\n\n\n7\n\n\n4\n\n\n6\n\n\n5\n\n\n5\n\n\n4\n\n\n6\n\n\n5\n\n\n4\n\n\n4\n\n\n6\n\n\n4\n\n\n3\n\n\n5\n\n\n3\n\n\n6\n\n\n3\n\n\n7\n\n\n4\n\n\n4\n\n\n4\n\n\n5\n\n\n5\n\n\n7\n\n\n3\n\n\n4\n\n\n4\n\n\n5\n\n\n4\n\n\n3\n\n\n5\n\n\n7\n\n\n3\n\n\n5\n\n\n3\n\n\n3\n\n\n4\n\n\n4\n\n\n7\n\n\n7\n\n\n4\n\n\n4\n\n\n7\n\n\n5\n\n\n4\n\n\n7\n\n\n5\n\n\n5\n\n\n4\n\n\n4\n\n\n2\n\n\n3\n\n\n5\n\n\n4\n\n\n6\n\n\n3\n\n\n6\n\n\n3\n\n\n4\n\n\n3\n\n\n4\n\n\n4\n\n\n5\n\n\n4\n\n\n4\n\n\n5\n\n\n4\n\n\n3\n\n\n3\n\n\n7\n\n\n4\n\n\n4\n\n\n5\n\n\n3\n\n\n3\n\n\n4\n\n\n7\n\n\n3\n\n\n4\n\n\n4\n\n\n4\n\n\n3\n\n\n5\n\n\n4\n\n\n5\n\n\n1\n\n\n5\n\n\n4\n\n\n3\n\n\n5\n\n\n5\n\n\n7\n\n\n4\n\n\n7\n\n\n7\n\n\n3\n\n\n4\n\n\n4\n\n\n3\n\n\n5\n\n\n5\n\n\n5\n\n\n5\n\n\n5\n\n\n5\n\n\n5\n\n\n5\n\n\n2\n\n\n5\n\n\n5\n\n\n3\n\n\n3\n\n\n6\n\n\n3\n\n\n3\n\n\n5\n\n\n4\n\n\n3\n\n\n7\n\n\n4\n\n\n4\n\n\n4\n\n\n4\n\n\n3\n\n\n3\n\n\n3\n\n\n2\n\n\n3\n\n\n4\n\n\n4\n\n\n4\n\n\n5\n\n\n6\n\n\n7\n\n\n4\n\n\n7\n\n\n5\n\n\n5\n\n\n7\n\n\n3\n\n\n4\n\n\n6\n\n\n4\n\n\n4\n\n\n5\n\n\n7\n\n\n4\n\n\n2\n\n\n5\n\n\n7\n\n\n4\n\n\n5\n\n\n4\n\n\n4\n\n\n6\n\n\n7\n\n\n7\n\n\n2\n\n\n3\n\n\n5\n\n\n4\n\n\n4\n\n\n5\n\n\n6\n\n\n4\n\n\n6\n\n\n6\n\n\n4\n\n\n4\n\n\n4\n\n\n5\n\n\n4\n\n\n4\n\n\n4\n\n\n4\n\n\n4\n\n\n5\n\n\n5\n\n\n3\n\n\n5\n\n\n5\n\n\n3\n\n\n4\n\n\n3\n\n\n5\n\n\n5\n\n\n6\n\n\n5\n\n\n5\n\n\n7\n\n\n2\n\n\n4\n\n\n5\n\n\n5\n\n\n5\n\n\n2\n\n\n5\n\n\n7\n\n\n4\n\n\n7\n\n\n2\n\n\n4\n\n\n5\n\n\n7\n\n\n7\n\n\n6\n\n\n4\n\n\n7\n\n\n3\n\n\n3\n\n\n6\n\n\n7\n\n\n7\n\n\n6\n\n\n1\n\n\n4\n\n\n5\n\n\n6\n\n\n6\n\n\n4\n\n\n5\n\n\n4\n\n\n3\n\n\n5\n\n\n5\n\n\n4\n\n\n4\n\n\n3\n\n\n7\n\n\n3\n\n\n5\n\n\n7\n\n\n5\n\n\n5\n\n\n6\n\n\n4\n\n\n5\n\n\n5\n\n\n7\n\n\n4\n\n\n2\n\n\n4\n\n\n4\n\n\n5\n\n\n4\n\n\n5\n\n\n4\n\n\n3\n\n\n2\n\n\n5\n\n\n2\n\n\n2\n\n\n5\n\n\n4\n\n\n7\n\n\n4\n\n\n3\n\n\n7\n\n\n4\n\n\n2\n\n\n4\n\n\n4\n\n\n5\n\n\n7\n\n\n4\n\n\n6\n\n\n4\n\n\n4\n\n\n7\n\n\n2\n\n\n7\n\n\n4\n\n\n5\n\n\n4\n\n\n4\n\n\n5\n\n\n4\n\n\n7\n\n\n5\n\n\n4\n\n\n2\n\n\n5\n\n\n4\n\n\n2\n\n\n4\n\n\n6\n\n\n7\n\n\n4\n\n\n4\n\n\n7\n\n\n3\n\n\n4\n\n\n4\n\n\n5\n\n\n4\n\n\n4\n\n\n5\n\n\n7\n\n\n5\n\n\n6\n\n\n6\n\n\n5\n\n\n4\n\n\n3\n\n\n3\n\n\n4\n\n\n3\n\n\n4\n\n\n5\n\n\n5\n\n\n4\n\n\n5\n\n\n5\n\n\n3\n\n\n5\n\n\n7\n\n\n4\n\n\n3\n\n\n7\n\n\n5\n\n\n7\n\n\n6\n\n\n7\n\n\n5\n\n\n5\n\n\n7\n\n\n3\n\n\n4\n\n\n5\n\n\n6\n\n\n4\n\n\n4\n\n\n4\n\n\n2\n\n\n4\n\n\n5\n\n\n7\n\n\n3\n\n\n3\n\n\n5\n\n\n4\n\n\n4\n\n\n5\n\n\n2\n\n\n5\n\n\n4\n\n\n6\n\n\n4\n\n\n4\n\n\n7\n\n\n4\n\n\n7\n\n\n5\n\n\n4\n\n\n7\n\n\n5\n\n\n4\n\n\n4\n\n\n4\n\n\n4\n\n\n4\n\n\n4\n\n\n5\n\n\n4\n\n\n7\n\n\n6\n\n\n6\n\n\n4\n\n\n4\n\n\n4\n\n\n4\n\n\n4\n\n\n5\n\n\n6\n\n\n6\n\n\n2\n\n\n4\n\n\n5\n\n\n4\n\n\n7\n\n\n4\n\n\n7\n\n\n4\n\n\n2\n\n\n7\n\n\n4\n\n\n4\n\n\n3\n\n\n4\n\n\n3\n\n\n2\n\n\n7\n\n\n3\n\n\n3\n\n\n5\n\n\n4\n\n\n5\n\n\n3\n\n\n4\n\n\n4\n\n\n3\n\n\n4\n\n\n7\n\n\n4\n\n\n4\n\n\n3\n\n\n4\n\n\n2\n\n\n4\n\n\n3\n\n\n2\n\n\n7\n\n\n2\n\n\n4\n\n\n7\n\n\n3\n\n\n1\n\n\n6\n\n\n4\n\n\n7\n\n\n5\n\n\n2\n\n\n5\n\n\n4\n\n\n2\n\n\n4\n\n\n7\n\n\n2\n\n\n4\n\n\n5\n\n\n3\n\n\n7\n\n\n6\n\n\n4\n\n\n3\n\n\n7\n\n\n4\n\n\n4\n\n\n3\n\n\n4\n\n\n3\n\n\n4\n\n\n4\n\n\n3\n\n\n4\n\n\n7\n\n\n4\n\n\n4\n\n\n3\n\n\n4\n\n\n2\n\n\n4\n\n\n3\n\n\n2\n\n\n7\n\n\n2\n\n\n4\n\n\n7\n\n\n3\n\n\n1\n\n\n6\n\n\n4\n\n\n7\n\n\n5\n\n\n2\n\n\n5\n\n\n4\n\n\n2\n\n\n4\n\n\n7\n\n\n2\n\n\n4\n\n\n5\n\n\n3\n\n\n7\n\n\n6\n\n\n4\n\n\n3\n\n\n7\n\n\n4\n\n\n4\n\n\n3\n\n\n4\n\n\n\n\n25\n\n\n0\n\n\nhivpos\n\n\n \n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n\n\n26\n\n\n0\n\n\nage\n\n\n \n\n\n52\n\n\n54\n\n\n47\n\n\n44\n\n\n53\n\n\n36\n\n\n47\n\n\n30\n\n\n47\n\n\n40\n\n\n32\n\n\n53\n\n\n29\n\n\n40\n\n\n50\n\n\n51\n\n\n38\n\n\n61\n\n\n36\n\n\n51\n\n\n39\n\n\n53\n\n\n51\n\n\n48\n\n\n54\n\n\n36\n\n\n29\n\n\n47\n\n\n39\n\n\n52\n\n\n31\n\n\n41\n\n\n48\n\n\n37\n\n\n60\n\n\n49\n\n\n38\n\n\n49\n\n\n55\n\n\n48\n\n\n55\n\n\n37\n\n\n33\n\n\n47\n\n\n37\n\n\n51\n\n\n31\n\n\n39\n\n\n49\n\n\n36\n\n\n61\n\n\n49\n\n\n39\n\n\n52\n\n\n57\n\n\n38\n\n\n45\n\n\n42\n\n\n40\n\n\n68\n\n\n48\n\n\n47\n\n\n34\n\n\n38\n\n\n43\n\n\n38\n\n\n49\n\n\n52\n\n\n26\n\n\n36\n\n\n45\n\n\n41\n\n\n33\n\n\n46\n\n\n27\n\n\n61\n\n\n52\n\n\n44\n\n\n45\n\n\n48\n\n\n55\n\n\n33\n\n\n43\n\n\n57\n\n\n48\n\n\n41\n\n\n45\n\n\n24\n\n\n42\n\n\n39\n\n\n63\n\n\n41\n\n\n49\n\n\n55\n\n\n37\n\n\n36\n\n\n43\n\n\n43\n\n\n38\n\n\n26\n\n\n42\n\n\n24\n\n\n40\n\n\n35\n\n\n48\n\n\n48\n\n\n62\n\n\n50\n\n\n48\n\n\n44\n\n\n45\n\n\n38\n\n\n50\n\n\n36\n\n\n37\n\n\n62\n\n\n45\n\n\n32\n\n\n42\n\n\n35\n\n\n58\n\n\n48\n\n\n26\n\n\n65\n\n\n47\n\n\n47\n\n\n44\n\n\n39\n\n\n52\n\n\n46\n\n\n43\n\n\n37\n\n\n55\n\n\n57\n\n\n50\n\n\n51\n\n\n50\n\n\n49\n\n\n47\n\n\n42\n\n\n35\n\n\n33\n\n\n42\n\n\n46\n\n\n51\n\n\n47\n\n\n53\n\n\n42\n\n\n20\n\n\n44\n\n\n29\n\n\n34\n\n\n30\n\n\n39\n\n\n34\n\n\n47\n\n\n51\n\n\n54\n\n\n42\n\n\n58\n\n\n46\n\n\n39\n\n\n25\n\n\n33\n\n\n50\n\n\n60\n\n\n44\n\n\n38\n\n\n57\n\n\n26\n\n\n47\n\n\n42\n\n\n49\n\n\n37\n\n\n55\n\n\n44\n\n\n46\n\n\n51\n\n\n37\n\n\n57\n\n\n50\n\n\n57\n\n\n24\n\n\n48\n\n\n32\n\n\n73\n\n\n43\n\n\n30\n\n\n34\n\n\n47\n\n\n42\n\n\n36\n\n\n43\n\n\n54\n\n\n41\n\n\n45\n\n\n50\n\n\n44\n\n\n57\n\n\n32\n\n\n46\n\n\n39\n\n\n42\n\n\n45\n\n\n43\n\n\n52\n\n\n41\n\n\n51\n\n\n39\n\n\n36\n\n\n46\n\n\n48\n\n\n47\n\n\n46\n\n\n39\n\n\n37\n\n\n58\n\n\n44\n\n\n44\n\n\n49\n\n\n41\n\n\n41\n\n\n42\n\n\n46\n\n\n42\n\n\n39\n\n\n44\n\n\n53\n\n\n33\n\n\n46\n\n\n31\n\n\n32\n\n\n57\n\n\n38\n\n\n46\n\n\n44\n\n\n44\n\n\n40\n\n\n46\n\n\n38\n\n\n41\n\n\n58\n\n\n25\n\n\n47\n\n\n43\n\n\n38\n\n\n49\n\n\n29\n\n\n45\n\n\n50\n\n\n37\n\n\n26\n\n\n37\n\n\n51\n\n\n53\n\n\n40\n\n\n55\n\n\n32\n\n\n51\n\n\n36\n\n\n39\n\n\n41\n\n\n29\n\n\n47\n\n\n34\n\n\n38\n\n\n59\n\n\n33\n\n\n41\n\n\n39\n\n\n40\n\n\n53\n\n\n35\n\n\n44\n\n\n60\n\n\n37\n\n\n27\n\n\n42\n\n\n55\n\n\n44\n\n\n49\n\n\n45\n\n\n53\n\n\n26\n\n\n36\n\n\n47\n\n\n45\n\n\n46\n\n\n45\n\n\n56\n\n\n47\n\n\n39\n\n\n47\n\n\n30\n\n\n36\n\n\n32\n\n\n48\n\n\n54\n\n\n47\n\n\n48\n\n\n47\n\n\n41\n\n\n44\n\n\n40\n\n\n27\n\n\n31\n\n\n48\n\n\n40\n\n\n50\n\n\n64\n\n\n33\n\n\n41\n\n\n42\n\n\n43\n\n\n50\n\n\n40\n\n\n40\n\n\n36\n\n\n32\n\n\n37\n\n\n36\n\n\n48\n\n\n25\n\n\n37\n\n\n56\n\n\n58\n\n\n56\n\n\n41\n\n\n37\n\n\n49\n\n\n42\n\n\n38\n\n\n53\n\n\n48\n\n\n50\n\n\n51\n\n\n53\n\n\n37\n\n\n34\n\n\n32\n\n\n36\n\n\n44\n\n\n36\n\n\n47\n\n\n31\n\n\n38\n\n\n57\n\n\n43\n\n\n40\n\n\n50\n\n\n29\n\n\n40\n\n\n45\n\n\n52\n\n\n57\n\n\n50\n\n\n42\n\n\n51\n\n\n58\n\n\n43\n\n\n54\n\n\n38\n\n\n36\n\n\n34\n\n\n62\n\n\n37\n\n\n30\n\n\n50\n\n\n49\n\n\n57\n\n\n41\n\n\n40\n\n\n52\n\n\n47\n\n\n34\n\n\n34\n\n\n46\n\n\n46\n\n\n34\n\n\n27\n\n\n61\n\n\n36\n\n\n42\n\n\n48\n\n\n38\n\n\n61\n\n\n36\n\n\n46\n\n\n43\n\n\n50\n\n\n60\n\n\n28\n\n\n49\n\n\n43\n\n\n28\n\n\n44\n\n\n51\n\n\n49\n\n\n62\n\n\n36\n\n\n23\n\n\n38\n\n\n48\n\n\n44\n\n\n43\n\n\n61\n\n\n44\n\n\n27\n\n\n52\n\n\n55\n\n\n37\n\n\n39\n\n\n36\n\n\n51\n\n\n37\n\n\n44\n\n\n52\n\n\n41\n\n\n43\n\n\n45\n\n\n41\n\n\n45\n\n\n43\n\n\n41\n\n\n51\n\n\n38\n\n\n43\n\n\n53\n\n\n46\n\n\n34\n\n\n44\n\n\n38\n\n\n41\n\n\n36\n\n\n36\n\n\n44\n\n\n45\n\n\n52\n\n\n40\n\n\n45\n\n\n48\n\n\n56\n\n\n26\n\n\n30\n\n\n39\n\n\n38\n\n\n43\n\n\n23\n\n\n45\n\n\n31\n\n\n36\n\n\n47\n\n\n41\n\n\n41\n\n\n46\n\n\n39\n\n\n48\n\n\n38\n\n\n36\n\n\n37\n\n\n31\n\n\n47\n\n\n34\n\n\n47\n\n\n58\n\n\n40\n\n\n36\n\n\n38\n\n\n58\n\n\n39\n\n\n47\n\n\n53\n\n\n48\n\n\n57\n\n\n53\n\n\n50\n\n\n33\n\n\n51\n\n\n48\n\n\n39\n\n\n41\n\n\n54\n\n\n35\n\n\n47\n\n\n36\n\n\n29\n\n\n47\n\n\n27\n\n\n39\n\n\n42\n\n\n36\n\n\n52\n\n\n46\n\n\n32\n\n\n31\n\n\n41\n\n\n36\n\n\n59\n\n\n42\n\n\n48\n\n\n37\n\n\n60\n\n\n45\n\n\n49\n\n\n31\n\n\n49\n\n\n38\n\n\n60\n\n\n38\n\n\n38\n\n\n38\n\n\n57\n\n\n39\n\n\n47\n\n\n49\n\n\n49\n\n\n58\n\n\n54\n\n\n49\n\n\n36\n\n\n55\n\n\n48\n\n\n39\n\n\n41\n\n\n55\n\n\n37\n\n\n46\n\n\n37\n\n\n33\n\n\n47\n\n\n28\n\n\n37\n\n\n41\n\n\n34\n\n\n51\n\n\n43\n\n\n31\n\n\n31\n\n\n39\n\n\n32\n\n\n61\n\n\n43\n\n\n49\n\n\n36\n\n\n61\n\n\n44\n\n\n49\n\n\n31\n\n\n49\n\n\n39\n\n\n\n\n27\n\n\n0\n\n\nART\n\n\n \n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n\n\n28\n\n\n0\n\n\neverART\n\n\n \n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n\n\n29\n\n\n0\n\n\nyears\n\n\n \n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n\n\n30\n\n\n0\n\n\nhard_drugs\n\n\n \n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n0\n\n\n0\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n0\n\n\n0\n\n\n1\n\n\n1\n\n\n1\n\n\n0\n\n\n0\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n0\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n0\n\n\n1\n\n\n1\n\n\n0\n\n\n0\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n0\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n0\n\n\n1\n\n\n1\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n1\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n1\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n1\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n1\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n1\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n1\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n1\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n1\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n1\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n1\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n1\n\n\n1\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n1\n\n\n0\n\n\n0\n\n\n0\n\n\n1\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n1\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n1\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n1\n\n\n1\n\n\n0\n\n\n0\n\n\n1\n\n\n0\n\n\n0\n\n\n0\n\n\n1\n\n\n1\n\n\n0\n\n\n1\n\n\n0\n\n\n0\n\n\n1\n\n\n0\n\n\n0\n\n\n1\n\n\n1\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n1\n\n\n1\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n1\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n1\n\n\n1\n\n\n0\n\n\n0\n\n\n1\n\n\n0\n\n\n0\n\n\n0\n\n\n1\n\n\n1\n\n\n0\n\n\n1\n\n\n0\n\n\n0\n\n\n1\n\n\n0\n\n\n0\n\n\n1\n\n\n1\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n1\n\n\n1\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n\n\n31\n\n\n0\n\n\nincome\n\n\nIncome\n\n\n4\n\n\n2\n\n\n6\n\n\n1\n\n\n6\n\n\n4\n\n\n2\n\n\n4\n\n\n6\n\n\n1\n\n\n4\n\n\n1\n\n\n1\n\n\n1\n\n\n3\n\n\n5\n\n\n2\n\n\n3\n\n\n1\n\n\n2\n\n\n1\n\n\n4\n\n\n.\n\n\n6\n\n\n.\n\n\n4\n\n\n4\n\n\n6\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n.\n\n\n2\n\n\n.\n\n\n2\n\n\n1\n\n\n4\n\n\n2\n\n\n6\n\n\n6\n\n\n4\n\n\n4\n\n\n.\n\n\n.\n\n\n.\n\n\n1\n\n\n1\n\n\n5\n\n\n2\n\n\n.\n\n\n2\n\n\n1\n\n\n.\n\n\n3\n\n\n.\n\n\n.\n\n\n.\n\n\n4\n\n\n5\n\n\n5\n\n\n7\n\n\n3\n\n\n3\n\n\n4\n\n\n3\n\n\n2\n\n\n.\n\n\n.\n\n\n6\n\n\n5\n\n\n6\n\n\n3\n\n\n.\n\n\n7\n\n\n1\n\n\n7\n\n\n.\n\n\n2\n\n\n1\n\n\n7\n\n\n2\n\n\n5\n\n\n3\n\n\n5\n\n\n3\n\n\n1\n\n\n2\n\n\n6\n\n\n2\n\n\n4\n\n\n1\n\n\n5\n\n\n1\n\n\n4\n\n\n1\n\n\n4\n\n\n5\n\n\n4\n\n\n.\n\n\n6\n\n\n1\n\n\n.\n\n\n6\n\n\n.\n\n\n5\n\n\n.\n\n\n3\n\n\n6\n\n\n6\n\n\n5\n\n\n5\n\n\n6\n\n\n.\n\n\n6\n\n\n3\n\n\n2\n\n\n4\n\n\n3\n\n\n.\n\n\n3\n\n\n3\n\n\n.\n\n\n.\n\n\n7\n\n\n7\n\n\n3\n\n\n1\n\n\n4\n\n\n6\n\n\n6\n\n\n.\n\n\n7\n\n\n4\n\n\n3\n\n\n9\n\n\n3\n\n\n.\n\n\n2\n\n\n7\n\n\n2\n\n\n1\n\n\n6\n\n\n.\n\n\n2\n\n\n.\n\n\n2\n\n\n3\n\n\n.\n\n\n5\n\n\n3\n\n\n4\n\n\n1\n\n\n6\n\n\n2\n\n\n.\n\n\n9\n\n\n1\n\n\n7\n\n\n1\n\n\n7\n\n\n6\n\n\n1\n\n\n.\n\n\n.\n\n\n2\n\n\n.\n\n\n.\n\n\n7\n\n\n3\n\n\n6\n\n\n6\n\n\n2\n\n\n.\n\n\n1\n\n\n3\n\n\n6\n\n\n1\n\n\n9\n\n\n3\n\n\n5\n\n\n4\n\n\n3\n\n\n5\n\n\n1\n\n\n7\n\n\n.\n\n\n4\n\n\n4\n\n\n.\n\n\n.\n\n\n1\n\n\n1\n\n\n2\n\n\n.\n\n\n3\n\n\n4\n\n\n3\n\n\n6\n\n\n5\n\n\n6\n\n\n5\n\n\n.\n\n\n.\n\n\n5\n\n\n1\n\n\n1\n\n\n2\n\n\n4\n\n\n3\n\n\n6\n\n\n6\n\n\n.\n\n\n.\n\n\n.\n\n\n3\n\n\n3\n\n\n.\n\n\n4\n\n\n6\n\n\n1\n\n\n6\n\n\n6\n\n\n6\n\n\n1\n\n\n1\n\n\n1\n\n\n.\n\n\n.\n\n\n.\n\n\n9\n\n\n.\n\n\n7\n\n\n4\n\n\n1\n\n\n1\n\n\n1\n\n\n.\n\n\n3\n\n\n1\n\n\n1\n\n\n5\n\n\n.\n\n\n9\n\n\n5\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n.\n\n\n.\n\n\n.\n\n\n2\n\n\n5\n\n\n7\n\n\n6\n\n\n.\n\n\n9\n\n\n4\n\n\n4\n\n\n2\n\n\n7\n\n\n.\n\n\n7\n\n\n4\n\n\n1\n\n\n.\n\n\n3\n\n\n.\n\n\n9\n\n\n.\n\n\n.\n\n\n6\n\n\n5\n\n\n6\n\n\n1\n\n\n3\n\n\n.\n\n\n.\n\n\n6\n\n\n5\n\n\n1\n\n\n5\n\n\n.\n\n\n.\n\n\n6\n\n\n3\n\n\n5\n\n\n3\n\n\n2\n\n\n2\n\n\n4\n\n\n.\n\n\n4\n\n\n.\n\n\n9\n\n\n4\n\n\n3\n\n\n6\n\n\n7\n\n\n6\n\n\n3\n\n\n3\n\n\n6\n\n\n7\n\n\n4\n\n\n6\n\n\n1\n\n\n2\n\n\n.\n\n\n1\n\n\n.\n\n\n4\n\n\n.\n\n\n.\n\n\n.\n\n\n2\n\n\n.\n\n\n4\n\n\n5\n\n\n1\n\n\n6\n\n\n3\n\n\n4\n\n\n6\n\n\n.\n\n\n.\n\n\n7\n\n\n3\n\n\n.\n\n\n7\n\n\n1\n\n\n6\n\n\n3\n\n\n7\n\n\n5\n\n\n1\n\n\n6\n\n\n3\n\n\n.\n\n\n4\n\n\n6\n\n\n4\n\n\n4\n\n\n.\n\n\n6\n\n\n3\n\n\n.\n\n\n7\n\n\n.\n\n\n1\n\n\n1\n\n\n6\n\n\n7\n\n\n.\n\n\n.\n\n\n2\n\n\n.\n\n\n9\n\n\n.\n\n\n1\n\n\n.\n\n\n2\n\n\n.\n\n\n2\n\n\n3\n\n\n4\n\n\n.\n\n\n7\n\n\n3\n\n\n1\n\n\n3\n\n\n.\n\n\n6\n\n\n3\n\n\n.\n\n\n7\n\n\n.\n\n\n2\n\n\n1\n\n\n3\n\n\n6\n\n\n.\n\n\n.\n\n\n2\n\n\n6\n\n\n5\n\n\n.\n\n\n6\n\n\n1\n\n\n1\n\n\n.\n\n\n6\n\n\n1\n\n\n2\n\n\n7\n\n\n5\n\n\n1\n\n\n4\n\n\n4\n\n\n1\n\n\n2\n\n\n4\n\n\n5\n\n\n1\n\n\n3\n\n\n5\n\n\n3\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n2\n\n\n.\n\n\n6\n\n\n5\n\n\n6\n\n\n6\n\n\n6\n\n\n6\n\n\n2\n\n\n5\n\n\n.\n\n\n.\n\n\n5\n\n\n.\n\n\n6\n\n\n.\n\n\n6\n\n\n4\n\n\n6\n\n\n.\n\n\n5\n\n\n2\n\n\n9\n\n\n4\n\n\n5\n\n\n3\n\n\n6\n\n\n4\n\n\n5\n\n\n1\n\n\n.\n\n\n3\n\n\n1\n\n\n2\n\n\n3\n\n\n6\n\n\n4\n\n\n1\n\n\n6\n\n\n.\n\n\n2\n\n\n.\n\n\n1\n\n\n.\n\n\n1\n\n\n.\n\n\n3\n\n\n5\n\n\n2\n\n\n1\n\n\n7\n\n\n3\n\n\n3\n\n\n.\n\n\n1\n\n\n1\n\n\n4\n\n\n5\n\n\n4\n\n\n.\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n6\n\n\n1\n\n\n.\n\n\n6\n\n\n.\n\n\n1\n\n\n4\n\n\n4\n\n\n6\n\n\n7\n\n\n.\n\n\n4\n\n\n3\n\n\n1\n\n\n6\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n3\n\n\n6\n\n\n5\n\n\n.\n\n\n3\n\n\n2\n\n\n2\n\n\n4\n\n\n3\n\n\n1\n\n\n3\n\n\n3\n\n\n3\n\n\n1\n\n\n1\n\n\n4\n\n\n5\n\n\n4\n\n\n.\n\n\n1\n\n\n.\n\n\n1\n\n\n1\n\n\n2\n\n\n6\n\n\n.\n\n\n.\n\n\n6\n\n\n1\n\n\n1\n\n\n4\n\n\n4\n\n\n.\n\n\n.\n\n\n1\n\n\n4\n\n\n.\n\n\n.\n\n\n6\n\n\n7\n\n\n.\n\n\n1\n\n\n.\n\n\n3\n\n\n.\n\n\n.\n\n\n2\n\n\n3\n\n\n2\n\n\n2\n\n\n4\n\n\n.\n\n\n1\n\n\n\n\n32\n\n\n0\n\n\nHASHF\n\n\nHash/Marijuana Use Since Last Visit\n\n\n.\n\n\n0\n\n\n4\n\n\n4\n\n\n3\n\n\n.\n\n\n1\n\n\n2\n\n\n0\n\n\n2\n\n\n0\n\n\n0\n\n\n1\n\n\n3\n\n\n0\n\n\n0\n\n\n3\n\n\n4\n\n\n.\n\n\n0\n\n\n.\n\n\n.\n\n\n0\n\n\n4\n\n\n3\n\n\n.\n\n\n2\n\n\n0\n\n\n2\n\n\n0\n\n\n1\n\n\n3\n\n\n0\n\n\n3\n\n\n4\n\n\n0\n\n\n.\n\n\n.\n\n\n0\n\n\n4\n\n\n3\n\n\n.\n\n\n2\n\n\n0\n\n\n2\n\n\n0\n\n\n1\n\n\n3\n\n\n0\n\n\n3\n\n\n4\n\n\n0\n\n\n.\n\n\n2\n\n\n0\n\n\n4\n\n\n1\n\n\n0\n\n\n0\n\n\n0\n\n\n4\n\n\n0\n\n\n4\n\n\n4\n\n\n4\n\n\n3\n\n\n0\n\n\n0\n\n\n2\n\n\n0\n\n\n0\n\n\n4\n\n\n1\n\n\n0\n\n\n4\n\n\n1\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n4\n\n\n4\n\n\n0\n\n\n2\n\n\n4\n\n\n1\n\n\n0\n\n\n.\n\n\n0\n\n\n2\n\n\n0\n\n\n0\n\n\n2\n\n\n0\n\n\n2\n\n\n0\n\n\n1\n\n\n0\n\n\n2\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n2\n\n\n0\n\n\n2\n\n\n0\n\n\n4\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n2\n\n\n4\n\n\n0\n\n\n1\n\n\n0\n\n\n.\n\n\n1\n\n\n0\n\n\n4\n\n\n0\n\n\n0\n\n\n0\n\n\n.\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n4\n\n\n0\n\n\n0\n\n\n4\n\n\n1\n\n\n0\n\n\n4\n\n\n0\n\n\n0\n\n\n2\n\n\n4\n\n\n.\n\n\n0\n\n\n2\n\n\n0\n\n\n0\n\n\n4\n\n\n0\n\n\n4\n\n\n3\n\n\n1\n\n\n0\n\n\n4\n\n\n0\n\n\n0\n\n\n4\n\n\n1\n\n\n3\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n1\n\n\n4\n\n\n4\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n4\n\n\n.\n\n\n1\n\n\n0\n\n\n.\n\n\n0\n\n\n1\n\n\n2\n\n\n1\n\n\n3\n\n\n1\n\n\n2\n\n\n0\n\n\n2\n\n\n0\n\n\n0\n\n\n3\n\n\n0\n\n\n1\n\n\n0\n\n\n0\n\n\n3\n\n\n0\n\n\n4\n\n\n.\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n2\n\n\n0\n\n\n0\n\n\n3\n\n\n0\n\n\n4\n\n\n0\n\n\n0\n\n\n1\n\n\n0\n\n\n0\n\n\n3\n\n\n0\n\n\n.\n\n\n0\n\n\n2\n\n\n0\n\n\n0\n\n\n0\n\n\n4\n\n\n0\n\n\n0\n\n\n0\n\n\n1\n\n\n0\n\n\n1\n\n\n0\n\n\n4\n\n\n0\n\n\n0\n\n\n3\n\n\n0\n\n\n0\n\n\n3\n\n\n2\n\n\n3\n\n\n0\n\n\n2\n\n\n0\n\n\n0\n\n\n.\n\n\n0\n\n\n0\n\n\n0\n\n\n2\n\n\n1\n\n\n0\n\n\n2\n\n\n0\n\n\n0\n\n\n0\n\n\n2\n\n\n0\n\n\n2\n\n\n2\n\n\n1\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n.\n\n\n1\n\n\n0\n\n\n0\n\n\n3\n\n\n0\n\n\n0\n\n\n0\n\n\n3\n\n\n1\n\n\n1\n\n\n0\n\n\n0\n\n\n4\n\n\n0\n\n\n0\n\n\n3\n\n\n.\n\n\n0\n\n\n3\n\n\n.\n\n\n0\n\n\n2\n\n\n1\n\n\n0\n\n\n0\n\n\n2\n\n\n.\n\n\n1\n\n\n.\n\n\n1\n\n\n0\n\n\n.\n\n\n1\n\n\n0\n\n\n1\n\n\n0\n\n\n0\n\n\n2\n\n\n0\n\n\n2\n\n\n3\n\n\n0\n\n\n2\n\n\n0\n\n\n4\n\n\n0\n\n\n4\n\n\n0\n\n\n.\n\n\n4\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n2\n\n\n0\n\n\n0\n\n\n.\n\n\n2\n\n\n2\n\n\n.\n\n\n0\n\n\n0\n\n\n4\n\n\n2\n\n\n0\n\n\n0\n\n\n4\n\n\n.\n\n\n0\n\n\n4\n\n\n0\n\n\n1\n\n\n0\n\n\n0\n\n\n1\n\n\n1\n\n\n3\n\n\n2\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n3\n\n\n0\n\n\n0\n\n\n2\n\n\n4\n\n\n0\n\n\n2\n\n\n0\n\n\n.\n\n\n3\n\n\n0\n\n\n4\n\n\n3\n\n\n0\n\n\n2\n\n\n.\n\n\n4\n\n\n0\n\n\n2\n\n\n0\n\n\n0\n\n\n1\n\n\n0\n\n\n0\n\n\n3\n\n\n0\n\n\n0\n\n\n0\n\n\n3\n\n\n0\n\n\n4\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n.\n\n\n2\n\n\n4\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n1\n\n\n3\n\n\n0\n\n\n4\n\n\n4\n\n\n4\n\n\n0\n\n\n1\n\n\n0\n\n\n4\n\n\n0\n\n\n0\n\n\n.\n\n\n0\n\n\n0\n\n\n0\n\n\n2\n\n\n2\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n1\n\n\n0\n\n\n0\n\n\n0\n\n\n2\n\n\n0\n\n\n4\n\n\n0\n\n\n2\n\n\n2\n\n\n0\n\n\n4\n\n\n0\n\n\n0\n\n\n0\n\n\n3\n\n\n1\n\n\n0\n\n\n0\n\n\n4\n\n\n2\n\n\n0\n\n\n3\n\n\n0\n\n\n0\n\n\n2\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n2\n\n\n0\n\n\n.\n\n\n0\n\n\n4\n\n\n.\n\n\n0\n\n\n2\n\n\n2\n\n\n.\n\n\n0\n\n\n0\n\n\n4\n\n\n3\n\n\n0\n\n\n0\n\n\n2\n\n\n0\n\n\n.\n\n\n4\n\n\n4\n\n\n0\n\n\n2\n\n\n2\n\n\n0\n\n\n4\n\n\n0\n\n\n0\n\n\n3\n\n\n3\n\n\n0\n\n\n.\n\n\n2\n\n\n0\n\n\n0\n\n\n2\n\n\n4\n\n\n2\n\n\n0\n\n\n4\n\n\n4\n\n\n1\n\n\n3\n\n\n0\n\n\n0\n\n\n3\n\n\n0\n\n\n3\n\n\n4\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n.\n\n\n0\n\n\n4\n\n\n3\n\n\n0\n\n\n0\n\n\n2\n\n\n0\n\n\n.\n\n\n4\n\n\n4\n\n\n0\n\n\n2\n\n\n2\n\n\n0\n\n\n4\n\n\n0\n\n\n0\n\n\n3\n\n\n3\n\n\n0\n\n\n.\n\n\n2\n\n\n0\n\n\n0\n\n\n2\n\n\n4\n\n\n2\n\n\n0\n\n\n4\n\n\n4\n\n\n1\n\n\n3\n\n\n0\n\n\n0\n\n\n3\n\n\n0\n\n\n3\n\n\n4\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n.\n\n\n\n\n33\n\n\n0\n\n\nADH\n\n\n \n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n1\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n\n\n34\n\n\n1\n\n\nnewid\n\n\nID\n\n\n1\n\n\n2\n\n\n3\n\n\n4\n\n\n5\n\n\n6\n\n\n7\n\n\n8\n\n\n9\n\n\n10\n\n\n11\n\n\n12\n\n\n13\n\n\n14\n\n\n15\n\n\n16\n\n\n17\n\n\n18\n\n\n19\n\n\n20\n\n\n21\n\n\n22\n\n\n23\n\n\n24\n\n\n25\n\n\n26\n\n\n27\n\n\n28\n\n\n29\n\n\n30\n\n\n31\n\n\n32\n\n\n33\n\n\n34\n\n\n35\n\n\n36\n\n\n37\n\n\n38\n\n\n39\n\n\n40\n\n\n41\n\n\n42\n\n\n43\n\n\n44\n\n\n45\n\n\n46\n\n\n47\n\n\n48\n\n\n49\n\n\n50\n\n\n51\n\n\n52\n\n\n53\n\n\n54\n\n\n55\n\n\n56\n\n\n57\n\n\n58\n\n\n59\n\n\n60\n\n\n61\n\n\n62\n\n\n63\n\n\n64\n\n\n65\n\n\n66\n\n\n67\n\n\n68\n\n\n69\n\n\n70\n\n\n71\n\n\n72\n\n\n73\n\n\n74\n\n\n75\n\n\n76\n\n\n77\n\n\n78\n\n\n79\n\n\n80\n\n\n81\n\n\n82\n\n\n83\n\n\n84\n\n\n85\n\n\n86\n\n\n87\n\n\n88\n\n\n89\n\n\n90\n\n\n91\n\n\n92\n\n\n93\n\n\n94\n\n\n95\n\n\n96\n\n\n97\n\n\n98\n\n\n99\n\n\n100\n\n\n101\n\n\n102\n\n\n103\n\n\n104\n\n\n105\n\n\n106\n\n\n107\n\n\n108\n\n\n109\n\n\n110\n\n\n111\n\n\n112\n\n\n113\n\n\n114\n\n\n115\n\n\n116\n\n\n117\n\n\n118\n\n\n119\n\n\n120\n\n\n121\n\n\n122\n\n\n123\n\n\n124\n\n\n125\n\n\n126\n\n\n127\n\n\n128\n\n\n129\n\n\n130\n\n\n131\n\n\n132\n\n\n133\n\n\n134\n\n\n135\n\n\n136\n\n\n137\n\n\n138\n\n\n139\n\n\n140\n\n\n141\n\n\n142\n\n\n143\n\n\n144\n\n\n145\n\n\n146\n\n\n147\n\n\n148\n\n\n149\n\n\n150\n\n\n151\n\n\n152\n\n\n153\n\n\n154\n\n\n155\n\n\n156\n\n\n157\n\n\n158\n\n\n159\n\n\n160\n\n\n161\n\n\n162\n\n\n163\n\n\n164\n\n\n165\n\n\n166\n\n\n167\n\n\n168\n\n\n169\n\n\n170\n\n\n171\n\n\n172\n\n\n173\n\n\n174\n\n\n175\n\n\n176\n\n\n177\n\n\n178\n\n\n179\n\n\n180\n\n\n181\n\n\n182\n\n\n183\n\n\n184\n\n\n185\n\n\n186\n\n\n187\n\n\n188\n\n\n189\n\n\n190\n\n\n191\n\n\n192\n\n\n193\n\n\n194\n\n\n195\n\n\n196\n\n\n197\n\n\n198\n\n\n199\n\n\n200\n\n\n201\n\n\n202\n\n\n203\n\n\n204\n\n\n205\n\n\n206\n\n\n207\n\n\n208\n\n\n209\n\n\n210\n\n\n211\n\n\n212\n\n\n213\n\n\n214\n\n\n215\n\n\n216\n\n\n217\n\n\n218\n\n\n219\n\n\n220\n\n\n221\n\n\n222\n\n\n223\n\n\n224\n\n\n225\n\n\n226\n\n\n227\n\n\n228\n\n\n229\n\n\n230\n\n\n231\n\n\n232\n\n\n233\n\n\n234\n\n\n235\n\n\n236\n\n\n237\n\n\n238\n\n\n239\n\n\n240\n\n\n241\n\n\n242\n\n\n243\n\n\n244\n\n\n245\n\n\n246\n\n\n247\n\n\n248\n\n\n249\n\n\n250\n\n\n251\n\n\n252\n\n\n253\n\n\n254\n\n\n255\n\n\n256\n\n\n257\n\n\n258\n\n\n259\n\n\n260\n\n\n261\n\n\n262\n\n\n263\n\n\n264\n\n\n265\n\n\n266\n\n\n267\n\n\n268\n\n\n269\n\n\n270\n\n\n271\n\n\n272\n\n\n273\n\n\n274\n\n\n275\n\n\n276\n\n\n277\n\n\n278\n\n\n279\n\n\n280\n\n\n281\n\n\n282\n\n\n283\n\n\n284\n\n\n285\n\n\n286\n\n\n287\n\n\n288\n\n\n289\n\n\n290\n\n\n291\n\n\n292\n\n\n293\n\n\n294\n\n\n295\n\n\n296\n\n\n297\n\n\n298\n\n\n299\n\n\n300\n\n\n301\n\n\n302\n\n\n303\n\n\n304\n\n\n305\n\n\n306\n\n\n307\n\n\n308\n\n\n309\n\n\n310\n\n\n311\n\n\n312\n\n\n313\n\n\n314\n\n\n315\n\n\n316\n\n\n317\n\n\n318\n\n\n319\n\n\n320\n\n\n321\n\n\n322\n\n\n323\n\n\n324\n\n\n325\n\n\n326\n\n\n327\n\n\n328\n\n\n329\n\n\n330\n\n\n331\n\n\n332\n\n\n333\n\n\n334\n\n\n335\n\n\n336\n\n\n337\n\n\n338\n\n\n339\n\n\n340\n\n\n341\n\n\n342\n\n\n343\n\n\n344\n\n\n345\n\n\n346\n\n\n347\n\n\n348\n\n\n349\n\n\n350\n\n\n351\n\n\n352\n\n\n353\n\n\n354\n\n\n355\n\n\n356\n\n\n357\n\n\n358\n\n\n359\n\n\n360\n\n\n361\n\n\n362\n\n\n363\n\n\n364\n\n\n365\n\n\n366\n\n\n367\n\n\n368\n\n\n369\n\n\n370\n\n\n371\n\n\n372\n\n\n373\n\n\n374\n\n\n375\n\n\n376\n\n\n377\n\n\n378\n\n\n379\n\n\n380\n\n\n381\n\n\n382\n\n\n383\n\n\n384\n\n\n385\n\n\n386\n\n\n387\n\n\n388\n\n\n389\n\n\n390\n\n\n391\n\n\n392\n\n\n393\n\n\n394\n\n\n395\n\n\n396\n\n\n397\n\n\n398\n\n\n399\n\n\n400\n\n\n401\n\n\n402\n\n\n403\n\n\n404\n\n\n405\n\n\n406\n\n\n407\n\n\n408\n\n\n409\n\n\n410\n\n\n411\n\n\n412\n\n\n413\n\n\n414\n\n\n415\n\n\n416\n\n\n417\n\n\n418\n\n\n419\n\n\n420\n\n\n421\n\n\n422\n\n\n423\n\n\n424\n\n\n425\n\n\n426\n\n\n427\n\n\n428\n\n\n429\n\n\n430\n\n\n431\n\n\n432\n\n\n433\n\n\n434\n\n\n435\n\n\n436\n\n\n437\n\n\n438\n\n\n439\n\n\n440\n\n\n441\n\n\n442\n\n\n443\n\n\n444\n\n\n445\n\n\n446\n\n\n447\n\n\n448\n\n\n449\n\n\n450\n\n\n451\n\n\n452\n\n\n453\n\n\n454\n\n\n455\n\n\n456\n\n\n457\n\n\n458\n\n\n459\n\n\n460\n\n\n461\n\n\n462\n\n\n463\n\n\n464\n\n\n465\n\n\n466\n\n\n467\n\n\n468\n\n\n469\n\n\n470\n\n\n471\n\n\n472\n\n\n473\n\n\n474\n\n\n475\n\n\n476\n\n\n477\n\n\n478\n\n\n479\n\n\n480\n\n\n481\n\n\n482\n\n\n483\n\n\n484\n\n\n485\n\n\n486\n\n\n487\n\n\n488\n\n\n489\n\n\n490\n\n\n491\n\n\n492\n\n\n493\n\n\n494\n\n\n495\n\n\n496\n\n\n497\n\n\n498\n\n\n499\n\n\n500\n\n\n501\n\n\n502\n\n\n503\n\n\n504\n\n\n505\n\n\n506\n\n\n507\n\n\n508\n\n\n509\n\n\n510\n\n\n511\n\n\n512\n\n\n513\n\n\n514\n\n\n515\n\n\n516\n\n\n517\n\n\n518\n\n\n519\n\n\n520\n\n\n521\n\n\n522\n\n\n523\n\n\n524\n\n\n525\n\n\n526\n\n\n527\n\n\n528\n\n\n529\n\n\n530\n\n\n531\n\n\n532\n\n\n533\n\n\n534\n\n\n535\n\n\n536\n\n\n537\n\n\n538\n\n\n539\n\n\n540\n\n\n541\n\n\n542\n\n\n543\n\n\n544\n\n\n545\n\n\n546\n\n\n547\n\n\n548\n\n\n549\n\n\n550\n\n\n\n\n35\n\n\n1\n\n\nAGG_MENT\n\n\nAggregate Mental QOL Score\n\n\n58.20753602\n\n\n48.71790642\n\n\n44.42011462\n\n\n31.15971215\n\n\n56.21993476\n\n\n53.84956108\n\n\n41.32938101\n\n\n30.53194073\n\n\n58.07473197\n\n\n57.1417356\n\n\n25.70015642\n\n\n45.76302915\n\n\n33.50968079\n\n\n35.82080909\n\n\n39.09628459\n\n\n59.33987011\n\n\n45.37660139\n\n\n51.30398929\n\n\n28.30757664\n\n\n23.33462079\n\n\n20.6085118\n\n\n62.28336178\n\n\n51.61324651\n\n\n31.98246753\n\n\n56.73641306\n\n\n60.07505279\n\n\n28.23811123\n\n\n55.23836529\n\n\n59.69158593\n\n\n64.25839322\n\n\n36.95915403\n\n\n36.80452883\n\n\n59.73670575\n\n\n42.33888869\n\n\n53.01971932\n\n\n25.78591579\n\n\n23.25374474\n\n\n62.61075577\n\n\n47.99326333\n\n\n37.8206828\n\n\n54.31389725\n\n\n61.96330996\n\n\n28.75680431\n\n\n54.11936111\n\n\n60.51489432\n\n\n51.96244013\n\n\n36.92764879\n\n\n36.94013129\n\n\n58.31519903\n\n\n39.9496879\n\n\n50.74753027\n\n\n25.53774799\n\n\n20.63371637\n\n\n60.90548962\n\n\n42.38921392\n\n\n58.67048049\n\n\n37.7951985\n\n\n62.08072475\n\n\n45.93200701\n\n\n53.60570092\n\n\n30.85480961\n\n\n44.68819831\n\n\n29.95018922\n\n\n20.69436314\n\n\n58.77260942\n\n\n25.70806017\n\n\n52.04456009\n\n\n42.10139303\n\n\n54.56210171\n\n\n53.52510623\n\n\n35.72405584\n\n\n49.08031262\n\n\n59.2482235\n\n\n61.71997008\n\n\n47.41026934\n\n\n56.69074435\n\n\n56.8120144\n\n\n53.68381515\n\n\n.\n\n\n61.54625512\n\n\n50.48553378\n\n\n59.69357966\n\n\n57.68238151\n\n\n32.73557752\n\n\n39.6668649\n\n\n37.66535563\n\n\n28.7075251\n\n\n53.8305108\n\n\n52.18251184\n\n\n44.53577833\n\n\n59.71629518\n\n\n55.12024035\n\n\n57.18799434\n\n\n58.59787438\n\n\n38.19800034\n\n\n54.55803511\n\n\n50.69448517\n\n\n44.99930773\n\n\n53.5917356\n\n\n51.4823665\n\n\n55.81220503\n\n\n43.62909144\n\n\n51.25533748\n\n\n55.70204643\n\n\n60.98503613\n\n\n57.65401071\n\n\n61.78983232\n\n\n45.65781272\n\n\n61.60902604\n\n\n58.72094316\n\n\n34.89049162\n\n\n52.84176847\n\n\n47.99151526\n\n\n53.52613085\n\n\n38.74095996\n\n\n60.40817002\n\n\n34.283481\n\n\n.\n\n\n51.10206781\n\n\n58.53380114\n\n\n50.88050653\n\n\n62.88442444\n\n\n55.81165976\n\n\n50.02496135\n\n\n49.18248082\n\n\n30.63752469\n\n\n33.00395043\n\n\n36.63421005\n\n\n59.62496036\n\n\n57.03523782\n\n\n32.05810211\n\n\n55.26251481\n\n\n61.6579261\n\n\n45.77710792\n\n\n55.4773143\n\n\n51.92789187\n\n\n55.01908602\n\n\n61.08915017\n\n\n57.5063042\n\n\n33.26513637\n\n\n50.78671716\n\n\n48.21725637\n\n\n60.46269645\n\n\n57.73662679\n\n\n47.45499445\n\n\n58.51013379\n\n\n45.61705297\n\n\n56.19189359\n\n\n30.95523327\n\n\n15.08436083\n\n\n54.78679353\n\n\n23.88690856\n\n\n36.41745586\n\n\n53.58481704\n\n\n.\n\n\n55.44121603\n\n\n40.24193953\n\n\n25.72552976\n\n\n62.10801966\n\n\n41.08809163\n\n\n57.92780037\n\n\n47.67493897\n\n\n52.86709905\n\n\n24.34457016\n\n\n39.22485225\n\n\n49.78535656\n\n\n60.00537342\n\n\n60.89976714\n\n\n30.1601812\n\n\n47.3232644\n\n\n30.31491899\n\n\n53.86081358\n\n\n35.63263425\n\n\n50.4811125\n\n\n60.33431334\n\n\n52.90752549\n\n\n52.69610511\n\n\n15.41144797\n\n\n53.87821635\n\n\n56.03944775\n\n\n37.71871922\n\n\n44.00482273\n\n\n37.46591901\n\n\n35.93462256\n\n\n37.8757628\n\n\n60.31192023\n\n\n36.79894615\n\n\n59.23635433\n\n\n36.31864166\n\n\n54.1280648\n\n\n50.52878505\n\n\n66.86330134\n\n\n47.87337173\n\n\n48.25175498\n\n\n20.31218504\n\n\n58.59306082\n\n\n29.11702267\n\n\n52.32365936\n\n\n46.54950368\n\n\n51.85860482\n\n\n41.52048717\n\n\n57.96301037\n\n\n52.87306006\n\n\n54.6465808\n\n\n53.6338761\n\n\n27.48978956\n\n\n33.30538271\n\n\n37.4917224\n\n\n41.8960176\n\n\n46.14498202\n\n\n62.65843799\n\n\n60.47185277\n\n\n44.77640059\n\n\n55.39878112\n\n\n60.29124941\n\n\n55.03501782\n\n\n14.97480827\n\n\n56.22730363\n\n\n55.52832764\n\n\n24.16361296\n\n\n51.86601122\n\n\n36.0081604\n\n\n47.13665868\n\n\n51.41137466\n\n\n54.64667635\n\n\n44.51116981\n\n\n30.35827111\n\n\n53.67702511\n\n\n60.09774775\n\n\n44.14221595\n\n\n52.48685566\n\n\n60.58821932\n\n\n44.55069542\n\n\n48.64137248\n\n\n38.15308208\n\n\n56.71078419\n\n\n48.64793324\n\n\n53.7402939\n\n\n53.73384233\n\n\n55.36114609\n\n\n30.9898261\n\n\n60.53654495\n\n\n52.4972117\n\n\n35.01574337\n\n\n56.12619513\n\n\n63.63127694\n\n\n38.23138845\n\n\n22.66625142\n\n\n57.01828593\n\n\n54.14010619\n\n\n52.67447061\n\n\n31.72750825\n\n\n15.58220792\n\n\n49.02774544\n\n\n59.73618477\n\n\n28.19836684\n\n\n55.95928083\n\n\n27.62446146\n\n\n27.74824665\n\n\n59.66577193\n\n\n50.5648121\n\n\n41.51995748\n\n\n20.53389067\n\n\n57.14988088\n\n\n60.05842956\n\n\n46.81019321\n\n\n58.85573363\n\n\n58.058884\n\n\n56.81317234\n\n\n53.26127836\n\n\n53.05470555\n\n\n57.0820711\n\n\n51.8669334\n\n\n53.12226017\n\n\n58.95433121\n\n\n25.25645337\n\n\n31.84584625\n\n\n52.36654359\n\n\n54.80392704\n\n\n53.47447786\n\n\n56.03640091\n\n\n34.05278148\n\n\n65.45588261\n\n\n34.46777081\n\n\n53.69304516\n\n\n36.94332933\n\n\n26.67261454\n\n\n52.84404014\n\n\n59.97449625\n\n\n53.9779315\n\n\n41.42220147\n\n\n40.81167898\n\n\n52.51306611\n\n\n29.79198005\n\n\n26.61290192\n\n\n40.7503856\n\n\n54.58435765\n\n\n57.33639432\n\n\n57.47431259\n\n\n42.39579245\n\n\n56.563323\n\n\n35.88567029\n\n\n47.24045475\n\n\n56.47067491\n\n\n32.81962746\n\n\n35.10917749\n\n\n50.36181574\n\n\n57.73981947\n\n\n15.63377234\n\n\n42.82986566\n\n\n38.30121519\n\n\n53.63800767\n\n\n14.52088418\n\n\n51.67550659\n\n\n65.3503919\n\n\n58.25449024\n\n\n54.55745874\n\n\n19.56711522\n\n\n25.5456675\n\n\n51.48689425\n\n\n70.83849011\n\n\n45.90938204\n\n\n52.19480551\n\n\n63.74341339\n\n\n56.21187378\n\n\n27.90521254\n\n\n36.70841194\n\n\n58.86984541\n\n\n30.93994131\n\n\n55.44514552\n\n\n52.89089285\n\n\n38.57422884\n\n\n56.79726723\n\n\n50.85334886\n\n\n54.07721032\n\n\n53.74414052\n\n\n45.68502291\n\n\n50.57621911\n\n\n14.57303496\n\n\n49.08881995\n\n\n58.34772096\n\n\n50.33497452\n\n\n19.36683566\n\n\n42.31287076\n\n\n55.506049\n\n\n30.60382826\n\n\n36.54671057\n\n\n35.39819567\n\n\n60.63069223\n\n\n58.14511923\n\n\n34.3029444\n\n\n36.09905797\n\n\n14.73941668\n\n\n62.0313313\n\n\n52.15961053\n\n\n40.31017902\n\n\n45.10649146\n\n\n39.52725587\n\n\n60.74921455\n\n\n53.39808211\n\n\n32.8026672\n\n\n59.03190194\n\n\n56.13067596\n\n\n27.29682436\n\n\n55.5015804\n\n\n50.79468491\n\n\n57.91265633\n\n\n14.5482773\n\n\n52.397525\n\n\n57.55536753\n\n\n41.82079132\n\n\n57.40277938\n\n\n36.38502495\n\n\n56.15319152\n\n\n56.80710656\n\n\n58.28760918\n\n\n58.84645012\n\n\n50.23592855\n\n\n52.96592202\n\n\n44.91865821\n\n\n42.73599167\n\n\n55.46767741\n\n\n50.32858461\n\n\n51.57908749\n\n\n21.66215661\n\n\n55.14647578\n\n\n55.55000033\n\n\n34.70460261\n\n\n51.2445366\n\n\n45.40346878\n\n\n14.58042164\n\n\n35.63498193\n\n\n54.62929617\n\n\n60.28637931\n\n\n50.10080336\n\n\n62.30819959\n\n\n59.51708563\n\n\n19.12988663\n\n\n47.75597834\n\n\n22.71661447\n\n\n34.12726977\n\n\n43.47366014\n\n\n26.15631462\n\n\n53.10288139\n\n\n57.50552751\n\n\n51.31586979\n\n\n26.31241326\n\n\n49.37758648\n\n\n46.07474778\n\n\n52.16925375\n\n\n58.80976178\n\n\n51.33565585\n\n\n28.58232911\n\n\n36.87626343\n\n\n57.09263823\n\n\n25.18827758\n\n\n58.5357533\n\n\n50.28077598\n\n\n59.28975039\n\n\n45.07692831\n\n\n51.94555281\n\n\n59.28656441\n\n\n28.20961141\n\n\n30.31709551\n\n\n23.21348629\n\n\n57.04397268\n\n\n29.16820047\n\n\n42.26883544\n\n\n52.00669243\n\n\n64.16946989\n\n\n59.26019461\n\n\n55.08914779\n\n\n58.83959825\n\n\n52.85333882\n\n\n31.20771196\n\n\n42.02068435\n\n\n44.07518914\n\n\n56.99642877\n\n\n36.19369178\n\n\n58.34134648\n\n\n49.91987709\n\n\n56.6909813\n\n\n61.5539284\n\n\n62.01877434\n\n\n33.95799806\n\n\n22.46092721\n\n\n27.0489022\n\n\n42.57607227\n\n\n51.70968333\n\n\n50.36439841\n\n\n52.92939151\n\n\n59.46893889\n\n\n50.49365334\n\n\n60.96242501\n\n\n25.98822792\n\n\n21.27161791\n\n\n58.2691881\n\n\n23.30554798\n\n\n55.34473822\n\n\n52.9481665\n\n\n62.68509896\n\n\n37.53021265\n\n\n56.7813699\n\n\n62.03580977\n\n\n43.07361035\n\n\n25.10360363\n\n\n31.93560301\n\n\n56.01287029\n\n\n57.09152696\n\n\n53.96441808\n\n\n60.73586597\n\n\n60.6334499\n\n\n54.69951682\n\n\n39.42667177\n\n\n60.3351686\n\n\n54.24919922\n\n\n73.31224001\n\n\n51.11506927\n\n\n32.28212351\n\n\n60.2319663\n\n\n56.85678612\n\n\n57.10095918\n\n\n24.23083275\n\n\n32.80502582\n\n\n58.3609927\n\n\n29.86688277\n\n\n54.73237586\n\n\n32.58872355\n\n\n57.75839718\n\n\n15.61966261\n\n\n30.30445565\n\n\n61.64158224\n\n\n48.52479547\n\n\n55.63619716\n\n\n35.02200131\n\n\n38.25434576\n\n\n27.33618698\n\n\n44.01431306\n\n\n50.64176611\n\n\n61.30902256\n\n\n41.17155428\n\n\n51.52233298\n\n\n55.99104237\n\n\n25.12712895\n\n\n51.65078626\n\n\n14.6576473\n\n\n22.09582621\n\n\n45.3618849\n\n\n27.80366128\n\n\n35.13086106\n\n\n56.70897245\n\n\n58.3180929\n\n\n52.33477318\n\n\n62.19442206\n\n\n61.49099571\n\n\n55.21255695\n\n\n43.76399388\n\n\n48.49741882\n\n\n56.29778638\n\n\n57.85004675\n\n\n49.26636403\n\n\n38.59155038\n\n\n60.93379979\n\n\n59.73947669\n\n\n56.2996025\n\n\n24.47112246\n\n\n33.63550422\n\n\n59.18405387\n\n\n29.68892192\n\n\n54.09338866\n\n\n34.05929608\n\n\n58.2022158\n\n\n16.14386504\n\n\n30.58800682\n\n\n51.44998579\n\n\n49.9688964\n\n\n54.17734455\n\n\n35.87282666\n\n\n37.15939168\n\n\n28.44718395\n\n\n42.90220461\n\n\n48.10232444\n\n\n58.89010471\n\n\n39.74210095\n\n\n49.08582067\n\n\n59.49544635\n\n\n25.77895468\n\n\n52.80063948\n\n\n12.38716248\n\n\n21.25537405\n\n\n\n\n36\n\n\n1\n\n\nAGG_PHYS\n\n\nAggregate Physical QOL Score\n\n\n41.29346502\n\n\n38.03806704\n\n\n62.71705483\n\n\n44.62882869\n\n\n30.47054901\n\n\n57.91396015\n\n\n61.09096244\n\n\n64.30965386\n\n\n54.64363002\n\n\n40.13534583\n\n\n51.92545397\n\n\n42.99788574\n\n\n49.65604224\n\n\n57.77780394\n\n\n56.5470728\n\n\n54.27142408\n\n\n43.62685247\n\n\n45.84662105\n\n\n29.64560874\n\n\n34.25073636\n\n\n63.19184148\n\n\n40.81894768\n\n\n37.01247801\n\n\n57.71074521\n\n\n31.1083321\n\n\n50.98833972\n\n\n66.15846876\n\n\n58.10240437\n\n\n43.12667992\n\n\n41.62103974\n\n\n50.87044216\n\n\n58.26029595\n\n\n53.30653699\n\n\n45.99854048\n\n\n49.27217569\n\n\n32.26106772\n\n\n63.53867962\n\n\n42.04976648\n\n\n38.49185551\n\n\n59.55817324\n\n\n32.44674306\n\n\n62.74336269\n\n\n65.10750306\n\n\n54.9182513\n\n\n40.54960904\n\n\n33.81334425\n\n\n50.33507923\n\n\n57.10491794\n\n\n53.49509181\n\n\n44.79760284\n\n\n49.84473731\n\n\n33.88133014\n\n\n65.91054248\n\n\n54.93302089\n\n\n29.90180277\n\n\n53.2926279\n\n\n37.74173218\n\n\n51.53107434\n\n\n55.21725231\n\n\n52.50653327\n\n\n44.21279312\n\n\n57.58883321\n\n\n65.48869884\n\n\n50.14216471\n\n\n53.23715075\n\n\n61.7181817\n\n\n58.92077105\n\n\n60.28327129\n\n\n57.05286919\n\n\n59.53779866\n\n\n30.41539896\n\n\n56.69176935\n\n\n52.5693695\n\n\n59.14718908\n\n\n58.0075156\n\n\n55.23772578\n\n\n54.86624283\n\n\n51.2068502\n\n\n.\n\n\n23.68679361\n\n\n50.00228459\n\n\n57.74353848\n\n\n54.39524467\n\n\n43.43039817\n\n\n57.84664729\n\n\n50.02760066\n\n\n39.05518395\n\n\n53.52864057\n\n\n43.06904795\n\n\n45.00470529\n\n\n57.69835694\n\n\n44.30106314\n\n\n55.17802896\n\n\n44.23957031\n\n\n48.40077012\n\n\n45.31549505\n\n\n55.69915398\n\n\n53.01765207\n\n\n47.53538901\n\n\n50.26187456\n\n\n55.49277523\n\n\n54.81308792\n\n\n56.90404013\n\n\n55.84368436\n\n\n42.33533609\n\n\n53.93493016\n\n\n50.37598816\n\n\n33.80779629\n\n\n53.96104859\n\n\n55.78898892\n\n\n63.74363703\n\n\n54.44659034\n\n\n52.53011343\n\n\n42.77767158\n\n\n50.18877253\n\n\n52.70937765\n\n\n42.52181192\n\n\n.\n\n\n37.84166967\n\n\n52.18648376\n\n\n36.93157733\n\n\n49.63702033\n\n\n42.38066409\n\n\n55.79495015\n\n\n49.21017267\n\n\n41.72346257\n\n\n47.97044363\n\n\n54.08904692\n\n\n40.51845654\n\n\n54.66570173\n\n\n54.32659012\n\n\n51.49024353\n\n\n60.24372677\n\n\n55.89370477\n\n\n49.15937543\n\n\n47.09282848\n\n\n55.51499956\n\n\n57.55873195\n\n\n53.51060591\n\n\n59.36065441\n\n\n55.91433858\n\n\n33.28752478\n\n\n58.40247928\n\n\n58.95949414\n\n\n69.2625684\n\n\n52.98343007\n\n\n46.70288258\n\n\n56.01015908\n\n\n61.16616347\n\n\n68.91305405\n\n\n58.35426019\n\n\n63.38921066\n\n\n59.37392559\n\n\n50.68487113\n\n\n.\n\n\n56.03858093\n\n\n57.13768399\n\n\n46.4791902\n\n\n59.0242036\n\n\n22.38691148\n\n\n59.54388284\n\n\n59.02001296\n\n\n52.98678149\n\n\n59.56731652\n\n\n31.24106156\n\n\n49.7948294\n\n\n49.38269932\n\n\n46.85650111\n\n\n62.88094415\n\n\n52.54139478\n\n\n53.73931478\n\n\n26.78114056\n\n\n33.24689304\n\n\n52.38536084\n\n\n22.39266214\n\n\n53.65875075\n\n\n48.6541083\n\n\n30.77275479\n\n\n50.90701679\n\n\n40.59503183\n\n\n48.55210312\n\n\n58.45703301\n\n\n53.60464867\n\n\n54.86704263\n\n\n48.84264566\n\n\n48.81727992\n\n\n58.62028083\n\n\n54.97321064\n\n\n52.2827949\n\n\n52.85319418\n\n\n53.40362784\n\n\n46.67991762\n\n\n55.14164968\n\n\n37.51923379\n\n\n47.95255976\n\n\n55.31484913\n\n\n32.20507704\n\n\n46.43927029\n\n\n42.61155189\n\n\n69.62514731\n\n\n49.52939315\n\n\n55.60606888\n\n\n35.89420098\n\n\n55.84511009\n\n\n53.81893978\n\n\n26.29014132\n\n\n59.97027166\n\n\n27.57430208\n\n\n19.27791518\n\n\n37.70681434\n\n\n55.96756888\n\n\n55.52806284\n\n\n63.38710385\n\n\n30.60770079\n\n\n37.8003757\n\n\n31.78723515\n\n\n41.11114539\n\n\n55.88579902\n\n\n48.71191516\n\n\n57.24652582\n\n\n56.19274687\n\n\n52.24687424\n\n\n47.06251927\n\n\n25.7328849\n\n\n26.09187541\n\n\n51.86449611\n\n\n43.53122096\n\n\n55.54741782\n\n\n61.08356007\n\n\n50.74764341\n\n\n34.91218667\n\n\n57.11058565\n\n\n58.4332847\n\n\n54.62517794\n\n\n49.03580717\n\n\n51.91542834\n\n\n51.43872177\n\n\n37.2119362\n\n\n58.25485925\n\n\n55.17859096\n\n\n62.56748968\n\n\n44.21697793\n\n\n59.01211778\n\n\n48.02919507\n\n\n39.04271164\n\n\n32.94707064\n\n\n59.14798363\n\n\n54.18112954\n\n\n59.42000754\n\n\n53.03545751\n\n\n55.35410049\n\n\n55.71659842\n\n\n68.94193163\n\n\n57.70045386\n\n\n57.15863713\n\n\n59.80236073\n\n\n52.57998903\n\n\n49.56720802\n\n\n56.9944944\n\n\n55.2062179\n\n\n54.94871521\n\n\n52.85736361\n\n\n48.46055988\n\n\n58.57305568\n\n\n53.75269178\n\n\n36.84613917\n\n\n55.78012965\n\n\n38.69382675\n\n\n49.29028195\n\n\n51.72703913\n\n\n58.97556651\n\n\n31.08594531\n\n\n54.37037117\n\n\n56.18165266\n\n\n58.73612096\n\n\n58.72203196\n\n\n60.43846783\n\n\n56.62209058\n\n\n52.97189415\n\n\n53.44997203\n\n\n33.6644041\n\n\n38.52323225\n\n\n58.7226087\n\n\n60.43523236\n\n\n57.11922075\n\n\n51.79152585\n\n\n64.27836818\n\n\n55.10661379\n\n\n47.6126059\n\n\n53.67072047\n\n\n61.19430129\n\n\n63.7052781\n\n\n56.9061975\n\n\n65.20899444\n\n\n27.73249211\n\n\n37.33051963\n\n\n54.91480883\n\n\n60.69504182\n\n\n54.41781665\n\n\n51.20952902\n\n\n53.01407267\n\n\n47.04722464\n\n\n47.47779176\n\n\n61.10061013\n\n\n34.54771762\n\n\n56.03695012\n\n\n49.46172886\n\n\n41.15633202\n\n\n30.9229381\n\n\n20.40715839\n\n\n46.04484812\n\n\n58.15214422\n\n\n61.14948179\n\n\n57.82972706\n\n\n48.9487516\n\n\n56.46700503\n\n\n47.75665445\n\n\n44.49636353\n\n\n52.79286523\n\n\n50.41726135\n\n\n47.6036911\n\n\n54.95588488\n\n\n55.1893953\n\n\n41.57563586\n\n\n44.67260395\n\n\n15.37383463\n\n\n33.66447187\n\n\n51.02419992\n\n\n37.69646124\n\n\n52.4514588\n\n\n56.44545784\n\n\n50.91583225\n\n\n58.9772415\n\n\n33.07077938\n\n\n50.44286692\n\n\n40.475036\n\n\n41.38580382\n\n\n55.75910974\n\n\n67.85146376\n\n\n39.1997267\n\n\n57.16017559\n\n\n53.98371205\n\n\n66.03829216\n\n\n56.2522724\n\n\n50.87987745\n\n\n50.43822212\n\n\n40.13254295\n\n\n25.23800108\n\n\n55.96496696\n\n\n59.43549907\n\n\n51.13495209\n\n\n57.83524533\n\n\n67.72429439\n\n\n52.88372059\n\n\n27.6127854\n\n\n56.20277795\n\n\n59.3207721\n\n\n54.28841376\n\n\n44.01972766\n\n\n53.44378394\n\n\n37.04259552\n\n\n57.84388118\n\n\n55.17331783\n\n\n65.40203122\n\n\n56.06720849\n\n\n51.80312457\n\n\n52.99956805\n\n\n52.95110099\n\n\n54.68943273\n\n\n58.36350746\n\n\n50.64879861\n\n\n53.94414319\n\n\n48.91257106\n\n\n53.63463298\n\n\n55.68996087\n\n\n55.78119189\n\n\n54.22049919\n\n\n45.07168179\n\n\n57.23940014\n\n\n41.53842619\n\n\n43.47584061\n\n\n55.98897795\n\n\n54.94422212\n\n\n31.08825357\n\n\n39.0177808\n\n\n51.1155253\n\n\n53.7592364\n\n\n63.98981222\n\n\n57.58493311\n\n\n57.83619067\n\n\n44.55098701\n\n\n64.14942249\n\n\n40.75458016\n\n\n9.11661401\n\n\n58.43374343\n\n\n58.69821531\n\n\n54.69936182\n\n\n51.70581198\n\n\n46.13198039\n\n\n54.94735104\n\n\n52.66757345\n\n\n43.10956471\n\n\n52.50822507\n\n\n41.77837492\n\n\n58.96677239\n\n\n47.47843864\n\n\n46.4657485\n\n\n56.07190234\n\n\n55.95973939\n\n\n55.10783418\n\n\n45.89940651\n\n\n48.32026733\n\n\n29.15563821\n\n\n31.53107847\n\n\n31.66623554\n\n\n57.46370727\n\n\n49.7769765\n\n\n54.0373801\n\n\n55.37088014\n\n\n45.12773871\n\n\n53.92091917\n\n\n50.65688363\n\n\n64.29799564\n\n\n59.2293714\n\n\n34.88412601\n\n\n53.08280031\n\n\n39.68844306\n\n\n45.96624881\n\n\n38.19220086\n\n\n57.00844251\n\n\n47.82026112\n\n\n38.05341183\n\n\n51.71540314\n\n\n56.2002058\n\n\n48.41641537\n\n\n15.03601805\n\n\n23.0999094\n\n\n54.60575016\n\n\n54.58241493\n\n\n51.60532941\n\n\n54.93946605\n\n\n54.46311286\n\n\n50.97108624\n\n\n57.72669219\n\n\n46.58638415\n\n\n34.44345506\n\n\n61.79881291\n\n\n70.43723736\n\n\n58.86372087\n\n\n47.75409421\n\n\n55.05211623\n\n\n57.17927881\n\n\n29.99995905\n\n\n37.21059063\n\n\n37.68520066\n\n\n63.51314335\n\n\n56.63376354\n\n\n63.84093276\n\n\n55.84130189\n\n\n59.25184229\n\n\n50.04076237\n\n\n38.89262533\n\n\n58.82548459\n\n\n51.47219048\n\n\n30.04579563\n\n\n54.67312184\n\n\n55.04340416\n\n\n42.12337538\n\n\n42.95137671\n\n\n47.39060674\n\n\n41.21077504\n\n\n41.23189809\n\n\n55.39573843\n\n\n23.32746292\n\n\n25.94664296\n\n\n51.02010966\n\n\n48.10919588\n\n\n38.42192531\n\n\n57.12456371\n\n\n40.34853064\n\n\n50.39923229\n\n\n30.97887517\n\n\n56.70325431\n\n\n40.01591725\n\n\n50.09434197\n\n\n65.82592605\n\n\n56.99400778\n\n\n33.86074798\n\n\n41.26312879\n\n\n61.0149999\n\n\n36.5378451\n\n\n41.15667524\n\n\n54.68322093\n\n\n51.75451611\n\n\n51.95326761\n\n\n57.04418811\n\n\n66.99255549\n\n\n44.9339516\n\n\n54.92282046\n\n\n54.51926266\n\n\n45.47228797\n\n\n48.35949722\n\n\n49.76413091\n\n\n31.14100457\n\n\n50.34140112\n\n\n43.44593443\n\n\n63.55661662\n\n\n28.44394782\n\n\n47.27685434\n\n\n54.92573215\n\n\n44.46332612\n\n\n43.19624376\n\n\n47.08943136\n\n\n40.96553844\n\n\n41.20195157\n\n\n55.24700859\n\n\n23.31479985\n\n\n27.0351765\n\n\n54.78726379\n\n\n49.68161013\n\n\n38.52863299\n\n\n59.38117723\n\n\n38.10865491\n\n\n51.64816472\n\n\n31.30938136\n\n\n57.44072898\n\n\n37.38231715\n\n\n61.9364941\n\n\n66.67043088\n\n\n55.621744\n\n\n35.59954435\n\n\n39.52191444\n\n\n61.39252707\n\n\n36.71705472\n\n\n35.48722639\n\n\n51.87920391\n\n\n51.31131899\n\n\n49.83240016\n\n\n58.21841631\n\n\n65.85393024\n\n\n44.07740786\n\n\n53.15717691\n\n\n54.42330234\n\n\n43.54997043\n\n\n49.72631908\n\n\n33.40751574\n\n\n34.20364103\n\n\n50.17121839\n\n\n38.58162607\n\n\n66.03296933\n\n\n\n\n37\n\n\n1\n\n\nHASHV\n\n\nFrequency of Hash/Marijuana Use\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n.\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n2\n\n\n.\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n.\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n2\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n2\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n.\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n.\n\n\n1\n\n\n1\n\n\n.\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n2\n\n\n1\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n1\n\n\n2\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n.\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n.\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n2\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n.\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n.\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n.\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n.\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n.\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n.\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n.\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n\n\n38\n\n\n1\n\n\nBMI\n\n\nBMI\n\n\n26.06800717\n\n\n25.96576195\n\n\n28.35319907\n\n\n18.21865476\n\n\n24.97865422\n\n\n23.7531848\n\n\n26.56640456\n\n\n29.21564539\n\n\n22.63666132\n\n\n19.32619854\n\n\n19.82680928\n\n\n20.27133169\n\n\n23.30875183\n\n\n25.61309587\n\n\n21.58454288\n\n\n30.84426242\n\n\n24.22315094\n\n\n30.15605109\n\n\n32.97599448\n\n\n998.8723536\n\n\n.\n\n\n25.10229862\n\n\n28.37735021\n\n\n30.27654369\n\n\n24.46095074\n\n\n21.20057325\n\n\n29.72498026\n\n\n23.56807083\n\n\n19.49979773\n\n\n19.11882936\n\n\n22.78114511\n\n\n27.10264635\n\n\n30.35525513\n\n\n24.37753726\n\n\n31.98730869\n\n\n999.0320183\n\n\n.\n\n\n24.30687276\n\n\n26.96343831\n\n\n26.72919349\n\n\n24.71993917\n\n\n23.48153557\n\n\n30.92230072\n\n\n21.72082824\n\n\n19.45008514\n\n\n19.95252021\n\n\n23.46478513\n\n\n25.7980622\n\n\n30.00366186\n\n\n24.67194324\n\n\n27.50166564\n\n\n999.6144867\n\n\n.\n\n\n24.90393397\n\n\n22.3662383\n\n\n38.89617368\n\n\n26.03576694\n\n\n31.64746685\n\n\n29.38887022\n\n\n24.21441709\n\n\n22.20325066\n\n\n27.26619273\n\n\n26.28658081\n\n\n30.34641725\n\n\n23.14444962\n\n\n19.29940658\n\n\n27.00767194\n\n\n24.95029679\n\n\n32.93492391\n\n\n24.26047687\n\n\n19.78703095\n\n\n25.91671285\n\n\n23.54004052\n\n\n24.81341974\n\n\n25.38232497\n\n\n27.63929398\n\n\n23.38853089\n\n\n22.78314002\n\n\n29.60040387\n\n\n28.68551415\n\n\n29.50935747\n\n\n25.91510912\n\n\n22.61444767\n\n\n25.63307585\n\n\n27.81865884\n\n\n21.18866589\n\n\n30.42068788\n\n\n17.0667334\n\n\n23.74740586\n\n\n.\n\n\n21.37786415\n\n\n25.12245016\n\n\n29.67528221\n\n\n20.6853351\n\n\n22.07942558\n\n\n35.52766663\n\n\n25.68450133\n\n\n.\n\n\n28.47664712\n\n\n25.61164819\n\n\n28.69476327\n\n\n.\n\n\n26.23423986\n\n\n21.97494402\n\n\n29.51507012\n\n\n31.66622487\n\n\n34.3849906\n\n\n28.38349868\n\n\n26.09817414\n\n\n29.71911353\n\n\n24.81985639\n\n\n22.77183245\n\n\n22.73046372\n\n\n24.91056205\n\n\n26.93871254\n\n\n.\n\n\n21.15035214\n\n\n24.32599205\n\n\n23.50172059\n\n\n20.55328507\n\n\n22.6829856\n\n\n22.88996585\n\n\n23.9785533\n\n\n25.06612306\n\n\n22.62789141\n\n\n21.51797566\n\n\n18.28851385\n\n\n20.38981426\n\n\n25.46709997\n\n\n25.46762968\n\n\n39.29654068\n\n\n29.82951158\n\n\n29.10734112\n\n\n21.84436475\n\n\n.\n\n\n24.16355704\n\n\n39.2594979\n\n\n.\n\n\n18.36929851\n\n\n27.28970417\n\n\n16.86896402\n\n\n23.00365755\n\n\n29.91683078\n\n\n21.73802393\n\n\n.\n\n\n25.21453082\n\n\n18.4992495\n\n\n23.45931656\n\n\n20.02336738\n\n\n36.5916123\n\n\n-1\n\n\n24.78894381\n\n\n31.5704387\n\n\n24.30407646\n\n\n23.7153711\n\n\n24.16986116\n\n\n32.26102399\n\n\n.\n\n\n.\n\n\n19.52664103\n\n\n20.20071767\n\n\n27.89883545\n\n\n23.44008146\n\n\n28.65460924\n\n\n23.50476691\n\n\n20.0026874\n\n\n26.60717662\n\n\n25.26692727\n\n\n31.05232516\n\n\n21.21300291\n\n\n23.11442257\n\n\n21.14297528\n\n\n20.49546467\n\n\n27.63631619\n\n\n29.08502321\n\n\n18.86516831\n\n\n24.53771674\n\n\n.\n\n\n27.11323357\n\n\n23.18392919\n\n\n25.3740699\n\n\n18.88798491\n\n\n23.11917863\n\n\n24.09593465\n\n\n.\n\n\n20.55925425\n\n\n20.55033738\n\n\n25.44316808\n\n\n27.00324576\n\n\n35.75474746\n\n\n27.10655879\n\n\n28.41959241\n\n\n25.61100906\n\n\n25.74114177\n\n\n22.6787383\n\n\n23.12313674\n\n\n23.19303473\n\n\n27.38161239\n\n\n24.42721458\n\n\n24.96238201\n\n\n31.3209053\n\n\n27.2418324\n\n\n28.87180265\n\n\n31.40387913\n\n\n28.13618565\n\n\n52.83200108\n\n\n.\n\n\n19.1475191\n\n\n18.92601692\n\n\n24.55336964\n\n\n.\n\n\n23.16185513\n\n\n28.19808579\n\n\n22.86738634\n\n\n26.4793665\n\n\n25.50395776\n\n\n22.97171356\n\n\n27.28733787\n\n\n.\n\n\n25.86142965\n\n\n27.81550396\n\n\n25.41758629\n\n\n29.3841401\n\n\n23.31688151\n\n\n27.37593644\n\n\n.\n\n\n18.48003655\n\n\n28.03299279\n\n\n21.81964853\n\n\n21.90071056\n\n\n21.48023542\n\n\n22.52083681\n\n\n32.37633391\n\n\n24.2994712\n\n\n39.97398303\n\n\n23.82407931\n\n\n-1\n\n\n21.04016765\n\n\n21.93229437\n\n\n20.32156911\n\n\n23.82709396\n\n\n28.42984563\n\n\n23.63758782\n\n\n26.66317736\n\n\n29.7871783\n\n\n20.45328693\n\n\n26.12614379\n\n\n.\n\n\n24.01051697\n\n\n20.50338106\n\n\n22.93502993\n\n\n15.94907484\n\n\n22.32890524\n\n\n.\n\n\n28.65654701\n\n\n.\n\n\n.\n\n\n-1\n\n\n27.67352694\n\n\n32.09934096\n\n\n25.26489314\n\n\n22.78496915\n\n\n19.46424875\n\n\n24.88397701\n\n\n29.81311982\n\n\n28.25227775\n\n\n32.08826931\n\n\n38.65914051\n\n\n23.25656561\n\n\n25.50323874\n\n\n20.26897643\n\n\n25.08566604\n\n\n35.79064871\n\n\n27.79921488\n\n\n24.19966298\n\n\n20.56620708\n\n\n21.18607757\n\n\n27.77930157\n\n\n25.04440237\n\n\n25.15198937\n\n\n22.06285248\n\n\n18.39836462\n\n\n25.98196667\n\n\n27.60948649\n\n\n22.70235129\n\n\n23.59385431\n\n\n25.10731367\n\n\n25.49591172\n\n\n28.73660806\n\n\n25.48820764\n\n\n26.53141906\n\n\n25.240448\n\n\n24.15118937\n\n\n29.13214251\n\n\n27.27886747\n\n\n24.11945752\n\n\n30.69995682\n\n\n24.1078779\n\n\n22.57218482\n\n\n23.29582716\n\n\n25.29452814\n\n\n22.81934695\n\n\n.\n\n\n.\n\n\n22.41075771\n\n\n.\n\n\n.\n\n\n19.29452213\n\n\n.\n\n\n23.94063712\n\n\n25.61306339\n\n\n.\n\n\n25.27897886\n\n\n21.60955418\n\n\n18.92856352\n\n\n24.80757793\n\n\n20.81839585\n\n\n24.38003957\n\n\n19.65948598\n\n\n29.93970644\n\n\n29.06454138\n\n\n28.04373163\n\n\n27.77368427\n\n\n26.87415373\n\n\n.\n\n\n27.06111492\n\n\n22.07283321\n\n\n24.66411569\n\n\n21.25546899\n\n\n21.02853516\n\n\n24.66647511\n\n\n19.67343041\n\n\n.\n\n\n20.765745\n\n\n43.70891905\n\n\n18.56434539\n\n\n20.35126802\n\n\n21.29446912\n\n\n30.3420679\n\n\n26.44258822\n\n\n29.70070737\n\n\n.\n\n\n31.25287012\n\n\n23.40761722\n\n\n21.30497829\n\n\n23.56576096\n\n\n.\n\n\n20.97126217\n\n\n.\n\n\n31.63275668\n\n\n23.24330532\n\n\n25.78177147\n\n\n26.57802507\n\n\n33.86495579\n\n\n33.85657049\n\n\n21.64816503\n\n\n25.76438795\n\n\n23.45984227\n\n\n25.38648178\n\n\n28.01379638\n\n\n23.82735628\n\n\n31.36090345\n\n\n999\n\n\n22.98272418\n\n\n25.00349948\n\n\n25.78447098\n\n\n29.60835074\n\n\n29.1969895\n\n\n22.72731883\n\n\n22.5646864\n\n\n25.95709107\n\n\n24.39166383\n\n\n16.36632199\n\n\n41.69309892\n\n\n28.20795384\n\n\n30.28645684\n\n\n29.09472819\n\n\n28.07357077\n\n\n22.26041649\n\n\n17.75841051\n\n\n28.60321468\n\n\n24.32225912\n\n\n.\n\n\n26.09369692\n\n\n22.19820782\n\n\n28.77441212\n\n\n35.04680226\n\n\n23.41189572\n\n\n21.7385706\n\n\n.\n\n\n24.66254127\n\n\n27.23470274\n\n\n.\n\n\n26.11232126\n\n\n22.49585319\n\n\n31.53348864\n\n\n30.47626679\n\n\n28.34689324\n\n\n.\n\n\n19.98898803\n\n\n23.67879897\n\n\n24.03890141\n\n\n34.73336894\n\n\n27.73570555\n\n\n31.25665366\n\n\n29.69288466\n\n\n33.07309324\n\n\n23.03830173\n\n\n29.45578341\n\n\n40.42919058\n\n\n23.3708753\n\n\n32.49817471\n\n\n32.92998871\n\n\n22.52525008\n\n\n18.31017725\n\n\n26.31260255\n\n\n31.05662635\n\n\n27.1414615\n\n\n26.29197994\n\n\n24.45293035\n\n\n25.17359645\n\n\n24.75901897\n\n\n23.2646736\n\n\n21.66760013\n\n\n999\n\n\n24.6655564\n\n\n25.09261338\n\n\n29.22136017\n\n\n19.04530818\n\n\n23.77700639\n\n\n28.30800481\n\n\n19.88013162\n\n\n28.52647285\n\n\n24.75293293\n\n\n23.18165683\n\n\n23.84158359\n\n\n.\n\n\n31.39361989\n\n\n27.63697221\n\n\n27.74355014\n\n\n21.76000361\n\n\n.\n\n\n22.26858218\n\n\n23.93236513\n\n\n26.38960703\n\n\n21.96744009\n\n\n.\n\n\n29.45289933\n\n\n20.93774407\n\n\n24.84362051\n\n\n25.06769978\n\n\n24.8835946\n\n\n.\n\n\n22.63539026\n\n\n25.8460716\n\n\n.\n\n\n28.94035559\n\n\n22.13490135\n\n\n25.89826938\n\n\n28.12203702\n\n\n31.3616119\n\n\n24.78823897\n\n\n28.03166941\n\n\n25.56051905\n\n\n24.58497896\n\n\n30.61119991\n\n\n18.97253817\n\n\n24.58617408\n\n\n18.20921595\n\n\n25.5261738\n\n\n28.8841452\n\n\n25.17463365\n\n\n25.77421287\n\n\n19.46671186\n\n\n26.62982004\n\n\n37.53737687\n\n\n27.11076128\n\n\n27.83875181\n\n\n29.10615014\n\n\n27.86820723\n\n\n24.93208174\n\n\n24.56035273\n\n\n19.38163732\n\n\n18.33992391\n\n\n21.67573448\n\n\n29.79192005\n\n\n23.3179443\n\n\n20.2403772\n\n\n19.82370896\n\n\n25.95856993\n\n\n20.23094602\n\n\n19.7490271\n\n\n.\n\n\n22.19075378\n\n\n23.74062371\n\n\n27.53645124\n\n\n22.4619877\n\n\n28.47401096\n\n\n.\n\n\n30.47318367\n\n\n24.40896489\n\n\n31.3103595\n\n\n23.59694525\n\n\n999\n\n\n25.58524177\n\n\n27.29940254\n\n\n.\n\n\n24.35545597\n\n\n29.26200293\n\n\n19.6772337\n\n\n24.64849497\n\n\n19.76755865\n\n\n27.51194871\n\n\n30.40278047\n\n\n24.68398259\n\n\n25.00580873\n\n\n20.86019639\n\n\n29.0083039\n\n\n35.65554328\n\n\n27.28175649\n\n\n27.06025704\n\n\n26.86154505\n\n\n27.63561304\n\n\n24.92811627\n\n\n23.95648654\n\n\n20.11673232\n\n\n18.19027457\n\n\n23.10657086\n\n\n30.74881679\n\n\n21.64005075\n\n\n20.81036296\n\n\n19.7199868\n\n\n24.61088563\n\n\n19.66638377\n\n\n20.44604571\n\n\n.\n\n\n22.48533437\n\n\n23.98503415\n\n\n25.85973443\n\n\n22.11218537\n\n\n29.09963218\n\n\n.\n\n\n30.38444125\n\n\n24.99743567\n\n\n28.11637953\n\n\n25.47150633\n\n\n999\n\n\n25.00659268\n\n\n24.74595378\n\n\n.\n\n\n\n\n39\n\n\n1\n\n\nHBP\n\n\nHigh Blood Pressure\n\n\n1\n\n\n2\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n3\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n3\n\n\n9\n\n\n1\n\n\n2\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n3\n\n\n1\n\n\n1\n\n\n1\n\n\n3\n\n\n9\n\n\n1\n\n\n2\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n3\n\n\n1\n\n\n1\n\n\n1\n\n\n3\n\n\n9\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n9\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n3\n\n\n9\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n9\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n2\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n9\n\n\n2\n\n\n2\n\n\n9\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n2\n\n\n9\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n9\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n3\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n3\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n3\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n9\n\n\n9\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n3\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n3\n\n\n2\n\n\n2\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n9\n\n\n1\n\n\n9\n\n\n2\n\n\n1\n\n\n3\n\n\n2\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n3\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n2\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n4\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n3\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n9\n\n\n2\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n3\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n3\n\n\n1\n\n\n2\n\n\n3\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n3\n\n\n1\n\n\n1\n\n\n9\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n9\n\n\n2\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n3\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n3\n\n\n1\n\n\n2\n\n\n3\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n3\n\n\n1\n\n\n1\n\n\n9\n\n\n\n\n40\n\n\n1\n\n\nDIAB\n\n\nDiabetes\n\n\n1\n\n\n9\n\n\n9\n\n\n3\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n3\n\n\n9\n\n\n1\n\n\n2\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n3\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n2\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n3\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n2\n\n\n9\n\n\n9\n\n\n3\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n3\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n3\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n3\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n2\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n3\n\n\n9\n\n\n3\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n3\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n2\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n3\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n3\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n3\n\n\n1\n\n\n3\n\n\n1\n\n\n1\n\n\n3\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n2\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n3\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n3\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n2\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n3\n\n\n3\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n3\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n3\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n3\n\n\n1\n\n\n1\n\n\n3\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n2\n\n\n9\n\n\n9\n\n\n9\n\n\n3\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n2\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n3\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n3\n\n\n9\n\n\n9\n\n\n9\n\n\n2\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n3\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n3\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n3\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n3\n\n\n3\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n2\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n2\n\n\n9\n\n\n9\n\n\n9\n\n\n3\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n3\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n3\n\n\n3\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n2\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n2\n\n\n9\n\n\n9\n\n\n9\n\n\n\n\n41\n\n\n1\n\n\nLIV34\n\n\nLiver Disease Stage 3/4\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n2\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n2\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n2\n\n\n9\n\n\n1\n\n\n2\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n2\n\n\n9\n\n\n1\n\n\n2\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n\n\n42\n\n\n1\n\n\nKID\n\n\nKidney Disease\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n3\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n2\n\n\n9\n\n\n9\n\n\n2\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n2\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n2\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n2\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n2\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n2\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n2\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n3\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n2\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n2\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n2\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n2\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n2\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n3\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n2\n\n\n9\n\n\n1\n\n\n3\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n2\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n2\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n2\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n2\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n2\n\n\n1\n\n\n2\n\n\n9\n\n\n9\n\n\n1\n\n\n2\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n2\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n2\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n2\n\n\n9\n\n\n9\n\n\n1\n\n\n2\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n2\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n2\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n\n\n43\n\n\n1\n\n\nFRP\n\n\nFrailty Related Phenotype\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n\n\n44\n\n\n1\n\n\nFP\n\n\nFrailty Phenotype\n\n\n1\n\n\n9\n\n\n1\n\n\n2\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n2\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n2\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n2\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n2\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n2\n\n\n9\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n2\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n2\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n2\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n2\n\n\n9\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n9\n\n\n9\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n2\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n2\n\n\n1\n\n\n\n\n45\n\n\n1\n\n\nTCHOL\n\n\nTotal Cholesterol\n\n\n131\n\n\nNA\n\n\n170\n\n\n197\n\n\n204\n\n\n216\n\n\n188\n\n\n171\n\n\n216\n\n\n140\n\n\n98\n\n\n98\n\n\n191\n\n\n144\n\n\n163\n\n\nNA\n\n\n222\n\n\n150\n\n\n138\n\n\n171\n\n\n216\n\n\n131\n\n\nNA\n\n\n170\n\n\n204\n\n\n216\n\n\n171\n\n\n216\n\n\n140\n\n\n98\n\n\n191\n\n\n144\n\n\nNA\n\n\n222\n\n\n150\n\n\n171\n\n\n216\n\n\n131\n\n\nNA\n\n\n170\n\n\n204\n\n\n216\n\n\n171\n\n\n216\n\n\n140\n\n\n98\n\n\n191\n\n\n144\n\n\nNA\n\n\n222\n\n\n150\n\n\n171\n\n\n216\n\n\n220\n\n\n162\n\n\nNA\n\n\n186\n\n\nNA\n\n\nNA\n\n\n190\n\n\nNA\n\n\n157\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n227\n\n\n157\n\n\n141\n\n\nNA\n\n\nNA\n\n\n217\n\n\n156\n\n\nNA\n\n\n144\n\n\n147\n\n\n199\n\n\nNA\n\n\n117\n\n\nNA\n\n\n184\n\n\nNA\n\n\n201\n\n\n209\n\n\nNA\n\n\nNA\n\n\n255\n\n\n122\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n163\n\n\n199\n\n\n128\n\n\n166\n\n\n209\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n147\n\n\n226\n\n\n189\n\n\n142\n\n\n183\n\n\nNA\n\n\n204\n\n\n172\n\n\n160\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n162\n\n\n224\n\n\n118\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n194\n\n\n162\n\n\n252\n\n\n244\n\n\n100\n\n\n136\n\n\n188\n\n\n131\n\n\n223\n\n\nNA\n\n\n136\n\n\n224\n\n\n180\n\n\nNA\n\n\n190\n\n\n196\n\n\nNA\n\n\n193\n\n\n189\n\n\nNA\n\n\n237\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n104\n\n\n208\n\n\n105\n\n\n155\n\n\n215\n\n\n176\n\n\nNA\n\n\n182\n\n\n164\n\n\n141\n\n\n150\n\n\n163\n\n\n150\n\n\nNA\n\n\n203\n\n\nNA\n\n\nNA\n\n\n118\n\n\n203\n\n\n159\n\n\nNA\n\n\n280\n\n\n153\n\n\nNA\n\n\n208\n\n\n198\n\n\nNA\n\n\n141\n\n\nNA\n\n\nNA\n\n\n174\n\n\n218\n\n\n150\n\n\n142\n\n\n175\n\n\n178\n\n\n191\n\n\nNA\n\n\n183\n\n\n229\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n267\n\n\nNA\n\n\n112\n\n\n143\n\n\nNA\n\n\n232\n\n\nNA\n\n\n137\n\n\n268\n\n\n274\n\n\n129\n\n\nNA\n\n\nNA\n\n\n337\n\n\nNA\n\n\nNA\n\n\n147\n\n\n161\n\n\n172\n\n\nNA\n\n\n176\n\n\nNA\n\n\nNA\n\n\n170\n\n\nNA\n\n\n116\n\n\n155\n\n\n203\n\n\nNA\n\n\nNA\n\n\n184\n\n\n169\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n217\n\n\nNA\n\n\n197\n\n\n267\n\n\n193\n\n\nNA\n\n\nNA\n\n\n190\n\n\n250\n\n\n107\n\n\nNA\n\n\n222\n\n\n115\n\n\nNA\n\n\n133\n\n\n227\n\n\n220\n\n\n200\n\n\n142\n\n\nNA\n\n\n242\n\n\n126\n\n\n143\n\n\n203\n\n\n181\n\n\n181\n\n\n143\n\n\nNA\n\n\n163\n\n\nNA\n\n\n233\n\n\nNA\n\n\nNA\n\n\n186\n\n\nNA\n\n\n72\n\n\n207\n\n\n161\n\n\n175\n\n\n222\n\n\n176\n\n\n200\n\n\n181\n\n\n198\n\n\nNA\n\n\n135\n\n\n148\n\n\n204\n\n\nNA\n\n\nNA\n\n\n292\n\n\n159\n\n\n131\n\n\nNA\n\n\n159\n\n\nNA\n\n\nNA\n\n\n153\n\n\n166\n\n\n152\n\n\n216\n\n\nNA\n\n\n183\n\n\n145\n\n\nNA\n\n\n213\n\n\n188\n\n\nNA\n\n\n188\n\n\n171\n\n\n155\n\n\n149\n\n\n184\n\n\nNA\n\n\n216\n\n\n164\n\n\nNA\n\n\n194\n\n\nNA\n\n\nNA\n\n\n235\n\n\n215\n\n\nNA\n\n\n140\n\n\nNA\n\n\n123\n\n\n219\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n145\n\n\n184\n\n\nNA\n\n\n98\n\n\n204\n\n\nNA\n\n\n308\n\n\n158\n\n\nNA\n\n\nNA\n\n\n135\n\n\nNA\n\n\n149\n\n\n125\n\n\n144\n\n\n204\n\n\n139\n\n\nNA\n\n\n219\n\n\n186\n\n\nNA\n\n\n98\n\n\nNA\n\n\nNA\n\n\n154\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n288\n\n\n136\n\n\nNA\n\n\n136\n\n\nNA\n\n\nNA\n\n\n191\n\n\n144\n\n\n167\n\n\n234\n\n\n141\n\n\n163\n\n\n205\n\n\n186\n\n\n229\n\n\n198\n\n\n236\n\n\nNA\n\n\n174\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n214\n\n\n182\n\n\n225\n\n\n189\n\n\n143\n\n\nNA\n\n\n181\n\n\n158\n\n\nNA\n\n\n164\n\n\n158\n\n\n216\n\n\n109\n\n\n162\n\n\n150\n\n\nNA\n\n\n150\n\n\n231\n\n\n136\n\n\n209\n\n\n131\n\n\n178\n\n\nNA\n\n\n195\n\n\n195\n\n\n154\n\n\nNA\n\n\n100\n\n\n216\n\n\n209\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n194\n\n\n149\n\n\n222\n\n\nNA\n\n\nNA\n\n\n186\n\n\n150\n\n\nNA\n\n\n149\n\n\n215\n\n\n232\n\n\n180\n\n\n249\n\n\n138\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n146\n\n\n348\n\n\nNA\n\n\nNA\n\n\n117\n\n\nNA\n\n\n225\n\n\nNA\n\n\n171\n\n\n136\n\n\n180\n\n\n252\n\n\nNA\n\n\nNA\n\n\n265\n\n\nNA\n\n\n200\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n171\n\n\nNA\n\n\n237\n\n\n175\n\n\n182\n\n\n172\n\n\n219\n\n\n154\n\n\n141\n\n\n257\n\n\n139\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n216\n\n\nNA\n\n\n159\n\n\n155\n\n\n169\n\n\n179\n\n\nNA\n\n\n119\n\n\n262\n\n\n162\n\n\nNA\n\n\nNA\n\n\n163\n\n\n128\n\n\nNA\n\n\n183\n\n\n131\n\n\n141\n\n\nNA\n\n\nNA\n\n\n267\n\n\n112\n\n\nNA\n\n\n170\n\n\n116\n\n\n222\n\n\n204\n\n\n159\n\n\n153\n\n\n216\n\n\n171\n\n\n216\n\n\n235\n\n\n140\n\n\nNA\n\n\n125\n\n\n98\n\n\nNA\n\n\n288\n\n\n191\n\n\n144\n\n\nNA\n\n\n150\n\n\n150\n\n\nNA\n\n\n222\n\n\n150\n\n\nNA\n\n\n171\n\n\nNA\n\n\nNA\n\n\n216\n\n\n162\n\n\nNA\n\n\nNA\n\n\n163\n\n\n128\n\n\nNA\n\n\n183\n\n\n131\n\n\n141\n\n\nNA\n\n\nNA\n\n\n267\n\n\n112\n\n\nNA\n\n\n170\n\n\n116\n\n\n222\n\n\n204\n\n\n159\n\n\n153\n\n\n216\n\n\n171\n\n\n216\n\n\n235\n\n\n140\n\n\nNA\n\n\n125\n\n\n98\n\n\nNA\n\n\n288\n\n\n191\n\n\n144\n\n\nNA\n\n\n150\n\n\n150\n\n\nNA\n\n\n222\n\n\n150\n\n\nNA\n\n\n171\n\n\nNA\n\n\nNA\n\n\n216\n\n\n\n\n46\n\n\n1\n\n\nTRIG\n\n\nTriglycerides\n\n\n107\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n192\n\n\nNA\n\n\n148\n\n\n74\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n87\n\n\n59\n\n\nNA\n\n\n118\n\n\n107\n\n\n89\n\n\nNA\n\n\nNA\n\n\n107\n\n\nNA\n\n\nNA\n\n\n192\n\n\nNA\n\n\n74\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n87\n\n\nNA\n\n\n118\n\n\n107\n\n\nNA\n\n\nNA\n\n\n107\n\n\nNA\n\n\nNA\n\n\n192\n\n\nNA\n\n\n74\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n87\n\n\nNA\n\n\n118\n\n\n107\n\n\nNA\n\n\nNA\n\n\n372\n\n\nNA\n\n\nNA\n\n\n146\n\n\nNA\n\n\nNA\n\n\n92\n\n\nNA\n\n\n150\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n86\n\n\n78\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n240\n\n\nNA\n\n\n58\n\n\n84\n\n\nNA\n\n\nNA\n\n\n85\n\n\nNA\n\n\n92\n\n\nNA\n\n\n142\n\n\n159\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n61\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n143\n\n\n417\n\n\n174\n\n\n89\n\n\n96\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n177\n\n\n298\n\n\nNA\n\n\nNA\n\n\n140\n\n\nNA\n\n\nNA\n\n\n107\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n127\n\n\n320\n\n\n164\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n57\n\n\n92\n\n\n197\n\n\n48\n\n\n183\n\n\n38\n\n\n107\n\n\nNA\n\n\nNA\n\n\n718\n\n\n101\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n106\n\n\n118\n\n\nNA\n\n\n596\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n327\n\n\n102\n\n\n201\n\n\nNA\n\n\nNA\n\n\n70\n\n\n108\n\n\n114\n\n\n98\n\n\n86\n\n\nNA\n\n\n70\n\n\nNA\n\n\nNA\n\n\n159\n\n\nNA\n\n\n53\n\n\nNA\n\n\n237\n\n\nNA\n\n\nNA\n\n\n116\n\n\n67\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n107\n\n\n44\n\n\n110\n\n\n122\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n117\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n55\n\n\n143\n\n\nNA\n\n\n402\n\n\nNA\n\n\n65\n\n\n223\n\n\n744\n\n\n52\n\n\nNA\n\n\nNA\n\n\n468\n\n\nNA\n\n\nNA\n\n\n125\n\n\n246\n\n\n129\n\n\nNA\n\n\n289\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n92\n\n\nNA\n\n\n134\n\n\nNA\n\n\nNA\n\n\n209\n\n\n49\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n83\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n46\n\n\n146\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n94\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n104\n\n\n115\n\n\nNA\n\n\n279\n\n\n73\n\n\n119\n\n\n71\n\n\n99\n\n\nNA\n\n\n74\n\n\nNA\n\n\n38\n\n\nNA\n\n\n110\n\n\nNA\n\n\nNA\n\n\n133\n\n\nNA\n\n\nNA\n\n\n140\n\n\nNA\n\n\n105\n\n\n87\n\n\nNA\n\n\n192\n\n\n196\n\n\n143\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n192\n\n\nNA\n\n\nNA\n\n\n139\n\n\n86\n\n\nNA\n\n\nNA\n\n\n245\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n78\n\n\nNA\n\n\nNA\n\n\n133\n\n\nNA\n\n\nNA\n\n\n96\n\n\n148\n\n\nNA\n\n\n344\n\n\n74\n\n\nNA\n\n\n448\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n59\n\n\nNA\n\n\nNA\n\n\n271\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n108\n\n\n174\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n62\n\n\n150\n\n\nNA\n\n\nNA\n\n\n438\n\n\nNA\n\n\n343\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n177\n\n\nNA\n\n\n175\n\n\n67\n\n\n83\n\n\n188\n\n\n98\n\n\nNA\n\n\n98\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n139\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n167\n\n\n86\n\n\nNA\n\n\n75\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n87\n\n\n93\n\n\n204\n\n\n142\n\n\n59\n\n\n99\n\n\n231\n\n\n235\n\n\n334\n\n\n151\n\n\nNA\n\n\n136\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n181\n\n\n65\n\n\nNA\n\n\n218\n\n\n58\n\n\nNA\n\n\n240\n\n\n96\n\n\n189\n\n\nNA\n\n\n136\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n185\n\n\n148\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n104\n\n\n316\n\n\n108\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n102\n\n\n72\n\n\n118\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n107\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n292\n\n\n229\n\n\n187\n\n\n89\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n117\n\n\n197\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n155\n\n\nNA\n\n\nNA\n\n\n106\n\n\nNA\n\n\n248\n\n\nNA\n\n\nNA\n\n\n133\n\n\nNA\n\n\n710\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n221\n\n\nNA\n\n\nNA\n\n\n226\n\n\n179\n\n\n156\n\n\n53\n\n\n93\n\n\n103\n\n\nNA\n\n\n69\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n55\n\n\n420\n\n\n170\n\n\nNA\n\n\nNA\n\n\n71\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n143\n\n\n174\n\n\nNA\n\n\n140\n\n\n107\n\n\n108\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n55\n\n\nNA\n\n\nNA\n\n\n92\n\n\nNA\n\n\n192\n\n\n86\n\n\nNA\n\n\nNA\n\n\n74\n\n\nNA\n\n\n271\n\n\nNA\n\n\nNA\n\n\n67\n\n\nNA\n\n\nNA\n\n\n167\n\n\nNA\n\n\n87\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n118\n\n\n107\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n143\n\n\n174\n\n\nNA\n\n\n140\n\n\n107\n\n\n108\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n55\n\n\nNA\n\n\nNA\n\n\n92\n\n\nNA\n\n\n192\n\n\n86\n\n\nNA\n\n\nNA\n\n\n74\n\n\nNA\n\n\n271\n\n\nNA\n\n\nNA\n\n\n67\n\n\nNA\n\n\nNA\n\n\n167\n\n\nNA\n\n\n87\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n118\n\n\n107\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n\n\n47\n\n\n1\n\n\nLDL\n\n\nLDL\n\n\n66\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n121\n\n\n104\n\n\nNA\n\n\n74\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n75\n\n\n49\n\n\nNA\n\n\n132\n\n\n100\n\n\n81\n\n\nNA\n\n\nNA\n\n\n66\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n104\n\n\nNA\n\n\n74\n\n\nNA\n\n\nNA\n\n\n75\n\n\nNA\n\n\n132\n\n\n100\n\n\nNA\n\n\nNA\n\n\n66\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n104\n\n\nNA\n\n\n74\n\n\nNA\n\n\nNA\n\n\n75\n\n\nNA\n\n\n132\n\n\n100\n\n\nNA\n\n\nNA\n\n\n146\n\n\nNA\n\n\nNA\n\n\n97\n\n\nNA\n\n\nNA\n\n\n129\n\n\nNA\n\n\n90\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n96\n\n\n68\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n67\n\n\nNA\n\n\n55\n\n\n74\n\n\nNA\n\n\nNA\n\n\n57\n\n\nNA\n\n\n126\n\n\nNA\n\n\n124\n\n\n135\n\n\nNA\n\n\nNA\n\n\n151\n\n\n70\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n95\n\n\nNA\n\n\n67\n\n\n71\n\n\n154\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n92\n\n\n130\n\n\nNA\n\n\n82\n\n\n106\n\n\nNA\n\n\nNA\n\n\n118\n\n\n92\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n104\n\n\n93\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n102\n\n\n193\n\n\n158\n\n\n60\n\n\n66\n\n\n141\n\n\n66\n\n\nNA\n\n\nNA\n\n\n56\n\n\n135\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n133\n\n\n132\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n61\n\n\n125\n\n\n101\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n88\n\n\n99\n\n\n101\n\n\n101\n\n\nNA\n\n\n125\n\n\nNA\n\n\nNA\n\n\n31\n\n\nNA\n\n\n86\n\n\nNA\n\n\n173\n\n\nNA\n\n\nNA\n\n\n131\n\n\n143\n\n\nNA\n\n\n95\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n95\n\n\n90\n\n\n114\n\n\n120\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n128\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n42\n\n\n47\n\n\nNA\n\n\n145\n\n\nNA\n\n\n68\n\n\nNA\n\n\nNA\n\n\n68\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n75\n\n\n74\n\n\n102\n\n\nNA\n\n\n80\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n66\n\n\nNA\n\n\n137\n\n\nNA\n\n\nNA\n\n\n111\n\n\n110\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n134\n\n\nNA\n\n\nNA\n\n\n150\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n141\n\n\n178\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n50\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n120\n\n\n89\n\n\nNA\n\n\n144\n\n\n75\n\n\n85\n\n\n129\n\n\n115\n\n\n114\n\n\n84\n\n\nNA\n\n\n95\n\n\nNA\n\n\n161\n\n\nNA\n\n\nNA\n\n\n121\n\n\nNA\n\n\nNA\n\n\n142\n\n\nNA\n\n\n108\n\n\n153\n\n\nNA\n\n\n119\n\n\n111\n\n\n137\n\n\nNA\n\n\nNA\n\n\n73\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n217\n\n\n74\n\n\nNA\n\n\nNA\n\n\n69\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n88\n\n\nNA\n\n\nNA\n\n\n116\n\n\nNA\n\n\nNA\n\n\n145\n\n\n121\n\n\nNA\n\n\nNA\n\n\n104\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n126\n\n\nNA\n\n\nNA\n\n\n145\n\n\nNA\n\n\nNA\n\n\n74\n\n\nNA\n\n\n68\n\n\n140\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n76\n\n\n120\n\n\nNA\n\n\nNA\n\n\n83\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n62\n\n\nNA\n\n\n78\n\n\n75\n\n\n78\n\n\n130\n\n\n51\n\n\nNA\n\n\n148\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n87\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n199\n\n\n80\n\n\nNA\n\n\n65\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n75\n\n\n90\n\n\n158\n\n\n73\n\n\n49\n\n\n137\n\n\n90\n\n\n144\n\n\n97\n\n\n164\n\n\nNA\n\n\n106\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n112\n\n\n81\n\n\nNA\n\n\n100\n\n\n80\n\n\nNA\n\n\n78\n\n\n106\n\n\n137\n\n\n52\n\n\n88\n\n\nNA\n\n\nNA\n\n\n90\n\n\n114\n\n\nNA\n\n\n131\n\n\n50\n\n\nNA\n\n\nNA\n\n\n94\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n36\n\n\n104\n\n\n130\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n125\n\n\n93\n\n\n132\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n100\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n93\n\n\n145\n\n\n81\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n158\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n151\n\n\nNA\n\n\nNA\n\n\n81\n\n\nNA\n\n\n161\n\n\nNA\n\n\nNA\n\n\n101\n\n\nNA\n\n\n90\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n84\n\n\nNA\n\n\nNA\n\n\n97\n\n\n103\n\n\n99\n\n\n98\n\n\n95\n\n\n87\n\n\nNA\n\n\n84\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n91\n\n\nNA\n\n\n92\n\n\nNA\n\n\nNA\n\n\n71\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n95\n\n\n67\n\n\nNA\n\n\n106\n\n\n66\n\n\n88\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n42\n\n\nNA\n\n\nNA\n\n\n66\n\n\nNA\n\n\nNA\n\n\n74\n\n\nNA\n\n\nNA\n\n\n104\n\n\nNA\n\n\n145\n\n\n74\n\n\nNA\n\n\n75\n\n\nNA\n\n\nNA\n\n\n199\n\n\nNA\n\n\n75\n\n\nNA\n\n\nNA\n\n\n90\n\n\nNA\n\n\n132\n\n\n100\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n95\n\n\n67\n\n\nNA\n\n\n106\n\n\n66\n\n\n88\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n42\n\n\nNA\n\n\nNA\n\n\n66\n\n\nNA\n\n\nNA\n\n\n74\n\n\nNA\n\n\nNA\n\n\n104\n\n\nNA\n\n\n145\n\n\n74\n\n\nNA\n\n\n75\n\n\nNA\n\n\nNA\n\n\n199\n\n\nNA\n\n\n75\n\n\nNA\n\n\nNA\n\n\n90\n\n\nNA\n\n\n132\n\n\n100\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n\n\n48\n\n\n1\n\n\nDYSLIP\n\n\nDyslipidemia\n\n\n1\n\n\n9\n\n\n2\n\n\n4\n\n\n9\n\n\n9\n\n\n2\n\n\n1\n\n\n9\n\n\n9\n\n\n2\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n2\n\n\n2\n\n\n9\n\n\n4\n\n\n1\n\n\n9\n\n\n2\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n2\n\n\n9\n\n\n4\n\n\n1\n\n\n9\n\n\n2\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n2\n\n\n9\n\n\n4\n\n\n2\n\n\n2\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n2\n\n\n9\n\n\n2\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n2\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n2\n\n\n2\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n2\n\n\n9\n\n\n2\n\n\n2\n\n\n9\n\n\n9\n\n\n9\n\n\n2\n\n\n9\n\n\n9\n\n\n9\n\n\n2\n\n\n2\n\n\n2\n\n\n1\n\n\n2\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n2\n\n\n2\n\n\n9\n\n\n2\n\n\n1\n\n\n9\n\n\n2\n\n\n2\n\n\n2\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n2\n\n\n2\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n2\n\n\n2\n\n\n2\n\n\n9\n\n\n2\n\n\n1\n\n\n9\n\n\n9\n\n\n2\n\n\n2\n\n\n2\n\n\n9\n\n\n9\n\n\n2\n\n\n9\n\n\n2\n\n\n2\n\n\n9\n\n\n2\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n2\n\n\n9\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n9\n\n\n9\n\n\n9\n\n\n2\n\n\n2\n\n\n1\n\n\n2\n\n\n9\n\n\n2\n\n\n9\n\n\n9\n\n\n2\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n2\n\n\n9\n\n\n2\n\n\n9\n\n\n9\n\n\n2\n\n\n4\n\n\n9\n\n\n3\n\n\n2\n\n\n2\n\n\n1\n\n\n2\n\n\n2\n\n\n2\n\n\n9\n\n\n9\n\n\n2\n\n\n9\n\n\n9\n\n\n9\n\n\n4\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n9\n\n\n9\n\n\n2\n\n\n9\n\n\n9\n\n\n2\n\n\n2\n\n\n1\n\n\n9\n\n\n2\n\n\n9\n\n\n9\n\n\n2\n\n\n9\n\n\n2\n\n\n2\n\n\n2\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n2\n\n\n9\n\n\n4\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n2\n\n\n2\n\n\n2\n\n\n9\n\n\n2\n\n\n1\n\n\n9\n\n\n3\n\n\n9\n\n\n9\n\n\n2\n\n\n2\n\n\n9\n\n\n9\n\n\n2\n\n\n2\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n2\n\n\n9\n\n\n9\n\n\n2\n\n\n9\n\n\n3\n\n\n2\n\n\n2\n\n\n1\n\n\n2\n\n\n9\n\n\n2\n\n\n2\n\n\n2\n\n\n9\n\n\n9\n\n\n2\n\n\n9\n\n\n9\n\n\n9\n\n\n2\n\n\n1\n\n\n9\n\n\n9\n\n\n2\n\n\n9\n\n\n9\n\n\n9\n\n\n2\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n2\n\n\n2\n\n\n9\n\n\n2\n\n\n1\n\n\n9\n\n\n2\n\n\n4\n\n\n9\n\n\n9\n\n\n2\n\n\n9\n\n\n1\n\n\n4\n\n\n9\n\n\n2\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n2\n\n\n2\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n2\n\n\n9\n\n\n2\n\n\n2\n\n\n9\n\n\n2\n\n\n4\n\n\n9\n\n\n9\n\n\n2\n\n\n9\n\n\n2\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n9\n\n\n2\n\n\n2\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n2\n\n\n9\n\n\n4\n\n\n9\n\n\n9\n\n\n2\n\n\n2\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n2\n\n\n2\n\n\n1\n\n\n4\n\n\n2\n\n\n1\n\n\n9\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n1\n\n\n2\n\n\n9\n\n\n2\n\n\n9\n\n\n2\n\n\n2\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n2\n\n\n9\n\n\n1\n\n\n2\n\n\n2\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n4\n\n\n2\n\n\n9\n\n\n2\n\n\n9\n\n\n9\n\n\n2\n\n\n2\n\n\n2\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n2\n\n\n9\n\n\n9\n\n\n2\n\n\n9\n\n\n2\n\n\n9\n\n\n9\n\n\n2\n\n\n9\n\n\n2\n\n\n9\n\n\n9\n\n\n2\n\n\n9\n\n\n2\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n2\n\n\n9\n\n\n9\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n1\n\n\n2\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n3\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n4\n\n\n9\n\n\n1\n\n\n2\n\n\n2\n\n\n2\n\n\n9\n\n\n2\n\n\n4\n\n\n2\n\n\n9\n\n\n9\n\n\n2\n\n\n2\n\n\n9\n\n\n1\n\n\n1\n\n\n2\n\n\n9\n\n\n4\n\n\n4\n\n\n1\n\n\n9\n\n\n2\n\n\n2\n\n\n2\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n2\n\n\n9\n\n\n9\n\n\n2\n\n\n9\n\n\n4\n\n\n2\n\n\n9\n\n\n1\n\n\n9\n\n\n2\n\n\n2\n\n\n9\n\n\n9\n\n\n2\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n4\n\n\n2\n\n\n9\n\n\n9\n\n\n2\n\n\n2\n\n\n9\n\n\n1\n\n\n1\n\n\n2\n\n\n9\n\n\n4\n\n\n4\n\n\n1\n\n\n9\n\n\n2\n\n\n2\n\n\n2\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n2\n\n\n9\n\n\n9\n\n\n2\n\n\n9\n\n\n4\n\n\n2\n\n\n9\n\n\n1\n\n\n9\n\n\n2\n\n\n2\n\n\n9\n\n\n9\n\n\n2\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n4\n\n\n\n\n49\n\n\n1\n\n\nCESD\n\n\nCESD Depression Score\n\n\n2\n\n\n18\n\n\n22\n\n\n25\n\n\n0\n\n\n3\n\n\n15\n\n\n10\n\n\n6\n\n\n14\n\n\n23\n\n\n14\n\n\n6\n\n\n9\n\n\n9\n\n\n4\n\n\n20\n\n\n10\n\n\n22\n\n\n37\n\n\n23\n\n\n2\n\n\n18\n\n\n21\n\n\n0\n\n\n5\n\n\n10\n\n\n6\n\n\n14\n\n\n13\n\n\n6\n\n\n9\n\n\n4\n\n\n20\n\n\n10\n\n\n37\n\n\n23\n\n\n2\n\n\n18\n\n\n20\n\n\n0\n\n\n6\n\n\n10\n\n\n6\n\n\n14\n\n\n14\n\n\n6\n\n\n9\n\n\n4\n\n\n20\n\n\n10\n\n\n37\n\n\n23\n\n\n1\n\n\n26\n\n\n3\n\n\n-1\n\n\n1\n\n\n11\n\n\n8\n\n\n44\n\n\n14\n\n\n12\n\n\n28\n\n\n4\n\n\n31\n\n\n3\n\n\n11\n\n\n5\n\n\n5\n\n\n19\n\n\n5\n\n\n1\n\n\n4\n\n\n13\n\n\n12\n\n\n1\n\n\n4\n\n\n7\n\n\n4\n\n\n9\n\n\n2\n\n\n0\n\n\n30\n\n\n7\n\n\n21\n\n\n26\n\n\n5\n\n\n11\n\n\n20\n\n\n1\n\n\n15\n\n\n2\n\n\n9\n\n\n10\n\n\n10\n\n\n1\n\n\n7\n\n\n8\n\n\n10\n\n\n0\n\n\n10\n\n\n15\n\n\n8\n\n\n23\n\n\n1\n\n\n1\n\n\n23\n\n\n0\n\n\n0\n\n\n7\n\n\n9\n\n\n1\n\n\n12\n\n\n21\n\n\n0\n\n\n16\n\n\n14\n\n\n9\n\n\n2\n\n\n13\n\n\n4\n\n\n16\n\n\n5\n\n\n15\n\n\n25\n\n\n22\n\n\n15\n\n\n2\n\n\n1\n\n\n23\n\n\n6\n\n\n4\n\n\n3\n\n\n0\n\n\n3\n\n\n5\n\n\n3\n\n\n1\n\n\n21\n\n\n10\n\n\n14\n\n\n0\n\n\n1\n\n\n7\n\n\n0\n\n\n3\n\n\n2\n\n\n28\n\n\n43\n\n\n6\n\n\n26\n\n\n9\n\n\n5\n\n\n8\n\n\n2\n\n\n10\n\n\n39\n\n\n0\n\n\n14\n\n\n6\n\n\n10\n\n\n13\n\n\n29\n\n\n25\n\n\n23\n\n\n0\n\n\n2\n\n\n7\n\n\n16\n\n\n17\n\n\n13\n\n\n29\n\n\n7\n\n\n8\n\n\n0\n\n\n9\n\n\n53\n\n\n8\n\n\n7\n\n\n28\n\n\n11\n\n\n27\n\n\n22\n\n\n20\n\n\n5\n\n\n17\n\n\n0\n\n\n15\n\n\n7\n\n\n5\n\n\n1\n\n\n12\n\n\n18\n\n\n31\n\n\n0\n\n\n28\n\n\n0\n\n\n8\n\n\n1\n\n\n13\n\n\n0\n\n\n10\n\n\n3\n\n\n6\n\n\n49\n\n\n21\n\n\n37\n\n\n25\n\n\n15\n\n\n0\n\n\n2\n\n\n22\n\n\n5\n\n\n11\n\n\n12\n\n\n44\n\n\n0\n\n\n5\n\n\n20\n\n\n1\n\n\n17\n\n\n13\n\n\n12\n\n\n14\n\n\n15\n\n\n25\n\n\n1\n\n\n3\n\n\n18\n\n\n9\n\n\n9\n\n\n8\n\n\n11\n\n\n18\n\n\n1\n\n\n10\n\n\n16\n\n\n13\n\n\n2\n\n\n20\n\n\n2\n\n\n13\n\n\n14\n\n\n9\n\n\n2\n\n\n18\n\n\n53\n\n\n6\n\n\n3\n\n\n12\n\n\n23\n\n\n35\n\n\n2\n\n\n2\n\n\n10\n\n\n7\n\n\n38\n\n\n-1\n\n\n1\n\n\n9\n\n\n19\n\n\n28\n\n\n10\n\n\n0\n\n\n12\n\n\n0\n\n\n6\n\n\n5\n\n\n4\n\n\n5\n\n\n0\n\n\n7\n\n\n1\n\n\n0\n\n\n28\n\n\n27\n\n\n3\n\n\n1\n\n\n6\n\n\n6\n\n\n25\n\n\n7\n\n\n14\n\n\n3\n\n\n11\n\n\n7\n\n\n11\n\n\n0\n\n\n4\n\n\n15\n\n\n6\n\n\n12\n\n\n10\n\n\n49\n\n\n33\n\n\n4\n\n\n0\n\n\n6\n\n\n10\n\n\n3\n\n\n16\n\n\n13\n\n\n0\n\n\n14\n\n\n24\n\n\n11\n\n\n14\n\n\n48\n\n\n17\n\n\n19\n\n\n5\n\n\n40\n\n\n2\n\n\n4\n\n\n16\n\n\n6\n\n\n33\n\n\n23\n\n\n8\n\n\n2\n\n\n15\n\n\n4\n\n\n5\n\n\n1\n\n\n35\n\n\n12\n\n\n2\n\n\n33\n\n\n2\n\n\n7\n\n\n9\n\n\n1\n\n\n1\n\n\n6\n\n\n6\n\n\n14\n\n\n8\n\n\n41\n\n\n14\n\n\n5\n\n\n5\n\n\n30\n\n\n23\n\n\n6\n\n\n23\n\n\n15\n\n\n12\n\n\n0\n\n\n0\n\n\n6\n\n\n9\n\n\n30\n\n\n0\n\n\n12\n\n\n9\n\n\n7\n\n\n13\n\n\n-1\n\n\n5\n\n\n13\n\n\n0\n\n\n0\n\n\n22\n\n\n2\n\n\n4\n\n\n6\n\n\n37\n\n\n9\n\n\n4\n\n\n9\n\n\n2\n\n\n17\n\n\n2\n\n\n6\n\n\n4\n\n\n3\n\n\n21\n\n\n2\n\n\n23\n\n\n19\n\n\n3\n\n\n-1\n\n\n13\n\n\n35\n\n\n4\n\n\n9\n\n\n20\n\n\n5\n\n\n7\n\n\n35\n\n\n12\n\n\n3\n\n\n18\n\n\n13\n\n\n0\n\n\n4\n\n\n44\n\n\n4\n\n\n31\n\n\n37\n\n\n20\n\n\n23\n\n\n7\n\n\n0\n\n\n10\n\n\n35\n\n\n3\n\n\n12\n\n\n4\n\n\n10\n\n\n15\n\n\n22\n\n\n17\n\n\n5\n\n\n16\n\n\n1\n\n\n5\n\n\n0\n\n\n17\n\n\n19\n\n\n4\n\n\n15\n\n\n23\n\n\n37\n\n\n3\n\n\n38\n\n\n18\n\n\n18\n\n\n0\n\n\n4\n\n\n13\n\n\n0\n\n\n6\n\n\n28\n\n\n28\n\n\n27\n\n\n1\n\n\n21\n\n\n1\n\n\n12\n\n\n1\n\n\n6\n\n\n3\n\n\n24\n\n\n19\n\n\n17\n\n\n12\n\n\n5\n\n\n3\n\n\n7\n\n\n1\n\n\n7\n\n\n19\n\n\n32\n\n\n23\n\n\n1\n\n\n36\n\n\n8\n\n\n4\n\n\n6\n\n\n20\n\n\n7\n\n\n1\n\n\n26\n\n\n29\n\n\n31\n\n\n15\n\n\n9\n\n\n8\n\n\n23\n\n\n2\n\n\n2\n\n\n14\n\n\n10\n\n\n7\n\n\n1\n\n\n18\n\n\n21\n\n\n11\n\n\n1\n\n\n0\n\n\n28\n\n\n25\n\n\n5\n\n\n10\n\n\n6\n\n\n14\n\n\n14\n\n\n40\n\n\n33\n\n\n13\n\n\n5\n\n\n6\n\n\n6\n\n\n9\n\n\n22\n\n\n19\n\n\n-1\n\n\n4\n\n\n20\n\n\n10\n\n\n3\n\n\n37\n\n\n3\n\n\n33\n\n\n23\n\n\n26\n\n\n28\n\n\n30\n\n\n15\n\n\n9\n\n\n8\n\n\n23\n\n\n2\n\n\n2\n\n\n14\n\n\n8\n\n\n7\n\n\n2\n\n\n18\n\n\n20\n\n\n11\n\n\n1\n\n\n0\n\n\n28\n\n\n25\n\n\n6\n\n\n10\n\n\n6\n\n\n14\n\n\n14\n\n\n40\n\n\n33\n\n\n14\n\n\n5\n\n\n6\n\n\n6\n\n\n9\n\n\n22\n\n\n19\n\n\n-1\n\n\n4\n\n\n20\n\n\n10\n\n\n5\n\n\n37\n\n\n3\n\n\n31\n\n\n23\n\n\n\n\n50\n\n\n1\n\n\nSMOKE\n\n\nSmoking Status\n\n\n3\n\n\n3\n\n\n2\n\n\n3\n\n\n3\n\n\n1\n\n\n3\n\n\n2\n\n\n2\n\n\n3\n\n\n3\n\n\n3\n\n\n3\n\n\n3\n\n\n3\n\n\n2\n\n\n3\n\n\n3\n\n\n3\n\n\n3\n\n\n1\n\n\n3\n\n\n3\n\n\n2\n\n\n3\n\n\n1\n\n\n2\n\n\n2\n\n\n3\n\n\n3\n\n\n3\n\n\n3\n\n\n2\n\n\n3\n\n\n3\n\n\n3\n\n\n1\n\n\n3\n\n\n3\n\n\n2\n\n\n3\n\n\n1\n\n\n2\n\n\n2\n\n\n3\n\n\n3\n\n\n3\n\n\n3\n\n\n2\n\n\n3\n\n\n3\n\n\n3\n\n\n1\n\n\n1\n\n\n3\n\n\n2\n\n\n3\n\n\n2\n\n\n3\n\n\n2\n\n\n2\n\n\n3\n\n\n1\n\n\n1\n\n\n2\n\n\n3\n\n\n2\n\n\n3\n\n\n2\n\n\n1\n\n\n2\n\n\n3\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n2\n\n\n2\n\n\n1\n\n\n2\n\n\n2\n\n\n2\n\n\n3\n\n\n3\n\n\n2\n\n\n2\n\n\n3\n\n\n2\n\n\n2\n\n\n2\n\n\n1\n\n\n3\n\n\n2\n\n\n3\n\n\n1\n\n\n3\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n3\n\n\n1\n\n\n2\n\n\n2\n\n\n2\n\n\n1\n\n\n2\n\n\n3\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n3\n\n\n1\n\n\n2\n\n\n3\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n3\n\n\n2\n\n\n3\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n3\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n3\n\n\n2\n\n\n3\n\n\n3\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n2\n\n\n3\n\n\n1\n\n\n1\n\n\n3\n\n\n3\n\n\n2\n\n\n1\n\n\n2\n\n\n3\n\n\n1\n\n\n3\n\n\n1\n\n\n3\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n3\n\n\n2\n\n\n2\n\n\n2\n\n\n3\n\n\n2\n\n\n2\n\n\n3\n\n\n2\n\n\n1\n\n\n3\n\n\n1\n\n\n2\n\n\n2\n\n\n2\n\n\n1\n\n\n2\n\n\n3\n\n\n3\n\n\n2\n\n\n2\n\n\n3\n\n\n3\n\n\n3\n\n\n2\n\n\n2\n\n\n2\n\n\n3\n\n\n1\n\n\n3\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n3\n\n\n3\n\n\n3\n\n\n3\n\n\n3\n\n\n2\n\n\n2\n\n\n3\n\n\n3\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n2\n\n\n3\n\n\n3\n\n\n3\n\n\n1\n\n\n1\n\n\n3\n\n\n3\n\n\n2\n\n\n3\n\n\n3\n\n\n3\n\n\n2\n\n\n3\n\n\n2\n\n\n1\n\n\n1\n\n\n3\n\n\n2\n\n\n2\n\n\n1\n\n\n3\n\n\n3\n\n\n3\n\n\n1\n\n\n1\n\n\n3\n\n\n3\n\n\n2\n\n\n1\n\n\n2\n\n\n2\n\n\n3\n\n\n2\n\n\n3\n\n\n2\n\n\n2\n\n\n3\n\n\n3\n\n\n3\n\n\n2\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n2\n\n\n3\n\n\n3\n\n\n3\n\n\n2\n\n\n1\n\n\n3\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n3\n\n\n1\n\n\n2\n\n\n2\n\n\n3\n\n\n3\n\n\n2\n\n\n2\n\n\n3\n\n\n3\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n3\n\n\n1\n\n\n2\n\n\n3\n\n\n3\n\n\n1\n\n\n2\n\n\n2\n\n\n3\n\n\n1\n\n\n3\n\n\n2\n\n\n2\n\n\n2\n\n\n3\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n3\n\n\n2\n\n\n3\n\n\n3\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n3\n\n\n3\n\n\n2\n\n\n3\n\n\n3\n\n\n2\n\n\n2\n\n\n3\n\n\n3\n\n\n2\n\n\n2\n\n\n1\n\n\n2\n\n\n3\n\n\n1\n\n\n2\n\n\n2\n\n\n2\n\n\n3\n\n\n3\n\n\n1\n\n\n2\n\n\n3\n\n\n2\n\n\n1\n\n\n1\n\n\n3\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n3\n\n\n2\n\n\n2\n\n\n2\n\n\n3\n\n\n3\n\n\n1\n\n\n1\n\n\n3\n\n\n3\n\n\n3\n\n\n2\n\n\n3\n\n\n3\n\n\n3\n\n\n2\n\n\n2\n\n\n3\n\n\n2\n\n\n3\n\n\n1\n\n\n1\n\n\n3\n\n\n2\n\n\n3\n\n\n2\n\n\n3\n\n\n2\n\n\n3\n\n\n2\n\n\n1\n\n\n3\n\n\n1\n\n\n3\n\n\n2\n\n\n2\n\n\n1\n\n\n2\n\n\n3\n\n\n1\n\n\n2\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n3\n\n\n2\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n3\n\n\n3\n\n\n3\n\n\n1\n\n\n2\n\n\n3\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n3\n\n\n2\n\n\n3\n\n\n3\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n3\n\n\n3\n\n\n1\n\n\n2\n\n\n3\n\n\n2\n\n\n3\n\n\n2\n\n\n3\n\n\n2\n\n\n3\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n3\n\n\n3\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n3\n\n\n3\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n3\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n2\n\n\n3\n\n\n1\n\n\n3\n\n\n3\n\n\n2\n\n\n2\n\n\n3\n\n\n3\n\n\n1\n\n\n1\n\n\n3\n\n\n1\n\n\n3\n\n\n3\n\n\n3\n\n\n1\n\n\n2\n\n\n3\n\n\n3\n\n\n3\n\n\n3\n\n\n2\n\n\n3\n\n\n3\n\n\n2\n\n\n3\n\n\n3\n\n\n3\n\n\n3\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n3\n\n\n1\n\n\n1\n\n\n3\n\n\n2\n\n\n1\n\n\n3\n\n\n3\n\n\n3\n\n\n2\n\n\n1\n\n\n2\n\n\n3\n\n\n3\n\n\n2\n\n\n3\n\n\n1\n\n\n3\n\n\n1\n\n\n3\n\n\n1\n\n\n3\n\n\n3\n\n\n3\n\n\n1\n\n\n2\n\n\n3\n\n\n3\n\n\n3\n\n\n3\n\n\n2\n\n\n3\n\n\n3\n\n\n2\n\n\n3\n\n\n3\n\n\n3\n\n\n3\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n3\n\n\n1\n\n\n1\n\n\n3\n\n\n2\n\n\n1\n\n\n3\n\n\n3\n\n\n3\n\n\n2\n\n\n1\n\n\n2\n\n\n3\n\n\n3\n\n\n2\n\n\n3\n\n\n1\n\n\n3\n\n\n1\n\n\n\n\n51\n\n\n1\n\n\nDKGRP\n\n\nDrinking Group\n\n\n3\n\n\n3\n\n\n0\n\n\n2\n\n\n1\n\n\n1\n\n\n3\n\n\n0\n\n\n0\n\n\n2\n\n\n0\n\n\n2\n\n\n1\n\n\n0\n\n\n0\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n0\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n0\n\n\n0\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n0\n\n\n1\n\n\n1\n\n\n1\n\n\n0\n\n\n1\n\n\n1\n\n\n1\n\n\n3\n\n\n1\n\n\n1\n\n\n3\n\n\n2\n\n\n1\n\n\n2\n\n\n0\n\n\n2\n\n\n0\n\n\n2\n\n\n1\n\n\n0\n\n\n0\n\n\n2\n\n\n2\n\n\n0\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n0\n\n\n2\n\n\n1\n\n\n2\n\n\n0\n\n\n1\n\n\n0\n\n\n1\n\n\n1\n\n\n0\n\n\n0\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n0\n\n\n2\n\n\n3\n\n\n1\n\n\n3\n\n\n1\n\n\n1\n\n\n0\n\n\n.\n\n\n2\n\n\n0\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n3\n\n\n.\n\n\n0\n\n\n0\n\n\n1\n\n\n3\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n2\n\n\n0\n\n\n1\n\n\n0\n\n\n1\n\n\n0\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n3\n\n\n1\n\n\n1\n\n\n3\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n0\n\n\n2\n\n\n1\n\n\n.\n\n\n0\n\n\n3\n\n\n1\n\n\n0\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n3\n\n\n0\n\n\n2\n\n\n1\n\n\n0\n\n\n.\n\n\n0\n\n\n1\n\n\n2\n\n\n3\n\n\n0\n\n\n1\n\n\n2\n\n\n1\n\n\n3\n\n\n1\n\n\n0\n\n\n2\n\n\n2\n\n\n1\n\n\n0\n\n\n0\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n0\n\n\n1\n\n\n0\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n3\n\n\n1\n\n\n2\n\n\n1\n\n\n.\n\n\n3\n\n\n1\n\n\n1\n\n\n2\n\n\n0\n\n\n2\n\n\n2\n\n\n2\n\n\n1\n\n\n2\n\n\n2\n\n\n2\n\n\n0\n\n\n0\n\n\n3\n\n\n1\n\n\n1\n\n\n0\n\n\n2\n\n\n0\n\n\n1\n\n\n3\n\n\n3\n\n\n1\n\n\n0\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n.\n\n\n1\n\n\n2\n\n\n1\n\n\n0\n\n\n2\n\n\n1\n\n\n0\n\n\n1\n\n\n2\n\n\n0\n\n\n1\n\n\n3\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n0\n\n\n1\n\n\n1\n\n\n1\n\n\n0\n\n\n1\n\n\n2\n\n\n0\n\n\n0\n\n\n1\n\n\n1\n\n\n0\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n0\n\n\n1\n\n\n1\n\n\n0\n\n\n1\n\n\n0\n\n\n1\n\n\n3\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n0\n\n\n.\n\n\n1\n\n\n1\n\n\n0\n\n\n3\n\n\n0\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n3\n\n\n1\n\n\n1\n\n\n0\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n0\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n0\n\n\n1\n\n\n0\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n0\n\n\n1\n\n\n1\n\n\n0\n\n\n0\n\n\n1\n\n\n1\n\n\n2\n\n\n0\n\n\n.\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n3\n\n\n0\n\n\n3\n\n\n1\n\n\n0\n\n\n0\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n0\n\n\n0\n\n\n2\n\n\n0\n\n\n1\n\n\n0\n\n\n0\n\n\n1\n\n\n0\n\n\n1\n\n\n1\n\n\n2\n\n\n0\n\n\n0\n\n\n1\n\n\n3\n\n\n2\n\n\n0\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n0\n\n\n0\n\n\n1\n\n\n0\n\n\n0\n\n\n0\n\n\n2\n\n\n2\n\n\n0\n\n\n0\n\n\n0\n\n\n3\n\n\n1\n\n\n.\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n0\n\n\n2\n\n\n2\n\n\n1\n\n\n0\n\n\n0\n\n\n1\n\n\n2\n\n\n1\n\n\n0\n\n\n1\n\n\n1\n\n\n0\n\n\n3\n\n\n1\n\n\n1\n\n\n0\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n0\n\n\n0\n\n\n1\n\n\n0\n\n\n2\n\n\n0\n\n\n0\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n0\n\n\n0\n\n\n2\n\n\n1\n\n\n0\n\n\n0\n\n\n1\n\n\n2\n\n\n1\n\n\n0\n\n\n2\n\n\n0\n\n\n0\n\n\n1\n\n\n0\n\n\n0\n\n\n2\n\n\n1\n\n\n1\n\n\n0\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n0\n\n\n3\n\n\n0\n\n\n2\n\n\n1\n\n\n1\n\n\n0\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n0\n\n\n1\n\n\n2\n\n\n0\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n2\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n0\n\n\n1\n\n\n3\n\n\n2\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n3\n\n\n1\n\n\n1\n\n\n0\n\n\n2\n\n\n0\n\n\n3\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n0\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n0\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n0\n\n\n3\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n0\n\n\n1\n\n\n0\n\n\n1\n\n\n0\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n3\n\n\n1\n\n\n1\n\n\n0\n\n\n3\n\n\n3\n\n\n2\n\n\n2\n\n\n\n\n52\n\n\n1\n\n\nHEROPIATE\n\n\nHeroin or Opiate Use Since Last Visit\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n.\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n.\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n.\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n.\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n-9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n.\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n.\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n\n\n53\n\n\n1\n\n\nIDU\n\n\nIntravenous Drug Usage Since Last Visit\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n\n\n54\n\n\n1\n\n\nLEU3N\n\n\nCD4+ T Cell Count\n\n\n262.0060502\n\n\n459.4561691\n\n\n488.9100325\n\n\n159.6297053\n\n\n92.6633977\n\n\n745.6517188\n\n\n1222.422198\n\n\n749.3410624\n\n\n304.9145642\n\n\n90.7628563\n\n\n342.6178646\n\n\n104.1200862\n\n\n454.7998272\n\n\n360.2315415\n\n\n314.2334052\n\n\n691.3178124\n\n\n639.4747542\n\n\n337.1387048\n\n\n126.8444754\n\n\n615.0916257\n\n\n1030.013266\n\n\n249.368614\n\n\n332.6279843\n\n\n434.2566211\n\n\n83.57586756\n\n\n761.5803396\n\n\n631.9302337\n\n\n390.5807862\n\n\n101.1188646\n\n\n125.1387413\n\n\n457.1058366\n\n\n347.0354018\n\n\n698.5483337\n\n\n570.1950358\n\n\n339.5179819\n\n\n613.8196083\n\n\n1034.355269\n\n\n226.343928\n\n\n421.575391\n\n\n446.3201385\n\n\n91.00067039\n\n\n761.1545822\n\n\n585.9644539\n\n\n417.7407109\n\n\n75.81206172\n\n\n92.73093942\n\n\n524.9657778\n\n\n413.8989704\n\n\n690.759471\n\n\n538.5817368\n\n\n338.4909136\n\n\n618.1545843\n\n\n1031.613571\n\n\n811.9669494\n\n\n415.53517\n\n\n161.8995461\n\n\n252.138746\n\n\n378.1063976\n\n\n241.1140965\n\n\n428.6235558\n\n\n477.5043991\n\n\n472.2518119\n\n\n978.3997918\n\n\n309.4734741\n\n\n783.6031177\n\n\n255.863637\n\n\n299.3991921\n\n\n293.7438199\n\n\n667.4378329\n\n\n542.0179675\n\n\n171.2003285\n\n\n271.9441778\n\n\n347.9145835\n\n\n135.435387\n\n\n628.2019292\n\n\n478.1489182\n\n\n698.5016255\n\n\n.\n\n\n371.9976896\n\n\n457.3159314\n\n\n665.3026371\n\n\n1323.473202\n\n\n266.647968\n\n\n508.4016926\n\n\n285.251359\n\n\n138.9680372\n\n\n609.3519444\n\n\n566.1990705\n\n\n532.838299\n\n\n694.1656983\n\n\n119.6278588\n\n\n713.7186854\n\n\n504.483081\n\n\n155.3352852\n\n\n385.8170914\n\n\n384.7486159\n\n\n211.6668731\n\n\n.\n\n\n950.9861673\n\n\n247.1469072\n\n\n1058.631989\n\n\n299.8067159\n\n\n723.405987\n\n\n583.9001541\n\n\n482.8555351\n\n\n284.4361576\n\n\n638.6313945\n\n\n630.9743106\n\n\n155.8404244\n\n\n425.8768194\n\n\n598.3937978\n\n\n281.0585068\n\n\n607.129907\n\n\n66.62045413\n\n\n320.415702\n\n\n479.2373551\n\n\n746.6463567\n\n\n743.9996542\n\n\n381.5893698\n\n\n708.7482369\n\n\n658.195458\n\n\n576.3474914\n\n\n760.9189509\n\n\n520.9766234\n\n\n447.454173\n\n\n100.2962706\n\n\n167.9382298\n\n\n814.9429615\n\n\n260.8224888\n\n\n395.1952199\n\n\n146.8734416\n\n\n484.7639035\n\n\n355.2316594\n\n\n294.1312143\n\n\n.\n\n\n966.7359639\n\n\n1126.340948\n\n\n.\n\n\n557.8420161\n\n\n392.5823184\n\n\n526.8044252\n\n\n121.8191992\n\n\n88.44505664\n\n\n491.5085449\n\n\n.\n\n\n771.5861623\n\n\n587.3998361\n\n\n310.175915\n\n\n409.8845653\n\n\n486.4897731\n\n\n485.2729585\n\n\n557.7302319\n\n\n263.1986325\n\n\n679.089184\n\n\n98.51227711\n\n\n468.7466029\n\n\n422.8803005\n\n\n482.6145556\n\n\n65.54820111\n\n\n189.345845\n\n\n431.67067\n\n\n713.1751705\n\n\n423.5591756\n\n\n473.3904402\n\n\n1147.53073\n\n\n403.7270467\n\n\n551.2058011\n\n\n450.2182586\n\n\n452.7246507\n\n\n331.4484553\n\n\n452.7175192\n\n\n310.1873153\n\n\n132.7746726\n\n\n258.6723343\n\n\n616.9881641\n\n\n382.9944817\n\n\n250.5168348\n\n\n278.3574873\n\n\n977.7784305\n\n\n747.9825395\n\n\n224.0963737\n\n\n320.0624154\n\n\n457.4632196\n\n\n300.7815128\n\n\n448.8154323\n\n\n866.9092975\n\n\n416.3110559\n\n\n924.4882282\n\n\n290.7012648\n\n\n636.7033572\n\n\n946.0717381\n\n\n372.0587579\n\n\n198.7904278\n\n\n455.9707039\n\n\n1015.937556\n\n\n76.60112462\n\n\n512.7494571\n\n\n363.7443602\n\n\n70.41999763\n\n\n564.4386138\n\n\n458.7413906\n\n\n812.9871682\n\n\n483.0122164\n\n\n883.5011245\n\n\n454.8373898\n\n\n742.1333549\n\n\n621.9829709\n\n\n217.8797446\n\n\n48.75554153\n\n\n497.1875695\n\n\n269.1688821\n\n\n1118.145614\n\n\n490.5446967\n\n\n234.8733719\n\n\n346.3528422\n\n\n558.8179925\n\n\n529.839443\n\n\n237.2760753\n\n\n495.8617988\n\n\n466.4092031\n\n\n397.3831214\n\n\n460.4596143\n\n\n364.4537755\n\n\n462.2584042\n\n\n517.2596775\n\n\n464.4941655\n\n\n157.5254042\n\n\n414.2086142\n\n\n806.5634678\n\n\n539.7174256\n\n\n362.6676699\n\n\n388.5258012\n\n\n300.5992341\n\n\n198.1362678\n\n\n573.743298\n\n\n714.7072297\n\n\n333.1831397\n\n\n549.4605811\n\n\n660.8552565\n\n\n540.7060371\n\n\n726.8204432\n\n\n347.2377641\n\n\n535.7063955\n\n\n166.3461281\n\n\n534.8635382\n\n\n235.0855819\n\n\n434.1884542\n\n\n621.8607427\n\n\n157.5878634\n\n\n137.5887702\n\n\n364.1938227\n\n\n602.3333873\n\n\n746.6993967\n\n\n.\n\n\n1109.288914\n\n\n.\n\n\n264.5556617\n\n\n469.7419314\n\n\n369.8756205\n\n\n571.8054594\n\n\n188.0390795\n\n\n539.8717889\n\n\n640.913342\n\n\n876.7879414\n\n\n580.6760313\n\n\n474.4125382\n\n\n928.4773089\n\n\n506.2690861\n\n\n613.2441914\n\n\n317.9746102\n\n\n543.5422314\n\n\n92.16637237\n\n\n147.3123347\n\n\n958.874296\n\n\n453.2799019\n\n\n404.7129126\n\n\n882.4865444\n\n\n663.3289945\n\n\n171.4184273\n\n\n588.5749488\n\n\n348.4951476\n\n\n529.2812102\n\n\n101.1944772\n\n\n428.6774255\n\n\n748.2670458\n\n\n736.3085149\n\n\n330.8805547\n\n\n743.0911884\n\n\n416.1069684\n\n\n540.3256423\n\n\n1223.24407\n\n\n599.1372288\n\n\n216.066349\n\n\n749.9675975\n\n\n108.5204535\n\n\n648.4912464\n\n\n889.69346\n\n\n421.3014936\n\n\n307.6959159\n\n\n563.3764144\n\n\n828.8115901\n\n\n677.3531623\n\n\n.\n\n\n548.7348084\n\n\n1097.515777\n\n\n536.468955\n\n\n.\n\n\n91.3723269\n\n\n523.3007793\n\n\n551.0901479\n\n\n921.5328362\n\n\n.\n\n\n63.02200973\n\n\n218.0928213\n\n\n205.1465147\n\n\n575.2569332\n\n\n76.1706763\n\n\n695.4283125\n\n\n340.95969\n\n\n703.348422\n\n\n681.113044\n\n\n581.7583909\n\n\n564.4304549\n\n\n651.2152311\n\n\n.\n\n\n373.7112987\n\n\n575.5821606\n\n\n153.4114172\n\n\n212.7160809\n\n\n948.7965541\n\n\n459.9803025\n\n\n67.58492594\n\n\n384.7206152\n\n\n378.764768\n\n\n203.8384945\n\n\n527.1752314\n\n\n108.7824255\n\n\n442.3708662\n\n\n763.0143917\n\n\n510.9384228\n\n\n321.0178908\n\n\n.\n\n\n1160.401251\n\n\n233.0175683\n\n\n744.5269277\n\n\n284.7057259\n\n\n361.5718503\n\n\n426.0940632\n\n\n.\n\n\n264.0510835\n\n\n452.6190482\n\n\n362.1169278\n\n\n312.5973267\n\n\n526.4703038\n\n\n1297.3667\n\n\n312.4015452\n\n\n768.9070443\n\n\n1116.489894\n\n\n377.5475433\n\n\n120.1206843\n\n\n438.1396647\n\n\n918.5839165\n\n\n307.7684247\n\n\n455.5481865\n\n\n830.0923243\n\n\n696.7268852\n\n\n521.9708303\n\n\n859.1215105\n\n\n163.7669565\n\n\n286.3706493\n\n\n.\n\n\n377.2346728\n\n\n348.5881916\n\n\n541.7845757\n\n\n1085.884271\n\n\n631.2695028\n\n\n454.6772749\n\n\n234.1648117\n\n\n87.33175109\n\n\n536.6964844\n\n\n548.6232483\n\n\n779.0104973\n\n\n567.8804007\n\n\n531.0039796\n\n\n184.3040909\n\n\n343.5252133\n\n\n280.7521888\n\n\n1014.579118\n\n\n835.3896702\n\n\n754.9066552\n\n\n428.7500452\n\n\n713.1856414\n\n\n.\n\n\n582.5828906\n\n\n739.1319394\n\n\n368.5518187\n\n\n691.8976426\n\n\n364.2180976\n\n\n572.483479\n\n\n418.8451418\n\n\n723.5413514\n\n\n638.4486772\n\n\n323.1195346\n\n\n320.5536665\n\n\n607.5044249\n\n\n336.1602355\n\n\n492.2358563\n\n\n484.4028861\n\n\n352.0617505\n\n\n.\n\n\n372.1636639\n\n\n477.7020366\n\n\n127.8964457\n\n\n71.15749593\n\n\n63.46702667\n\n\n549.8246177\n\n\n263.366029\n\n\n388.4296044\n\n\n649.8408352\n\n\n510.8731616\n\n\n259.9464996\n\n\n364.2226337\n\n\n511.1894567\n\n\n475.8353673\n\n\n613.1888502\n\n\n653.3820694\n\n\n282.2760376\n\n\n545.6481844\n\n\n181.750715\n\n\n354.1760953\n\n\n567.8593868\n\n\n50.34363665\n\n\n601.6712968\n\n\n902.8929317\n\n\n467.6141437\n\n\n248.6789646\n\n\n159.0094314\n\n\n233.5840535\n\n\n813.7667489\n\n\n203.1191893\n\n\n604.904081\n\n\n317.2530007\n\n\n210.3231801\n\n\n263.706035\n\n\n317.5607166\n\n\n305.3563874\n\n\n454.2167141\n\n\n428.4516769\n\n\n775.1232288\n\n\n343.8704944\n\n\n413.3654175\n\n\n864.9595427\n\n\n81.51761314\n\n\n179.6812341\n\n\n408.3747705\n\n\n1031.311815\n\n\n373.8828388\n\n\n557.7827358\n\n\n615.2497169\n\n\n616.0661969\n\n\n1036.434541\n\n\n835.6238976\n\n\n381.1655385\n\n\n499.8214461\n\n\n412.4061555\n\n\n294.3767443\n\n\n246.3175564\n\n\n717.8265371\n\n\n149.1410573\n\n\n954.0507176\n\n\n480.5352395\n\n\n248.4964994\n\n\n467.6687868\n\n\n190.9401341\n\n\n597.9835803\n\n\n634.6597738\n\n\n377.7176268\n\n\n331.4452058\n\n\n433.6807568\n\n\n349.7810041\n\n\n709.2745951\n\n\n84.67050672\n\n\n400.8935458\n\n\n532.544436\n\n\n761.3323399\n\n\n631.4107541\n\n\n390.4840441\n\n\n1095.140261\n\n\n103.3463486\n\n\n64.32917191\n\n\n206.729542\n\n\n126.4054132\n\n\n.\n\n\n740.3535532\n\n\n456.7356745\n\n\n345.8798769\n\n\n454.8062818\n\n\n548.5189204\n\n\n566.1619066\n\n\n693.5102797\n\n\n571.7250037\n\n\n336.8739623\n\n\n351.544874\n\n\n616.1251021\n\n\n343.5478873\n\n\n422.4623425\n\n\n1032.729878\n\n\n412.0809612\n\n\n316.9634441\n\n\n237.9355696\n\n\n714.2259335\n\n\n152.4583406\n\n\n951.4887044\n\n\n481.0765939\n\n\n228.0416527\n\n\n467.100867\n\n\n190.6965003\n\n\n570.2013542\n\n\n633.82585\n\n\n377.52599\n\n\n424.7623618\n\n\n443.3320017\n\n\n350.4443622\n\n\n713.3250101\n\n\n89.02273751\n\n\n400.8177396\n\n\n529.4765414\n\n\n763.1170837\n\n\n585.7413884\n\n\n419.0665149\n\n\n1098.687928\n\n\n77.09125306\n\n\n65.02324628\n\n\n208.2101566\n\n\n93.24884043\n\n\n.\n\n\n742.436195\n\n\n525.4331939\n\n\n412.4556798\n\n\n458.2865205\n\n\n550.9328456\n\n\n564.7457921\n\n\n691.8465191\n\n\n538.5068496\n\n\n335.6836267\n\n\n334.7533786\n\n\n616.8155906\n\n\n345.6064175\n\n\n445.0087337\n\n\n1031.836777\n\n\n\n\n55\n\n\n1\n\n\nVLOAD\n\n\nViral Load;';\n\n\n27\n\n\n21\n\n\n2020\n\n\n26.64731734\n\n\n30389\n\n\n7870\n\n\n11\n\n\n11\n\n\n1.986387856\n\n\n687753.3627\n\n\n6566\n\n\n6514\n\n\n9\n\n\n80\n\n\n16.84844688\n\n\n869\n\n\n3\n\n\n33\n\n\n40\n\n\n20\n\n\n5\n\n\n36\n\n\n13\n\n\n1991\n\n\n30384\n\n\n7881\n\n\n10\n\n\n7.205760531\n\n\n687745.6134\n\n\n6504\n\n\n45\n\n\n59\n\n\n864\n\n\n24\n\n\n43\n\n\n10\n\n\n17\n\n\n26\n\n\n19\n\n\n2005\n\n\n30385\n\n\n7860\n\n\n14\n\n\n1.972775712\n\n\n687739.1644\n\n\n6536\n\n\n24\n\n\n66\n\n\n875\n\n\n49\n\n\n43\n\n\n23\n\n\n4\n\n\n7.891102846\n\n\n8\n\n\n16\n\n\n115207\n\n\n2.959163567\n\n\n78077\n\n\n543\n\n\n209.6074194\n\n\n9.617281594\n\n\n620\n\n\n53521\n\n\n11347\n\n\n181606\n\n\n3\n\n\n18.98796622\n\n\n15055\n\n\n154.3696994\n\n\n138\n\n\n36\n\n\n47\n\n\n87449\n\n\n0.493193928\n\n\n47\n\n\n8.13769981\n\n\n.\n\n\n44\n\n\n5985\n\n\n41.42828994\n\n\n2\n\n\n140\n\n\n62.09350515\n\n\n7\n\n\n279727\n\n\n39\n\n\n90\n\n\n20.71414497\n\n\n466\n\n\n8223\n\n\n22\n\n\n11.83665427\n\n\n250\n\n\n31774\n\n\n18282\n\n\n34\n\n\n.\n\n\n8196\n\n\n174957\n\n\n9.124087666\n\n\n163\n\n\n299126\n\n\n6270\n\n\n14\n\n\n44\n\n\n97.65239772\n\n\n35\n\n\n1678.338937\n\n\n12465\n\n\n159.7948326\n\n\n513\n\n\n602.4363829\n\n\n1339\n\n\n2864\n\n\n85\n\n\n4235\n\n\n3109\n\n\n350638\n\n\n16\n\n\n814\n\n\n630641\n\n\n3\n\n\n10.60366945\n\n\n12.08325123\n\n\n2148\n\n\n145773\n\n\n17340\n\n\n26\n\n\n48\n\n\n7.323163841\n\n\n4560\n\n\n2092\n\n\n37\n\n\n.\n\n\n14\n\n\n32\n\n\n.\n\n\n1\n\n\n5709\n\n\n9310\n\n\n1083.936622\n\n\n2.995839753\n\n\n1604\n\n\n.\n\n\n44\n\n\n40.49576423\n\n\n26\n\n\n7\n\n\n42\n\n\n63.44336396\n\n\n2290\n\n\n3188\n\n\n6.411521062\n\n\n1539\n\n\n39\n\n\n99984\n\n\n25\n\n\n21.70053283\n\n\n1665\n\n\n26.63247211\n\n\n1618.662471\n\n\n67\n\n\n56\n\n\n851\n\n\n48015\n\n\n31450\n\n\n36\n\n\n6.164924099\n\n\n1427\n\n\n9.37068463\n\n\n49.31939279\n\n\n54\n\n\n18603.02836\n\n\n225\n\n\n653105\n\n\n10800\n\n\n15\n\n\n4\n\n\n11\n\n\n3\n\n\n369\n\n\n9\n\n\n10795\n\n\n99\n\n\n2.465969639\n\n\n61800\n\n\n40\n\n\n13407\n\n\n121\n\n\n29\n\n\n2702\n\n\n41\n\n\n21\n\n\n46\n\n\n31621\n\n\n4\n\n\n24\n\n\n4.192148387\n\n\n40771\n\n\n1750\n\n\n3\n\n\n10.85026641\n\n\n.\n\n\n22.68692068\n\n\n17\n\n\n20\n\n\n12\n\n\n.\n\n\n68\n\n\n6.658118026\n\n\n4.685342315\n\n\n2017\n\n\n282\n\n\n2492\n\n\n13\n\n\n35\n\n\n247.0241618\n\n\n1228\n\n\n47\n\n\n3\n\n\n11.98335901\n\n\n19393.61823\n\n\n.\n\n\n77\n\n\n4\n\n\n25.64731734\n\n\n20\n\n\n12953\n\n\n9759\n\n\n59398\n\n\n17\n\n\n8.384296774\n\n\n2725\n\n\n31\n\n\n13413\n\n\n.\n\n\n172\n\n\n31\n\n\n12\n\n\n17973\n\n\n11.34346034\n\n\n5031\n\n\n3793.647693\n\n\n6465\n\n\n59621.91367\n\n\n4.685342315\n\n\n14853\n\n\n17\n\n\n3098\n\n\n20124\n\n\n690\n\n\n19\n\n\n.\n\n\n4.192148387\n\n\n.\n\n\n15.78220569\n\n\n1699\n\n\n180.8810802\n\n\n1\n\n\n22\n\n\n0.246596964\n\n\n23\n\n\n2193\n\n\n30\n\n\n63.44336396\n\n\n20\n\n\n3\n\n\n1\n\n\n22\n\n\n2168\n\n\n30386\n\n\n5516.374083\n\n\n6\n\n\n11.83665427\n\n\n67.49294038\n\n\n6\n\n\n18\n\n\n1.726178748\n\n\n160.7812205\n\n\n5739.790933\n\n\n1416\n\n\n52048\n\n\n4.192148387\n\n\n7868\n\n\n5237.719514\n\n\n13\n\n\n13\n\n\n5353\n\n\n13\n\n\n6\n\n\n111\n\n\n12927\n\n\n8\n\n\n46638.97166\n\n\n43\n\n\n29\n\n\n23886\n\n\n0.986387856\n\n\n4919\n\n\n30.82462049\n\n\n21\n\n\n.\n\n\n2.959163567\n\n\n28.35865085\n\n\n2663\n\n\n22.44032372\n\n\n687750.3627\n\n\n4681\n\n\n12698\n\n\n9\n\n\n.\n\n\n9317\n\n\n24631\n\n\n68602\n\n\n24\n\n\n24\n\n\n629\n\n\n6562\n\n\n.\n\n\n12216\n\n\n201.9629135\n\n\n35\n\n\n156\n\n\n.\n\n\n7\n\n\n1651\n\n\n95\n\n\n68052\n\n\n1\n\n\n9.37068463\n\n\n21505\n\n\n3036\n\n\n212\n\n\n0.246596964\n\n\n36714\n\n\n6513\n\n\n3181.594029\n\n\n738.3727677\n\n\n5\n\n\n42\n\n\n.\n\n\n41\n\n\n1.349858808\n\n\n1.23298482\n\n\n2.712566603\n\n\n23\n\n\n38\n\n\n.\n\n\n3702\n\n\n8\n\n\n77\n\n\n8687.857637\n\n\n10.60366945\n\n\n3\n\n\n14.84844688\n\n\n49\n\n\n556\n\n\n180\n\n\n144\n\n\n43.19548184\n\n\n154\n\n\n9\n\n\n12473\n\n\n1699\n\n\n1166\n\n\n3\n\n\n4\n\n\n4.931939279\n\n\n485\n\n\n18\n\n\n45\n\n\n1815\n\n\n15\n\n\n4\n\n\n17\n\n\n105.7900975\n\n\n46\n\n\n24665\n\n\n39\n\n\n1257\n\n\n10.60366945\n\n\n7.891102846\n\n\n968\n\n\n153.8839041\n\n\n52.26076014\n\n\n50\n\n\n13\n\n\n28.35865085\n\n\n44\n\n\n36\n\n\n34714\n\n\n.\n\n\n90\n\n\n9\n\n\n0.493193928\n\n\n868\n\n\n4656\n\n\n25\n\n\n67\n\n\n14\n\n\n2\n\n\n5306\n\n\n4630\n\n\n25\n\n\n31\n\n\n11086\n\n\n50\n\n\n28\n\n\n.\n\n\n3024\n\n\n28\n\n\n34\n\n\n2\n\n\n506503\n\n\n8.630893738\n\n\n14705\n\n\n28123\n\n\n2458\n\n\n777.5202273\n\n\n22\n\n\n211169\n\n\n94\n\n\n705\n\n\n20\n\n\n0.246596964\n\n\n35.096329\n\n\n5424\n\n\n20\n\n\n8.384296774\n\n\n512\n\n\n91674\n\n\n38\n\n\n7993\n\n\n7651\n\n\n65\n\n\n185783\n\n\n178\n\n\n37\n\n\n8.099152845\n\n\n40\n\n\n26\n\n\n93\n\n\n41\n\n\n4922.935071\n\n\n103408.6337\n\n\n46\n\n\n35071\n\n\n1125.961737\n\n\n193261\n\n\n5\n\n\n115.9005731\n\n\n.\n\n\n8675\n\n\n4480\n\n\n3\n\n\n21\n\n\n411\n\n\n6\n\n\n15\n\n\n24040\n\n\n16167\n\n\n371535\n\n\n3.452357495\n\n\n5\n\n\n53527\n\n\n181597\n\n\n35\n\n\n244\n\n\n8189\n\n\n19\n\n\n36\n\n\n13\n\n\n1666\n\n\n4441\n\n\n133\n\n\n2690\n\n\n10\n\n\n1990\n\n\n2495\n\n\n13415\n\n\n30382\n\n\n14.84844688\n\n\n1411\n\n\n7879\n\n\n8\n\n\n3.205760531\n\n\n30.82462049\n\n\n687743.6134\n\n\n9314\n\n\n68050\n\n\n6501\n\n\n.\n\n\n8.877490702\n\n\n44\n\n\n57\n\n\n12486\n\n\n1258\n\n\n11.83665427\n\n\n860\n\n\n22\n\n\n42\n\n\n211177\n\n\n7\n\n\n193262\n\n\n4938\n\n\n15\n\n\n18\n\n\n53524\n\n\n181605\n\n\n35\n\n\n245\n\n\n8181\n\n\n12\n\n\n25\n\n\n26\n\n\n1667\n\n\n2482\n\n\n139\n\n\n2703\n\n\n17\n\n\n2003\n\n\n2474\n\n\n13406\n\n\n30382\n\n\n63.44336396\n\n\n1415\n\n\n7860\n\n\n11\n\n\n1.972775712\n\n\n32.55079924\n\n\n687734.1644\n\n\n9309\n\n\n68048\n\n\n6534\n\n\n.\n\n\n4.931939279\n\n\n19\n\n\n64\n\n\n12470\n\n\n1257\n\n\n4.438745351\n\n\n874\n\n\n46\n\n\n41\n\n\n211175\n\n\n20\n\n\n193274\n\n\n7929\n\n\n2\n\n\n\n\n56\n\n\n1\n\n\nRACE\n\n\n \n\n\n1\n\n\n3\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n7\n\n\n1\n\n\n1\n\n\n3\n\n\n3\n\n\n3\n\n\n8\n\n\n3\n\n\n3\n\n\n1\n\n\n1\n\n\n3\n\n\n3\n\n\n3\n\n\n1\n\n\n1\n\n\n3\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n3\n\n\n3\n\n\n8\n\n\n3\n\n\n1\n\n\n1\n\n\n3\n\n\n3\n\n\n1\n\n\n1\n\n\n3\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n3\n\n\n3\n\n\n8\n\n\n3\n\n\n1\n\n\n1\n\n\n3\n\n\n3\n\n\n1\n\n\n1\n\n\n3\n\n\n1\n\n\n1\n\n\n1\n\n\n3\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n3\n\n\n3\n\n\n1\n\n\n1\n\n\n1\n\n\n3\n\n\n1\n\n\n2\n\n\n7\n\n\n7\n\n\n1\n\n\n1\n\n\n3\n\n\n1\n\n\n1\n\n\n3\n\n\n1\n\n\n1\n\n\n3\n\n\n1\n\n\n1\n\n\n1\n\n\n3\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n3\n\n\n8\n\n\n1\n\n\n3\n\n\n3\n\n\n3\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n3\n\n\n1\n\n\n1\n\n\n3\n\n\n1\n\n\n3\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n3\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n3\n\n\n3\n\n\n1\n\n\n1\n\n\n1\n\n\n3\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n3\n\n\n7\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n3\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n3\n\n\n1\n\n\n1\n\n\n1\n\n\n3\n\n\n1\n\n\n1\n\n\n1\n\n\n7\n\n\n1\n\n\n1\n\n\n3\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n8\n\n\n3\n\n\n3\n\n\n1\n\n\n4\n\n\n1\n\n\n1\n\n\n3\n\n\n1\n\n\n1\n\n\n1\n\n\n3\n\n\n3\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n3\n\n\n3\n\n\n1\n\n\n1\n\n\n1\n\n\n3\n\n\n3\n\n\n3\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n3\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n3\n\n\n3\n\n\n1\n\n\n3\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n3\n\n\n3\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n3\n\n\n1\n\n\n1\n\n\n1\n\n\n3\n\n\n1\n\n\n1\n\n\n1\n\n\n4\n\n\n1\n\n\n1\n\n\n3\n\n\n1\n\n\n3\n\n\n1\n\n\n3\n\n\n3\n\n\n3\n\n\n3\n\n\n3\n\n\n2\n\n\n1\n\n\n3\n\n\n1\n\n\n1\n\n\n3\n\n\n1\n\n\n3\n\n\n3\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n3\n\n\n1\n\n\n1\n\n\n8\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n3\n\n\n1\n\n\n3\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n3\n\n\n1\n\n\n1\n\n\n3\n\n\n3\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n8\n\n\n3\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n3\n\n\n3\n\n\n7\n\n\n1\n\n\n1\n\n\n1\n\n\n3\n\n\n8\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n3\n\n\n1\n\n\n3\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n3\n\n\n1\n\n\n3\n\n\n3\n\n\n2\n\n\n1\n\n\n3\n\n\n1\n\n\n3\n\n\n1\n\n\n1\n\n\n3\n\n\n1\n\n\n3\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n3\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n3\n\n\n2\n\n\n1\n\n\n8\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n3\n\n\n1\n\n\n3\n\n\n1\n\n\n1\n\n\n8\n\n\n3\n\n\n1\n\n\n1\n\n\n3\n\n\n3\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n3\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n3\n\n\n3\n\n\n3\n\n\n3\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n3\n\n\n3\n\n\n1\n\n\n1\n\n\n1\n\n\n3\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n3\n\n\n8\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n3\n\n\n1\n\n\n1\n\n\n1\n\n\n3\n\n\n3\n\n\n1\n\n\n3\n\n\n3\n\n\n1\n\n\n3\n\n\n1\n\n\n3\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n3\n\n\n1\n\n\n8\n\n\n1\n\n\n1\n\n\n1\n\n\n3\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n3\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n3\n\n\n3\n\n\n4\n\n\n8\n\n\n3\n\n\n3\n\n\n1\n\n\n2\n\n\n8\n\n\n1\n\n\n1\n\n\n1\n\n\n8\n\n\n1\n\n\n1\n\n\n3\n\n\n8\n\n\n1\n\n\n1\n\n\n1\n\n\n3\n\n\n1\n\n\n3\n\n\n3\n\n\n1\n\n\n8\n\n\n3\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n3\n\n\n1\n\n\n3\n\n\n3\n\n\n1\n\n\n3\n\n\n3\n\n\n1\n\n\n3\n\n\n8\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n3\n\n\n3\n\n\n1\n\n\n3\n\n\n1\n\n\n1\n\n\n8\n\n\n3\n\n\n1\n\n\n3\n\n\n1\n\n\n1\n\n\n1\n\n\n3\n\n\n1\n\n\n3\n\n\n2\n\n\n8\n\n\n1\n\n\n3\n\n\n3\n\n\n1\n\n\n8\n\n\n3\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n3\n\n\n1\n\n\n3\n\n\n3\n\n\n1\n\n\n3\n\n\n3\n\n\n1\n\n\n3\n\n\n8\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n3\n\n\n3\n\n\n1\n\n\n3\n\n\n1\n\n\n1\n\n\n8\n\n\n3\n\n\n1\n\n\n3\n\n\n1\n\n\n1\n\n\n1\n\n\n3\n\n\n1\n\n\n3\n\n\n2\n\n\n8\n\n\n1\n\n\n\n\n57\n\n\n1\n\n\nEDUCBAS\n\n\n \n\n\n4\n\n\n2\n\n\n7\n\n\n5\n\n\n7\n\n\n6\n\n\n5\n\n\n4\n\n\n7\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n4\n\n\n4\n\n\n6\n\n\n4\n\n\n3\n\n\n4\n\n\n4\n\n\n4\n\n\n4\n\n\n2\n\n\n7\n\n\n7\n\n\n6\n\n\n4\n\n\n7\n\n\n2\n\n\n2\n\n\n2\n\n\n4\n\n\n6\n\n\n4\n\n\n3\n\n\n4\n\n\n4\n\n\n4\n\n\n2\n\n\n7\n\n\n7\n\n\n6\n\n\n4\n\n\n7\n\n\n2\n\n\n2\n\n\n2\n\n\n4\n\n\n6\n\n\n4\n\n\n3\n\n\n4\n\n\n4\n\n\n6\n\n\n3\n\n\n4\n\n\n4\n\n\n5\n\n\n5\n\n\n4\n\n\n7\n\n\n5\n\n\n6\n\n\n4\n\n\n5\n\n\n4\n\n\n4\n\n\n7\n\n\n4\n\n\n6\n\n\n5\n\n\n5\n\n\n4\n\n\n6\n\n\n5\n\n\n4\n\n\n4\n\n\n6\n\n\n4\n\n\n3\n\n\n5\n\n\n3\n\n\n6\n\n\n3\n\n\n7\n\n\n4\n\n\n4\n\n\n4\n\n\n5\n\n\n5\n\n\n7\n\n\n3\n\n\n4\n\n\n4\n\n\n5\n\n\n4\n\n\n3\n\n\n5\n\n\n7\n\n\n3\n\n\n5\n\n\n3\n\n\n3\n\n\n4\n\n\n4\n\n\n7\n\n\n7\n\n\n4\n\n\n4\n\n\n7\n\n\n5\n\n\n4\n\n\n7\n\n\n5\n\n\n5\n\n\n4\n\n\n4\n\n\n2\n\n\n3\n\n\n5\n\n\n4\n\n\n6\n\n\n3\n\n\n6\n\n\n3\n\n\n4\n\n\n3\n\n\n4\n\n\n4\n\n\n5\n\n\n4\n\n\n4\n\n\n5\n\n\n4\n\n\n3\n\n\n3\n\n\n7\n\n\n4\n\n\n4\n\n\n5\n\n\n3\n\n\n3\n\n\n4\n\n\n7\n\n\n3\n\n\n4\n\n\n4\n\n\n4\n\n\n3\n\n\n5\n\n\n4\n\n\n5\n\n\n1\n\n\n5\n\n\n4\n\n\n3\n\n\n5\n\n\n5\n\n\n7\n\n\n4\n\n\n7\n\n\n7\n\n\n3\n\n\n4\n\n\n4\n\n\n3\n\n\n5\n\n\n5\n\n\n5\n\n\n5\n\n\n5\n\n\n5\n\n\n5\n\n\n5\n\n\n2\n\n\n5\n\n\n5\n\n\n3\n\n\n3\n\n\n6\n\n\n3\n\n\n3\n\n\n5\n\n\n4\n\n\n3\n\n\n7\n\n\n4\n\n\n4\n\n\n4\n\n\n4\n\n\n3\n\n\n3\n\n\n3\n\n\n2\n\n\n3\n\n\n4\n\n\n4\n\n\n4\n\n\n5\n\n\n6\n\n\n7\n\n\n4\n\n\n7\n\n\n5\n\n\n5\n\n\n7\n\n\n3\n\n\n4\n\n\n6\n\n\n4\n\n\n4\n\n\n5\n\n\n7\n\n\n4\n\n\n2\n\n\n5\n\n\n7\n\n\n4\n\n\n5\n\n\n4\n\n\n4\n\n\n6\n\n\n7\n\n\n7\n\n\n2\n\n\n3\n\n\n5\n\n\n4\n\n\n4\n\n\n5\n\n\n6\n\n\n4\n\n\n6\n\n\n6\n\n\n4\n\n\n4\n\n\n4\n\n\n5\n\n\n4\n\n\n4\n\n\n4\n\n\n4\n\n\n4\n\n\n5\n\n\n5\n\n\n3\n\n\n5\n\n\n5\n\n\n3\n\n\n4\n\n\n3\n\n\n5\n\n\n5\n\n\n6\n\n\n5\n\n\n5\n\n\n7\n\n\n2\n\n\n4\n\n\n5\n\n\n5\n\n\n5\n\n\n2\n\n\n5\n\n\n7\n\n\n4\n\n\n7\n\n\n2\n\n\n4\n\n\n5\n\n\n7\n\n\n7\n\n\n6\n\n\n4\n\n\n7\n\n\n3\n\n\n3\n\n\n6\n\n\n7\n\n\n7\n\n\n6\n\n\n1\n\n\n4\n\n\n5\n\n\n6\n\n\n6\n\n\n4\n\n\n5\n\n\n4\n\n\n3\n\n\n5\n\n\n5\n\n\n4\n\n\n4\n\n\n3\n\n\n7\n\n\n3\n\n\n5\n\n\n7\n\n\n5\n\n\n5\n\n\n6\n\n\n4\n\n\n5\n\n\n5\n\n\n7\n\n\n4\n\n\n2\n\n\n4\n\n\n4\n\n\n5\n\n\n4\n\n\n5\n\n\n4\n\n\n3\n\n\n2\n\n\n5\n\n\n2\n\n\n2\n\n\n5\n\n\n4\n\n\n7\n\n\n4\n\n\n3\n\n\n7\n\n\n4\n\n\n2\n\n\n4\n\n\n4\n\n\n5\n\n\n7\n\n\n4\n\n\n6\n\n\n4\n\n\n4\n\n\n7\n\n\n2\n\n\n7\n\n\n4\n\n\n5\n\n\n4\n\n\n4\n\n\n5\n\n\n4\n\n\n7\n\n\n5\n\n\n4\n\n\n2\n\n\n5\n\n\n4\n\n\n2\n\n\n4\n\n\n6\n\n\n7\n\n\n4\n\n\n4\n\n\n7\n\n\n3\n\n\n4\n\n\n4\n\n\n5\n\n\n4\n\n\n4\n\n\n5\n\n\n7\n\n\n5\n\n\n6\n\n\n6\n\n\n5\n\n\n4\n\n\n3\n\n\n3\n\n\n4\n\n\n3\n\n\n4\n\n\n5\n\n\n5\n\n\n4\n\n\n5\n\n\n5\n\n\n3\n\n\n5\n\n\n7\n\n\n4\n\n\n3\n\n\n7\n\n\n5\n\n\n7\n\n\n6\n\n\n7\n\n\n5\n\n\n5\n\n\n7\n\n\n3\n\n\n4\n\n\n5\n\n\n6\n\n\n4\n\n\n4\n\n\n4\n\n\n2\n\n\n4\n\n\n5\n\n\n7\n\n\n3\n\n\n3\n\n\n5\n\n\n4\n\n\n4\n\n\n5\n\n\n2\n\n\n5\n\n\n4\n\n\n6\n\n\n4\n\n\n4\n\n\n7\n\n\n4\n\n\n7\n\n\n5\n\n\n4\n\n\n7\n\n\n5\n\n\n4\n\n\n4\n\n\n4\n\n\n4\n\n\n4\n\n\n4\n\n\n5\n\n\n4\n\n\n7\n\n\n6\n\n\n6\n\n\n4\n\n\n4\n\n\n4\n\n\n4\n\n\n4\n\n\n5\n\n\n6\n\n\n6\n\n\n2\n\n\n4\n\n\n5\n\n\n4\n\n\n7\n\n\n4\n\n\n7\n\n\n4\n\n\n2\n\n\n7\n\n\n4\n\n\n4\n\n\n3\n\n\n4\n\n\n3\n\n\n2\n\n\n7\n\n\n3\n\n\n3\n\n\n5\n\n\n4\n\n\n5\n\n\n3\n\n\n4\n\n\n4\n\n\n3\n\n\n4\n\n\n7\n\n\n4\n\n\n4\n\n\n3\n\n\n4\n\n\n2\n\n\n4\n\n\n3\n\n\n2\n\n\n7\n\n\n2\n\n\n4\n\n\n7\n\n\n3\n\n\n1\n\n\n6\n\n\n4\n\n\n7\n\n\n5\n\n\n2\n\n\n5\n\n\n4\n\n\n2\n\n\n4\n\n\n7\n\n\n2\n\n\n4\n\n\n5\n\n\n3\n\n\n7\n\n\n6\n\n\n4\n\n\n3\n\n\n7\n\n\n4\n\n\n4\n\n\n3\n\n\n4\n\n\n3\n\n\n4\n\n\n4\n\n\n3\n\n\n4\n\n\n7\n\n\n4\n\n\n4\n\n\n3\n\n\n4\n\n\n2\n\n\n4\n\n\n3\n\n\n2\n\n\n7\n\n\n2\n\n\n4\n\n\n7\n\n\n3\n\n\n1\n\n\n6\n\n\n4\n\n\n7\n\n\n5\n\n\n2\n\n\n5\n\n\n4\n\n\n2\n\n\n4\n\n\n7\n\n\n2\n\n\n4\n\n\n5\n\n\n3\n\n\n7\n\n\n6\n\n\n4\n\n\n3\n\n\n7\n\n\n4\n\n\n4\n\n\n3\n\n\n4\n\n\n\n\n58\n\n\n1\n\n\nhivpos\n\n\n \n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n\n\n59\n\n\n1\n\n\nage\n\n\n \n\n\n53\n\n\n55\n\n\n48\n\n\n45\n\n\n54\n\n\n37\n\n\n48\n\n\n31\n\n\n48\n\n\n41\n\n\n33\n\n\n54\n\n\n30\n\n\n41\n\n\n51\n\n\n52\n\n\n39\n\n\n62\n\n\n37\n\n\n52\n\n\n40\n\n\n54\n\n\n52\n\n\n49\n\n\n55\n\n\n37\n\n\n30\n\n\n48\n\n\n40\n\n\n53\n\n\n32\n\n\n42\n\n\n49\n\n\n38\n\n\n61\n\n\n50\n\n\n39\n\n\n50\n\n\n56\n\n\n49\n\n\n56\n\n\n38\n\n\n34\n\n\n48\n\n\n38\n\n\n52\n\n\n32\n\n\n40\n\n\n50\n\n\n37\n\n\n62\n\n\n50\n\n\n40\n\n\n53\n\n\n58\n\n\n39\n\n\n46\n\n\n43\n\n\n41\n\n\n69\n\n\n49\n\n\n48\n\n\n35\n\n\n39\n\n\n44\n\n\n39\n\n\n50\n\n\n53\n\n\n27\n\n\n37\n\n\n46\n\n\n42\n\n\n34\n\n\n47\n\n\n28\n\n\n62\n\n\n53\n\n\n45\n\n\n46\n\n\n49\n\n\n56\n\n\n34\n\n\n44\n\n\n58\n\n\n49\n\n\n42\n\n\n46\n\n\n25\n\n\n43\n\n\n40\n\n\n64\n\n\n42\n\n\n50\n\n\n56\n\n\n38\n\n\n37\n\n\n44\n\n\n44\n\n\n39\n\n\n27\n\n\n43\n\n\n25\n\n\n41\n\n\n36\n\n\n49\n\n\n49\n\n\n63\n\n\n51\n\n\n49\n\n\n45\n\n\n46\n\n\n39\n\n\n51\n\n\n37\n\n\n38\n\n\n63\n\n\n46\n\n\n33\n\n\n43\n\n\n36\n\n\n59\n\n\n49\n\n\n27\n\n\n66\n\n\n48\n\n\n48\n\n\n45\n\n\n40\n\n\n53\n\n\n47\n\n\n44\n\n\n38\n\n\n56\n\n\n58\n\n\n51\n\n\n52\n\n\n51\n\n\n50\n\n\n48\n\n\n43\n\n\n36\n\n\n34\n\n\n43\n\n\n47\n\n\n52\n\n\n48\n\n\n54\n\n\n43\n\n\n21\n\n\n45\n\n\n30\n\n\n35\n\n\n31\n\n\n40\n\n\n35\n\n\n48\n\n\n52\n\n\n55\n\n\n43\n\n\n59\n\n\n47\n\n\n40\n\n\n26\n\n\n34\n\n\n51\n\n\n61\n\n\n45\n\n\n39\n\n\n58\n\n\n27\n\n\n48\n\n\n43\n\n\n50\n\n\n38\n\n\n56\n\n\n45\n\n\n47\n\n\n52\n\n\n38\n\n\n58\n\n\n51\n\n\n58\n\n\n25\n\n\n49\n\n\n33\n\n\n74\n\n\n44\n\n\n31\n\n\n35\n\n\n48\n\n\n43\n\n\n37\n\n\n44\n\n\n55\n\n\n42\n\n\n46\n\n\n51\n\n\n45\n\n\n58\n\n\n33\n\n\n47\n\n\n40\n\n\n43\n\n\n46\n\n\n44\n\n\n53\n\n\n42\n\n\n52\n\n\n40\n\n\n37\n\n\n47\n\n\n49\n\n\n48\n\n\n47\n\n\n40\n\n\n38\n\n\n59\n\n\n45\n\n\n45\n\n\n50\n\n\n42\n\n\n42\n\n\n43\n\n\n47\n\n\n43\n\n\n40\n\n\n45\n\n\n54\n\n\n34\n\n\n47\n\n\n32\n\n\n33\n\n\n58\n\n\n39\n\n\n47\n\n\n45\n\n\n45\n\n\n41\n\n\n47\n\n\n39\n\n\n42\n\n\n59\n\n\n26\n\n\n48\n\n\n44\n\n\n39\n\n\n50\n\n\n30\n\n\n46\n\n\n51\n\n\n38\n\n\n27\n\n\n38\n\n\n52\n\n\n54\n\n\n41\n\n\n56\n\n\n33\n\n\n52\n\n\n37\n\n\n40\n\n\n42\n\n\n30\n\n\n48\n\n\n35\n\n\n39\n\n\n60\n\n\n34\n\n\n42\n\n\n40\n\n\n41\n\n\n54\n\n\n36\n\n\n45\n\n\n61\n\n\n38\n\n\n28\n\n\n43\n\n\n56\n\n\n45\n\n\n50\n\n\n46\n\n\n54\n\n\n27\n\n\n37\n\n\n48\n\n\n46\n\n\n47\n\n\n46\n\n\n57\n\n\n48\n\n\n40\n\n\n48\n\n\n31\n\n\n37\n\n\n33\n\n\n49\n\n\n55\n\n\n48\n\n\n49\n\n\n48\n\n\n42\n\n\n45\n\n\n41\n\n\n28\n\n\n32\n\n\n49\n\n\n41\n\n\n51\n\n\n65\n\n\n34\n\n\n42\n\n\n43\n\n\n44\n\n\n51\n\n\n41\n\n\n41\n\n\n37\n\n\n33\n\n\n38\n\n\n37\n\n\n49\n\n\n26\n\n\n38\n\n\n57\n\n\n59\n\n\n57\n\n\n42\n\n\n38\n\n\n50\n\n\n43\n\n\n39\n\n\n54\n\n\n49\n\n\n51\n\n\n52\n\n\n54\n\n\n38\n\n\n35\n\n\n33\n\n\n37\n\n\n45\n\n\n37\n\n\n48\n\n\n32\n\n\n39\n\n\n58\n\n\n44\n\n\n41\n\n\n51\n\n\n30\n\n\n41\n\n\n46\n\n\n53\n\n\n58\n\n\n51\n\n\n43\n\n\n52\n\n\n59\n\n\n44\n\n\n55\n\n\n39\n\n\n37\n\n\n35\n\n\n63\n\n\n38\n\n\n31\n\n\n51\n\n\n50\n\n\n58\n\n\n42\n\n\n41\n\n\n53\n\n\n48\n\n\n35\n\n\n35\n\n\n47\n\n\n47\n\n\n35\n\n\n28\n\n\n62\n\n\n37\n\n\n43\n\n\n49\n\n\n39\n\n\n62\n\n\n37\n\n\n47\n\n\n44\n\n\n51\n\n\n61\n\n\n29\n\n\n50\n\n\n44\n\n\n29\n\n\n45\n\n\n52\n\n\n50\n\n\n63\n\n\n37\n\n\n24\n\n\n39\n\n\n49\n\n\n45\n\n\n44\n\n\n62\n\n\n45\n\n\n28\n\n\n53\n\n\n56\n\n\n38\n\n\n40\n\n\n37\n\n\n52\n\n\n38\n\n\n45\n\n\n53\n\n\n42\n\n\n44\n\n\n46\n\n\n42\n\n\n46\n\n\n44\n\n\n42\n\n\n52\n\n\n39\n\n\n44\n\n\n54\n\n\n47\n\n\n35\n\n\n45\n\n\n39\n\n\n42\n\n\n37\n\n\n37\n\n\n45\n\n\n46\n\n\n53\n\n\n41\n\n\n46\n\n\n49\n\n\n57\n\n\n27\n\n\n31\n\n\n40\n\n\n39\n\n\n44\n\n\n24\n\n\n46\n\n\n32\n\n\n37\n\n\n48\n\n\n42\n\n\n42\n\n\n47\n\n\n40\n\n\n49\n\n\n39\n\n\n37\n\n\n38\n\n\n32\n\n\n48\n\n\n35\n\n\n48\n\n\n59\n\n\n41\n\n\n37\n\n\n39\n\n\n59\n\n\n40\n\n\n48\n\n\n54\n\n\n49\n\n\n58\n\n\n54\n\n\n51\n\n\n34\n\n\n52\n\n\n49\n\n\n40\n\n\n42\n\n\n55\n\n\n36\n\n\n48\n\n\n37\n\n\n30\n\n\n48\n\n\n28\n\n\n40\n\n\n43\n\n\n37\n\n\n53\n\n\n47\n\n\n33\n\n\n32\n\n\n42\n\n\n37\n\n\n60\n\n\n43\n\n\n49\n\n\n38\n\n\n61\n\n\n46\n\n\n50\n\n\n32\n\n\n50\n\n\n39\n\n\n61\n\n\n39\n\n\n39\n\n\n39\n\n\n58\n\n\n40\n\n\n48\n\n\n50\n\n\n50\n\n\n59\n\n\n55\n\n\n50\n\n\n37\n\n\n56\n\n\n49\n\n\n40\n\n\n42\n\n\n56\n\n\n38\n\n\n47\n\n\n38\n\n\n34\n\n\n48\n\n\n29\n\n\n38\n\n\n42\n\n\n35\n\n\n52\n\n\n44\n\n\n32\n\n\n32\n\n\n40\n\n\n33\n\n\n62\n\n\n44\n\n\n50\n\n\n37\n\n\n62\n\n\n45\n\n\n50\n\n\n32\n\n\n50\n\n\n40\n\n\n\n\n60\n\n\n1\n\n\nART\n\n\n \n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n\n\n61\n\n\n1\n\n\neverART\n\n\n \n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n\n\n62\n\n\n1\n\n\nyears\n\n\n \n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n\n\n63\n\n\n1\n\n\nhard_drugs\n\n\n \n\n\n0\n\n\n0\n\n\n1\n\n\n1\n\n\n1\n\n\n0\n\n\n0\n\n\n0\n\n\n1\n\n\n0\n\n\n1\n\n\n1\n\n\n0\n\n\n1\n\n\n0\n\n\n0\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n0\n\n\n0\n\n\n1\n\n\n1\n\n\n0\n\n\n0\n\n\n1\n\n\n0\n\n\n1\n\n\n0\n\n\n1\n\n\n0\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n0\n\n\n0\n\n\n1\n\n\n1\n\n\n0\n\n\n0\n\n\n1\n\n\n0\n\n\n1\n\n\n0\n\n\n1\n\n\n0\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n1\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n1\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n1\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n1\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n1\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n1\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n1\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n1\n\n\n0\n\n\n0\n\n\n0\n\n\n1\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n1\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n1\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n1\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n1\n\n\n0\n\n\n0\n\n\n1\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n1\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n1\n\n\n0\n\n\n0\n\n\n0\n\n\n1\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n1\n\n\n1\n\n\n0\n\n\n1\n\n\n0\n\n\n0\n\n\n1\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n1\n\n\n0\n\n\n0\n\n\n1\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n1\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n1\n\n\n0\n\n\n0\n\n\n0\n\n\n1\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n1\n\n\n1\n\n\n0\n\n\n1\n\n\n0\n\n\n0\n\n\n1\n\n\n\n\n64\n\n\n1\n\n\nincome\n\n\nIncome\n\n\n4\n\n\n1\n\n\n6\n\n\n1\n\n\n2\n\n\n4\n\n\n2\n\n\n.\n\n\n6\n\n\n1\n\n\n.\n\n\n1\n\n\n1\n\n\n2\n\n\n.\n\n\n5\n\n\n2\n\n\n.\n\n\n.\n\n\n2\n\n\n1\n\n\n4\n\n\n.\n\n\n6\n\n\n2\n\n\n4\n\n\n4\n\n\n6\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n5\n\n\n2\n\n\n.\n\n\n2\n\n\n1\n\n\n4\n\n\n1\n\n\n6\n\n\n2\n\n\n4\n\n\n.\n\n\n6\n\n\n1\n\n\n1\n\n\n1\n\n\n.\n\n\n.\n\n\n.\n\n\n5\n\n\n.\n\n\n1\n\n\n6\n\n\n3\n\n\n4\n\n\n9\n\n\n6\n\n\n4\n\n\n5\n\n\n6\n\n\n7\n\n\n1\n\n\n3\n\n\n9\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n6\n\n\n3\n\n\n2\n\n\n3\n\n\n1\n\n\n7\n\n\n.\n\n\n.\n\n\n.\n\n\n2\n\n\n1\n\n\n.\n\n\n2\n\n\n5\n\n\n3\n\n\n.\n\n\n3\n\n\n1\n\n\n2\n\n\n.\n\n\n3\n\n\n5\n\n\n.\n\n\n6\n\n\n1\n\n\n.\n\n\n1\n\n\n3\n\n\n.\n\n\n5\n\n\n.\n\n\n.\n\n\n1\n\n\n.\n\n\n5\n\n\n.\n\n\n4\n\n\n7\n\n\n2\n\n\n.\n\n\n.\n\n\n6\n\n\n5\n\n\n6\n\n\n.\n\n\n6\n\n\n2\n\n\n2\n\n\n.\n\n\n4\n\n\n3\n\n\n3\n\n\n3\n\n\n1\n\n\n.\n\n\n7\n\n\n7\n\n\n3\n\n\n1\n\n\n4\n\n\n5\n\n\n6\n\n\n3\n\n\n.\n\n\n1\n\n\n3\n\n\n9\n\n\n3\n\n\n5\n\n\n2\n\n\n7\n\n\n3\n\n\n.\n\n\n6\n\n\n5\n\n\n.\n\n\n1\n\n\n1\n\n\n.\n\n\n2\n\n\n5\n\n\n1\n\n\n4\n\n\n1\n\n\n6\n\n\n.\n\n\n1\n\n\n9\n\n\n.\n\n\n7\n\n\n1\n\n\n7\n\n\n6\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n3\n\n\n4\n\n\n6\n\n\n2\n\n\n7\n\n\n6\n\n\n.\n\n\n.\n\n\n.\n\n\n3\n\n\n6\n\n\n1\n\n\n1\n\n\n4\n\n\n5\n\n\n4\n\n\n3\n\n\n5\n\n\n1\n\n\n.\n\n\n4\n\n\n5\n\n\n.\n\n\n1\n\n\n3\n\n\n.\n\n\n1\n\n\n1\n\n\n.\n\n\n3\n\n\n4\n\n\n.\n\n\n6\n\n\n.\n\n\n6\n\n\n5\n\n\n7\n\n\n.\n\n\n6\n\n\n.\n\n\n.\n\n\n.\n\n\n1\n\n\n3\n\n\n6\n\n\n.\n\n\n6\n\n\n2\n\n\n1\n\n\n4\n\n\n3\n\n\n4\n\n\n.\n\n\n5\n\n\n1\n\n\n6\n\n\n.\n\n\n.\n\n\n.\n\n\n4\n\n\n1\n\n\n3\n\n\n4\n\n\n3\n\n\n2\n\n\n3\n\n\n7\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n5\n\n\n3\n\n\n2\n\n\n1\n\n\n.\n\n\n3\n\n\n6\n\n\n.\n\n\n1\n\n\n7\n\n\n1\n\n\n1\n\n\n1\n\n\n4\n\n\n2\n\n\n2\n\n\n5\n\n\n7\n\n\n6\n\n\n.\n\n\n9\n\n\n4\n\n\n.\n\n\n2\n\n\n7\n\n\n2\n\n\n7\n\n\n3\n\n\n1\n\n\n4\n\n\n.\n\n\n3\n\n\n.\n\n\n3\n\n\n2\n\n\n6\n\n\n5\n\n\n7\n\n\n1\n\n\n2\n\n\n5\n\n\n7\n\n\n.\n\n\n6\n\n\n1\n\n\n5\n\n\n7\n\n\n4\n\n\n6\n\n\n3\n\n\n5\n\n\n3\n\n\n3\n\n\n2\n\n\n5\n\n\n9\n\n\n4\n\n\n.\n\n\n1\n\n\n4\n\n\n3\n\n\n6\n\n\n7\n\n\n6\n\n\n3\n\n\n3\n\n\n6\n\n\n7\n\n\n.\n\n\n6\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n3\n\n\n4\n\n\n1\n\n\n3\n\n\n.\n\n\n2\n\n\n1\n\n\n.\n\n\n5\n\n\n.\n\n\n6\n\n\n4\n\n\n4\n\n\n6\n\n\n2\n\n\n1\n\n\n.\n\n\n.\n\n\n5\n\n\n7\n\n\n1\n\n\n4\n\n\n.\n\n\n.\n\n\n4\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n4\n\n\n6\n\n\n4\n\n\n4\n\n\n7\n\n\n6\n\n\n2\n\n\n.\n\n\n7\n\n\n5\n\n\n1\n\n\n2\n\n\n6\n\n\n7\n\n\n2\n\n\n2\n\n\n2\n\n\n4\n\n\n9\n\n\n.\n\n\n1\n\n\n3\n\n\n4\n\n\n.\n\n\n.\n\n\n.\n\n\n5\n\n\n4\n\n\n.\n\n\n3\n\n\n1\n\n\n.\n\n\n1\n\n\n5\n\n\n3\n\n\n7\n\n\n.\n\n\n.\n\n\n3\n\n\n.\n\n\n.\n\n\n6\n\n\n6\n\n\n4\n\n\n.\n\n\n6\n\n\n4\n\n\n.\n\n\n6\n\n\n.\n\n\n1\n\n\n5\n\n\n1\n\n\n1\n\n\n.\n\n\n.\n\n\n.\n\n\n1\n\n\n4\n\n\n.\n\n\n1\n\n\n2\n\n\n.\n\n\n5\n\n\n1\n\n\n5\n\n\n5\n\n\n3\n\n\n4\n\n\n6\n\n\n1\n\n\n4\n\n\n1\n\n\n4\n\n\n9\n\n\n.\n\n\n5\n\n\n9\n\n\n6\n\n\n6\n\n\n5\n\n\n2\n\n\n5\n\n\n1\n\n\n.\n\n\n6\n\n\n9\n\n\n6\n\n\n.\n\n\n6\n\n\n.\n\n\n6\n\n\n5\n\n\n9\n\n\n2\n\n\n9\n\n\n4\n\n\n5\n\n\n.\n\n\n.\n\n\n4\n\n\n4\n\n\n.\n\n\n3\n\n\n1\n\n\n1\n\n\n3\n\n\n3\n\n\n6\n\n\n4\n\n\n1\n\n\n6\n\n\n6\n\n\n2\n\n\n3\n\n\n1\n\n\n3\n\n\n1\n\n\n.\n\n\n4\n\n\n5\n\n\n.\n\n\n.\n\n\n7\n\n\n3\n\n\n3\n\n\n3\n\n\n.\n\n\n1\n\n\n5\n\n\n5\n\n\n4\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n.\n\n\n1\n\n\n6\n\n\n.\n\n\n.\n\n\n2\n\n\n1\n\n\n.\n\n\n4\n\n\n4\n\n\n6\n\n\n.\n\n\n1\n\n\n4\n\n\n.\n\n\n1\n\n\n6\n\n\n7\n\n\n1\n\n\n2\n\n\n5\n\n\n2\n\n\n.\n\n\n.\n\n\n2\n\n\n5\n\n\n2\n\n\n.\n\n\n4\n\n\n.\n\n\n1\n\n\n.\n\n\n3\n\n\n3\n\n\n1\n\n\n1\n\n\n.\n\n\n5\n\n\n4\n\n\n1\n\n\n.\n\n\n.\n\n\n1\n\n\n1\n\n\n1\n\n\n6\n\n\n1\n\n\n1\n\n\n2\n\n\n.\n\n\n1\n\n\n4\n\n\n4\n\n\n6\n\n\n.\n\n\n1\n\n\n4\n\n\n2\n\n\n.\n\n\n6\n\n\n7\n\n\n1\n\n\n2\n\n\n5\n\n\n.\n\n\n.\n\n\n5\n\n\n2\n\n\n.\n\n\n2\n\n\n2\n\n\n4\n\n\n3\n\n\n1\n\n\n\n\n65\n\n\n1\n\n\nHASHF\n\n\nHash/Marijuana Use Since Last Visit\n\n\n4\n\n\n0\n\n\n4\n\n\n2\n\n\n3\n\n\n.\n\n\n1\n\n\n1\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n3\n\n\n4\n\n\n3\n\n\n0\n\n\n1\n\n\n0\n\n\n0\n\n\n0\n\n\n4\n\n\n4\n\n\n0\n\n\n4\n\n\n3\n\n\n.\n\n\n1\n\n\n0\n\n\n0\n\n\n0\n\n\n3\n\n\n4\n\n\n0\n\n\n1\n\n\n0\n\n\n0\n\n\n4\n\n\n4\n\n\n0\n\n\n4\n\n\n3\n\n\n.\n\n\n1\n\n\n0\n\n\n0\n\n\n0\n\n\n3\n\n\n4\n\n\n0\n\n\n1\n\n\n0\n\n\n0\n\n\n4\n\n\n3\n\n\n0\n\n\n0\n\n\n2\n\n\n0\n\n\n0\n\n\n4\n\n\n4\n\n\n0\n\n\n4\n\n\n4\n\n\n4\n\n\n3\n\n\n0\n\n\n0\n\n\n2\n\n\n0\n\n\n0\n\n\n4\n\n\n1\n\n\n0\n\n\n0\n\n\n1\n\n\n0\n\n\n3\n\n\n0\n\n\n0\n\n\n4\n\n\n0\n\n\n0\n\n\n1\n\n\n0\n\n\n1\n\n\n0\n\n\n0\n\n\n2\n\n\n4\n\n\n0\n\n\n4\n\n\n2\n\n\n0\n\n\n1\n\n\n0\n\n\n1\n\n\n0\n\n\n2\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n2\n\n\n0\n\n\n3\n\n\n0\n\n\n0\n\n\n0\n\n\n4\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n2\n\n\n0\n\n\n.\n\n\n1\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n4\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n4\n\n\n0\n\n\n0\n\n\n1\n\n\n2\n\n\n0\n\n\n0\n\n\n2\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n3\n\n\n1\n\n\n0\n\n\n4\n\n\n4\n\n\n0\n\n\n4\n\n\n1\n\n\n2\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n1\n\n\n0\n\n\n4\n\n\n0\n\n\n0\n\n\n0\n\n\n4\n\n\n0\n\n\n0\n\n\n2\n\n\n0\n\n\n0\n\n\n0\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n0\n\n\n2\n\n\n0\n\n\n0\n\n\n3\n\n\n0\n\n\n1\n\n\n0\n\n\n0\n\n\n3\n\n\n0\n\n\n0\n\n\n0\n\n\n4\n\n\n0\n\n\n0\n\n\n0\n\n\n2\n\n\n0\n\n\n0\n\n\n2\n\n\n0\n\n\n4\n\n\n0\n\n\n0\n\n\n1\n\n\n0\n\n\n0\n\n\n2\n\n\n0\n\n\n0\n\n\n0\n\n\n1\n\n\n0\n\n\n0\n\n\n0\n\n\n2\n\n\n0\n\n\n0\n\n\n0\n\n\n3\n\n\n0\n\n\n1\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n3\n\n\n0\n\n\n0\n\n\n3\n\n\n2\n\n\n3\n\n\n0\n\n\n2\n\n\n2\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n2\n\n\n1\n\n\n0\n\n\n1\n\n\n0\n\n\n0\n\n\n0\n\n\n1\n\n\n4\n\n\n2\n\n\n1\n\n\n1\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n1\n\n\n0\n\n\n0\n\n\n3\n\n\n0\n\n\n0\n\n\n0\n\n\n1\n\n\n1\n\n\n1\n\n\n0\n\n\n0\n\n\n4\n\n\n0\n\n\n0\n\n\n4\n\n\n.\n\n\n3\n\n\n4\n\n\n0\n\n\n0\n\n\n1\n\n\n1\n\n\n0\n\n\n4\n\n\n1\n\n\n0\n\n\n1\n\n\n0\n\n\n1\n\n\n0\n\n\n.\n\n\n1\n\n\n0\n\n\n2\n\n\n0\n\n\n0\n\n\n4\n\n\n0\n\n\n0\n\n\n2\n\n\n0\n\n\n3\n\n\n0\n\n\n4\n\n\n0\n\n\n4\n\n\n0\n\n\n0\n\n\n2\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n4\n\n\n4\n\n\n0\n\n\n0\n\n\n0\n\n\n2\n\n\n2\n\n\n0\n\n\n0\n\n\n0\n\n\n4\n\n\n2\n\n\n0\n\n\n0\n\n\n4\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n1\n\n\n0\n\n\n0\n\n\n1\n\n\n3\n\n\n4\n\n\n2\n\n\n0\n\n\n0\n\n\n3\n\n\n0\n\n\n0\n\n\n2\n\n\n0\n\n\n0\n\n\n2\n\n\n2\n\n\n4\n\n\n2\n\n\n0\n\n\n0\n\n\n4\n\n\n0\n\n\n3\n\n\n3\n\n\n0\n\n\n2\n\n\n0\n\n\n2\n\n\n0\n\n\n2\n\n\n0\n\n\n0\n\n\n2\n\n\n0\n\n\n0\n\n\n2\n\n\n0\n\n\n0\n\n\n0\n\n\n4\n\n\n0\n\n\n3\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n4\n\n\n0\n\n\n0\n\n\n4\n\n\n0\n\n\n0\n\n\n1\n\n\n1\n\n\n0\n\n\n4\n\n\n0\n\n\n0\n\n\n0\n\n\n1\n\n\n0\n\n\n4\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n4\n\n\n2\n\n\n0\n\n\n4\n\n\n0\n\n\n0\n\n\n0\n\n\n1\n\n\n0\n\n\n0\n\n\n0\n\n\n2\n\n\n0\n\n\n3\n\n\n0\n\n\n2\n\n\n3\n\n\n0\n\n\n4\n\n\n4\n\n\n0\n\n\n4\n\n\n3\n\n\n1\n\n\n4\n\n\n0\n\n\n2\n\n\n2\n\n\n0\n\n\n3\n\n\n0\n\n\n0\n\n\n2\n\n\n4\n\n\n0\n\n\n0\n\n\n0\n\n\n2\n\n\n0\n\n\n4\n\n\n0\n\n\n4\n\n\n0\n\n\n0\n\n\n2\n\n\n2\n\n\n.\n\n\n0\n\n\n0\n\n\n4\n\n\n3\n\n\n4\n\n\n0\n\n\n2\n\n\n0\n\n\n4\n\n\n0\n\n\n4\n\n\n0\n\n\n1\n\n\n2\n\n\n0\n\n\n4\n\n\n0\n\n\n0\n\n\n3\n\n\n1\n\n\n0\n\n\n.\n\n\n1\n\n\n0\n\n\n0\n\n\n0\n\n\n4\n\n\n4\n\n\n0\n\n\n4\n\n\n0\n\n\n3\n\n\n4\n\n\n4\n\n\n0\n\n\n2\n\n\n0\n\n\n1\n\n\n0\n\n\n0\n\n\n0\n\n\n4\n\n\n0\n\n\n4\n\n\n0\n\n\n4\n\n\n3\n\n\n4\n\n\n0\n\n\n2\n\n\n0\n\n\n4\n\n\n0\n\n\n4\n\n\n0\n\n\n1\n\n\n2\n\n\n0\n\n\n4\n\n\n0\n\n\n0\n\n\n3\n\n\n1\n\n\n0\n\n\n.\n\n\n1\n\n\n0\n\n\n0\n\n\n0\n\n\n4\n\n\n4\n\n\n0\n\n\n4\n\n\n0\n\n\n3\n\n\n4\n\n\n4\n\n\n0\n\n\n2\n\n\n0\n\n\n1\n\n\n0\n\n\n0\n\n\n0\n\n\n4\n\n\n0\n\n\n4\n\n\n\n\n66\n\n\n1\n\n\nADH\n\n\n \n\n\n2\n\n\n1\n\n\n1\n\n\n3\n\n\n1\n\n\n2\n\n\n2\n\n\n2\n\n\n1\n\n\n3\n\n\n3\n\n\n2\n\n\n1\n\n\n2\n\n\n3\n\n\n3\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n2\n\n\n1\n\n\n3\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n2\n\n\n1\n\n\n3\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n3\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n3\n\n\n2\n\n\n1\n\n\n2\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n3\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n4\n\n\n1\n\n\n1\n\n\n2\n\n\n3\n\n\n1\n\n\n3\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n4\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n3\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n3\n\n\n1\n\n\n2\n\n\n2\n\n\n3\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n3\n\n\n3\n\n\n3\n\n\n3\n\n\n1\n\n\n2\n\n\n1\n\n\n3\n\n\n1\n\n\n2\n\n\n2\n\n\n3\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n2\n\n\n4\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n2\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n3\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n3\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n3\n\n\n1\n\n\n1\n\n\n3\n\n\n3\n\n\n2\n\n\n3\n\n\n2\n\n\n3\n\n\n3\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n2\n\n\n3\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n3\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n3\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n3\n\n\n4\n\n\n2\n\n\n4\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n3\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n3\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n2\n\n\n4\n\n\n2\n\n\n1\n\n\n3\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n2\n\n\n2\n\n\n1\n\n\n2\n\n\n2\n\n\n3\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n2\n\n\n1\n\n\n2\n\n\n2\n\n\n2\n\n\n3\n\n\n1\n\n\n3\n\n\n2\n\n\n2\n\n\n3\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n4\n\n\n2\n\n\n1\n\n\n3\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n3\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n3\n\n\n1\n\n\n1\n\n\n3\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n3\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n3\n\n\n4\n\n\n2\n\n\n2\n\n\n2\n\n\n4\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n3\n\n\n1\n\n\n2\n\n\n2\n\n\n2\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n2\n\n\n3\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n3\n\n\n1\n\n\n2\n\n\n3\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n2\n\n\n3\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n3\n\n\n3\n\n\n2\n\n\n2\n\n\n2\n\n\n1\n\n\n2\n\n\n3\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n3\n\n\n2\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n3\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n3\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n3\n\n\n2\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n3\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n\n\n67\n\n\n2\n\n\nnewid\n\n\nID\n\n\n1\n\n\n2\n\n\n3\n\n\n4\n\n\n5\n\n\n6\n\n\n7\n\n\n8\n\n\n9\n\n\n10\n\n\n11\n\n\n12\n\n\n13\n\n\n14\n\n\n15\n\n\n16\n\n\n17\n\n\n18\n\n\n19\n\n\n20\n\n\n21\n\n\n22\n\n\n23\n\n\n24\n\n\n25\n\n\n26\n\n\n27\n\n\n28\n\n\n29\n\n\n30\n\n\n31\n\n\n32\n\n\n33\n\n\n34\n\n\n35\n\n\n36\n\n\n37\n\n\n38\n\n\n39\n\n\n40\n\n\n41\n\n\n42\n\n\n43\n\n\n44\n\n\n45\n\n\n46\n\n\n47\n\n\n48\n\n\n49\n\n\n50\n\n\n51\n\n\n52\n\n\n53\n\n\n54\n\n\n55\n\n\n56\n\n\n57\n\n\n58\n\n\n59\n\n\n60\n\n\n61\n\n\n62\n\n\n63\n\n\n64\n\n\n65\n\n\n66\n\n\n67\n\n\n68\n\n\n69\n\n\n70\n\n\n71\n\n\n72\n\n\n73\n\n\n74\n\n\n75\n\n\n76\n\n\n77\n\n\n78\n\n\n79\n\n\n80\n\n\n81\n\n\n82\n\n\n83\n\n\n84\n\n\n85\n\n\n86\n\n\n87\n\n\n88\n\n\n89\n\n\n90\n\n\n91\n\n\n92\n\n\n93\n\n\n94\n\n\n95\n\n\n96\n\n\n97\n\n\n98\n\n\n99\n\n\n100\n\n\n101\n\n\n102\n\n\n103\n\n\n104\n\n\n105\n\n\n106\n\n\n107\n\n\n108\n\n\n109\n\n\n110\n\n\n111\n\n\n112\n\n\n113\n\n\n114\n\n\n115\n\n\n116\n\n\n117\n\n\n118\n\n\n119\n\n\n120\n\n\n121\n\n\n122\n\n\n123\n\n\n124\n\n\n125\n\n\n126\n\n\n127\n\n\n128\n\n\n129\n\n\n130\n\n\n131\n\n\n132\n\n\n133\n\n\n134\n\n\n135\n\n\n136\n\n\n137\n\n\n138\n\n\n139\n\n\n140\n\n\n141\n\n\n142\n\n\n143\n\n\n144\n\n\n145\n\n\n146\n\n\n147\n\n\n148\n\n\n149\n\n\n150\n\n\n151\n\n\n152\n\n\n153\n\n\n154\n\n\n155\n\n\n156\n\n\n157\n\n\n158\n\n\n159\n\n\n160\n\n\n161\n\n\n162\n\n\n163\n\n\n164\n\n\n165\n\n\n166\n\n\n167\n\n\n168\n\n\n169\n\n\n170\n\n\n171\n\n\n172\n\n\n173\n\n\n174\n\n\n175\n\n\n176\n\n\n177\n\n\n178\n\n\n179\n\n\n180\n\n\n181\n\n\n182\n\n\n183\n\n\n184\n\n\n185\n\n\n186\n\n\n187\n\n\n188\n\n\n189\n\n\n190\n\n\n191\n\n\n192\n\n\n193\n\n\n194\n\n\n195\n\n\n196\n\n\n197\n\n\n198\n\n\n199\n\n\n200\n\n\n201\n\n\n202\n\n\n203\n\n\n204\n\n\n205\n\n\n206\n\n\n207\n\n\n208\n\n\n209\n\n\n210\n\n\n211\n\n\n212\n\n\n213\n\n\n214\n\n\n215\n\n\n216\n\n\n217\n\n\n218\n\n\n219\n\n\n220\n\n\n221\n\n\n222\n\n\n223\n\n\n224\n\n\n225\n\n\n226\n\n\n227\n\n\n228\n\n\n229\n\n\n230\n\n\n231\n\n\n232\n\n\n233\n\n\n234\n\n\n235\n\n\n236\n\n\n237\n\n\n238\n\n\n239\n\n\n240\n\n\n241\n\n\n242\n\n\n243\n\n\n244\n\n\n245\n\n\n246\n\n\n247\n\n\n248\n\n\n249\n\n\n250\n\n\n251\n\n\n252\n\n\n253\n\n\n254\n\n\n255\n\n\n256\n\n\n257\n\n\n258\n\n\n259\n\n\n260\n\n\n261\n\n\n262\n\n\n263\n\n\n264\n\n\n265\n\n\n266\n\n\n267\n\n\n268\n\n\n269\n\n\n270\n\n\n271\n\n\n272\n\n\n273\n\n\n274\n\n\n275\n\n\n276\n\n\n277\n\n\n278\n\n\n279\n\n\n280\n\n\n281\n\n\n282\n\n\n283\n\n\n284\n\n\n285\n\n\n286\n\n\n287\n\n\n288\n\n\n289\n\n\n290\n\n\n291\n\n\n292\n\n\n293\n\n\n294\n\n\n295\n\n\n296\n\n\n297\n\n\n298\n\n\n299\n\n\n300\n\n\n301\n\n\n302\n\n\n303\n\n\n304\n\n\n305\n\n\n306\n\n\n307\n\n\n308\n\n\n309\n\n\n310\n\n\n311\n\n\n312\n\n\n313\n\n\n314\n\n\n315\n\n\n316\n\n\n317\n\n\n318\n\n\n319\n\n\n320\n\n\n321\n\n\n322\n\n\n323\n\n\n324\n\n\n325\n\n\n326\n\n\n327\n\n\n328\n\n\n329\n\n\n330\n\n\n331\n\n\n332\n\n\n333\n\n\n334\n\n\n335\n\n\n336\n\n\n337\n\n\n338\n\n\n339\n\n\n340\n\n\n341\n\n\n342\n\n\n343\n\n\n344\n\n\n345\n\n\n346\n\n\n347\n\n\n348\n\n\n349\n\n\n350\n\n\n351\n\n\n352\n\n\n353\n\n\n354\n\n\n355\n\n\n356\n\n\n357\n\n\n358\n\n\n359\n\n\n360\n\n\n361\n\n\n362\n\n\n363\n\n\n364\n\n\n365\n\n\n366\n\n\n367\n\n\n368\n\n\n369\n\n\n370\n\n\n371\n\n\n372\n\n\n373\n\n\n374\n\n\n375\n\n\n376\n\n\n377\n\n\n378\n\n\n379\n\n\n380\n\n\n381\n\n\n382\n\n\n383\n\n\n384\n\n\n385\n\n\n386\n\n\n387\n\n\n388\n\n\n389\n\n\n390\n\n\n391\n\n\n392\n\n\n393\n\n\n394\n\n\n395\n\n\n396\n\n\n397\n\n\n398\n\n\n399\n\n\n400\n\n\n401\n\n\n402\n\n\n403\n\n\n404\n\n\n405\n\n\n406\n\n\n407\n\n\n408\n\n\n409\n\n\n410\n\n\n411\n\n\n412\n\n\n413\n\n\n414\n\n\n415\n\n\n416\n\n\n417\n\n\n418\n\n\n419\n\n\n420\n\n\n421\n\n\n422\n\n\n423\n\n\n424\n\n\n425\n\n\n426\n\n\n427\n\n\n428\n\n\n429\n\n\n430\n\n\n431\n\n\n432\n\n\n433\n\n\n434\n\n\n435\n\n\n436\n\n\n437\n\n\n438\n\n\n439\n\n\n440\n\n\n441\n\n\n442\n\n\n443\n\n\n444\n\n\n445\n\n\n446\n\n\n447\n\n\n448\n\n\n449\n\n\n450\n\n\n451\n\n\n452\n\n\n453\n\n\n454\n\n\n455\n\n\n456\n\n\n457\n\n\n458\n\n\n459\n\n\n460\n\n\n461\n\n\n462\n\n\n463\n\n\n464\n\n\n465\n\n\n466\n\n\n467\n\n\n468\n\n\n469\n\n\n470\n\n\n471\n\n\n472\n\n\n473\n\n\n474\n\n\n475\n\n\n476\n\n\n477\n\n\n478\n\n\n479\n\n\n480\n\n\n481\n\n\n482\n\n\n483\n\n\n484\n\n\n485\n\n\n486\n\n\n487\n\n\n488\n\n\n489\n\n\n490\n\n\n491\n\n\n492\n\n\n493\n\n\n494\n\n\n495\n\n\n496\n\n\n497\n\n\n498\n\n\n499\n\n\n500\n\n\n501\n\n\n502\n\n\n503\n\n\n504\n\n\n505\n\n\n506\n\n\n507\n\n\n508\n\n\n509\n\n\n510\n\n\n511\n\n\n512\n\n\n513\n\n\n514\n\n\n515\n\n\n516\n\n\n517\n\n\n518\n\n\n519\n\n\n520\n\n\n521\n\n\n522\n\n\n523\n\n\n524\n\n\n525\n\n\n526\n\n\n527\n\n\n528\n\n\n529\n\n\n530\n\n\n531\n\n\n532\n\n\n533\n\n\n534\n\n\n535\n\n\n536\n\n\n537\n\n\n538\n\n\n539\n\n\n540\n\n\n541\n\n\n542\n\n\n543\n\n\n544\n\n\n545\n\n\n546\n\n\n547\n\n\n548\n\n\n549\n\n\n550\n\n\n\n\n68\n\n\n2\n\n\nAGG_MENT\n\n\nAggregate Mental QOL Score\n\n\n59.65135974\n\n\n45.41482533\n\n\n41.70078566\n\n\n52.68222906\n\n\n66.5062864\n\n\n50.26009928\n\n\n46.53768446\n\n\n23.47187955\n\n\n24.58592849\n\n\n40.42751974\n\n\n35.70460086\n\n\n56.67807667\n\n\n48.97333796\n\n\n41.82840016\n\n\n57.8339564\n\n\n50.6422377\n\n\n58.62175414\n\n\n58.69127543\n\n\n23.66342691\n\n\n25.47932307\n\n\n17.17665349\n\n\n58.66342301\n\n\n47.60549443\n\n\n39.04482517\n\n\n66.50948828\n\n\n50.9612698\n\n\n21.90716631\n\n\n19.90768484\n\n\n40.31165233\n\n\n54.9059996\n\n\n49.93729544\n\n\n41.77636339\n\n\n52.45878377\n\n\n58.59144725\n\n\n56.60029333\n\n\n25.01715303\n\n\n14.66083821\n\n\n60.94107926\n\n\n43.03519952\n\n\n40.80118433\n\n\n63.87149654\n\n\n50.14522007\n\n\n24.16872191\n\n\n24.99892903\n\n\n40.83024641\n\n\n55.56917325\n\n\n49.84834649\n\n\n39.8009723\n\n\n50.38734161\n\n\n60.49463178\n\n\n56.48349745\n\n\n28.49354493\n\n\n17.66599985\n\n\n58.80780179\n\n\n39.22915878\n\n\n61.50818038\n\n\n32.95718365\n\n\n60.31709727\n\n\n28.25921125\n\n\n56.8034703\n\n\n51.01550514\n\n\n53.35145828\n\n\n42.48754958\n\n\n19.81057275\n\n\n53.71062541\n\n\n48.87277857\n\n\n55.67739223\n\n\n29.90305443\n\n\n58.31795784\n\n\n56.34121062\n\n\n30.44253053\n\n\n43.64824432\n\n\n54.95099558\n\n\n55.77490433\n\n\n37.71854716\n\n\n42.15365145\n\n\n54.04429628\n\n\n51.779953\n\n\n40.1392794\n\n\n.\n\n\n49.40022993\n\n\n59.07408431\n\n\n58.00828552\n\n\n38.68270072\n\n\n54.11032328\n\n\n44.28779295\n\n\n38.53107799\n\n\n38.19331279\n\n\n46.03517591\n\n\n48.06964818\n\n\n58.66032458\n\n\n48.15738863\n\n\n55.72638793\n\n\n54.45976117\n\n\n45.83624666\n\n\n57.00342862\n\n\n56.31784906\n\n\n42.99846228\n\n\n58.85520586\n\n\n54.97381223\n\n\n52.42746296\n\n\n38.47250122\n\n\n36.78632748\n\n\n54.8958905\n\n\n57.31249625\n\n\n54.22317581\n\n\n60.53781593\n\n\n45.59463461\n\n\n59.57977573\n\n\n55.32401716\n\n\n41.72988452\n\n\n55.13993037\n\n\n55.20823737\n\n\n35.13669202\n\n\n17.7627831\n\n\n59.17480385\n\n\n39.65238641\n\n\n.\n\n\n51.08749044\n\n\n56.82198871\n\n\n45.59257909\n\n\n57.89022025\n\n\n39.91495261\n\n\n59.027519\n\n\n50.0698502\n\n\n38.08874734\n\n\n31.37664743\n\n\n39.20509757\n\n\n57.92091332\n\n\n53.69463794\n\n\n50.27836511\n\n\n55.19805145\n\n\n60.82394258\n\n\n58.41806584\n\n\n55.64164542\n\n\n43.80020137\n\n\n52.84383348\n\n\n57.87169567\n\n\n62.44009423\n\n\n14.37031464\n\n\n55.53484344\n\n\n64.997051\n\n\n60.85407779\n\n\n60.70032772\n\n\n56.99430218\n\n\n57.63195358\n\n\n46.43525038\n\n\n44.69051162\n\n\n31.08528663\n\n\n35.08371984\n\n\n54.83487188\n\n\n.\n\n\n47.8864312\n\n\n58.87753751\n\n\n50.56207874\n\n\n42.2222284\n\n\n40.19740688\n\n\n53.70070691\n\n\n60.48123366\n\n\n56.03462754\n\n\n48.14495389\n\n\n56.19316967\n\n\n.\n\n\n36.68256518\n\n\n37.0911889\n\n\n45.43689968\n\n\n60.39143774\n\n\n37.67196506\n\n\n52.94254358\n\n\n52.38777706\n\n\n57.33676835\n\n\n45.91470027\n\n\n38.69344399\n\n\n51.84505548\n\n\n52.30891574\n\n\n61.85148022\n\n\n51.24474428\n\n\n11.27975263\n\n\n49.51167788\n\n\n41.82054337\n\n\n38.75910796\n\n\n47.3299383\n\n\n39.96647904\n\n\n29.40800346\n\n\n46.53217529\n\n\n59.65222388\n\n\n45.58604901\n\n\n35.0623551\n\n\n45.91539965\n\n\n51.42426846\n\n\n49.68781164\n\n\n66.73736449\n\n\n30.48413951\n\n\n45.001939\n\n\n44.253038\n\n\n59.24397041\n\n\n42.94866848\n\n\n51.23168246\n\n\n56.75214841\n\n\n57.65725943\n\n\n45.21107874\n\n\n60.44341414\n\n\n35.29418166\n\n\n58.79554553\n\n\n48.08828194\n\n\n23.75606445\n\n\n38.86495162\n\n\n31.81389548\n\n\n33.66211351\n\n\n34.45755736\n\n\n56.26253071\n\n\n60.7615031\n\n\n40.93171496\n\n\n52.82423746\n\n\n50.85872262\n\n\n40.00539083\n\n\n54.14371238\n\n\n50.20680454\n\n\n55.78914588\n\n\n29.83544256\n\n\n52.97850059\n\n\n36.70180914\n\n\n31.04960118\n\n\n58.79559273\n\n\n57.5383861\n\n\n28.53977045\n\n\n52.81592912\n\n\n58.66211927\n\n\n59.43861515\n\n\n35.75965714\n\n\n55.18804376\n\n\n49.24182522\n\n\n54.34253686\n\n\n58.38274334\n\n\n.\n\n\n51.56255437\n\n\n55.40358712\n\n\n54.41476202\n\n\n56.40007677\n\n\n58.47543583\n\n\n34.32087859\n\n\n61.72680486\n\n\n47.57579281\n\n\n38.262747\n\n\n54.67692342\n\n\n53.54160757\n\n\n35.36883748\n\n\n36.30715762\n\n\n62.60830699\n\n\n57.0610304\n\n\n45.75668619\n\n\n35.30231189\n\n\n15.01073883\n\n\n53.78113156\n\n\n59.51121944\n\n\n25.49979123\n\n\n53.17037798\n\n\n29.17025133\n\n\n17.07826727\n\n\n58.30532738\n\n\n54.96651273\n\n\n50.53030702\n\n\n32.70198509\n\n\n59.64343921\n\n\n54.93220633\n\n\n48.18301205\n\n\n57.25594936\n\n\n54.37635078\n\n\n50.43332629\n\n\n54.67193399\n\n\n48.33843903\n\n\n65.33253174\n\n\n58.67464859\n\n\n57.18984007\n\n\n59.27203502\n\n\n20.6472121\n\n\n36.15625288\n\n\n54.94538951\n\n\n56.35810795\n\n\n53.92788377\n\n\n61.094414\n\n\n38.33620189\n\n\n56.88267438\n\n\n38.52974428\n\n\n51.53914698\n\n\n43.43589435\n\n\n53.8342169\n\n\n47.26587362\n\n\n53.21746045\n\n\n56.40570483\n\n\n44.98028441\n\n\n47.41413128\n\n\n30.39305717\n\n\n22.26945842\n\n\n52.71830738\n\n\n28.94691297\n\n\n54.57882302\n\n\n56.13971436\n\n\n24.39641349\n\n\n46.4557864\n\n\n55.8597219\n\n\n35.30198781\n\n\n55.2258182\n\n\n62.3824534\n\n\n49.13125113\n\n\n50.28277182\n\n\n51.74970338\n\n\n38.57091721\n\n\n11.20635117\n\n\n32.04392058\n\n\n52.19017464\n\n\n56.62429775\n\n\n41.08830298\n\n\n58.16666378\n\n\n59.15999372\n\n\n58.46791865\n\n\n47.96053503\n\n\n54.13850311\n\n\n35.58372504\n\n\n54.49571816\n\n\n59.19869263\n\n\n50.02202393\n\n\n55.09160251\n\n\n55.4725676\n\n\n57.23879193\n\n\n41.71708132\n\n\n54.37649606\n\n\n57.2448269\n\n\n29.98260197\n\n\n38.18893497\n\n\n55.25069711\n\n\n47.78135867\n\n\n54.01545366\n\n\n55.80003459\n\n\n57.86673655\n\n\n54.98861052\n\n\n56.04014539\n\n\n48.89709926\n\n\n30.45382272\n\n\n56.44806746\n\n\n58.3731579\n\n\n56.23445421\n\n\n35.18385304\n\n\n32.66626078\n\n\n47.68002847\n\n\n49.28170513\n\n\n44.83688669\n\n\n44.28789419\n\n\n59.7240182\n\n\n58.84764483\n\n\n49.69664545\n\n\n41.55076645\n\n\n46.7616228\n\n\n59.65370141\n\n\n54.69442551\n\n\n57.1326159\n\n\n50.94881205\n\n\n45.1135825\n\n\n59.69550434\n\n\n55.63741347\n\n\n30.76008957\n\n\n58.98170444\n\n\n56.26333842\n\n\n54.94627505\n\n\n.\n\n\n54.40757913\n\n\n21.46129363\n\n\n10.51014272\n\n\n47.18161913\n\n\n58.34586253\n\n\n51.45120996\n\n\n59.74481054\n\n\n28.64019361\n\n\n55.51363807\n\n\n54.11821486\n\n\n58.44913547\n\n\n55.2805667\n\n\n51.23520566\n\n\n57.51893384\n\n\n32.29174359\n\n\n48.14815165\n\n\n54.74588159\n\n\n53.12683465\n\n\n52.49486571\n\n\n40.04293127\n\n\n50.92264975\n\n\n51.21461974\n\n\n56.14441827\n\n\n52.19310334\n\n\n50.38409228\n\n\n21.04112784\n\n\n54.55899401\n\n\n57.48881902\n\n\n43.91209515\n\n\n62.64689315\n\n\n58.18298139\n\n\n51.1332706\n\n\n38.8398467\n\n\n58.49304099\n\n\n16.67654367\n\n\n42.71524584\n\n\n59.53362193\n\n\n50.90190156\n\n\n55.99355913\n\n\n60.96021062\n\n\n57.98136992\n\n\n33.59217539\n\n\n52.86749794\n\n\n48.82018661\n\n\n59.8075194\n\n\n56.62789019\n\n\n48.31173289\n\n\n23.72611168\n\n\n38.49142276\n\n\n35.32848584\n\n\n54.74408703\n\n\n61.69340002\n\n\n16.45686703\n\n\n59.93962482\n\n\n47.5815286\n\n\n19.6887645\n\n\n54.61870663\n\n\n35.49574203\n\n\n25.28565842\n\n\n26.01004554\n\n\n47.04304657\n\n\n20.72756324\n\n\n39.83976363\n\n\n38.5452829\n\n\n62.1520387\n\n\n57.10842366\n\n\n58.73368824\n\n\n52.73643526\n\n\n54.06236239\n\n\n31.27578631\n\n\n40.55750302\n\n\n42.42659106\n\n\n58.64031963\n\n\n42.32486386\n\n\n47.30183555\n\n\n56.56193603\n\n\n57.54729595\n\n\n59.47490438\n\n\n62.85125722\n\n\n53.84105539\n\n\n33.66195477\n\n\n37.42300373\n\n\n40.2809636\n\n\n57.0312401\n\n\n49.90187652\n\n\n53.21757533\n\n\n56.84075639\n\n\n38.95990199\n\n\n54.52384385\n\n\n20.6580785\n\n\n17.10025881\n\n\n60.07067293\n\n\n18.74362704\n\n\n57.68226617\n\n\n56.16192439\n\n\n58.65283328\n\n\n41.51343844\n\n\n56.87470466\n\n\n58.82516067\n\n\n39.95723321\n\n\n21.73975462\n\n\n51.13969645\n\n\n47.37086503\n\n\n56.29979955\n\n\n56.64796694\n\n\n56.79310781\n\n\n59.38588416\n\n\n42.71138913\n\n\n54.67128178\n\n\n54.2074642\n\n\n51.49674093\n\n\n64.52070738\n\n\n47.61068363\n\n\n41.8948384\n\n\n53.19596193\n\n\n52.30830091\n\n\n65.21213693\n\n\n20.9555014\n\n\n35.17391418\n\n\n51.83274072\n\n\n22.46937275\n\n\n21.31429147\n\n\n48.76196058\n\n\n41.46924036\n\n\n40.03523627\n\n\n29.48185193\n\n\n54.37900069\n\n\n57.40511047\n\n\n48.23347795\n\n\n48.78445384\n\n\n41.33330219\n\n\n53.57828188\n\n\n48.94678094\n\n\n52.02337201\n\n\n52.0139046\n\n\n57.17539194\n\n\n56.78412304\n\n\n57.02621524\n\n\n26.49661368\n\n\n51.08109662\n\n\n19.26643524\n\n\n16.70362269\n\n\n39.46594016\n\n\n20.72231851\n\n\n49.96297703\n\n\n47.58264908\n\n\n54.42945259\n\n\n57.27960983\n\n\n57.83822884\n\n\n61.1108359\n\n\n42.37044777\n\n\n57.41329348\n\n\n51.08550178\n\n\n49.81999037\n\n\n64.09463882\n\n\n44.74215946\n\n\n40.29470157\n\n\n51.73953188\n\n\n52.33952349\n\n\n63.25154789\n\n\n19.92073688\n\n\n35.47765339\n\n\n50.63336873\n\n\n22.81473557\n\n\n23.50404706\n\n\n48.02539025\n\n\n40.35447712\n\n\n40.76416726\n\n\n29.66632685\n\n\n55.84056918\n\n\n57.96192783\n\n\n48.03145888\n\n\n49.55538925\n\n\n41.5776787\n\n\n53.1029269\n\n\n47.45757236\n\n\n52.45169386\n\n\n49.73669712\n\n\n60.23212601\n\n\n57.23358054\n\n\n56.84953296\n\n\n27.72082613\n\n\n47.74273925\n\n\n19.30304554\n\n\n17.96139442\n\n\n\n\n69\n\n\n2\n\n\nAGG_PHYS\n\n\nAggregate Physical QOL Score\n\n\n48.54453213\n\n\n37.32203958\n\n\n58.51450134\n\n\n51.50532947\n\n\n18.82349891\n\n\n55.95667802\n\n\n58.80015723\n\n\n62.31112887\n\n\n44.93715846\n\n\n51.17592208\n\n\n44.29373175\n\n\n33.89621373\n\n\n37.80197962\n\n\n40.31417905\n\n\n42.3825101\n\n\n53.29184854\n\n\n37.16403543\n\n\n53.61760255\n\n\n29.90342696\n\n\n41.97060941\n\n\n63.17989214\n\n\n50.46482617\n\n\n40.79881936\n\n\n58.78267161\n\n\n19.37072096\n\n\n55.13330937\n\n\n64.6473298\n\n\n45.33275674\n\n\n49.48754432\n\n\n36.7449984\n\n\n38.25631119\n\n\n38.69180097\n\n\n54.63373513\n\n\n35.65026857\n\n\n52.3432864\n\n\n42.56726887\n\n\n63.33804362\n\n\n50.59890308\n\n\n37.63919247\n\n\n59.37696628\n\n\n18.67329093\n\n\n54.38964021\n\n\n63.06390808\n\n\n46.93891551\n\n\n51.51429127\n\n\n36.894349\n\n\n39.4468133\n\n\n37.49439484\n\n\n55.36390317\n\n\n34.44384146\n\n\n55.11051526\n\n\n42.15687816\n\n\n62.0772059\n\n\n54.26095198\n\n\n27.80343776\n\n\n54.88860773\n\n\n39.24885819\n\n\n54.43726553\n\n\n36.56912751\n\n\n55.2716719\n\n\n45.19308202\n\n\n46.63295975\n\n\n60.01611704\n\n\n60.13479549\n\n\n58.93701177\n\n\n54.74633125\n\n\n56.93948681\n\n\n66.68034698\n\n\n54.35083944\n\n\n54.64243641\n\n\n38.69943311\n\n\n60.31738506\n\n\n58.11221859\n\n\n55.9463943\n\n\n57.56464739\n\n\n45.56764476\n\n\n59.88665378\n\n\n53.9823103\n\n\n44.98571504\n\n\n.\n\n\n50.46027028\n\n\n59.48623521\n\n\n58.03724434\n\n\n41.14199378\n\n\n51.73762348\n\n\n49.7688214\n\n\n36.15435142\n\n\n60.32759796\n\n\n51.15875349\n\n\n46.45174807\n\n\n52.97446739\n\n\n45.35281881\n\n\n59.74779958\n\n\n52.67471109\n\n\n42.71575136\n\n\n53.3488494\n\n\n53.57865383\n\n\n39.93013097\n\n\n52.35489484\n\n\n50.32335022\n\n\n55.59933924\n\n\n46.19286337\n\n\n54.22098038\n\n\n53.72354514\n\n\n57.47739718\n\n\n46.35886124\n\n\n43.39840936\n\n\n44.77639513\n\n\n53.03422465\n\n\n53.72826491\n\n\n53.20420752\n\n\n53.16952267\n\n\n53.83850285\n\n\n65.07759815\n\n\n51.23079462\n\n\n53.82368885\n\n\n40.10385762\n\n\n.\n\n\n52.3787484\n\n\n55.66949656\n\n\n23.79423763\n\n\n56.98149972\n\n\n59.20121015\n\n\n55.09594669\n\n\n53.01110333\n\n\n47.0391072\n\n\n45.75204577\n\n\n59.27784121\n\n\n48.56925582\n\n\n55.22839456\n\n\n48.31136197\n\n\n55.36959871\n\n\n53.45144035\n\n\n51.74797046\n\n\n45.53375201\n\n\n53.7982742\n\n\n57.15653943\n\n\n55.73144391\n\n\n52.51991074\n\n\n55.26700933\n\n\n56.11145567\n\n\n52.05647244\n\n\n53.52676905\n\n\n52.69205579\n\n\n60.44764058\n\n\n56.35070615\n\n\n43.40394074\n\n\n55.93603848\n\n\n61.28994581\n\n\n62.59771647\n\n\n60.66802482\n\n\n.\n\n\n49.274185\n\n\n52.33626542\n\n\n58.77354887\n\n\n46.75660197\n\n\n56.28739453\n\n\n38.16958173\n\n\n58.1696503\n\n\n17.70032703\n\n\n55.83226053\n\n\n56.95825394\n\n\n.\n\n\n38.21770894\n\n\n31.5513396\n\n\n54.23105808\n\n\n44.01075708\n\n\n40.12435951\n\n\n53.29779112\n\n\n56.17194295\n\n\n49.6398367\n\n\n28.64646786\n\n\n31.86271474\n\n\n49.50920425\n\n\n22.66041368\n\n\n52.99480015\n\n\n53.66576844\n\n\n38.27385499\n\n\n59.67908152\n\n\n52.53639476\n\n\n55.57893935\n\n\n47.94912484\n\n\n52.15523657\n\n\n51.28087447\n\n\n51.81123016\n\n\n46.4858302\n\n\n60.05827474\n\n\n61.95795404\n\n\n37.12384887\n\n\n44.68699843\n\n\n52.60298735\n\n\n45.68070522\n\n\n51.36933015\n\n\n36.3847059\n\n\n56.05001083\n\n\n56.76883777\n\n\n26.15727719\n\n\n46.05723754\n\n\n48.13784505\n\n\n57.01776308\n\n\n48.45195169\n\n\n53.58038327\n\n\n42.15041295\n\n\n54.18726713\n\n\n53.73463271\n\n\n39.3136758\n\n\n54.7007424\n\n\n34.63848872\n\n\n33.46390812\n\n\n37.38869862\n\n\n56.62325213\n\n\n55.05760016\n\n\n59.74373607\n\n\n35.50411895\n\n\n36.85726574\n\n\n53.99990401\n\n\n38.144681\n\n\n60.17604128\n\n\n48.52143672\n\n\n44.26968165\n\n\n57.69397613\n\n\n45.61123301\n\n\n58.06736229\n\n\n14.81651011\n\n\n36.42385941\n\n\n62.62035456\n\n\n51.91382617\n\n\n53.16781661\n\n\n56.63771935\n\n\n58.18005161\n\n\n30.51455715\n\n\n56.86584618\n\n\n56.59953343\n\n\n58.71825155\n\n\n.\n\n\n46.95482279\n\n\n52.8287454\n\n\n52.81698191\n\n\n56.50447544\n\n\n53.30413279\n\n\n60.29063498\n\n\n40.89996641\n\n\n61.3895996\n\n\n29.30508191\n\n\n52.81967256\n\n\n23.75663701\n\n\n59.4963798\n\n\n56.63603878\n\n\n54.83806261\n\n\n53.01523725\n\n\n60.2298188\n\n\n58.49967306\n\n\n61.34434318\n\n\n54.69360419\n\n\n55.41733192\n\n\n59.9678154\n\n\n55.46130957\n\n\n49.04399469\n\n\n58.16200744\n\n\n54.92756865\n\n\n54.94162111\n\n\n52.92097079\n\n\n52.62746632\n\n\n58.41715635\n\n\n55.33029578\n\n\n48.81594027\n\n\n53.16660964\n\n\n39.4162865\n\n\n53.23647532\n\n\n54.71869035\n\n\n61.05733361\n\n\n19.37360377\n\n\n51.39912523\n\n\n55.02911232\n\n\n56.64893781\n\n\n54.11552097\n\n\n54.40566455\n\n\n56.39935857\n\n\n55.6283582\n\n\n56.21035705\n\n\n45.62958322\n\n\n34.87897547\n\n\n50.26352341\n\n\n59.42432692\n\n\n55.23846661\n\n\n51.2120533\n\n\n56.3362477\n\n\n48.20186864\n\n\n48.13249302\n\n\n54.22472008\n\n\n58.81818813\n\n\n61.57061646\n\n\n55.41407716\n\n\n63.85868447\n\n\n46.50185878\n\n\n48.14919202\n\n\n50.20095288\n\n\n56.90202068\n\n\n44.24045509\n\n\n60.18656116\n\n\n55.24485501\n\n\n42.88280886\n\n\n45.55093566\n\n\n58.54357736\n\n\n20.26001409\n\n\n55.83992514\n\n\n56.29681555\n\n\n50.79762142\n\n\n38.77217418\n\n\n27.09633025\n\n\n38.65615557\n\n\n53.39942074\n\n\n33.89618406\n\n\n56.59201902\n\n\n54.20653799\n\n\n54.77584\n\n\n55.10739925\n\n\n48.28721047\n\n\n43.20589545\n\n\n51.99822162\n\n\n51.29438552\n\n\n51.20008431\n\n\n60.40584291\n\n\n56.3158132\n\n\n49.90558846\n\n\n25.68647563\n\n\n33.31734886\n\n\n54.78493094\n\n\n36.1200218\n\n\n53.26207743\n\n\n54.2248614\n\n\n42.45317818\n\n\n56.02616162\n\n\n53.57679013\n\n\n47.16648644\n\n\n52.15903928\n\n\n35.70557381\n\n\n56.01870188\n\n\n57.7696652\n\n\n46.31848933\n\n\n57.10703288\n\n\n52.47305857\n\n\n59.62512166\n\n\n64.17518882\n\n\n53.72867042\n\n\n43.69712796\n\n\n53.46262774\n\n\n48.305421\n\n\n54.81391666\n\n\n56.46926201\n\n\n39.36250436\n\n\n38.51193963\n\n\n55.40838997\n\n\n52.87926518\n\n\n30.9068803\n\n\n42.19121475\n\n\n56.0221704\n\n\n55.79328019\n\n\n52.51072831\n\n\n52.50232349\n\n\n49.14272207\n\n\n58.01979971\n\n\n59.1420783\n\n\n60.93401225\n\n\n.\n\n\n53.36116278\n\n\n64.43923806\n\n\n67.68282064\n\n\n47.44928746\n\n\n56.68648144\n\n\n55.22209175\n\n\n57.62106925\n\n\n47.76271595\n\n\n54.84770856\n\n\n55.30959506\n\n\n55.76971377\n\n\n55.35452344\n\n\n53.26285275\n\n\n58.72951869\n\n\n46.34355548\n\n\n38.02641378\n\n\n56.0235803\n\n\n54.01389277\n\n\n39.99346241\n\n\n40.38674079\n\n\n55.48677815\n\n\n53.32967664\n\n\n57.75295387\n\n\n57.2197485\n\n\n55.97326253\n\n\n38.78537748\n\n\n58.1078728\n\n\n57.17633861\n\n\n17.69219507\n\n\n47.2540263\n\n\n57.39310837\n\n\n54.59165977\n\n\n48.25786048\n\n\n47.74174237\n\n\n53.31672482\n\n\n40.50344679\n\n\n37.06113422\n\n\n47.44782798\n\n\n48.31929592\n\n\n54.77148063\n\n\n54.21198588\n\n\n58.4555573\n\n\n47.78406647\n\n\n56.23923695\n\n\n50.03857717\n\n\n46.35527335\n\n\n53.28381669\n\n\n27.91380609\n\n\n22.12152502\n\n\n33.78690379\n\n\n54.53907422\n\n\n47.72265642\n\n\n68.87474169\n\n\n54.53692053\n\n\n45.98733229\n\n\n65.93397401\n\n\n35.95793931\n\n\n60.32710181\n\n\n60.1914368\n\n\n42.81446332\n\n\n41.98962178\n\n\n39.73473699\n\n\n39.06159127\n\n\n32.85394745\n\n\n58.98891847\n\n\n52.47638299\n\n\n51.26120071\n\n\n20.6378771\n\n\n58.00606951\n\n\n46.15970883\n\n\n21.33654614\n\n\n33.83306267\n\n\n55.74270342\n\n\n54.30194334\n\n\n38.10020025\n\n\n54.4272594\n\n\n50.99776359\n\n\n51.36114215\n\n\n58.31331457\n\n\n50.79126414\n\n\n39.3366191\n\n\n52.98654382\n\n\n54.59976557\n\n\n58.22704499\n\n\n55.58523169\n\n\n58.89100235\n\n\n59.22795115\n\n\n49.76472438\n\n\n15.47930598\n\n\n40.45953771\n\n\n63.31764527\n\n\n54.10162049\n\n\n52.8630443\n\n\n56.47921222\n\n\n55.84686558\n\n\n41.78368244\n\n\n38.58335058\n\n\n53.14851544\n\n\n56.22968512\n\n\n26.42841445\n\n\n59.87383778\n\n\n54.28598566\n\n\n44.70012854\n\n\n53.15991797\n\n\n53.49601498\n\n\n54.86879928\n\n\n50.058313\n\n\n46.99197355\n\n\n15.8222927\n\n\n23.63548385\n\n\n42.58313605\n\n\n46.11083667\n\n\n38.61768111\n\n\n58.45311743\n\n\n37.47515339\n\n\n49.19110285\n\n\n19.87064079\n\n\n56.85101543\n\n\n33.79904827\n\n\n54.94683015\n\n\n63.86416738\n\n\n45.8081066\n\n\n20.7151777\n\n\n50.0525685\n\n\n33.15703362\n\n\n37.73525253\n\n\n36.9714504\n\n\n50.1794763\n\n\n54.00328658\n\n\n38.97511538\n\n\n38.32416886\n\n\n58.15349309\n\n\n36.41792223\n\n\n55.14364065\n\n\n55.58625185\n\n\n36.26972605\n\n\n52.40062027\n\n\n34.79666856\n\n\n41.52918041\n\n\n55.60166734\n\n\n42.17205661\n\n\n62.64358763\n\n\n27.51379334\n\n\n58.64434823\n\n\n54.47494885\n\n\n42.07188579\n\n\n54.23808849\n\n\n53.51109334\n\n\n55.451048\n\n\n50.14490357\n\n\n46.73424336\n\n\n15.94313281\n\n\n22.02224775\n\n\n44.15628744\n\n\n46.36818328\n\n\n37.90010584\n\n\n58.17722864\n\n\n38.61167569\n\n\n48.65899053\n\n\n18.195961\n\n\n54.34063468\n\n\n34.23861677\n\n\n53.81085519\n\n\n63.29617763\n\n\n45.45858937\n\n\n19.89597839\n\n\n51.24280804\n\n\n34.28940357\n\n\n38.51719566\n\n\n35.86717035\n\n\n53.07985439\n\n\n52.68494989\n\n\n39.6738891\n\n\n39.69051295\n\n\n59.65980958\n\n\n36.88305058\n\n\n54.85819418\n\n\n54.42897289\n\n\n35.86703372\n\n\n53.82528582\n\n\n35.12220343\n\n\n41.43358806\n\n\n55.24317213\n\n\n40.98869961\n\n\n63.56650385\n\n\n\n\n70\n\n\n2\n\n\nHASHV\n\n\nFrequency of Hash/Marijuana Use\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n.\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n2\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n1\n\n\n2\n\n\n.\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n.\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n2\n\n\n2\n\n\n.\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n.\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n.\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n.\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n.\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n.\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n2\n\n\n1\n\n\n.\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n2\n\n\n.\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n.\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n.\n\n\n1\n\n\n2\n\n\n2\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n2\n\n\n2\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n2\n\n\n1\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n.\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n.\n\n\n1\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n.\n\n\n2\n\n\n\n\n71\n\n\n2\n\n\nBMI\n\n\nBMI\n\n\n27.16420923\n\n\n26.96036793\n\n\n28.18509751\n\n\n20.28485484\n\n\n20.80192959\n\n\n22.41000894\n\n\n25.46551381\n\n\n31.36061504\n\n\n23.93400584\n\n\n21.07192056\n\n\n.\n\n\n20.25611118\n\n\n22.48906873\n\n\n26.64129603\n\n\n20.93600827\n\n\n30.25431222\n\n\n23.1812287\n\n\n26.73805037\n\n\n34.70866681\n\n\n998.8512852\n\n\n30.0084378\n\n\n25.54188066\n\n\n28.45378539\n\n\n29.31100018\n\n\n22.50568715\n\n\n21.39584369\n\n\n28.57876082\n\n\n22.55871711\n\n\n18.95861488\n\n\n21.14861022\n\n\n19.51836193\n\n\n25.37753904\n\n\n30.4691107\n\n\n24.7462187\n\n\n28.06058273\n\n\n1000.114647\n\n\n26.91574079\n\n\n25.33042529\n\n\n28.18113485\n\n\n27.28934952\n\n\n21.89439124\n\n\n22.54073192\n\n\n32.59591503\n\n\n23.25633774\n\n\n18.95345757\n\n\n19.06541618\n\n\n23.45637925\n\n\n26.7219056\n\n\n30.41635157\n\n\n26.01101799\n\n\n27.9083614\n\n\n999.5550573\n\n\n27.37114718\n\n\n.\n\n\n26.37870494\n\n\n36.21202997\n\n\n23.4682649\n\n\n30.01736652\n\n\n26.00772531\n\n\n25.59975172\n\n\n23.39399312\n\n\n24.90362324\n\n\n23.4939203\n\n\n31.00841859\n\n\n24.89247461\n\n\n19.4026479\n\n\n28.39192105\n\n\n22.1078539\n\n\n32.21844351\n\n\n24.02500916\n\n\n20.69258197\n\n\n25.21932109\n\n\n24.89981059\n\n\n24.84642328\n\n\n26.66828382\n\n\n27.37145391\n\n\n20.17249682\n\n\n24.58631648\n\n\n31.69358402\n\n\n27.75109464\n\n\n28.42807723\n\n\n28.13518265\n\n\n20.42436365\n\n\n23.6386924\n\n\n28.01566567\n\n\n.\n\n\n30.10984505\n\n\n16.61994836\n\n\n.\n\n\n23.92077368\n\n\n23.51005331\n\n\n24.4173064\n\n\n29.86535787\n\n\n19.65325391\n\n\n22.89730781\n\n\n36.30084622\n\n\n26.52178932\n\n\n.\n\n\n27.18675255\n\n\n28.90289877\n\n\n26.94975812\n\n\n23.1933723\n\n\n25.784311\n\n\n23.37899133\n\n\n27.67878066\n\n\n35.88491967\n\n\n39.58390987\n\n\n25.87281161\n\n\n26.87004918\n\n\n28.5841569\n\n\n20.92408221\n\n\n24.83583678\n\n\n22.29699137\n\n\n23.34785894\n\n\n.\n\n\n.\n\n\n22.32810014\n\n\n.\n\n\n22.56586793\n\n\n24.76767878\n\n\n23.2222675\n\n\n23.83790329\n\n\n24.76852926\n\n\n-1\n\n\n24.2525239\n\n\n21.40491563\n\n\n18.78919795\n\n\n.\n\n\n26.49900089\n\n\n23.81934516\n\n\n36.00043725\n\n\n29.74638687\n\n\n29.54560634\n\n\n23.78137233\n\n\n.\n\n\n23.01878\n\n\n41.13526074\n\n\n.\n\n\n20.89599588\n\n\n28.12540235\n\n\n18.59647308\n\n\n20.61279578\n\n\n29.29364363\n\n\n21.86331744\n\n\n.\n\n\n25.49740256\n\n\n18.94221654\n\n\n23.74580158\n\n\n19.39956944\n\n\n33.88456923\n\n\n22.38151925\n\n\n25.67429921\n\n\n33.75067315\n\n\n22.43776976\n\n\n27.25216966\n\n\n-1\n\n\n29.09631038\n\n\n39.77888252\n\n\n29.71020099\n\n\n19.69868518\n\n\n22.15563229\n\n\n24.12812266\n\n\n25.06721495\n\n\n23.1152224\n\n\n21.2943742\n\n\n20.18366955\n\n\n28.60249815\n\n\n27.59039794\n\n\n31.74351291\n\n\n20.87303427\n\n\n24.31805997\n\n\n.\n\n\n20.65004697\n\n\n26.28639768\n\n\n29.38705517\n\n\n19.18107815\n\n\n24.48514688\n\n\n.\n\n\n26.88803544\n\n\n22.25393457\n\n\n26.17359553\n\n\n20.23341504\n\n\n23.74721143\n\n\n24.14268057\n\n\n29.5027734\n\n\n18.3274927\n\n\n21.15721844\n\n\n25.0471692\n\n\n.\n\n\n33.98938474\n\n\n24.81098798\n\n\n28.19027415\n\n\n27.19730945\n\n\n27.70972555\n\n\n24.41219013\n\n\n26.69298194\n\n\n24.17507215\n\n\n28.1651686\n\n\n23.785999\n\n\n25.68356101\n\n\n32.07433024\n\n\n25.04404027\n\n\n31.29638222\n\n\n31.9784961\n\n\n28.90162257\n\n\n44.29591187\n\n\n28.63031066\n\n\n.\n\n\n.\n\n\n23.05132676\n\n\n26.39940234\n\n\n25.18514832\n\n\n27.33230641\n\n\n22.47674874\n\n\n29.88601907\n\n\n26.45550379\n\n\n24.74877551\n\n\n24.89601754\n\n\n.\n\n\n24.30096262\n\n\n27.82133248\n\n\n26.10730478\n\n\n.\n\n\n21.50850347\n\n\n24.31868828\n\n\n.\n\n\n20.67330758\n\n\n26.72144887\n\n\n20.08660394\n\n\n21.1072291\n\n\n19.85574847\n\n\n21.62981671\n\n\n26.32956157\n\n\n25.99291873\n\n\n37.84153226\n\n\n26.23926945\n\n\n35.2885235\n\n\n20.8820298\n\n\n24.06146944\n\n\n17.94502997\n\n\n24.6179348\n\n\n29.04838289\n\n\n23.3634697\n\n\n26.20650872\n\n\n27.83824139\n\n\n22.52252101\n\n\n24.12835679\n\n\n23.25994647\n\n\n21.35973756\n\n\n18.69816895\n\n\n.\n\n\n18.02263512\n\n\n24.48765083\n\n\n.\n\n\n30.79641384\n\n\n.\n\n\n.\n\n\n999\n\n\n24.29341715\n\n\n31.94896064\n\n\n23.60131479\n\n\n23.49041576\n\n\n22.49588867\n\n\n25.77136268\n\n\n30.17659224\n\n\n29.08070674\n\n\n30.35171171\n\n\n39.19706389\n\n\n22.82679255\n\n\n25.35538705\n\n\n20.873141\n\n\n20.87649981\n\n\n34.15591156\n\n\n31.45747084\n\n\n22.87282378\n\n\n19.15160445\n\n\n20.95526694\n\n\n28.56723487\n\n\n26.82623641\n\n\n22.38089956\n\n\n22.01090348\n\n\n20.38948099\n\n\n.\n\n\n24.25521902\n\n\n22.29335603\n\n\n23.87565792\n\n\n28.45832778\n\n\n25.91078245\n\n\n28.23865567\n\n\n27.20649112\n\n\n25.70117167\n\n\n24.74731184\n\n\n23.95039696\n\n\n31.70459802\n\n\n.\n\n\n25.48151389\n\n\n32.6558507\n\n\n22.08544232\n\n\n23.44802432\n\n\n22.7263371\n\n\n21.49326381\n\n\n23.01104604\n\n\n28.76094408\n\n\n.\n\n\n21.6878738\n\n\n-1\n\n\n.\n\n\n21.62271599\n\n\n18.02948103\n\n\n.\n\n\n24.92677311\n\n\n.\n\n\n24.99247286\n\n\n20.71640526\n\n\n.\n\n\n.\n\n\n21.91438394\n\n\n22.7422951\n\n\n.\n\n\n32.28362449\n\n\n29.73550417\n\n\n23.77064566\n\n\n28.86940561\n\n\n25.70926573\n\n\n.\n\n\n23.67910222\n\n\n20.43926666\n\n\n20.79840497\n\n\n21.3561065\n\n\n21.64835402\n\n\n24.07756277\n\n\n18.5114061\n\n\n.\n\n\n20.76899709\n\n\n43.6214588\n\n\n23.78296225\n\n\n19.81136254\n\n\n22.86094876\n\n\n29.91885734\n\n\n23.63717766\n\n\n29.12175063\n\n\n.\n\n\n33.95961375\n\n\n21.76145638\n\n\n21.79593286\n\n\n25.72416666\n\n\n.\n\n\n20.73316301\n\n\n.\n\n\n29.46650695\n\n\n21.99813869\n\n\n27.03951874\n\n\n28.38223496\n\n\n36.00892811\n\n\n34.75739676\n\n\n20.85032726\n\n\n27.01940831\n\n\n25.57251734\n\n\n22.10213496\n\n\n31.87813544\n\n\n25.05550455\n\n\n28.50389963\n\n\n999\n\n\n23.50689518\n\n\n25.94645273\n\n\n25.86198801\n\n\n31.16049789\n\n\n30.18500627\n\n\n25.51402695\n\n\n24.35123184\n\n\n26.14906742\n\n\n23.47827116\n\n\n32.52301588\n\n\n40.87002525\n\n\n27.67134213\n\n\n32.08220311\n\n\n27.50205262\n\n\n27.64997575\n\n\n.\n\n\n17.95834207\n\n\n30.21563057\n\n\n22.65785975\n\n\n28.58988691\n\n\n26.09211031\n\n\n20.43625678\n\n\n28.26162747\n\n\n33.92867379\n\n\n28.31615331\n\n\n23.30899234\n\n\n.\n\n\n23.24712811\n\n\n28.63244566\n\n\n24.24959922\n\n\n26.8880357\n\n\n20.80855523\n\n\n28.27081319\n\n\n29.69674712\n\n\n28.86226311\n\n\n20.42014196\n\n\n21.2527861\n\n\n26.22669778\n\n\n23.60403725\n\n\n34.34353678\n\n\n28.7416662\n\n\n29.01917773\n\n\n27.00498988\n\n\n30.62697138\n\n\n26.74667346\n\n\n31.40196155\n\n\n38.5769143\n\n\n24.74718944\n\n\n31.96077184\n\n\n34.43023916\n\n\n.\n\n\n.\n\n\n.\n\n\n26.222232\n\n\n29.58234438\n\n\n26.81982722\n\n\n26.28450201\n\n\n25.85010033\n\n\n25.26233037\n\n\n22.47991453\n\n\n23.41611416\n\n\n999\n\n\n22.35557262\n\n\n28.41369024\n\n\n31.06518505\n\n\n16.84781098\n\n\n.\n\n\n28.80147918\n\n\n.\n\n\n29.94039006\n\n\n28.116523\n\n\n23.43216886\n\n\n23.94884546\n\n\n.\n\n\n27.54355497\n\n\n.\n\n\n26.46121303\n\n\n.\n\n\n23.10211316\n\n\n22.59281203\n\n\n24.36465456\n\n\n22.1878524\n\n\n21.26847866\n\n\n22.26423475\n\n\n30.65961879\n\n\n21.19198709\n\n\n24.33625495\n\n\n999\n\n\n23.7301267\n\n\n.\n\n\n21.05210794\n\n\n23.935653\n\n\n29.50955176\n\n\n30.78327831\n\n\n25.34275166\n\n\n.\n\n\n30.18812865\n\n\n33.43448764\n\n\n25.31414258\n\n\n26.7615941\n\n\n27.23203965\n\n\n25.38116989\n\n\n29.91126609\n\n\n19.26082964\n\n\n25.17517249\n\n\n19.65776479\n\n\n29.35412296\n\n\n29.01351798\n\n\n26.32266806\n\n\n-1\n\n\n21.45260568\n\n\n27.85761122\n\n\n34.35580704\n\n\n27.57888716\n\n\n28.9065307\n\n\n29.37284025\n\n\n28.76314027\n\n\n24.58803539\n\n\n22.90376283\n\n\n19.95631774\n\n\n18.89567102\n\n\n21.09640492\n\n\n29.58180819\n\n\n22.8859962\n\n\n19.77258055\n\n\n19.10813443\n\n\n25.10216112\n\n\n19.71358264\n\n\n21.5499826\n\n\n.\n\n\n23.24599072\n\n\n19.32261327\n\n\n25.28773594\n\n\n25.25305263\n\n\n31.93928974\n\n\n31.60412407\n\n\n29.88856876\n\n\n24.34515677\n\n\n27.9725462\n\n\n25.10113515\n\n\n999\n\n\n23.82324495\n\n\n25.06929596\n\n\n27.71814884\n\n\n27.23958478\n\n\n29.60910945\n\n\n19.61986601\n\n\n26.32731255\n\n\n19.91558338\n\n\n28.43584636\n\n\n27.98779281\n\n\n25.0732554\n\n\n-1\n\n\n19.41456289\n\n\n28.49737298\n\n\n34.81387075\n\n\n27.13312619\n\n\n28.40772335\n\n\n26.52381448\n\n\n28.39676701\n\n\n27.38768545\n\n\n22.19785187\n\n\n19.5495792\n\n\n19.93956244\n\n\n22.75523831\n\n\n32.05751155\n\n\n22.61282601\n\n\n21.88688688\n\n\n19.09017917\n\n\n26.24179768\n\n\n21.04637794\n\n\n19.47538438\n\n\n.\n\n\n23.79357939\n\n\n23.06066983\n\n\n26.2361762\n\n\n24.38052126\n\n\n29.09195228\n\n\n30.44565201\n\n\n30.55843854\n\n\n24.7982166\n\n\n27.62838612\n\n\n25.90623754\n\n\n999\n\n\n24.52528845\n\n\n25.84396484\n\n\n28.25922617\n\n\n\n\n72\n\n\n2\n\n\nHBP\n\n\nHigh Blood Pressure\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n9\n\n\n2\n\n\n1\n\n\n9\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n2\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n9\n\n\n2\n\n\n1\n\n\n9\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n3\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n3\n\n\n1\n\n\n1\n\n\n1\n\n\n4\n\n\n2\n\n\n1\n\n\n2\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n9\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n2\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n4\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n9\n\n\n1\n\n\n2\n\n\n9\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n9\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n9\n\n\n1\n\n\n9\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n3\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n9\n\n\n3\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n3\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n3\n\n\n1\n\n\n9\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n9\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n\n\n73\n\n\n2\n\n\nDIAB\n\n\nDiabetes\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n2\n\n\n9\n\n\n1\n\n\n1\n\n\n3\n\n\n2\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n2\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n2\n\n\n9\n\n\n9\n\n\n2\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n3\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n2\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n2\n\n\n9\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n3\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n2\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n3\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n3\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n2\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n2\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n3\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n3\n\n\n1\n\n\n2\n\n\n3\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n2\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n3\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n3\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n2\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n3\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n2\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n2\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n2\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n2\n\n\n9\n\n\n1\n\n\n9\n\n\n2\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n2\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n2\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n2\n\n\n9\n\n\n1\n\n\n9\n\n\n\n\n74\n\n\n2\n\n\nLIV34\n\n\nLiver Disease Stage 3/4\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n2\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n2\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n2\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n\n\n75\n\n\n2\n\n\nKID\n\n\nKidney Disease\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n2\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n2\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n2\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n2\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n2\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n2\n\n\n2\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n2\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n2\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n2\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n2\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n2\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n2\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n2\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n2\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n2\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n2\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n2\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n2\n\n\n9\n\n\n9\n\n\n1\n\n\n2\n\n\n9\n\n\n1\n\n\n1\n\n\n2\n\n\n9\n\n\n2\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n2\n\n\n9\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n2\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n2\n\n\n9\n\n\n9\n\n\n1\n\n\n2\n\n\n9\n\n\n1\n\n\n1\n\n\n2\n\n\n9\n\n\n2\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n2\n\n\n9\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n2\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n\n\n76\n\n\n2\n\n\nFRP\n\n\nFrailty Related Phenotype\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n\n\n77\n\n\n2\n\n\nFP\n\n\nFrailty Phenotype\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n2\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n2\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n2\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n9\n\n\n1\n\n\n1\n\n\n2\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n\n\n78\n\n\n2\n\n\nTCHOL\n\n\nTotal Cholesterol\n\n\n180\n\n\n134\n\n\n180\n\n\n251\n\n\nNA\n\n\n151\n\n\n211\n\n\n195\n\n\n191\n\n\n133\n\n\n97\n\n\n116\n\n\n201\n\n\n117\n\n\n128\n\n\n144\n\n\n205\n\n\n178\n\n\n177\n\n\n187\n\n\n162\n\n\n180\n\n\n134\n\n\n180\n\n\nNA\n\n\n151\n\n\n195\n\n\n191\n\n\n133\n\n\n116\n\n\n201\n\n\n117\n\n\n144\n\n\n205\n\n\n178\n\n\n187\n\n\n162\n\n\n180\n\n\n134\n\n\n180\n\n\nNA\n\n\n151\n\n\n195\n\n\n191\n\n\n133\n\n\n116\n\n\n201\n\n\n117\n\n\n144\n\n\n205\n\n\n178\n\n\n187\n\n\n162\n\n\n122\n\n\n227\n\n\nNA\n\n\n220\n\n\n231\n\n\nNA\n\n\n220\n\n\nNA\n\n\n173\n\n\n168\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n187\n\n\n157\n\n\n134\n\n\nNA\n\n\nNA\n\n\n219\n\n\n166\n\n\nNA\n\n\n146\n\n\n142\n\n\n200\n\n\nNA\n\n\n167\n\n\nNA\n\n\n182\n\n\nNA\n\n\n150\n\n\n217\n\n\n242\n\n\nNA\n\n\nNA\n\n\n125\n\n\n375\n\n\nNA\n\n\nNA\n\n\n166\n\n\n237\n\n\n141\n\n\nNA\n\n\n202\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n146\n\n\n191\n\n\n211\n\n\nNA\n\n\n220\n\n\nNA\n\n\n238\n\n\n228\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n147\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n191\n\n\n252\n\n\n176\n\n\nNA\n\n\n319\n\n\nNA\n\n\n220\n\n\n156\n\n\n229\n\n\nNA\n\n\n104\n\n\n137\n\n\nNA\n\n\n180\n\n\n242\n\n\nNA\n\n\n256\n\n\n194\n\n\n194\n\n\nNA\n\n\n203\n\n\n199\n\n\nNA\n\n\n213\n\n\n206\n\n\nNA\n\n\n189\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n226\n\n\n155\n\n\n238\n\n\n122\n\n\n120\n\n\n221\n\n\n146\n\n\nNA\n\n\n171\n\n\n170\n\n\n143\n\n\nNA\n\n\nNA\n\n\n202\n\n\nNA\n\n\n210\n\n\nNA\n\n\nNA\n\n\n151\n\n\n177\n\n\n187\n\n\nNA\n\n\n279\n\n\nNA\n\n\nNA\n\n\n290\n\n\n238\n\n\n222\n\n\nNA\n\n\n138\n\n\nNA\n\n\n174\n\n\n271\n\n\n173\n\n\n158\n\n\n230\n\n\n181\n\n\n180\n\n\n216\n\n\n252\n\n\n245\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n272\n\n\nNA\n\n\n109\n\n\n134\n\n\n134\n\n\n223\n\n\nNA\n\n\n110\n\n\n287\n\n\n613\n\n\n114\n\n\n278\n\n\nNA\n\n\n304\n\n\nNA\n\n\nNA\n\n\n136\n\n\n156\n\n\n227\n\n\nNA\n\n\n187\n\n\nNA\n\n\nNA\n\n\n180\n\n\nNA\n\n\n144\n\n\n181\n\n\nNA\n\n\nNA\n\n\n161\n\n\n247\n\n\n174\n\n\n226\n\n\nNA\n\n\nNA\n\n\n193\n\n\nNA\n\n\n251\n\n\n206\n\n\n178\n\n\nNA\n\n\nNA\n\n\n176\n\n\n246\n\n\n118\n\n\nNA\n\n\n225\n\n\n133\n\n\nNA\n\n\n116\n\n\n197\n\n\n171\n\n\n230\n\n\n135\n\n\nNA\n\n\n212\n\n\n156\n\n\n147\n\n\n214\n\n\n157\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n167\n\n\nNA\n\n\n199\n\n\nNA\n\n\n273\n\n\n115\n\n\n190\n\n\n57\n\n\n229\n\n\n182\n\n\n191\n\n\n209\n\n\n196\n\n\n196\n\n\n154\n\n\n191\n\n\nNA\n\n\n177\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n326\n\n\n151\n\n\n131\n\n\nNA\n\n\n177\n\n\nNA\n\n\nNA\n\n\n161\n\n\n220\n\n\n109\n\n\n151\n\n\nNA\n\n\n216\n\n\n167\n\n\nNA\n\n\n241\n\n\n211\n\n\nNA\n\n\n302\n\n\n195\n\n\n161\n\n\n154\n\n\n220\n\n\nNA\n\n\n191\n\n\n205\n\n\nNA\n\n\n248\n\n\n295\n\n\nNA\n\n\n199\n\n\n189\n\n\n259\n\n\n133\n\n\nNA\n\n\nNA\n\n\n249\n\n\nNA\n\n\nNA\n\n\n160\n\n\nNA\n\n\n152\n\n\n218\n\n\nNA\n\n\n97\n\n\n244\n\n\n198\n\n\n258\n\n\n194\n\n\n123\n\n\n167\n\n\n177\n\n\nNA\n\n\n136\n\n\n130\n\n\n150\n\n\n180\n\n\n145\n\n\n232\n\n\n196\n\n\n187\n\n\nNA\n\n\n116\n\n\n282\n\n\n206\n\n\n172\n\n\nNA\n\n\nNA\n\n\n274\n\n\n244\n\n\n200\n\n\n164\n\n\nNA\n\n\n167\n\n\nNA\n\n\n311\n\n\n201\n\n\n117\n\n\n182\n\n\n202\n\n\n144\n\n\n128\n\n\nNA\n\n\n170\n\n\n225\n\n\n194\n\n\n255\n\n\nNA\n\n\n202\n\n\nNA\n\n\nNA\n\n\n215\n\n\n202\n\n\n231\n\n\n223\n\n\n237\n\n\n162\n\n\n153\n\n\n156\n\n\n131\n\n\n171\n\n\n143\n\n\n207\n\n\n264\n\n\nNA\n\n\n124\n\n\n114\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n131\n\n\n249\n\n\n151\n\n\n196\n\n\n224\n\n\n172\n\n\n213\n\n\n202\n\n\nNA\n\n\n133\n\n\n207\n\n\n207\n\n\n144\n\n\n204\n\n\n231\n\n\n222\n\n\n170\n\n\n205\n\n\n168\n\n\nNA\n\n\n181\n\n\n178\n\n\nNA\n\n\n160\n\n\n204\n\n\n178\n\n\n203\n\n\n228\n\n\n177\n\n\nNA\n\n\nNA\n\n\n229\n\n\nNA\n\n\n224\n\n\nNA\n\n\nNA\n\n\n164\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n187\n\n\n136\n\n\n189\n\n\n362\n\n\nNA\n\n\n171\n\n\nNA\n\n\n125\n\n\n350\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n155\n\n\n192\n\n\nNA\n\n\n195\n\n\n200\n\n\n211\n\n\n155\n\n\n214\n\n\n143\n\n\n139\n\n\n268\n\n\n215\n\n\nNA\n\n\nNA\n\n\n145\n\n\nNA\n\n\n180\n\n\nNA\n\n\n138\n\n\n162\n\n\n237\n\n\n195\n\n\n159\n\n\n173\n\n\nNA\n\n\nNA\n\n\n168\n\n\n200\n\n\n227\n\n\nNA\n\n\nNA\n\n\n166\n\n\n141\n\n\nNA\n\n\n220\n\n\n180\n\n\n143\n\n\nNA\n\n\n138\n\n\n272\n\n\n109\n\n\n134\n\n\n180\n\n\n144\n\n\n225\n\n\nNA\n\n\n151\n\n\n161\n\n\n151\n\n\n195\n\n\n191\n\n\n199\n\n\n133\n\n\nNA\n\n\n130\n\n\n116\n\n\nNA\n\n\n200\n\n\n201\n\n\n117\n\n\nNA\n\n\n114\n\n\nNA\n\n\n144\n\n\n205\n\n\n178\n\n\nNA\n\n\n187\n\n\nNA\n\n\n138\n\n\n162\n\n\n227\n\n\nNA\n\n\nNA\n\n\n166\n\n\n141\n\n\nNA\n\n\n220\n\n\n180\n\n\n143\n\n\nNA\n\n\n138\n\n\n272\n\n\n109\n\n\n134\n\n\n180\n\n\n144\n\n\n225\n\n\nNA\n\n\n151\n\n\n161\n\n\n151\n\n\n195\n\n\n191\n\n\n199\n\n\n133\n\n\nNA\n\n\n130\n\n\n116\n\n\nNA\n\n\n200\n\n\n201\n\n\n117\n\n\nNA\n\n\n114\n\n\nNA\n\n\n144\n\n\n205\n\n\n178\n\n\nNA\n\n\n187\n\n\nNA\n\n\n138\n\n\n162\n\n\n\n\n79\n\n\n2\n\n\nTRIG\n\n\nTriglycerides\n\n\n233\n\n\nNA\n\n\n82\n\n\n260\n\n\nNA\n\n\n125\n\n\n127\n\n\n84\n\n\nNA\n\n\nNA\n\n\n36\n\n\n42\n\n\n143\n\n\nNA\n\n\n80\n\n\nNA\n\n\n78\n\n\n86\n\n\nNA\n\n\n62\n\n\nNA\n\n\n233\n\n\nNA\n\n\n82\n\n\nNA\n\n\n125\n\n\n84\n\n\nNA\n\n\nNA\n\n\n42\n\n\n143\n\n\nNA\n\n\nNA\n\n\n78\n\n\n86\n\n\n62\n\n\nNA\n\n\n233\n\n\nNA\n\n\n82\n\n\nNA\n\n\n125\n\n\n84\n\n\nNA\n\n\nNA\n\n\n42\n\n\n143\n\n\nNA\n\n\nNA\n\n\n78\n\n\n86\n\n\n62\n\n\nNA\n\n\n81\n\n\n67\n\n\nNA\n\n\n107\n\n\n254\n\n\nNA\n\n\n132\n\n\nNA\n\n\n113\n\n\n150\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n114\n\n\n58\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n125\n\n\nNA\n\n\nNA\n\n\n54\n\n\nNA\n\n\nNA\n\n\n218\n\n\nNA\n\n\n110\n\n\nNA\n\n\n124\n\n\nNA\n\n\n270\n\n\nNA\n\n\nNA\n\n\n91\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n207\n\n\n166\n\n\n129\n\n\nNA\n\n\n145\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n245\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n58\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n154\n\n\n240\n\n\n294\n\n\nNA\n\n\n899\n\n\nNA\n\n\n59\n\n\nNA\n\n\n117\n\n\nNA\n\n\n88\n\n\n120\n\n\nNA\n\n\n233\n\n\nNA\n\n\nNA\n\n\n356\n\n\n146\n\n\n189\n\n\nNA\n\n\nNA\n\n\n101\n\n\nNA\n\n\n109\n\n\n115\n\n\nNA\n\n\n66\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n525\n\n\nNA\n\n\n106\n\n\n54\n\n\n173\n\n\n95\n\n\n132\n\n\nNA\n\n\n121\n\n\nNA\n\n\n68\n\n\nNA\n\n\nNA\n\n\n89\n\n\nNA\n\n\n62\n\n\nNA\n\n\nNA\n\n\n66\n\n\n93\n\n\n70\n\n\nNA\n\n\n183\n\n\nNA\n\n\nNA\n\n\n224\n\n\n62\n\n\n107\n\n\nNA\n\n\n113\n\n\nNA\n\n\n61\n\n\nNA\n\n\n130\n\n\n56\n\n\n127\n\n\n200\n\n\nNA\n\n\n144\n\n\nNA\n\n\n108\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n525\n\n\nNA\n\n\nNA\n\n\n66\n\n\nNA\n\n\n227\n\n\nNA\n\n\n64\n\n\n574\n\n\n179\n\n\n28\n\n\n772\n\n\nNA\n\n\n325\n\n\nNA\n\n\nNA\n\n\n132\n\n\n161\n\n\n250\n\n\nNA\n\n\n454\n\n\nNA\n\n\nNA\n\n\n82\n\n\nNA\n\n\n168\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n110\n\n\n438\n\n\n83\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n103\n\n\nNA\n\n\n260\n\n\n353\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n471\n\n\n89\n\n\nNA\n\n\nNA\n\n\n178\n\n\n99\n\n\nNA\n\n\n81\n\n\nNA\n\n\nNA\n\n\n92\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n627\n\n\n49\n\n\n59\n\n\n242\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n71\n\n\nNA\n\n\n115\n\n\nNA\n\n\nNA\n\n\n91\n\n\n72\n\n\n73\n\n\n171\n\n\n185\n\n\n142\n\n\n128\n\n\n78\n\n\n135\n\n\n134\n\n\nNA\n\n\nNA\n\n\n84\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n85\n\n\n87\n\n\n92\n\n\nNA\n\n\n290\n\n\nNA\n\n\nNA\n\n\n142\n\n\n145\n\n\nNA\n\n\n125\n\n\nNA\n\n\n303\n\n\n113\n\n\nNA\n\n\nNA\n\n\n127\n\n\nNA\n\n\n896\n\n\n84\n\n\n205\n\n\nNA\n\n\n288\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n149\n\n\n104\n\n\nNA\n\n\n183\n\n\nNA\n\n\n330\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n105\n\n\nNA\n\n\nNA\n\n\n110\n\n\nNA\n\n\n67\n\n\n136\n\n\nNA\n\n\n36\n\n\n536\n\n\n86\n\n\n346\n\n\nNA\n\n\nNA\n\n\n145\n\n\n128\n\n\nNA\n\n\n118\n\n\nNA\n\n\n65\n\n\n329\n\n\n97\n\n\n62\n\n\n150\n\n\nNA\n\n\nNA\n\n\n42\n\n\n504\n\n\n179\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n173\n\n\n113\n\n\n114\n\n\n176\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n509\n\n\n143\n\n\nNA\n\n\n354\n\n\nNA\n\n\n105\n\n\n80\n\n\nNA\n\n\n138\n\n\n158\n\n\n276\n\n\n167\n\n\nNA\n\n\n108\n\n\nNA\n\n\nNA\n\n\n135\n\n\n78\n\n\nNA\n\n\n220\n\n\n272\n\n\n85\n\n\n108\n\n\n358\n\n\n81\n\n\n261\n\n\nNA\n\n\nNA\n\n\n507\n\n\nNA\n\n\n103\n\n\n245\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n238\n\n\n157\n\n\nNA\n\n\n301\n\n\nNA\n\n\n110\n\n\n148\n\n\nNA\n\n\n147\n\n\n217\n\n\n195\n\n\nNA\n\n\n119\n\n\nNA\n\n\n78\n\n\n96\n\n\n78\n\n\n260\n\n\nNA\n\n\nNA\n\n\n86\n\n\nNA\n\n\n102\n\n\nNA\n\n\n280\n\n\n308\n\n\n269\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n299\n\n\nNA\n\n\nNA\n\n\n156\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n62\n\n\n179\n\n\n119\n\n\nNA\n\n\nNA\n\n\n54\n\n\nNA\n\n\n94\n\n\n119\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n113\n\n\n230\n\n\nNA\n\n\n86\n\n\n191\n\n\n157\n\n\n112\n\n\n42\n\n\n98\n\n\n56\n\n\n83\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n77\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n134\n\n\nNA\n\n\n97\n\n\n268\n\n\nNA\n\n\n151\n\n\nNA\n\n\nNA\n\n\n80\n\n\n162\n\n\n67\n\n\nNA\n\n\nNA\n\n\n207\n\n\n129\n\n\nNA\n\n\nNA\n\n\n233\n\n\n68\n\n\nNA\n\n\n113\n\n\n525\n\n\nNA\n\n\nNA\n\n\n82\n\n\n168\n\n\n178\n\n\nNA\n\n\n87\n\n\n142\n\n\n125\n\n\n84\n\n\nNA\n\n\n183\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n42\n\n\nNA\n\n\n114\n\n\n143\n\n\nNA\n\n\nNA\n\n\n245\n\n\nNA\n\n\nNA\n\n\n78\n\n\n86\n\n\nNA\n\n\n62\n\n\nNA\n\n\n134\n\n\nNA\n\n\n67\n\n\nNA\n\n\nNA\n\n\n207\n\n\n129\n\n\nNA\n\n\nNA\n\n\n233\n\n\n68\n\n\nNA\n\n\n113\n\n\n525\n\n\nNA\n\n\nNA\n\n\n82\n\n\n168\n\n\n178\n\n\nNA\n\n\n87\n\n\n142\n\n\n125\n\n\n84\n\n\nNA\n\n\n183\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n42\n\n\nNA\n\n\n114\n\n\n143\n\n\nNA\n\n\nNA\n\n\n245\n\n\nNA\n\n\nNA\n\n\n78\n\n\n86\n\n\nNA\n\n\n62\n\n\nNA\n\n\n134\n\n\nNA\n\n\n\n\n80\n\n\n2\n\n\nLDL\n\n\nLDL\n\n\n86\n\n\nNA\n\n\n127\n\n\n152\n\n\nNA\n\n\n81\n\n\n142\n\n\n137\n\n\nNA\n\n\nNA\n\n\n63\n\n\n47\n\n\n107\n\n\nNA\n\n\n47\n\n\n87\n\n\n125\n\n\n122\n\n\nNA\n\n\n124\n\n\nNA\n\n\n86\n\n\nNA\n\n\n127\n\n\nNA\n\n\n81\n\n\n137\n\n\nNA\n\n\nNA\n\n\n47\n\n\n107\n\n\nNA\n\n\n87\n\n\n125\n\n\n122\n\n\n124\n\n\nNA\n\n\n86\n\n\nNA\n\n\n127\n\n\nNA\n\n\n81\n\n\n137\n\n\nNA\n\n\nNA\n\n\n47\n\n\n107\n\n\nNA\n\n\n87\n\n\n125\n\n\n122\n\n\n124\n\n\nNA\n\n\n74\n\n\n152\n\n\nNA\n\n\n125\n\n\n152\n\n\nNA\n\n\n138\n\n\nNA\n\n\n113\n\n\n107\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n116\n\n\n85\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n97\n\n\nNA\n\n\nNA\n\n\n75\n\n\nNA\n\n\nNA\n\n\n87\n\n\nNA\n\n\n114\n\n\nNA\n\n\n83\n\n\nNA\n\n\n166\n\n\nNA\n\n\nNA\n\n\n64\n\n\n95\n\n\nNA\n\n\nNA\n\n\n87\n\n\n160\n\n\n77\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n75\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n131\n\n\n145\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n156\n\n\nNA\n\n\n164\n\n\nNA\n\n\n53\n\n\n71\n\n\nNA\n\n\n86\n\n\nNA\n\n\nNA\n\n\n133\n\n\n114\n\n\n122\n\n\nNA\n\n\nNA\n\n\n142\n\n\nNA\n\n\n150\n\n\n132\n\n\nNA\n\n\n122\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n157\n\n\n71\n\n\n46\n\n\n139\n\n\n89\n\n\nNA\n\n\n98\n\n\n106\n\n\n96\n\n\nNA\n\n\nNA\n\n\n144\n\n\nNA\n\n\n124\n\n\nNA\n\n\nNA\n\n\n88\n\n\n106\n\n\n91\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n196\n\n\n168\n\n\n147\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n107\n\n\nNA\n\n\n110\n\n\n103\n\n\n155\n\n\n111\n\n\nNA\n\n\n141\n\n\nNA\n\n\n147\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n73\n\n\nNA\n\n\n154\n\n\nNA\n\n\n46\n\n\nNA\n\n\nNA\n\n\n55\n\n\nNA\n\n\nNA\n\n\n204\n\n\nNA\n\n\nNA\n\n\n59\n\n\n85\n\n\n132\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n127\n\n\nNA\n\n\n65\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n111\n\n\nNA\n\n\n115\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n87\n\n\nNA\n\n\n152\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n176\n\n\nNA\n\n\nNA\n\n\n139\n\n\n67\n\n\nNA\n\n\n52\n\n\nNA\n\n\nNA\n\n\n212\n\n\nNA\n\n\nNA\n\n\n130\n\n\nNA\n\n\n91\n\n\n139\n\n\n66\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n86\n\n\nNA\n\n\n126\n\n\nNA\n\n\nNA\n\n\n70\n\n\n134\n\n\nNA\n\n\n158\n\n\n104\n\n\n121\n\n\n141\n\n\n131\n\n\n118\n\n\n95\n\n\nNA\n\n\nNA\n\n\n107\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n247\n\n\n71\n\n\n60\n\n\nNA\n\n\n80\n\n\nNA\n\n\nNA\n\n\n74\n\n\n131\n\n\nNA\n\n\n81\n\n\nNA\n\n\n116\n\n\n98\n\n\nNA\n\n\nNA\n\n\n142\n\n\nNA\n\n\nNA\n\n\n137\n\n\n76\n\n\nNA\n\n\n107\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n165\n\n\nNA\n\n\nNA\n\n\n124\n\n\nNA\n\n\n156\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n185\n\n\nNA\n\n\nNA\n\n\n88\n\n\nNA\n\n\n80\n\n\n157\n\n\nNA\n\n\n63\n\n\nNA\n\n\n121\n\n\nNA\n\n\nNA\n\n\n45\n\n\n97\n\n\n115\n\n\nNA\n\n\n73\n\n\nNA\n\n\n83\n\n\n84\n\n\n64\n\n\n151\n\n\n115\n\n\nNA\n\n\nNA\n\n\n47\n\n\n128\n\n\n134\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n188\n\n\n183\n\n\n109\n\n\n79\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n107\n\n\nNA\n\n\n100\n\n\nNA\n\n\n84\n\n\n47\n\n\nNA\n\n\n97\n\n\n150\n\n\n104\n\n\n180\n\n\nNA\n\n\n134\n\n\nNA\n\n\nNA\n\n\n136\n\n\n145\n\n\nNA\n\n\n147\n\n\n138\n\n\n98\n\n\n95\n\n\n53\n\n\n66\n\n\n96\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n63\n\n\n42\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n154\n\n\n82\n\n\nNA\n\n\n115\n\n\n90\n\n\n155\n\n\n129\n\n\nNA\n\n\n56\n\n\n67\n\n\n124\n\n\n87\n\n\n62\n\n\nNA\n\n\n139\n\n\n107\n\n\n125\n\n\n102\n\n\nNA\n\n\nNA\n\n\n122\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n99\n\n\n100\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n101\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n124\n\n\n69\n\n\n107\n\n\nNA\n\n\nNA\n\n\n96\n\n\nNA\n\n\n72\n\n\n101\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n101\n\n\n95\n\n\nNA\n\n\n119\n\n\n124\n\n\n136\n\n\n97\n\n\n78\n\n\n123\n\n\n81\n\n\n191\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n62\n\n\nNA\n\n\n113\n\n\nNA\n\n\n79\n\n\nNA\n\n\n167\n\n\n97\n\n\nNA\n\n\n94\n\n\nNA\n\n\nNA\n\n\n105\n\n\n130\n\n\n152\n\n\nNA\n\n\nNA\n\n\n87\n\n\n77\n\n\nNA\n\n\nNA\n\n\n86\n\n\n96\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n127\n\n\n65\n\n\n139\n\n\nNA\n\n\n71\n\n\n74\n\n\n81\n\n\n137\n\n\nNA\n\n\n124\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n47\n\n\nNA\n\n\n109\n\n\n107\n\n\nNA\n\n\nNA\n\n\n42\n\n\nNA\n\n\n87\n\n\n125\n\n\n122\n\n\nNA\n\n\n124\n\n\nNA\n\n\n79\n\n\nNA\n\n\n152\n\n\nNA\n\n\nNA\n\n\n87\n\n\n77\n\n\nNA\n\n\nNA\n\n\n86\n\n\n96\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n127\n\n\n65\n\n\n139\n\n\nNA\n\n\n71\n\n\n74\n\n\n81\n\n\n137\n\n\nNA\n\n\n124\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n47\n\n\nNA\n\n\n109\n\n\n107\n\n\nNA\n\n\nNA\n\n\n42\n\n\nNA\n\n\n87\n\n\n125\n\n\n122\n\n\nNA\n\n\n124\n\n\nNA\n\n\n79\n\n\nNA\n\n\n\n\n81\n\n\n2\n\n\nDYSLIP\n\n\nDyslipidemia\n\n\n2\n\n\n4\n\n\n2\n\n\n2\n\n\n4\n\n\n1\n\n\n2\n\n\n2\n\n\n9\n\n\n9\n\n\n2\n\n\n1\n\n\n2\n\n\n9\n\n\n1\n\n\n9\n\n\n2\n\n\n2\n\n\n9\n\n\n1\n\n\n2\n\n\n2\n\n\n4\n\n\n2\n\n\n4\n\n\n1\n\n\n2\n\n\n9\n\n\n9\n\n\n1\n\n\n2\n\n\n9\n\n\n9\n\n\n2\n\n\n2\n\n\n1\n\n\n2\n\n\n2\n\n\n4\n\n\n2\n\n\n4\n\n\n1\n\n\n2\n\n\n9\n\n\n9\n\n\n1\n\n\n2\n\n\n9\n\n\n9\n\n\n2\n\n\n2\n\n\n1\n\n\n2\n\n\n2\n\n\n2\n\n\n9\n\n\n2\n\n\n2\n\n\n9\n\n\n2\n\n\n9\n\n\n2\n\n\n2\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n2\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n2\n\n\n9\n\n\n2\n\n\n9\n\n\n1\n\n\n4\n\n\n2\n\n\n9\n\n\n9\n\n\n1\n\n\n2\n\n\n9\n\n\n9\n\n\n2\n\n\n2\n\n\n2\n\n\n3\n\n\n2\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n4\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n4\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n2\n\n\n2\n\n\n4\n\n\n9\n\n\n2\n\n\n9\n\n\n2\n\n\n9\n\n\n2\n\n\n2\n\n\n2\n\n\n1\n\n\n9\n\n\n2\n\n\n9\n\n\n9\n\n\n9\n\n\n2\n\n\n2\n\n\n9\n\n\n9\n\n\n2\n\n\n9\n\n\n2\n\n\n2\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n2\n\n\n9\n\n\n2\n\n\n1\n\n\n2\n\n\n2\n\n\n2\n\n\n9\n\n\n1\n\n\n9\n\n\n2\n\n\n4\n\n\n9\n\n\n2\n\n\n9\n\n\n2\n\n\n9\n\n\n9\n\n\n1\n\n\n1\n\n\n1\n\n\n9\n\n\n9\n\n\n2\n\n\n9\n\n\n2\n\n\n9\n\n\n2\n\n\n9\n\n\n2\n\n\n9\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n2\n\n\n2\n\n\n9\n\n\n9\n\n\n9\n\n\n2\n\n\n9\n\n\n9\n\n\n9\n\n\n2\n\n\n9\n\n\n9\n\n\n1\n\n\n4\n\n\n9\n\n\n9\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n2\n\n\n9\n\n\n2\n\n\n9\n\n\n9\n\n\n2\n\n\n2\n\n\n2\n\n\n9\n\n\n2\n\n\n9\n\n\n9\n\n\n2\n\n\n9\n\n\n2\n\n\n9\n\n\n2\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n2\n\n\n4\n\n\n9\n\n\n9\n\n\n9\n\n\n2\n\n\n2\n\n\n2\n\n\n4\n\n\n2\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n2\n\n\n2\n\n\n9\n\n\n2\n\n\n2\n\n\n1\n\n\n2\n\n\n2\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n2\n\n\n9\n\n\n2\n\n\n2\n\n\n2\n\n\n9\n\n\n2\n\n\n2\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n2\n\n\n2\n\n\n9\n\n\n1\n\n\n9\n\n\n4\n\n\n4\n\n\n9\n\n\n2\n\n\n1\n\n\n1\n\n\n9\n\n\n2\n\n\n9\n\n\n9\n\n\n1\n\n\n2\n\n\n9\n\n\n1\n\n\n9\n\n\n2\n\n\n1\n\n\n9\n\n\n2\n\n\n2\n\n\n9\n\n\n4\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n2\n\n\n2\n\n\n9\n\n\n2\n\n\n2\n\n\n2\n\n\n9\n\n\n9\n\n\n2\n\n\n2\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n2\n\n\n9\n\n\n2\n\n\n2\n\n\n1\n\n\n4\n\n\n4\n\n\n2\n\n\n1\n\n\n2\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n2\n\n\n1\n\n\n9\n\n\n2\n\n\n2\n\n\n9\n\n\n1\n\n\n9\n\n\n9\n\n\n2\n\n\n9\n\n\n9\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n9\n\n\n9\n\n\n9\n\n\n2\n\n\n2\n\n\n9\n\n\n9\n\n\n2\n\n\n2\n\n\n1\n\n\n4\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n9\n\n\n2\n\n\n9\n\n\n9\n\n\n9\n\n\n2\n\n\n9\n\n\n2\n\n\n2\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n9\n\n\n4\n\n\n9\n\n\n2\n\n\n4\n\n\n1\n\n\n2\n\n\n9\n\n\n9\n\n\n9\n\n\n2\n\n\n2\n\n\n2\n\n\n9\n\n\n9\n\n\n9\n\n\n2\n\n\n2\n\n\n9\n\n\n1\n\n\n2\n\n\n2\n\n\n9\n\n\n2\n\n\n2\n\n\n2\n\n\n1\n\n\n2\n\n\n9\n\n\n9\n\n\n9\n\n\n2\n\n\n4\n\n\n4\n\n\n9\n\n\n2\n\n\n2\n\n\n2\n\n\n9\n\n\n9\n\n\n9\n\n\n2\n\n\n9\n\n\n9\n\n\n9\n\n\n9\n\n\n2\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n9\n\n\n1\n\n\n9\n\n\n1\n\n\n2\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n2\n\n\n9\n\n\n1\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n1\n\n\n2\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n2\n\n\n9\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n4\n\n\n9\n\n\n1\n\n\n2\n\n\n2\n\n\n9\n\n\n9\n\n\n2\n\n\n2\n\n\n9\n\n\n9\n\n\n2\n\n\n2\n\n\n9\n\n\n2\n\n\n2\n\n\n9\n\n\n4\n\n\n2\n\n\n2\n\n\n2\n\n\n4\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n9\n\n\n2\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n2\n\n\n2\n\n\n9\n\n\n9\n\n\n2\n\n\n9\n\n\n9\n\n\n2\n\n\n2\n\n\n9\n\n\n1\n\n\n9\n\n\n2\n\n\n2\n\n\n2\n\n\n9\n\n\n9\n\n\n2\n\n\n2\n\n\n9\n\n\n9\n\n\n2\n\n\n2\n\n\n9\n\n\n2\n\n\n2\n\n\n9\n\n\n4\n\n\n2\n\n\n2\n\n\n2\n\n\n4\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n9\n\n\n2\n\n\n9\n\n\n9\n\n\n9\n\n\n1\n\n\n9\n\n\n2\n\n\n2\n\n\n9\n\n\n9\n\n\n2\n\n\n9\n\n\n9\n\n\n2\n\n\n2\n\n\n9\n\n\n1\n\n\n9\n\n\n2\n\n\n2\n\n\n\n\n82\n\n\n2\n\n\nCESD\n\n\nCESD Depression Score\n\n\n1\n\n\n18\n\n\n23\n\n\n13\n\n\n1\n\n\n4\n\n\n14\n\n\n32\n\n\n33\n\n\n15\n\n\n26\n\n\n10\n\n\n12\n\n\n28\n\n\n10\n\n\n11\n\n\n14\n\n\n5\n\n\n39\n\n\n44\n\n\n27\n\n\n1\n\n\n18\n\n\n23\n\n\n1\n\n\n4\n\n\n32\n\n\n33\n\n\n15\n\n\n10\n\n\n12\n\n\n28\n\n\n11\n\n\n14\n\n\n5\n\n\n44\n\n\n27\n\n\n1\n\n\n18\n\n\n23\n\n\n1\n\n\n4\n\n\n32\n\n\n33\n\n\n15\n\n\n10\n\n\n12\n\n\n28\n\n\n11\n\n\n14\n\n\n5\n\n\n44\n\n\n27\n\n\n0\n\n\n14\n\n\n0\n\n\n22\n\n\n2\n\n\n24\n\n\n2\n\n\n12\n\n\n7\n\n\n13\n\n\n29\n\n\n3\n\n\n15\n\n\n3\n\n\n26\n\n\n0\n\n\n2\n\n\n32\n\n\n4\n\n\n12\n\n\n5\n\n\n14\n\n\n13\n\n\n3\n\n\n9\n\n\n18\n\n\n5\n\n\n17\n\n\n1\n\n\n0\n\n\n30\n\n\n1\n\n\n26\n\n\n21\n\n\n20\n\n\n15\n\n\n12\n\n\n4\n\n\n11\n\n\n1\n\n\n4\n\n\n16\n\n\n9\n\n\n0\n\n\n25\n\n\n0\n\n\n5\n\n\n1\n\n\n19\n\n\n17\n\n\n8\n\n\n1\n\n\n7\n\n\n1\n\n\n18\n\n\n0\n\n\n1\n\n\n4\n\n\n6\n\n\n0\n\n\n17\n\n\n39\n\n\n6\n\n\n14\n\n\n11\n\n\n9\n\n\n1\n\n\n19\n\n\n3\n\n\n15\n\n\n3\n\n\n17\n\n\n29\n\n\n22\n\n\n16\n\n\n1\n\n\n0\n\n\n14\n\n\n4\n\n\n0\n\n\n0\n\n\n6\n\n\n8\n\n\n4\n\n\n3\n\n\n0\n\n\n34\n\n\n2\n\n\n3\n\n\n-1\n\n\n2\n\n\n0\n\n\n0\n\n\n9\n\n\n19\n\n\n20\n\n\n33\n\n\n3\n\n\n38\n\n\n11\n\n\n2\n\n\n13\n\n\n26\n\n\n17\n\n\n12\n\n\n0\n\n\n8\n\n\n14\n\n\n6\n\n\n11\n\n\n16\n\n\n23\n\n\n25\n\n\n0\n\n\n22\n\n\n7\n\n\n4\n\n\n0\n\n\n11\n\n\n27\n\n\n9\n\n\n13\n\n\n0\n\n\n9\n\n\n45\n\n\n8\n\n\n2\n\n\n26\n\n\n18\n\n\n23\n\n\n17\n\n\n10\n\n\n1\n\n\n12\n\n\n11\n\n\n18\n\n\n8\n\n\n9\n\n\n0\n\n\n12\n\n\n18\n\n\n24\n\n\n0\n\n\n29\n\n\n3\n\n\n3\n\n\n1\n\n\n15\n\n\n0\n\n\n22\n\n\n2\n\n\n9\n\n\n34\n\n\n25\n\n\n25\n\n\n15\n\n\n13\n\n\n0\n\n\n1\n\n\n23\n\n\n7\n\n\n27\n\n\n24\n\n\n10\n\n\n6\n\n\n4\n\n\n29\n\n\n0\n\n\n16\n\n\n21\n\n\n25\n\n\n12\n\n\n13\n\n\n13\n\n\n3\n\n\n7\n\n\n20\n\n\n5\n\n\n12\n\n\n3\n\n\n1\n\n\n37\n\n\n12\n\n\n8\n\n\n10\n\n\n7\n\n\n4\n\n\n22\n\n\n2\n\n\n18\n\n\n16\n\n\n10\n\n\n12\n\n\n6\n\n\n32\n\n\n3\n\n\n5\n\n\n14\n\n\n-1\n\n\n47\n\n\n1\n\n\n2\n\n\n12\n\n\n11\n\n\n31\n\n\n-1\n\n\n0\n\n\n10\n\n\n13\n\n\n17\n\n\n5\n\n\n0\n\n\n18\n\n\n0\n\n\n13\n\n\n5\n\n\n2\n\n\n4\n\n\n1\n\n\n5\n\n\n0\n\n\n3\n\n\n15\n\n\n22\n\n\n0\n\n\n4\n\n\n9\n\n\n2\n\n\n24\n\n\n6\n\n\n14\n\n\n4\n\n\n25\n\n\n0\n\n\n10\n\n\n3\n\n\n8\n\n\n14\n\n\n5\n\n\n16\n\n\n32\n\n\n5\n\n\n38\n\n\n2\n\n\n1\n\n\n33\n\n\n3\n\n\n2\n\n\n22\n\n\n4\n\n\n0\n\n\n20\n\n\n14\n\n\n9\n\n\n15\n\n\n50\n\n\n29\n\n\n4\n\n\n5\n\n\n22\n\n\n1\n\n\n3\n\n\n15\n\n\n7\n\n\n10\n\n\n26\n\n\n8\n\n\n1\n\n\n8\n\n\n2\n\n\n11\n\n\n0\n\n\n24\n\n\n13\n\n\n0\n\n\n30\n\n\n6\n\n\n6\n\n\n17\n\n\n3\n\n\n2\n\n\n7\n\n\n3\n\n\n10\n\n\n5\n\n\n21\n\n\n12\n\n\n0\n\n\n3\n\n\n29\n\n\n17\n\n\n12\n\n\n38\n\n\n18\n\n\n12\n\n\n0\n\n\n0\n\n\n12\n\n\n28\n\n\n4\n\n\n0\n\n\n32\n\n\n10\n\n\n7\n\n\n11\n\n\n7\n\n\n7\n\n\n17\n\n\n0\n\n\n0\n\n\n9\n\n\n18\n\n\n4\n\n\n28\n\n\n29\n\n\n18\n\n\n0\n\n\n18\n\n\n2\n\n\n17\n\n\n6\n\n\n5\n\n\n1\n\n\n6\n\n\n14\n\n\n0\n\n\n24\n\n\n24\n\n\n6\n\n\n5\n\n\n18\n\n\n17\n\n\n4\n\n\n7\n\n\n0\n\n\n7\n\n\n10\n\n\n31\n\n\n6\n\n\n2\n\n\n23\n\n\n3\n\n\n1\n\n\n11\n\n\n-1\n\n\n0\n\n\n48\n\n\n23\n\n\n14\n\n\n16\n\n\n4\n\n\n0\n\n\n5\n\n\n23\n\n\n9\n\n\n7\n\n\n1\n\n\n9\n\n\n12\n\n\n39\n\n\n20\n\n\n11\n\n\n7\n\n\n2\n\n\n31\n\n\n0\n\n\n8\n\n\n23\n\n\n7\n\n\n9\n\n\n27\n\n\n44\n\n\n6\n\n\n41\n\n\n9\n\n\n38\n\n\n0\n\n\n2\n\n\n1\n\n\n17\n\n\n5\n\n\n25\n\n\n23\n\n\n29\n\n\n1\n\n\n15\n\n\n16\n\n\n5\n\n\n2\n\n\n3\n\n\n4\n\n\n1\n\n\n27\n\n\n-1\n\n\n21\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n23\n\n\n26\n\n\n25\n\n\n27\n\n\n7\n\n\n38\n\n\n5\n\n\n2\n\n\n7\n\n\n14\n\n\n1\n\n\n3\n\n\n14\n\n\n29\n\n\n15\n\n\n11\n\n\n4\n\n\n0\n\n\n1\n\n\n1\n\n\n26\n\n\n8\n\n\n13\n\n\n8\n\n\n0\n\n\n18\n\n\n23\n\n\n27\n\n\n12\n\n\n1\n\n\n15\n\n\n24\n\n\n4\n\n\n32\n\n\n33\n\n\n20\n\n\n15\n\n\n22\n\n\n30\n\n\n10\n\n\n3\n\n\n12\n\n\n12\n\n\n28\n\n\n9\n\n\n24\n\n\n5\n\n\n11\n\n\n14\n\n\n5\n\n\n7\n\n\n44\n\n\n2\n\n\n25\n\n\n27\n\n\n14\n\n\n29\n\n\n15\n\n\n11\n\n\n4\n\n\n0\n\n\n1\n\n\n1\n\n\n26\n\n\n8\n\n\n13\n\n\n8\n\n\n0\n\n\n18\n\n\n23\n\n\n27\n\n\n12\n\n\n1\n\n\n15\n\n\n24\n\n\n4\n\n\n32\n\n\n33\n\n\n20\n\n\n15\n\n\n22\n\n\n30\n\n\n10\n\n\n3\n\n\n12\n\n\n12\n\n\n28\n\n\n9\n\n\n24\n\n\n5\n\n\n11\n\n\n14\n\n\n5\n\n\n7\n\n\n44\n\n\n2\n\n\n25\n\n\n27\n\n\n\n\n83\n\n\n2\n\n\nSMOKE\n\n\nSmoking Status\n\n\n3\n\n\n3\n\n\n2\n\n\n3\n\n\n3\n\n\n1\n\n\n3\n\n\n2\n\n\n2\n\n\n2\n\n\n3\n\n\n3\n\n\n3\n\n\n3\n\n\n3\n\n\n2\n\n\n3\n\n\n3\n\n\n3\n\n\n3\n\n\n1\n\n\n3\n\n\n3\n\n\n2\n\n\n3\n\n\n1\n\n\n2\n\n\n2\n\n\n2\n\n\n3\n\n\n3\n\n\n3\n\n\n2\n\n\n3\n\n\n3\n\n\n3\n\n\n1\n\n\n3\n\n\n3\n\n\n2\n\n\n3\n\n\n1\n\n\n2\n\n\n2\n\n\n2\n\n\n3\n\n\n3\n\n\n3\n\n\n2\n\n\n3\n\n\n3\n\n\n3\n\n\n1\n\n\n1\n\n\n3\n\n\n2\n\n\n3\n\n\n2\n\n\n3\n\n\n2\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n3\n\n\n2\n\n\n3\n\n\n2\n\n\n1\n\n\n2\n\n\n3\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n2\n\n\n2\n\n\n1\n\n\n2\n\n\n2\n\n\n2\n\n\n3\n\n\n3\n\n\n2\n\n\n2\n\n\n3\n\n\n3\n\n\n3\n\n\n2\n\n\n1\n\n\n3\n\n\n2\n\n\n3\n\n\n1\n\n\n3\n\n\n3\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n3\n\n\n1\n\n\n2\n\n\n2\n\n\n2\n\n\n1\n\n\n2\n\n\n3\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n3\n\n\n1\n\n\n2\n\n\n2\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n3\n\n\n1\n\n\n2\n\n\n1\n\n\n3\n\n\n2\n\n\n3\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n3\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n3\n\n\n2\n\n\n3\n\n\n3\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n2\n\n\n3\n\n\n1\n\n\n1\n\n\n3\n\n\n3\n\n\n2\n\n\n1\n\n\n2\n\n\n3\n\n\n1\n\n\n3\n\n\n1\n\n\n3\n\n\n1\n\n\n1\n\n\n2\n\n\n3\n\n\n3\n\n\n2\n\n\n2\n\n\n2\n\n\n3\n\n\n2\n\n\n2\n\n\n3\n\n\n2\n\n\n1\n\n\n3\n\n\n1\n\n\n3\n\n\n2\n\n\n2\n\n\n1\n\n\n2\n\n\n3\n\n\n3\n\n\n2\n\n\n2\n\n\n3\n\n\n3\n\n\n3\n\n\n2\n\n\n3\n\n\n2\n\n\n3\n\n\n1\n\n\n3\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n3\n\n\n3\n\n\n3\n\n\n3\n\n\n3\n\n\n2\n\n\n2\n\n\n3\n\n\n3\n\n\n3\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n3\n\n\n3\n\n\n3\n\n\n3\n\n\n1\n\n\n1\n\n\n3\n\n\n3\n\n\n2\n\n\n2\n\n\n3\n\n\n3\n\n\n2\n\n\n3\n\n\n2\n\n\n1\n\n\n1\n\n\n3\n\n\n2\n\n\n2\n\n\n2\n\n\n3\n\n\n3\n\n\n3\n\n\n1\n\n\n1\n\n\n3\n\n\n3\n\n\n2\n\n\n1\n\n\n2\n\n\n2\n\n\n3\n\n\n2\n\n\n3\n\n\n3\n\n\n3\n\n\n3\n\n\n3\n\n\n2\n\n\n2\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n2\n\n\n3\n\n\n3\n\n\n3\n\n\n2\n\n\n1\n\n\n3\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n3\n\n\n1\n\n\n2\n\n\n2\n\n\n3\n\n\n3\n\n\n2\n\n\n2\n\n\n3\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n3\n\n\n1\n\n\n2\n\n\n3\n\n\n3\n\n\n1\n\n\n2\n\n\n2\n\n\n3\n\n\n1\n\n\n3\n\n\n2\n\n\n2\n\n\n2\n\n\n3\n\n\n1\n\n\n3\n\n\n1\n\n\n1\n\n\n3\n\n\n2\n\n\n2\n\n\n3\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n3\n\n\n2\n\n\n2\n\n\n3\n\n\n3\n\n\n2\n\n\n2\n\n\n3\n\n\n2\n\n\n2\n\n\n2\n\n\n1\n\n\n2\n\n\n3\n\n\n1\n\n\n2\n\n\n2\n\n\n2\n\n\n3\n\n\n3\n\n\n1\n\n\n2\n\n\n3\n\n\n2\n\n\n1\n\n\n1\n\n\n3\n\n\n3\n\n\n2\n\n\n2\n\n\n1\n\n\n2\n\n\n3\n\n\n2\n\n\n2\n\n\n2\n\n\n3\n\n\n3\n\n\n1\n\n\n1\n\n\n2\n\n\n3\n\n\n3\n\n\n2\n\n\n3\n\n\n2\n\n\n3\n\n\n2\n\n\n2\n\n\n3\n\n\n2\n\n\n3\n\n\n1\n\n\n1\n\n\n3\n\n\n2\n\n\n3\n\n\n2\n\n\n2\n\n\n2\n\n\n3\n\n\n2\n\n\n1\n\n\n3\n\n\n1\n\n\n3\n\n\n2\n\n\n2\n\n\n1\n\n\n2\n\n\n3\n\n\n1\n\n\n2\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n3\n\n\n2\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n3\n\n\n3\n\n\n3\n\n\n1\n\n\n2\n\n\n3\n\n\n1\n\n\n3\n\n\n1\n\n\n2\n\n\n3\n\n\n2\n\n\n3\n\n\n3\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n3\n\n\n3\n\n\n1\n\n\n2\n\n\n3\n\n\n2\n\n\n3\n\n\n2\n\n\n3\n\n\n2\n\n\n3\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n3\n\n\n3\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n3\n\n\n3\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n3\n\n\n3\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n3\n\n\n3\n\n\n1\n\n\n3\n\n\n3\n\n\n2\n\n\n2\n\n\n2\n\n\n3\n\n\n1\n\n\n1\n\n\n3\n\n\n1\n\n\n3\n\n\n3\n\n\n3\n\n\n1\n\n\n2\n\n\n3\n\n\n3\n\n\n3\n\n\n3\n\n\n3\n\n\n3\n\n\n3\n\n\n2\n\n\n3\n\n\n3\n\n\n3\n\n\n3\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n3\n\n\n3\n\n\n1\n\n\n3\n\n\n3\n\n\n3\n\n\n2\n\n\n1\n\n\n2\n\n\n3\n\n\n3\n\n\n2\n\n\n3\n\n\n1\n\n\n3\n\n\n1\n\n\n3\n\n\n1\n\n\n3\n\n\n3\n\n\n3\n\n\n1\n\n\n2\n\n\n3\n\n\n3\n\n\n3\n\n\n3\n\n\n3\n\n\n3\n\n\n3\n\n\n2\n\n\n3\n\n\n3\n\n\n3\n\n\n3\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n3\n\n\n3\n\n\n1\n\n\n3\n\n\n3\n\n\n3\n\n\n2\n\n\n1\n\n\n2\n\n\n3\n\n\n3\n\n\n2\n\n\n3\n\n\n1\n\n\n3\n\n\n1\n\n\n\n\n84\n\n\n2\n\n\nDKGRP\n\n\nDrinking Group\n\n\n0\n\n\n2\n\n\n0\n\n\n0\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n.\n\n\n1\n\n\n1\n\n\n0\n\n\n2\n\n\n2\n\n\n0\n\n\n1\n\n\n1\n\n\n1\n\n\n3\n\n\n2\n\n\n2\n\n\n1\n\n\n.\n\n\n0\n\n\n1\n\n\n0\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n1\n\n\n2\n\n\n3\n\n\n1\n\n\n1\n\n\n3\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n0\n\n\n0\n\n\n1\n\n\n3\n\n\n0\n\n\n0\n\n\n1\n\n\n1\n\n\n0\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n3\n\n\n0\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n3\n\n\n1\n\n\n3\n\n\n0\n\n\n2\n\n\n1\n\n\n3\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n0\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n0\n\n\n1\n\n\n.\n\n\n1\n\n\n0\n\n\n1\n\n\n0\n\n\n1\n\n\n0\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n0\n\n\n0\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n0\n\n\n2\n\n\n2\n\n\n0\n\n\n1\n\n\n0\n\n\n0\n\n\n2\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n0\n\n\n1\n\n\n0\n\n\n2\n\n\n3\n\n\n1\n\n\n1\n\n\n0\n\n\n1\n\n\n0\n\n\n1\n\n\n1\n\n\n3\n\n\n0\n\n\n1\n\n\n0\n\n\n2\n\n\n1\n\n\n0\n\n\n1\n\n\n0\n\n\n2\n\n\n1\n\n\n0\n\n\n1\n\n\n0\n\n\n0\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n0\n\n\n1\n\n\n0\n\n\n2\n\n\n1\n\n\n2\n\n\n0\n\n\n0\n\n\n2\n\n\n0\n\n\n2\n\n\n3\n\n\n1\n\n\n0\n\n\n0\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n0\n\n\n0\n\n\n3\n\n\n2\n\n\n1\n\n\n2\n\n\n3\n\n\n1\n\n\n3\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n0\n\n\n1\n\n\n3\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n0\n\n\n0\n\n\n1\n\n\n1\n\n\n0\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n0\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n0\n\n\n1\n\n\n1\n\n\n0\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n3\n\n\n2\n\n\n1\n\n\n0\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n0\n\n\n0\n\n\n3\n\n\n0\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n0\n\n\n2\n\n\n1\n\n\n1\n\n\n0\n\n\n0\n\n\n0\n\n\n1\n\n\n0\n\n\n1\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n0\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n0\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n0\n\n\n1\n\n\n1\n\n\n1\n\n\n0\n\n\n0\n\n\n1\n\n\n1\n\n\n.\n\n\n1\n\n\n2\n\n\n2\n\n\n2\n\n\n1\n\n\n3\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n0\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n0\n\n\n2\n\n\n3\n\n\n1\n\n\n1\n\n\n0\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n0\n\n\n2\n\n\n0\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n0\n\n\n2\n\n\n0\n\n\n0\n\n\n2\n\n\n2\n\n\n1\n\n\n0\n\n\n2\n\n\n1\n\n\n0\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n0\n\n\n3\n\n\n0\n\n\n2\n\n\n2\n\n\n1\n\n\n.\n\n\n2\n\n\n3\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n0\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n3\n\n\n1\n\n\n0\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n3\n\n\n1\n\n\n2\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n1\n\n\n0\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n3\n\n\n3\n\n\n1\n\n\n2\n\n\n1\n\n\n0\n\n\n0\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n0\n\n\n0\n\n\n1\n\n\n1\n\n\n2\n\n\n3\n\n\n1\n\n\n1\n\n\n1\n\n\n0\n\n\n3\n\n\n1\n\n\n2\n\n\n0\n\n\n0\n\n\n1\n\n\n1\n\n\n1\n\n\n0\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n0\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n0\n\n\n0\n\n\n1\n\n\n3\n\n\n2\n\n\n2\n\n\n1\n\n\n0\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n.\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n2\n\n\n2\n\n\n0\n\n\n1\n\n\n1\n\n\n1\n\n\n.\n\n\n0\n\n\n1\n\n\n0\n\n\n1\n\n\n2\n\n\n1\n\n\n0\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n0\n\n\n3\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n0\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n3\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n0\n\n\n2\n\n\n1\n\n\n1\n\n\n0\n\n\n1\n\n\n1\n\n\n0\n\n\n1\n\n\n2\n\n\n0\n\n\n1\n\n\n3\n\n\n0\n\n\n0\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n\n\n85\n\n\n2\n\n\nHEROPIATE\n\n\nHeroin or Opiate Use Since Last Visit\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n\n\n86\n\n\n2\n\n\nIDU\n\n\nIntravenous Drug Usage Since Last Visit\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n\n\n87\n\n\n2\n\n\nLEU3N\n\n\nCD4+ T Cell Count\n\n\n345.4009918\n\n\n263.0692533\n\n\n405.1815688\n\n\n179.6409126\n\n\n59.62190057\n\n\n893.4327659\n\n\n1395.431806\n\n\n945.4961141\n\n\n448.28625\n\n\n69.36626023\n\n\n334.1167999\n\n\n113.7808136\n\n\n356.2013405\n\n\n452.5509974\n\n\n449.5860724\n\n\n900.7052042\n\n\n649.5694537\n\n\n408.7066812\n\n\n228.1243866\n\n\n860.9136418\n\n\n1040.039103\n\n\n329.4558309\n\n\n325.3915588\n\n\n294.4764196\n\n\n55.76719719\n\n\n895.930188\n\n\n971.435125\n\n\n453.6042423\n\n\n66.60302922\n\n\n110.9553245\n\n\n377.3995358\n\n\n443.6465901\n\n\n896.4046771\n\n\n661.6280291\n\n\n414.2272381\n\n\n857.011389\n\n\n1038.114985\n\n\n319.9109843\n\n\n311.8924223\n\n\n357.5457107\n\n\n70.57331457\n\n\n893.1849669\n\n\n961.6641331\n\n\n476.073338\n\n\n63.77244854\n\n\n119.8713716\n\n\n348.9052265\n\n\n381.7528215\n\n\n896.884669\n\n\n681.2146416\n\n\n402.3513041\n\n\n857.2072933\n\n\n1038.79061\n\n\n518.7174908\n\n\n396.9556753\n\n\n521.372059\n\n\n270.0615738\n\n\n370.5797012\n\n\n308.2001911\n\n\n380.765941\n\n\n759.6088405\n\n\n547.8964627\n\n\n1727.925579\n\n\n419.7538525\n\n\n697.3964047\n\n\n366.227543\n\n\n361.5258338\n\n\n369.3601221\n\n\n742.636462\n\n\n470.7067004\n\n\n226.387257\n\n\n212.8632613\n\n\n378.0324942\n\n\n128.1964816\n\n\n648.7867476\n\n\n619.8271148\n\n\n665.7235128\n\n\n170.7266279\n\n\n346.2472234\n\n\n463.3563826\n\n\n631.6275509\n\n\n911.0665526\n\n\n366.6269058\n\n\n518.430698\n\n\n288.2693608\n\n\n179.8677185\n\n\n879.8462431\n\n\n48.8869399\n\n\n1032.13292\n\n\n711.5599513\n\n\n218.8969125\n\n\n829.8804069\n\n\n517.2574063\n\n\n486.0409883\n\n\n378.8156047\n\n\n.\n\n\n297.9266232\n\n\n.\n\n\n846.0853454\n\n\n332.2531045\n\n\n1143.187324\n\n\n407.753458\n\n\n774.352906\n\n\n588.6476868\n\n\n633.9049112\n\n\n474.0790472\n\n\n416.9112042\n\n\n652.8635145\n\n\n236.4165302\n\n\n435.9018706\n\n\n1044.303785\n\n\n296.1750034\n\n\n825.1748785\n\n\n125.0006017\n\n\n310.6643423\n\n\n331.363118\n\n\n560.7733292\n\n\n1048.794699\n\n\n571.6895225\n\n\n697.2849206\n\n\n587.7572746\n\n\n778.8880676\n\n\n813.9035262\n\n\n511.7515275\n\n\n642.8106006\n\n\n121.8556482\n\n\n180.6908108\n\n\n.\n\n\n345.1727083\n\n\n421.8855467\n\n\n322.5563328\n\n\n537.9837377\n\n\n432.0849389\n\n\n315.7824844\n\n\n.\n\n\n1319.418821\n\n\n1382.625957\n\n\n329.6313919\n\n\n584.0113786\n\n\n499.8140765\n\n\n407.4587923\n\n\n351.3190707\n\n\n62.05839408\n\n\n758.013418\n\n\n.\n\n\n776.5153191\n\n\n542.9625608\n\n\n422.8385446\n\n\n529.9701482\n\n\n238.5883953\n\n\n401.8145724\n\n\n660.7225419\n\n\n530.3485478\n\n\n612.681383\n\n\n135.1580235\n\n\n504.9896435\n\n\n374.9692284\n\n\n576.752941\n\n\n247.8370683\n\n\n330.4909411\n\n\n498.649829\n\n\n362.7503143\n\n\n351.7326574\n\n\n666.3078363\n\n\n909.209384\n\n\n320.3094357\n\n\n472.8692738\n\n\n649.0670737\n\n\n520.6459165\n\n\n489.8418135\n\n\n492.52108\n\n\n362.960706\n\n\n243.5026454\n\n\n234.4353722\n\n\n714.3538421\n\n\n412.357751\n\n\n315.1388065\n\n\n336.5455415\n\n\n733.3613617\n\n\n800.2783543\n\n\n265.0607457\n\n\n614.8872842\n\n\n675.1709969\n\n\n476.3951713\n\n\n860.7453906\n\n\n1099.368117\n\n\n563.4330199\n\n\n958.997212\n\n\n236.9710138\n\n\n909.860415\n\n\n1056.824257\n\n\n493.1973641\n\n\n211.7518026\n\n\n261.9658963\n\n\n900.9320781\n\n\n42.29731282\n\n\n408.7736425\n\n\n407.5622191\n\n\n140.1695342\n\n\n582.8400023\n\n\n538.3116589\n\n\n776.5861742\n\n\n596.7721097\n\n\n806.6228478\n\n\n349.5428206\n\n\n751.8150633\n\n\n595.9117639\n\n\n293.0185887\n\n\n135.0222866\n\n\n581.062235\n\n\n357.5394316\n\n\n933.3516767\n\n\n407.879045\n\n\n184.7974391\n\n\n281.2833094\n\n\n772.1759603\n\n\n516.4850006\n\n\n383.0808176\n\n\n448.5016343\n\n\n554.265326\n\n\n851.6951002\n\n\n484.3664696\n\n\n405.8109213\n\n\n199.8706772\n\n\n820.9177962\n\n\n366.5082353\n\n\n179.396802\n\n\n528.5864732\n\n\n859.3409473\n\n\n593.8355587\n\n\n593.5976943\n\n\n346.4394653\n\n\n232.625812\n\n\n164.9059462\n\n\n706.4828322\n\n\n428.9648155\n\n\n427.9741595\n\n\n654.6964567\n\n\n775.9375824\n\n\n721.5633423\n\n\n781.2586221\n\n\n430.8886045\n\n\n601.6795132\n\n\n263.9813192\n\n\n552.2078187\n\n\n207.0237857\n\n\n321.5142668\n\n\n606.1282501\n\n\n98.27954597\n\n\n264.3984802\n\n\n.\n\n\n590.2357503\n\n\n952.758477\n\n\n.\n\n\n1560.152078\n\n\n1276.624746\n\n\n.\n\n\n381.9282481\n\n\n452.9140862\n\n\n722.7981662\n\n\n195.1198572\n\n\n466.1362529\n\n\n741.7434714\n\n\n1019.552285\n\n\n510.5187365\n\n\n428.1445536\n\n\n764.4294619\n\n\n678.1506971\n\n\n777.3306917\n\n\n338.552797\n\n\n544.4725297\n\n\n61.9704516\n\n\n292.6134627\n\n\n1116.696468\n\n\n584.4351552\n\n\n618.7013601\n\n\n1012.054167\n\n\n576.523613\n\n\n270.5674846\n\n\n952.154827\n\n\n553.7594836\n\n\n722.9454062\n\n\n102.3999118\n\n\n480.3291741\n\n\n893.366029\n\n\n612.6741569\n\n\n641.5808092\n\n\n683.2558063\n\n\n546.4012713\n\n\n730.9578183\n\n\n1395.958205\n\n\n585.6318442\n\n\n351.2002236\n\n\n947.8610293\n\n\n106.063039\n\n\n687.2733573\n\n\n998.4718311\n\n\n457.2652307\n\n\n449.0527471\n\n\n541.4156852\n\n\n438.7131443\n\n\n706.4861593\n\n\n512.351139\n\n\n652.6418885\n\n\n561.3139715\n\n\n592.1366985\n\n\n508.7142751\n\n\n68.29774567\n\n\n769.056782\n\n\n.\n\n\n699.7724537\n\n\n.\n\n\n353.4389813\n\n\n264.2781431\n\n\n.\n\n\n867.6446068\n\n\n99.01388044\n\n\n758.7060113\n\n\n332.1670977\n\n\n878.9903697\n\n\n777.0142046\n\n\n1003.642414\n\n\n775.0503112\n\n\n619.8485082\n\n\n724.6291867\n\n\n430.6426935\n\n\n557.296473\n\n\n355.0299166\n\n\n366.3883103\n\n\n771.905713\n\n\n518.6374024\n\n\n47.74783865\n\n\n460.1780894\n\n\n376.2937281\n\n\n137.9478851\n\n\n558.6397096\n\n\n114.3219451\n\n\n925.4749489\n\n\n779.6305923\n\n\n680.8197506\n\n\n462.6179408\n\n\n.\n\n\n997.4894647\n\n\n318.3755197\n\n\n681.9824356\n\n\n270.9185739\n\n\n346.917182\n\n\n914.6215163\n\n\n.\n\n\n267.7061095\n\n\n356.9023988\n\n\n451.8310788\n\n\n701.8371817\n\n\n775.1402027\n\n\n1505.613288\n\n\n447.9231129\n\n\n713.6924391\n\n\n1172.642045\n\n\n408.9949063\n\n\n141.3139031\n\n\n434.3116006\n\n\n825.328483\n\n\n448.9388798\n\n\n628.6388874\n\n\n889.2757119\n\n\n622.7506482\n\n\n618.2446945\n\n\n1142.875958\n\n\n403.1145253\n\n\n449.6269782\n\n\n325.8750069\n\n\n500.4186068\n\n\n262.1045251\n\n\n545.3680252\n\n\n795.5533644\n\n\n579.0105678\n\n\n478.3510872\n\n\n316.1733192\n\n\n112.153184\n\n\n642.6171166\n\n\n535.7817069\n\n\n724.0971363\n\n\n630.1255953\n\n\n763.6581113\n\n\n66.43379586\n\n\n492.6022033\n\n\n375.9789217\n\n\n987.1213009\n\n\n875.1903008\n\n\n1129.400975\n\n\n475.2713016\n\n\n678.1151335\n\n\n39.54212339\n\n\n666.4307241\n\n\n737.2919447\n\n\n605.2739228\n\n\n899.1153843\n\n\n311.6031428\n\n\n1016.556659\n\n\n538.6962445\n\n\n782.0157627\n\n\n648.3094382\n\n\n335.3570857\n\n\n506.6593532\n\n\n599.8573623\n\n\n409.3114121\n\n\n375.0875679\n\n\n520.3061253\n\n\n625.9381819\n\n\n468.0250023\n\n\n365.0513267\n\n\n762.3713398\n\n\n229.1847359\n\n\n70.20703104\n\n\n108.4059717\n\n\n620.6831094\n\n\n301.8673354\n\n\n462.1908843\n\n\n805.6960751\n\n\n435.7413874\n\n\n449.6778587\n\n\n463.5501718\n\n\n544.7167309\n\n\n430.727282\n\n\n856.5938154\n\n\n752.1432985\n\n\n235.5844003\n\n\n584.5022727\n\n\n246.3871102\n\n\n463.1319509\n\n\n782.978182\n\n\n104.0130492\n\n\n694.2648634\n\n\n933.7128272\n\n\n612.4466158\n\n\n267.5827259\n\n\n186.0582836\n\n\n251.1666705\n\n\n1063.270057\n\n\n283.88684\n\n\n798.9867246\n\n\n530.5984182\n\n\n300.3461659\n\n\n.\n\n\n366.7968685\n\n\n424.6248789\n\n\n603.1618637\n\n\n471.8028762\n\n\n915.3342353\n\n\n322.5266776\n\n\n596.0773801\n\n\n692.1018322\n\n\n407.7665369\n\n\n.\n\n\n491.1005999\n\n\n1038.803336\n\n\n613.4037721\n\n\n605.2048676\n\n\n643.0671049\n\n\n1102.413644\n\n\n.\n\n\n966.1161965\n\n\n634.9383145\n\n\n742.7640093\n\n\n400.5929107\n\n\n420.32719\n\n\n363.919257\n\n\n833.524644\n\n\n489.2840812\n\n\n852.0095416\n\n\n633.181547\n\n\n331.7689995\n\n\n504.6524463\n\n\n331.4727368\n\n\n710.2949387\n\n\n911.4783574\n\n\n485.7808549\n\n\n324.6733944\n\n\n295.5737519\n\n\n282.7493081\n\n\n431.4784656\n\n\n59.96954109\n\n\n621.7061549\n\n\n721.727278\n\n\n891.447387\n\n\n970.8615563\n\n\n451.6557981\n\n\n561.7307992\n\n\n66.03462176\n\n\n358.5825854\n\n\n364.4458264\n\n\n112.8697832\n\n\n.\n\n\n677.4037993\n\n\n375.246615\n\n\n444.8783879\n\n\n634.6551015\n\n\n542.8548705\n\n\n628.9965822\n\n\n896.9476772\n\n\n658.7381247\n\n\n411.1906611\n\n\n462.3593738\n\n\n856.1033583\n\n\n319.9117926\n\n\n488.6211928\n\n\n1037.257569\n\n\n402.4171067\n\n\n426.8582871\n\n\n364.2251802\n\n\n834.8080434\n\n\n486.3302955\n\n\n849.79992\n\n\n627.8089977\n\n\n318.0824841\n\n\n504.1167598\n\n\n333.8020413\n\n\n709.322973\n\n\n910.5074749\n\n\n489.2477241\n\n\n308.5509714\n\n\n356.5810065\n\n\n286.8132906\n\n\n429.4030841\n\n\n66.05653976\n\n\n622.3413086\n\n\n719.114542\n\n\n893.1047448\n\n\n961.2650563\n\n\n479.538482\n\n\n562.5223715\n\n\n63.81087723\n\n\n355.8554358\n\n\n365.0150034\n\n\n120.8285896\n\n\n.\n\n\n677.7709966\n\n\n348.3413317\n\n\n382.5158105\n\n\n634.169477\n\n\n536.0453344\n\n\n627.6560193\n\n\n897.9952335\n\n\n677.9818251\n\n\n408.4351536\n\n\n460.9949378\n\n\n857.6297975\n\n\n324.2283247\n\n\n491.3113029\n\n\n1039.795403\n\n\n\n\n88\n\n\n2\n\n\nVLOAD\n\n\nViral Load;';\n\n\n60\n\n\n48\n\n\n27.50917363\n\n\n27\n\n\n419.5\n\n\n53.5\n\n\n10.5\n\n\n2\n\n\n12.71335579\n\n\n424291\n\n\n18814.65713\n\n\n236\n\n\n30\n\n\n2615\n\n\n117.7379987\n\n\n380.7593245\n\n\n29\n\n\n26\n\n\n49\n\n\n1114.633516\n\n\n49.5\n\n\n53\n\n\n33\n\n\n4.69895446\n\n\n404.5\n\n\n27.5\n\n\n9\n\n\n4.84947723\n\n\n424288\n\n\n227\n\n\n43\n\n\n2610\n\n\n380.9098473\n\n\n21\n\n\n9\n\n\n1134.881398\n\n\n13.5\n\n\n62\n\n\n35\n\n\n25.96074194\n\n\n402\n\n\n19.5\n\n\n6\n\n\n7.93193928\n\n\n424292\n\n\n273.5\n\n\n47\n\n\n2652\n\n\n375.3614155\n\n\n36\n\n\n44\n\n\n1141.630692\n\n\n6\n\n\n1.479581784\n\n\n10.79887046\n\n\n48\n\n\n64838\n\n\n3.452357495\n\n\n18186\n\n\n2\n\n\n44.88064744\n\n\n0.739790892\n\n\n11\n\n\n648\n\n\n464\n\n\n55675\n\n\n46\n\n\n1.972775712\n\n\n7\n\n\n1058.147572\n\n\n9\n\n\n26\n\n\n28\n\n\n7747\n\n\n1.972775712\n\n\n14\n\n\n1.23298482\n\n\n379509\n\n\n517\n\n\n17756\n\n\n9.617281594\n\n\n39\n\n\n8.13769981\n\n\n14\n\n\n7\n\n\n18.89802331\n\n\n7\n\n\n5\n\n\n8.877490702\n\n\n454\n\n\n122\n\n\n28.34703496\n\n\n15\n\n\n131\n\n\n42442\n\n\n15\n\n\n10\n\n\n.\n\n\n116.6403639\n\n\n28\n\n\n12.08325123\n\n\n41.84562303\n\n\n18212\n\n\n7294\n\n\n3\n\n\n41.60888546\n\n\n.\n\n\n11\n\n\n51.2921685\n\n\n55\n\n\n543.2531116\n\n\n0.246596964\n\n\n34.52357495\n\n\n34988\n\n\n2286.66082\n\n\n2\n\n\n1888\n\n\n25\n\n\n28428.02649\n\n\n11\n\n\n1142\n\n\n4.438745351\n\n\n31\n\n\n10.85026641\n\n\n0.986387856\n\n\n9.124087666\n\n\n56.69406992\n\n\n.\n\n\n58\n\n\n0.246596964\n\n\n8.630893738\n\n\n21\n\n\n6.658118026\n\n\n17\n\n\n.\n\n\n16\n\n\n14\n\n\n40558\n\n\n37\n\n\n54.25798664\n\n\n800\n\n\n43\n\n\n69963.17724\n\n\n12\n\n\n.\n\n\n7.151311954\n\n\n42\n\n\n43\n\n\n8\n\n\n28\n\n\n43\n\n\n28\n\n\n6112\n\n\n9.617281594\n\n\n7\n\n\n6\n\n\n50\n\n\n85\n\n\n7.151311954\n\n\n821\n\n\n10.98474576\n\n\n3147.070454\n\n\n3527\n\n\n86\n\n\n375.2607485\n\n\n21\n\n\n500\n\n\n7\n\n\n11.83665427\n\n\n37\n\n\n52.26076014\n\n\n23.18011461\n\n\n48\n\n\n24.41309943\n\n\n1\n\n\n3213\n\n\n14\n\n\n4\n\n\n1\n\n\n2.712566603\n\n\n3.698954459\n\n\n17\n\n\n22\n\n\n982\n\n\n4\n\n\n10.85026641\n\n\n826\n\n\n35\n\n\n47663\n\n\n66\n\n\n19\n\n\n31\n\n\n17816.7864\n\n\n47\n\n\n2\n\n\n12215\n\n\n18\n\n\n116\n\n\n10.11047552\n\n\n6.90471499\n\n\n13\n\n\n3.945551423\n\n\n10.35707249\n\n\n778\n\n\n11.09686338\n\n\n.\n\n\n8\n\n\n169\n\n\n.\n\n\n8\n\n\n0.986387856\n\n\n8.13769981\n\n\n26.50917363\n\n\n318\n\n\n45211\n\n\n29\n\n\n31\n\n\n494\n\n\n1226\n\n\n3.698954459\n\n\n14\n\n\n8.384296774\n\n\n26.63247211\n\n\n710841\n\n\n13\n\n\n7\n\n\n23\n\n\n33\n\n\n6\n\n\n4438\n\n\n2288\n\n\n34\n\n\n16\n\n\n59416\n\n\n76\n\n\n84\n\n\n46\n\n\n31.04675257\n\n\n29\n\n\n2\n\n\n45.89519946\n\n\n23\n\n\n9\n\n\n25\n\n\n2\n\n\n.\n\n\n3.945551423\n\n\n2837\n\n\n111376.8502\n\n\n1806\n\n\n.\n\n\n1023\n\n\n5\n\n\n.\n\n\n11\n\n\n2568.307379\n\n\n26.63247211\n\n\n591\n\n\n31\n\n\n18\n\n\n10\n\n\n4.438745351\n\n\n31\n\n\n2.959163567\n\n\n3073\n\n\n31.04675257\n\n\n17\n\n\n20\n\n\n20\n\n\n15\n\n\n3866\n\n\n417.5\n\n\n393.8153514\n\n\n47\n\n\n12.3298482\n\n\n5\n\n\n5\n\n\n31\n\n\n9.863878558\n\n\n93.21365237\n\n\n90.50108577\n\n\n1675\n\n\n99.88955176\n\n\n2.219372675\n\n\n47.5\n\n\n0.493193928\n\n\n11\n\n\n10\n\n\n7295\n\n\n9\n\n\n7.5\n\n\n15\n\n\n3\n\n\n1\n\n\n51950.66607\n\n\n4\n\n\n10\n\n\n851\n\n\n11.71335579\n\n\n3.698954459\n\n\n12.08325123\n\n\n45\n\n\n14\n\n\n7.151311954\n\n\n0.986387856\n\n\n62\n\n\n5.425133207\n\n\n424288\n\n\n41\n\n\n.\n\n\n1.997226502\n\n\n.\n\n\n15\n\n\n1491\n\n\n.\n\n\n4\n\n\n46\n\n\n439\n\n\n18813.65713\n\n\n42\n\n\n35.95007704\n\n\n22.93351765\n\n\n6\n\n\n998\n\n\n11.09686338\n\n\n22\n\n\n1504\n\n\n37\n\n\n283\n\n\n9.863878558\n\n\n5.918327135\n\n\n67101\n\n\n2335\n\n\n39\n\n\n1.972775712\n\n\n3360\n\n\n235\n\n\n77.43144668\n\n\n92\n\n\n15\n\n\n45\n\n\n.\n\n\n9.863878558\n\n\n14.84844688\n\n\n4.931939279\n\n\n6.90471499\n\n\n32\n\n\n17\n\n\n.\n\n\n15229\n\n\n28\n\n\n2615\n\n\n20.96074194\n\n\n3.452357495\n\n\n20\n\n\n114.7379987\n\n\n35\n\n\n14\n\n\n41\n\n\n17\n\n\n12\n\n\n88\n\n\n1\n\n\n61\n\n\n3392\n\n\n4004\n\n\n18\n\n\n3\n\n\n5.425133207\n\n\n46\n\n\n121\n\n\n3\n\n\n22\n\n\n0.246596964\n\n\n41.84562303\n\n\n0.493193928\n\n\n7.891102846\n\n\n21\n\n\n4013\n\n\n26\n\n\n8\n\n\n5.918327135\n\n\n563.2274656\n\n\n2118\n\n\n60602\n\n\n13.31484335\n\n\n37\n\n\n48\n\n\n76.93825275\n\n\n14\n\n\n44\n\n\n15\n\n\n7943\n\n\n33\n\n\n20\n\n\n3.452357495\n\n\n379.7593245\n\n\n842\n\n\n900\n\n\n8305.681243\n\n\n87\n\n\n25\n\n\n10621\n\n\n63\n\n\n26\n\n\n26\n\n\n310\n\n\n17\n\n\n165\n\n\n8.384296774\n\n\n2356\n\n\n36\n\n\n48\n\n\n27\n\n\n1190\n\n\n9.37068463\n\n\n16712.12284\n\n\n30415\n\n\n118.1199457\n\n\n311.6985624\n\n\n39\n\n\n391\n\n\n36\n\n\n871\n\n\n1113.633516\n\n\n14.64632768\n\n\n21\n\n\n2.959163567\n\n\n1194\n\n\n2.712566603\n\n\n38\n\n\n37189\n\n\n8.13769981\n\n\n24.65969639\n\n\n38\n\n\n21\n\n\n199973\n\n\n40\n\n\n21\n\n\n22.94759973\n\n\n46\n\n\n49\n\n\n28\n\n\n45\n\n\n1380.90556\n\n\n22\n\n\n10.79887046\n\n\n1288\n\n\n286.7922691\n\n\n51939\n\n\n13\n\n\n43.15446869\n\n\n21\n\n\n43\n\n\n21\n\n\n47.5\n\n\n180\n\n\n44\n\n\n47\n\n\n14\n\n\n.\n\n\n5\n\n\n60\n\n\n5.425133207\n\n\n21.59774092\n\n\n638\n\n\n55657\n\n\n41.84562303\n\n\n138\n\n\n113.9277973\n\n\n48\n\n\n52\n\n\n7\n\n\n820\n\n\n20\n\n\n81\n\n\n25\n\n\n31\n\n\n3.69895446\n\n\n45221\n\n\n94\n\n\n402.5\n\n\n5\n\n\n1659\n\n\n22.5\n\n\n8\n\n\n1.84947723\n\n\n11.09686338\n\n\n424283\n\n\n20\n\n\n292\n\n\n225\n\n\n.\n\n\n3.205760531\n\n\n39\n\n\n2610\n\n\n56\n\n\n12\n\n\n564.2138535\n\n\n377.9098473\n\n\n20\n\n\n9\n\n\n378\n\n\n1133.881398\n\n\n51958\n\n\n21\n\n\n7.5\n\n\n26.99717615\n\n\n640\n\n\n55675\n\n\n14.84844688\n\n\n130\n\n\n114.4209913\n\n\n45\n\n\n60\n\n\n25\n\n\n812\n\n\n13\n\n\n78\n\n\n9\n\n\n33\n\n\n20.96074194\n\n\n45206\n\n\n78\n\n\n400\n\n\n6\n\n\n1665\n\n\n17.5\n\n\n2\n\n\n4.93193928\n\n\n3.205760531\n\n\n424286\n\n\n45\n\n\n275\n\n\n272.5\n\n\n.\n\n\n1.23298482\n\n\n46\n\n\n2650\n\n\n71\n\n\n11\n\n\n567.1730171\n\n\n372.3614155\n\n\n35\n\n\n42\n\n\n386\n\n\n1140.630692\n\n\n51955\n\n\n48\n\n\n5\n\n\n\n\n89\n\n\n2\n\n\nRACE\n\n\n \n\n\n1\n\n\n3\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n7\n\n\n1\n\n\n1\n\n\n3\n\n\n3\n\n\n3\n\n\n8\n\n\n3\n\n\n3\n\n\n1\n\n\n1\n\n\n3\n\n\n3\n\n\n3\n\n\n1\n\n\n1\n\n\n3\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n3\n\n\n3\n\n\n8\n\n\n3\n\n\n1\n\n\n1\n\n\n3\n\n\n3\n\n\n1\n\n\n1\n\n\n3\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n3\n\n\n3\n\n\n8\n\n\n3\n\n\n1\n\n\n1\n\n\n3\n\n\n3\n\n\n1\n\n\n1\n\n\n3\n\n\n1\n\n\n1\n\n\n1\n\n\n3\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n3\n\n\n3\n\n\n1\n\n\n1\n\n\n1\n\n\n3\n\n\n1\n\n\n2\n\n\n7\n\n\n7\n\n\n1\n\n\n1\n\n\n3\n\n\n1\n\n\n1\n\n\n3\n\n\n1\n\n\n1\n\n\n3\n\n\n1\n\n\n1\n\n\n1\n\n\n3\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n3\n\n\n8\n\n\n1\n\n\n3\n\n\n3\n\n\n3\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n3\n\n\n1\n\n\n1\n\n\n3\n\n\n1\n\n\n3\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n3\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n3\n\n\n3\n\n\n1\n\n\n1\n\n\n1\n\n\n3\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n3\n\n\n7\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n3\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n3\n\n\n1\n\n\n1\n\n\n1\n\n\n3\n\n\n1\n\n\n1\n\n\n1\n\n\n7\n\n\n1\n\n\n1\n\n\n3\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n8\n\n\n3\n\n\n3\n\n\n1\n\n\n4\n\n\n1\n\n\n1\n\n\n3\n\n\n1\n\n\n1\n\n\n1\n\n\n3\n\n\n3\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n3\n\n\n3\n\n\n1\n\n\n1\n\n\n1\n\n\n3\n\n\n3\n\n\n3\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n3\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n3\n\n\n3\n\n\n1\n\n\n3\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n3\n\n\n3\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n3\n\n\n1\n\n\n1\n\n\n1\n\n\n3\n\n\n1\n\n\n1\n\n\n1\n\n\n4\n\n\n1\n\n\n1\n\n\n3\n\n\n1\n\n\n3\n\n\n1\n\n\n3\n\n\n3\n\n\n3\n\n\n3\n\n\n3\n\n\n2\n\n\n1\n\n\n3\n\n\n1\n\n\n1\n\n\n3\n\n\n1\n\n\n3\n\n\n3\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n3\n\n\n1\n\n\n1\n\n\n8\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n3\n\n\n1\n\n\n3\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n3\n\n\n1\n\n\n1\n\n\n3\n\n\n3\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n8\n\n\n3\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n3\n\n\n3\n\n\n7\n\n\n1\n\n\n1\n\n\n1\n\n\n3\n\n\n8\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n3\n\n\n1\n\n\n3\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n3\n\n\n1\n\n\n3\n\n\n3\n\n\n2\n\n\n1\n\n\n3\n\n\n1\n\n\n3\n\n\n1\n\n\n1\n\n\n3\n\n\n1\n\n\n3\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n3\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n3\n\n\n2\n\n\n1\n\n\n8\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n3\n\n\n1\n\n\n3\n\n\n1\n\n\n1\n\n\n8\n\n\n3\n\n\n1\n\n\n1\n\n\n3\n\n\n3\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n3\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n3\n\n\n3\n\n\n3\n\n\n3\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n3\n\n\n3\n\n\n1\n\n\n1\n\n\n1\n\n\n3\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n3\n\n\n8\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n3\n\n\n1\n\n\n1\n\n\n1\n\n\n3\n\n\n3\n\n\n1\n\n\n3\n\n\n3\n\n\n1\n\n\n3\n\n\n1\n\n\n3\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n3\n\n\n1\n\n\n8\n\n\n1\n\n\n1\n\n\n1\n\n\n3\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n3\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n3\n\n\n3\n\n\n4\n\n\n8\n\n\n3\n\n\n3\n\n\n1\n\n\n2\n\n\n8\n\n\n1\n\n\n1\n\n\n1\n\n\n8\n\n\n1\n\n\n1\n\n\n3\n\n\n8\n\n\n1\n\n\n1\n\n\n1\n\n\n3\n\n\n1\n\n\n3\n\n\n3\n\n\n1\n\n\n8\n\n\n3\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n3\n\n\n1\n\n\n3\n\n\n3\n\n\n1\n\n\n3\n\n\n3\n\n\n1\n\n\n3\n\n\n8\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n3\n\n\n3\n\n\n1\n\n\n3\n\n\n1\n\n\n1\n\n\n8\n\n\n3\n\n\n1\n\n\n3\n\n\n1\n\n\n1\n\n\n1\n\n\n3\n\n\n1\n\n\n3\n\n\n2\n\n\n8\n\n\n1\n\n\n3\n\n\n3\n\n\n1\n\n\n8\n\n\n3\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n3\n\n\n1\n\n\n3\n\n\n3\n\n\n1\n\n\n3\n\n\n3\n\n\n1\n\n\n3\n\n\n8\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n3\n\n\n3\n\n\n1\n\n\n3\n\n\n1\n\n\n1\n\n\n8\n\n\n3\n\n\n1\n\n\n3\n\n\n1\n\n\n1\n\n\n1\n\n\n3\n\n\n1\n\n\n3\n\n\n2\n\n\n8\n\n\n1\n\n\n\n\n90\n\n\n2\n\n\nEDUCBAS\n\n\n \n\n\n4\n\n\n2\n\n\n7\n\n\n5\n\n\n7\n\n\n6\n\n\n5\n\n\n4\n\n\n7\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n4\n\n\n4\n\n\n6\n\n\n4\n\n\n3\n\n\n4\n\n\n4\n\n\n4\n\n\n4\n\n\n2\n\n\n7\n\n\n7\n\n\n6\n\n\n4\n\n\n7\n\n\n2\n\n\n2\n\n\n2\n\n\n4\n\n\n6\n\n\n4\n\n\n3\n\n\n4\n\n\n4\n\n\n4\n\n\n2\n\n\n7\n\n\n7\n\n\n6\n\n\n4\n\n\n7\n\n\n2\n\n\n2\n\n\n2\n\n\n4\n\n\n6\n\n\n4\n\n\n3\n\n\n4\n\n\n4\n\n\n6\n\n\n3\n\n\n4\n\n\n4\n\n\n5\n\n\n5\n\n\n4\n\n\n7\n\n\n5\n\n\n6\n\n\n4\n\n\n5\n\n\n4\n\n\n4\n\n\n7\n\n\n4\n\n\n6\n\n\n5\n\n\n5\n\n\n4\n\n\n6\n\n\n5\n\n\n4\n\n\n4\n\n\n6\n\n\n4\n\n\n3\n\n\n5\n\n\n3\n\n\n6\n\n\n3\n\n\n7\n\n\n4\n\n\n4\n\n\n4\n\n\n5\n\n\n5\n\n\n7\n\n\n3\n\n\n4\n\n\n4\n\n\n5\n\n\n4\n\n\n3\n\n\n5\n\n\n7\n\n\n3\n\n\n5\n\n\n3\n\n\n3\n\n\n4\n\n\n4\n\n\n7\n\n\n7\n\n\n4\n\n\n4\n\n\n7\n\n\n5\n\n\n4\n\n\n7\n\n\n5\n\n\n5\n\n\n4\n\n\n4\n\n\n2\n\n\n3\n\n\n5\n\n\n4\n\n\n6\n\n\n3\n\n\n6\n\n\n3\n\n\n4\n\n\n3\n\n\n4\n\n\n4\n\n\n5\n\n\n4\n\n\n4\n\n\n5\n\n\n4\n\n\n3\n\n\n3\n\n\n7\n\n\n4\n\n\n4\n\n\n5\n\n\n3\n\n\n3\n\n\n4\n\n\n7\n\n\n3\n\n\n4\n\n\n4\n\n\n4\n\n\n3\n\n\n5\n\n\n4\n\n\n5\n\n\n1\n\n\n5\n\n\n4\n\n\n3\n\n\n5\n\n\n5\n\n\n7\n\n\n4\n\n\n7\n\n\n7\n\n\n3\n\n\n4\n\n\n4\n\n\n3\n\n\n5\n\n\n5\n\n\n5\n\n\n5\n\n\n5\n\n\n5\n\n\n5\n\n\n5\n\n\n2\n\n\n5\n\n\n5\n\n\n3\n\n\n3\n\n\n6\n\n\n3\n\n\n3\n\n\n5\n\n\n4\n\n\n3\n\n\n7\n\n\n4\n\n\n4\n\n\n4\n\n\n4\n\n\n3\n\n\n3\n\n\n3\n\n\n2\n\n\n3\n\n\n4\n\n\n4\n\n\n4\n\n\n5\n\n\n6\n\n\n7\n\n\n4\n\n\n7\n\n\n5\n\n\n5\n\n\n7\n\n\n3\n\n\n4\n\n\n6\n\n\n4\n\n\n4\n\n\n5\n\n\n7\n\n\n4\n\n\n2\n\n\n5\n\n\n7\n\n\n4\n\n\n5\n\n\n4\n\n\n4\n\n\n6\n\n\n7\n\n\n7\n\n\n2\n\n\n3\n\n\n5\n\n\n4\n\n\n4\n\n\n5\n\n\n6\n\n\n4\n\n\n6\n\n\n6\n\n\n4\n\n\n4\n\n\n4\n\n\n5\n\n\n4\n\n\n4\n\n\n4\n\n\n4\n\n\n4\n\n\n5\n\n\n5\n\n\n3\n\n\n5\n\n\n5\n\n\n3\n\n\n4\n\n\n3\n\n\n5\n\n\n5\n\n\n6\n\n\n5\n\n\n5\n\n\n7\n\n\n2\n\n\n4\n\n\n5\n\n\n5\n\n\n5\n\n\n2\n\n\n5\n\n\n7\n\n\n4\n\n\n7\n\n\n2\n\n\n4\n\n\n5\n\n\n7\n\n\n7\n\n\n6\n\n\n4\n\n\n7\n\n\n3\n\n\n3\n\n\n6\n\n\n7\n\n\n7\n\n\n6\n\n\n1\n\n\n4\n\n\n5\n\n\n6\n\n\n6\n\n\n4\n\n\n5\n\n\n4\n\n\n3\n\n\n5\n\n\n5\n\n\n4\n\n\n4\n\n\n3\n\n\n7\n\n\n3\n\n\n5\n\n\n7\n\n\n5\n\n\n5\n\n\n6\n\n\n4\n\n\n5\n\n\n5\n\n\n7\n\n\n4\n\n\n2\n\n\n4\n\n\n4\n\n\n5\n\n\n4\n\n\n5\n\n\n4\n\n\n3\n\n\n2\n\n\n5\n\n\n2\n\n\n2\n\n\n5\n\n\n4\n\n\n7\n\n\n4\n\n\n3\n\n\n7\n\n\n4\n\n\n2\n\n\n4\n\n\n4\n\n\n5\n\n\n7\n\n\n4\n\n\n6\n\n\n4\n\n\n4\n\n\n7\n\n\n2\n\n\n7\n\n\n4\n\n\n5\n\n\n4\n\n\n4\n\n\n5\n\n\n4\n\n\n7\n\n\n5\n\n\n4\n\n\n2\n\n\n5\n\n\n4\n\n\n2\n\n\n4\n\n\n6\n\n\n7\n\n\n4\n\n\n4\n\n\n7\n\n\n3\n\n\n4\n\n\n4\n\n\n5\n\n\n4\n\n\n4\n\n\n5\n\n\n7\n\n\n5\n\n\n6\n\n\n6\n\n\n5\n\n\n4\n\n\n3\n\n\n3\n\n\n4\n\n\n3\n\n\n4\n\n\n5\n\n\n5\n\n\n4\n\n\n5\n\n\n5\n\n\n3\n\n\n5\n\n\n7\n\n\n4\n\n\n3\n\n\n7\n\n\n5\n\n\n7\n\n\n6\n\n\n7\n\n\n5\n\n\n5\n\n\n7\n\n\n3\n\n\n4\n\n\n5\n\n\n6\n\n\n4\n\n\n4\n\n\n4\n\n\n2\n\n\n4\n\n\n5\n\n\n7\n\n\n3\n\n\n3\n\n\n5\n\n\n4\n\n\n4\n\n\n5\n\n\n2\n\n\n5\n\n\n4\n\n\n6\n\n\n4\n\n\n4\n\n\n7\n\n\n4\n\n\n7\n\n\n5\n\n\n4\n\n\n7\n\n\n5\n\n\n4\n\n\n4\n\n\n4\n\n\n4\n\n\n4\n\n\n4\n\n\n5\n\n\n4\n\n\n7\n\n\n6\n\n\n6\n\n\n4\n\n\n4\n\n\n4\n\n\n4\n\n\n4\n\n\n5\n\n\n6\n\n\n6\n\n\n2\n\n\n4\n\n\n5\n\n\n4\n\n\n7\n\n\n4\n\n\n7\n\n\n4\n\n\n2\n\n\n7\n\n\n4\n\n\n4\n\n\n3\n\n\n4\n\n\n3\n\n\n2\n\n\n7\n\n\n3\n\n\n3\n\n\n5\n\n\n4\n\n\n5\n\n\n3\n\n\n4\n\n\n4\n\n\n3\n\n\n4\n\n\n7\n\n\n4\n\n\n4\n\n\n3\n\n\n4\n\n\n2\n\n\n4\n\n\n3\n\n\n2\n\n\n7\n\n\n2\n\n\n4\n\n\n7\n\n\n3\n\n\n1\n\n\n6\n\n\n4\n\n\n7\n\n\n5\n\n\n2\n\n\n5\n\n\n4\n\n\n2\n\n\n4\n\n\n7\n\n\n2\n\n\n4\n\n\n5\n\n\n3\n\n\n7\n\n\n6\n\n\n4\n\n\n3\n\n\n7\n\n\n4\n\n\n4\n\n\n3\n\n\n4\n\n\n3\n\n\n4\n\n\n4\n\n\n3\n\n\n4\n\n\n7\n\n\n4\n\n\n4\n\n\n3\n\n\n4\n\n\n2\n\n\n4\n\n\n3\n\n\n2\n\n\n7\n\n\n2\n\n\n4\n\n\n7\n\n\n3\n\n\n1\n\n\n6\n\n\n4\n\n\n7\n\n\n5\n\n\n2\n\n\n5\n\n\n4\n\n\n2\n\n\n4\n\n\n7\n\n\n2\n\n\n4\n\n\n5\n\n\n3\n\n\n7\n\n\n6\n\n\n4\n\n\n3\n\n\n7\n\n\n4\n\n\n4\n\n\n3\n\n\n4\n\n\n\n\n91\n\n\n2\n\n\nhivpos\n\n\n \n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n\n\n92\n\n\n2\n\n\nage\n\n\n \n\n\n54\n\n\n56\n\n\n49\n\n\n46\n\n\n55\n\n\n38\n\n\n49\n\n\n32\n\n\n49\n\n\n42\n\n\n34\n\n\n55\n\n\n31\n\n\n42\n\n\n52\n\n\n53\n\n\n40\n\n\n63\n\n\n38\n\n\n53\n\n\n41\n\n\n55\n\n\n53\n\n\n50\n\n\n56\n\n\n38\n\n\n31\n\n\n49\n\n\n41\n\n\n54\n\n\n33\n\n\n43\n\n\n50\n\n\n39\n\n\n62\n\n\n51\n\n\n40\n\n\n51\n\n\n57\n\n\n50\n\n\n57\n\n\n39\n\n\n35\n\n\n49\n\n\n39\n\n\n53\n\n\n33\n\n\n41\n\n\n51\n\n\n38\n\n\n63\n\n\n51\n\n\n41\n\n\n54\n\n\n59\n\n\n40\n\n\n47\n\n\n44\n\n\n42\n\n\n70\n\n\n50\n\n\n49\n\n\n36\n\n\n40\n\n\n45\n\n\n40\n\n\n51\n\n\n54\n\n\n28\n\n\n38\n\n\n47\n\n\n43\n\n\n35\n\n\n48\n\n\n29\n\n\n63\n\n\n54\n\n\n46\n\n\n47\n\n\n50\n\n\n57\n\n\n35\n\n\n45\n\n\n59\n\n\n50\n\n\n43\n\n\n47\n\n\n26\n\n\n44\n\n\n41\n\n\n65\n\n\n43\n\n\n51\n\n\n57\n\n\n39\n\n\n38\n\n\n45\n\n\n45\n\n\n40\n\n\n28\n\n\n44\n\n\n26\n\n\n42\n\n\n37\n\n\n50\n\n\n50\n\n\n64\n\n\n52\n\n\n50\n\n\n46\n\n\n47\n\n\n40\n\n\n52\n\n\n38\n\n\n39\n\n\n64\n\n\n47\n\n\n34\n\n\n44\n\n\n37\n\n\n60\n\n\n50\n\n\n28\n\n\n67\n\n\n49\n\n\n49\n\n\n46\n\n\n41\n\n\n54\n\n\n48\n\n\n45\n\n\n39\n\n\n57\n\n\n59\n\n\n52\n\n\n53\n\n\n52\n\n\n51\n\n\n49\n\n\n44\n\n\n37\n\n\n35\n\n\n44\n\n\n48\n\n\n53\n\n\n49\n\n\n55\n\n\n44\n\n\n22\n\n\n46\n\n\n31\n\n\n36\n\n\n32\n\n\n41\n\n\n36\n\n\n49\n\n\n53\n\n\n56\n\n\n44\n\n\n60\n\n\n48\n\n\n41\n\n\n27\n\n\n35\n\n\n52\n\n\n62\n\n\n46\n\n\n40\n\n\n59\n\n\n28\n\n\n49\n\n\n44\n\n\n51\n\n\n39\n\n\n57\n\n\n46\n\n\n48\n\n\n53\n\n\n39\n\n\n59\n\n\n52\n\n\n59\n\n\n26\n\n\n50\n\n\n34\n\n\n75\n\n\n45\n\n\n32\n\n\n36\n\n\n49\n\n\n44\n\n\n38\n\n\n45\n\n\n56\n\n\n43\n\n\n47\n\n\n52\n\n\n46\n\n\n59\n\n\n34\n\n\n48\n\n\n41\n\n\n44\n\n\n47\n\n\n45\n\n\n54\n\n\n43\n\n\n53\n\n\n41\n\n\n38\n\n\n48\n\n\n50\n\n\n49\n\n\n48\n\n\n41\n\n\n39\n\n\n60\n\n\n46\n\n\n46\n\n\n51\n\n\n43\n\n\n43\n\n\n44\n\n\n48\n\n\n44\n\n\n41\n\n\n46\n\n\n55\n\n\n35\n\n\n48\n\n\n33\n\n\n34\n\n\n59\n\n\n40\n\n\n48\n\n\n46\n\n\n46\n\n\n42\n\n\n48\n\n\n40\n\n\n43\n\n\n60\n\n\n27\n\n\n49\n\n\n45\n\n\n40\n\n\n51\n\n\n31\n\n\n47\n\n\n52\n\n\n39\n\n\n28\n\n\n39\n\n\n53\n\n\n55\n\n\n42\n\n\n57\n\n\n34\n\n\n53\n\n\n38\n\n\n41\n\n\n43\n\n\n31\n\n\n49\n\n\n36\n\n\n40\n\n\n61\n\n\n35\n\n\n43\n\n\n41\n\n\n42\n\n\n55\n\n\n37\n\n\n46\n\n\n62\n\n\n39\n\n\n29\n\n\n44\n\n\n57\n\n\n46\n\n\n51\n\n\n47\n\n\n55\n\n\n28\n\n\n38\n\n\n49\n\n\n47\n\n\n48\n\n\n47\n\n\n58\n\n\n49\n\n\n41\n\n\n49\n\n\n32\n\n\n38\n\n\n34\n\n\n50\n\n\n56\n\n\n49\n\n\n50\n\n\n49\n\n\n43\n\n\n46\n\n\n42\n\n\n29\n\n\n33\n\n\n50\n\n\n42\n\n\n52\n\n\n66\n\n\n35\n\n\n43\n\n\n44\n\n\n45\n\n\n52\n\n\n42\n\n\n42\n\n\n38\n\n\n34\n\n\n39\n\n\n38\n\n\n50\n\n\n27\n\n\n39\n\n\n58\n\n\n60\n\n\n58\n\n\n43\n\n\n39\n\n\n51\n\n\n44\n\n\n40\n\n\n55\n\n\n50\n\n\n52\n\n\n53\n\n\n55\n\n\n39\n\n\n36\n\n\n34\n\n\n38\n\n\n46\n\n\n38\n\n\n49\n\n\n33\n\n\n40\n\n\n59\n\n\n45\n\n\n42\n\n\n52\n\n\n31\n\n\n42\n\n\n47\n\n\n54\n\n\n59\n\n\n52\n\n\n44\n\n\n53\n\n\n60\n\n\n45\n\n\n56\n\n\n40\n\n\n38\n\n\n36\n\n\n64\n\n\n39\n\n\n32\n\n\n52\n\n\n51\n\n\n59\n\n\n43\n\n\n42\n\n\n54\n\n\n49\n\n\n36\n\n\n36\n\n\n48\n\n\n48\n\n\n36\n\n\n29\n\n\n63\n\n\n38\n\n\n44\n\n\n50\n\n\n40\n\n\n63\n\n\n38\n\n\n48\n\n\n45\n\n\n52\n\n\n62\n\n\n30\n\n\n51\n\n\n45\n\n\n30\n\n\n46\n\n\n53\n\n\n51\n\n\n64\n\n\n38\n\n\n25\n\n\n40\n\n\n50\n\n\n46\n\n\n45\n\n\n63\n\n\n46\n\n\n29\n\n\n54\n\n\n57\n\n\n39\n\n\n41\n\n\n38\n\n\n53\n\n\n39\n\n\n46\n\n\n54\n\n\n43\n\n\n45\n\n\n47\n\n\n43\n\n\n47\n\n\n45\n\n\n43\n\n\n53\n\n\n40\n\n\n45\n\n\n55\n\n\n48\n\n\n36\n\n\n46\n\n\n40\n\n\n43\n\n\n38\n\n\n38\n\n\n46\n\n\n47\n\n\n54\n\n\n42\n\n\n47\n\n\n50\n\n\n58\n\n\n28\n\n\n32\n\n\n41\n\n\n40\n\n\n45\n\n\n25\n\n\n47\n\n\n33\n\n\n38\n\n\n49\n\n\n43\n\n\n43\n\n\n48\n\n\n41\n\n\n50\n\n\n40\n\n\n38\n\n\n39\n\n\n33\n\n\n49\n\n\n36\n\n\n49\n\n\n60\n\n\n42\n\n\n38\n\n\n40\n\n\n60\n\n\n41\n\n\n49\n\n\n55\n\n\n50\n\n\n59\n\n\n55\n\n\n52\n\n\n35\n\n\n53\n\n\n50\n\n\n41\n\n\n43\n\n\n56\n\n\n37\n\n\n49\n\n\n38\n\n\n31\n\n\n49\n\n\n29\n\n\n41\n\n\n44\n\n\n38\n\n\n54\n\n\n48\n\n\n34\n\n\n33\n\n\n43\n\n\n38\n\n\n61\n\n\n44\n\n\n50\n\n\n39\n\n\n62\n\n\n47\n\n\n51\n\n\n33\n\n\n51\n\n\n40\n\n\n62\n\n\n40\n\n\n40\n\n\n40\n\n\n59\n\n\n41\n\n\n49\n\n\n51\n\n\n51\n\n\n60\n\n\n56\n\n\n51\n\n\n38\n\n\n57\n\n\n50\n\n\n41\n\n\n43\n\n\n57\n\n\n39\n\n\n48\n\n\n39\n\n\n35\n\n\n49\n\n\n30\n\n\n39\n\n\n43\n\n\n36\n\n\n53\n\n\n45\n\n\n33\n\n\n33\n\n\n41\n\n\n34\n\n\n63\n\n\n45\n\n\n51\n\n\n38\n\n\n63\n\n\n46\n\n\n51\n\n\n33\n\n\n51\n\n\n41\n\n\n\n\n93\n\n\n2\n\n\nART\n\n\n \n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n\n\n94\n\n\n2\n\n\neverART\n\n\n \n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n\n\n95\n\n\n2\n\n\nyears\n\n\n \n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n\n\n96\n\n\n2\n\n\nhard_drugs\n\n\n \n\n\n0\n\n\n0\n\n\n1\n\n\n0\n\n\n1\n\n\n1\n\n\n1\n\n\n0\n\n\n1\n\n\n0\n\n\n1\n\n\n1\n\n\n0\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n0\n\n\n0\n\n\n0\n\n\n1\n\n\n0\n\n\n0\n\n\n1\n\n\n1\n\n\n1\n\n\n0\n\n\n1\n\n\n0\n\n\n1\n\n\n0\n\n\n1\n\n\n1\n\n\n1\n\n\n0\n\n\n0\n\n\n1\n\n\n0\n\n\n0\n\n\n1\n\n\n1\n\n\n1\n\n\n0\n\n\n1\n\n\n0\n\n\n1\n\n\n0\n\n\n1\n\n\n1\n\n\n1\n\n\n0\n\n\n0\n\n\n1\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n1\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n1\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n1\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n1\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n1\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n1\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n1\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n1\n\n\n0\n\n\n0\n\n\n0\n\n\n1\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n1\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n1\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n1\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n1\n\n\n0\n\n\n0\n\n\n1\n\n\n0\n\n\n0\n\n\n1\n\n\n0\n\n\n1\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n1\n\n\n0\n\n\n0\n\n\n0\n\n\n1\n\n\n0\n\n\n0\n\n\n0\n\n\n1\n\n\n1\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n1\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n1\n\n\n0\n\n\n0\n\n\n1\n\n\n0\n\n\n0\n\n\n1\n\n\n0\n\n\n1\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n1\n\n\n0\n\n\n0\n\n\n0\n\n\n1\n\n\n0\n\n\n0\n\n\n0\n\n\n1\n\n\n1\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n1\n\n\n\n\n97\n\n\n2\n\n\nincome\n\n\nIncome\n\n\n4\n\n\n2\n\n\n6\n\n\n.\n\n\n9\n\n\n.\n\n\n2\n\n\n3\n\n\n6\n\n\n1\n\n\n.\n\n\n1\n\n\n.\n\n\n.\n\n\n1\n\n\n6\n\n\n.\n\n\n3\n\n\n.\n\n\n2\n\n\n.\n\n\n.\n\n\n2\n\n\n.\n\n\n9\n\n\n.\n\n\n3\n\n\n6\n\n\n.\n\n\n1\n\n\n.\n\n\n1\n\n\n6\n\n\n.\n\n\n3\n\n\n2\n\n\n2\n\n\n4\n\n\n2\n\n\n6\n\n\n9\n\n\n5\n\n\n3\n\n\n6\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n6\n\n\n1\n\n\n3\n\n\n.\n\n\n2\n\n\n6\n\n\n3\n\n\n4\n\n\n2\n\n\n6\n\n\n1\n\n\n4\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n3\n\n\n4\n\n\n1\n\n\n6\n\n\n.\n\n\n6\n\n\n2\n\n\n2\n\n\n3\n\n\n1\n\n\n.\n\n\n1\n\n\n7\n\n\n3\n\n\n.\n\n\n.\n\n\n7\n\n\n2\n\n\n6\n\n\n.\n\n\n5\n\n\n2\n\n\n1\n\n\n3\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n5\n\n\n3\n\n\n3\n\n\n.\n\n\n4\n\n\n5\n\n\n6\n\n\n2\n\n\n.\n\n\n3\n\n\n1\n\n\n5\n\n\n5\n\n\n6\n\n\n.\n\n\n3\n\n\n6\n\n\n1\n\n\n6\n\n\n6\n\n\n6\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n5\n\n\n4\n\n\n3\n\n\n3\n\n\n6\n\n\n.\n\n\n7\n\n\n7\n\n\n.\n\n\n.\n\n\n2\n\n\n4\n\n\n6\n\n\n.\n\n\n1\n\n\n.\n\n\n9\n\n\n3\n\n\n9\n\n\n3\n\n\n.\n\n\n2\n\n\n.\n\n\n.\n\n\n5\n\n\n.\n\n\n.\n\n\n4\n\n\n6\n\n\n1\n\n\n2\n\n\n2\n\n\n5\n\n\n1\n\n\n4\n\n\n1\n\n\n6\n\n\n3\n\n\n1\n\n\n4\n\n\n.\n\n\n7\n\n\n.\n\n\n7\n\n\n6\n\n\n.\n\n\n1\n\n\n1\n\n\n3\n\n\n3\n\n\n5\n\n\n7\n\n\n3\n\n\n6\n\n\n6\n\n\n.\n\n\n6\n\n\n.\n\n\n4\n\n\n9\n\n\n1\n\n\n2\n\n\n7\n\n\n.\n\n\n5\n\n\n3\n\n\n5\n\n\n.\n\n\n.\n\n\n.\n\n\n5\n\n\n3\n\n\n1\n\n\n3\n\n\n1\n\n\n1\n\n\n.\n\n\n2\n\n\n2\n\n\n4\n\n\n3\n\n\n6\n\n\n6\n\n\n9\n\n\n6\n\n\n7\n\n\n5\n\n\n6\n\n\n1\n\n\n.\n\n\n.\n\n\n1\n\n\n.\n\n\n7\n\n\n6\n\n\n6\n\n\n2\n\n\n1\n\n\n4\n\n\n3\n\n\n4\n\n\n.\n\n\n6\n\n\n2\n\n\n.\n\n\n6\n\n\n5\n\n\n1\n\n\n2\n\n\n.\n\n\n3\n\n\n.\n\n\n4\n\n\n2\n\n\n3\n\n\n4\n\n\n3\n\n\n.\n\n\n1\n\n\n2\n\n\n.\n\n\n3\n\n\n1\n\n\n1\n\n\n.\n\n\n4\n\n\n3\n\n\n5\n\n\n1\n\n\n7\n\n\n1\n\n\n1\n\n\n.\n\n\n.\n\n\n2\n\n\n2\n\n\n6\n\n\n5\n\n\n6\n\n\n6\n\n\n1\n\n\n3\n\n\n5\n\n\n4\n\n\n.\n\n\n2\n\n\n.\n\n\n.\n\n\n1\n\n\n4\n\n\n3\n\n\n.\n\n\n9\n\n\n4\n\n\n9\n\n\n6\n\n\n5\n\n\n7\n\n\n2\n\n\n.\n\n\n.\n\n\n7\n\n\n6\n\n\n6\n\n\n.\n\n\n4\n\n\n.\n\n\n5\n\n\n6\n\n\n.\n\n\n.\n\n\n3\n\n\n.\n\n\n2\n\n\n4\n\n\n9\n\n\n.\n\n\n2\n\n\n1\n\n\n4\n\n\n3\n\n\n6\n\n\n7\n\n\n6\n\n\n1\n\n\n3\n\n\n.\n\n\n.\n\n\n4\n\n\n6\n\n\n1\n\n\n3\n\n\n.\n\n\n7\n\n\n3\n\n\n4\n\n\n2\n\n\n5\n\n\n2\n\n\n2\n\n\n9\n\n\n1\n\n\n.\n\n\n7\n\n\n6\n\n\n.\n\n\n4\n\n\n.\n\n\n.\n\n\n2\n\n\n.\n\n\n.\n\n\n.\n\n\n7\n\n\n1\n\n\n4\n\n\n.\n\n\n7\n\n\n.\n\n\n.\n\n\n6\n\n\n4\n\n\n1\n\n\n.\n\n\n.\n\n\n7\n\n\n4\n\n\n7\n\n\n7\n\n\n.\n\n\n2\n\n\n.\n\n\n5\n\n\n.\n\n\n.\n\n\n6\n\n\n7\n\n\n2\n\n\n1\n\n\n1\n\n\n.\n\n\n1\n\n\n.\n\n\n1\n\n\n4\n\n\n3\n\n\n5\n\n\n1\n\n\n3\n\n\n5\n\n\n2\n\n\n7\n\n\n9\n\n\n1\n\n\n2\n\n\n1\n\n\n.\n\n\n3\n\n\n7\n\n\n7\n\n\n2\n\n\n3\n\n\n1\n\n\n4\n\n\n6\n\n\n6\n\n\n4\n\n\n1\n\n\n6\n\n\n3\n\n\n5\n\n\n.\n\n\n1\n\n\n1\n\n\n4\n\n\n1\n\n\n1\n\n\n2\n\n\n7\n\n\n6\n\n\n1\n\n\n4\n\n\n.\n\n\n.\n\n\n.\n\n\n4\n\n\n5\n\n\n2\n\n\n3\n\n\n5\n\n\n3\n\n\n.\n\n\n7\n\n\n9\n\n\n5\n\n\n1\n\n\n.\n\n\n3\n\n\n6\n\n\n6\n\n\n9\n\n\n6\n\n\n6\n\n\n.\n\n\n3\n\n\n4\n\n\n.\n\n\n2\n\n\n6\n\n\n9\n\n\n7\n\n\n.\n\n\n.\n\n\n.\n\n\n4\n\n\n6\n\n\n6\n\n\n4\n\n\n1\n\n\n.\n\n\n5\n\n\n.\n\n\n.\n\n\n4\n\n\n4\n\n\n1\n\n\n.\n\n\n.\n\n\n2\n\n\n.\n\n\n2\n\n\n6\n\n\n4\n\n\n1\n\n\n.\n\n\n4\n\n\n1\n\n\n1\n\n\n2\n\n\n.\n\n\n1\n\n\n2\n\n\n4\n\n\n.\n\n\n2\n\n\n3\n\n\n6\n\n\n3\n\n\n3\n\n\n4\n\n\n1\n\n\n.\n\n\n6\n\n\n5\n\n\n.\n\n\n.\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n.\n\n\n1\n\n\n1\n\n\n.\n\n\n.\n\n\n1\n\n\n5\n\n\n.\n\n\n6\n\n\n7\n\n\n.\n\n\n4\n\n\n2\n\n\n1\n\n\n7\n\n\n.\n\n\n1\n\n\n.\n\n\n5\n\n\n4\n\n\n.\n\n\n6\n\n\n1\n\n\n.\n\n\n.\n\n\n2\n\n\n4\n\n\n.\n\n\n.\n\n\n3\n\n\n3\n\n\n4\n\n\n1\n\n\n3\n\n\n6\n\n\n5\n\n\n4\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n.\n\n\n6\n\n\n1\n\n\n.\n\n\n9\n\n\n2\n\n\n1\n\n\n5\n\n\n.\n\n\n6\n\n\n.\n\n\n1\n\n\n4\n\n\n2\n\n\n1\n\n\n7\n\n\n7\n\n\n1\n\n\n1\n\n\n5\n\n\n4\n\n\n6\n\n\n6\n\n\n1\n\n\n3\n\n\n3\n\n\n.\n\n\n.\n\n\n1\n\n\n2\n\n\n\n\n98\n\n\n2\n\n\nHASHF\n\n\nHash/Marijuana Use Since Last Visit\n\n\n0\n\n\n0\n\n\n4\n\n\n0\n\n\n4\n\n\n4\n\n\n1\n\n\n2\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n1\n\n\n2\n\n\n2\n\n\n0\n\n\n2\n\n\n4\n\n\n0\n\n\n4\n\n\n0\n\n\n0\n\n\n0\n\n\n4\n\n\n4\n\n\n4\n\n\n2\n\n\n0\n\n\n0\n\n\n0\n\n\n1\n\n\n2\n\n\n0\n\n\n2\n\n\n4\n\n\n4\n\n\n0\n\n\n0\n\n\n0\n\n\n4\n\n\n4\n\n\n4\n\n\n2\n\n\n0\n\n\n0\n\n\n0\n\n\n1\n\n\n2\n\n\n0\n\n\n2\n\n\n4\n\n\n4\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n2\n\n\n0\n\n\n0\n\n\n4\n\n\n4\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n4\n\n\n0\n\n\n0\n\n\n2\n\n\n0\n\n\n0\n\n\n4\n\n\n2\n\n\n0\n\n\n4\n\n\n1\n\n\n0\n\n\n4\n\n\n0\n\n\n0\n\n\n4\n\n\n0\n\n\n0\n\n\n1\n\n\n0\n\n\n2\n\n\n0\n\n\n0\n\n\n4\n\n\n2\n\n\n4\n\n\n0\n\n\n2\n\n\n0\n\n\n1\n\n\n0\n\n\n1\n\n\n0\n\n\n1\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n2\n\n\n0\n\n\n4\n\n\n0\n\n\n4\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n2\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n2\n\n\n0\n\n\n0\n\n\n1\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n4\n\n\n0\n\n\n4\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n4\n\n\n1\n\n\n0\n\n\n4\n\n\n0\n\n\n0\n\n\n1\n\n\n4\n\n\n0\n\n\n0\n\n\n3\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n3\n\n\n1\n\n\n0\n\n\n4\n\n\n0\n\n\n0\n\n\n0\n\n\n1\n\n\n2\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n1\n\n\n0\n\n\n4\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n2\n\n\n0\n\n\n4\n\n\n0\n\n\n3\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n4\n\n\n4\n\n\n1\n\n\n0\n\n\n0\n\n\n4\n\n\n4\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n1\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n3\n\n\n0\n\n\n4\n\n\n0\n\n\n0\n\n\n1\n\n\n0\n\n\n0\n\n\n2\n\n\n0\n\n\n0\n\n\n0\n\n\n2\n\n\n0\n\n\n4\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n4\n\n\n0\n\n\n1\n\n\n0\n\n\n0\n\n\n3\n\n\n0\n\n\n4\n\n\n0\n\n\n0\n\n\n3\n\n\n2\n\n\n1\n\n\n4\n\n\n2\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n2\n\n\n2\n\n\n0\n\n\n2\n\n\n0\n\n\n0\n\n\n0\n\n\n1\n\n\n4\n\n\n3\n\n\n1\n\n\n1\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n1\n\n\n0\n\n\n0\n\n\n4\n\n\n4\n\n\n0\n\n\n0\n\n\n1\n\n\n1\n\n\n1\n\n\n0\n\n\n0\n\n\n4\n\n\n0\n\n\n0\n\n\n4\n\n\n4\n\n\n0\n\n\n0\n\n\n4\n\n\n0\n\n\n1\n\n\n1\n\n\n0\n\n\n0\n\n\n2\n\n\n0\n\n\n1\n\n\n0\n\n\n1\n\n\n0\n\n\n0\n\n\n1\n\n\n0\n\n\n1\n\n\n0\n\n\n0\n\n\n2\n\n\n0\n\n\n0\n\n\n1\n\n\n0\n\n\n4\n\n\n0\n\n\n4\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n4\n\n\n2\n\n\n0\n\n\n0\n\n\n0\n\n\n2\n\n\n2\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n2\n\n\n4\n\n\n4\n\n\n4\n\n\n0\n\n\n0\n\n\n4\n\n\n0\n\n\n1\n\n\n0\n\n\n0\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n0\n\n\n0\n\n\n2\n\n\n0\n\n\n0\n\n\n2\n\n\n0\n\n\n4\n\n\n2\n\n\n4\n\n\n3\n\n\n2\n\n\n0\n\n\n0\n\n\n4\n\n\n0\n\n\n3\n\n\n2\n\n\n0\n\n\n2\n\n\n0\n\n\n2\n\n\n0\n\n\n2\n\n\n0\n\n\n4\n\n\n2\n\n\n0\n\n\n0\n\n\n4\n\n\n0\n\n\n0\n\n\n0\n\n\n3\n\n\n0\n\n\n4\n\n\n4\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n2\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n1\n\n\n2\n\n\n0\n\n\n0\n\n\n0\n\n\n4\n\n\n0\n\n\n1\n\n\n0\n\n\n4\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n2\n\n\n4\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n1\n\n\n4\n\n\n0\n\n\n0\n\n\n2\n\n\n0\n\n\n4\n\n\n0\n\n\n4\n\n\n2\n\n\n0\n\n\n3\n\n\n2\n\n\n2\n\n\n0\n\n\n4\n\n\n2\n\n\n0\n\n\n0\n\n\n4\n\n\n2\n\n\n0\n\n\n3\n\n\n0\n\n\n0\n\n\n0\n\n\n3\n\n\n0\n\n\n0\n\n\n0\n\n\n2\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n2\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n4\n\n\n0\n\n\n0\n\n\n1\n\n\n0\n\n\n0\n\n\n0\n\n\n4\n\n\n0\n\n\n1\n\n\n0\n\n\n0\n\n\n4\n\n\n0\n\n\n3\n\n\n4\n\n\n1\n\n\n0\n\n\n4\n\n\n2\n\n\n0\n\n\n0\n\n\n0\n\n\n4\n\n\n2\n\n\n0\n\n\n4\n\n\n4\n\n\n1\n\n\n2\n\n\n3\n\n\n0\n\n\n4\n\n\n0\n\n\n2\n\n\n4\n\n\n0\n\n\n4\n\n\n3\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n4\n\n\n0\n\n\n0\n\n\n1\n\n\n0\n\n\n0\n\n\n0\n\n\n4\n\n\n0\n\n\n1\n\n\n0\n\n\n0\n\n\n4\n\n\n0\n\n\n3\n\n\n4\n\n\n1\n\n\n0\n\n\n4\n\n\n2\n\n\n0\n\n\n0\n\n\n0\n\n\n4\n\n\n2\n\n\n0\n\n\n4\n\n\n4\n\n\n1\n\n\n2\n\n\n3\n\n\n0\n\n\n4\n\n\n0\n\n\n2\n\n\n4\n\n\n0\n\n\n4\n\n\n3\n\n\n0\n\n\n0\n\n\n\n\n99\n\n\n2\n\n\nADH\n\n\n \n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n3\n\n\n2\n\n\n1\n\n\n1\n\n\n3\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n3\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n3\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n3\n\n\n2\n\n\n2\n\n\n3\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n3\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n4\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n3\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n4\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n4\n\n\n2\n\n\n1\n\n\n3\n\n\n1\n\n\n2\n\n\n1\n\n\n3\n\n\n1\n\n\n2\n\n\n3\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n3\n\n\n2\n\n\n2\n\n\n2\n\n\n3\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n3\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n3\n\n\n2\n\n\n2\n\n\n4\n\n\n2\n\n\n2\n\n\n1\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n1\n\n\n2\n\n\n2\n\n\n3\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n3\n\n\n2\n\n\n2\n\n\n1\n\n\n3\n\n\n2\n\n\n2\n\n\n2\n\n\n1\n\n\n2\n\n\n3\n\n\n2\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n1\n\n\n2\n\n\n2\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n4\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n2\n\n\n4\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n3\n\n\n2\n\n\n2\n\n\n3\n\n\n1\n\n\n2\n\n\n3\n\n\n2\n\n\n3\n\n\n2\n\n\n1\n\n\n3\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n3\n\n\n2\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n3\n\n\n2\n\n\n2\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n2\n\n\n4\n\n\n2\n\n\n2\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n4\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n3\n\n\n1\n\n\n3\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n2\n\n\n2\n\n\n3\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n3\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n3\n\n\n1\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n2\n\n\n3\n\n\n2\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n3\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n4\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n2\n\n\n3\n\n\n3\n\n\n2\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n3\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n3\n\n\n2\n\n\n3\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n3\n\n\n2\n\n\n2\n\n\n4\n\n\n2\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n1\n\n\n2\n\n\n2\n\n\n1\n\n\n2\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n2\n\n\n2\n\n\n1\n\n\n2\n\n\n3\n\n\n2\n\n\n2\n\n\n2\n\n\n3\n\n\n2\n\n\n2\n\n\n4\n\n\n2\n\n\n4\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n2\n\n\n2\n\n\n2\n\n\n1\n\n\n2\n\n\n3\n\n\n3\n\n\n2\n\n\n2\n\n\n\n\n\n\n\n\n\n\n\u001425                                                         The SAS System                      Sunday, November  3, 2024 06:51:00 AM\n\n441        ods listing close;ods html5 (id=saspy_internal) file=_tomods1 options(bitmap_mode='inline') device=svg style=HTMLBlue;\n441      ! ods graphics on / outputfmt=png;\n442        \n443        PROC SORT DATA = Proj2.data2;\n444          BY years newid;\n445          RUN;\n446        \n447        * Transpose the data out of wide format into long format;\n448        PROC TRANSPOSE DATA=Proj2.data2 OUT=Proj2.data_wide_2 PREFIX=Year;\n449            BY years;\n450            ID newid;\n451            VAR _ALL_;\n452        RUN;\n453        \n454        PROC PRINT DATA = Proj2.data_wide_2 (OBS = 100);\n455           RUN;\n456        \n457        \n458        \n459        ods html5 (id=saspy_internal) close;ods listing;\n460        \n\u001426                                                         The SAS System                      Sunday, November  3, 2024 06:51:00 AM\n\n461"
  },
  {
    "objectID": "Project_2/Project_2_R/Code/Project2.html",
    "href": "Project_2/Project_2_R/Code/Project2.html",
    "title": "Advanced Data Analysis - Project 2",
    "section": "",
    "text": "The aim of the current study is to assess how treatment response differs for HIV+ patients 2 years after initiating Highly Active Antiretroviral Therapy (HAART) based on hard drug usage (such as heroin or cocaine). This study is of particular scientific interest because it is unclear whether the use of hard drugs inhibits the immune system in humans; treatment strategies may differ based on these results. The researchers are interested in comparing subjects who never used hard drugs to current hard drug users (those that use hard drugs at year 2) or previous hard drug users (those who used drugs at year 0 or 1). Outcomes of interest are: viral load (HIV copies in a mL of blood), CD4+ T cell count (a measure of immunologic health), and aggregate physical and quality of life scores from the SF-36.\nThe clinical hypothesis is that, if hard drugs inhibit the immune system in humans, subjects who currently or previously used hard drugs will have higher viral load and lower CD4+ T cell counts than those who never used hard drugs. Additionally, the researchers are interested in knowing if potential differences between the drug use groups can be explained by differences in adherence to the treatment regimen. The researchers are agnostic on how quality of life changes after treatment, since side effects of the treatment are significant.\nThe project description provided by the PI is available below:"
  },
  {
    "objectID": "Project_2/Project_2_R/Code/Project2.html#labeling-categorical-variables",
    "href": "Project_2/Project_2_R/Code/Project2.html#labeling-categorical-variables",
    "title": "Advanced Data Analysis - Project 2",
    "section": "Labeling Categorical Variables",
    "text": "Labeling Categorical Variables\nLet’s factor and label our categorical variables so they are appropriately represented (and not doubles, which will yield incorrect results in models)\n\n# Converting all appropriate variables from doubles to categorical variables\n\ndata$HASHV &lt;- factor(data$HASHV,\n                     levels = c(1, 2),\n                     labels = c(\"No\", \"Yes\"))\n\ndata$HASHF &lt;- factor(data$HASHF,\n                     levels = c(0, 1, 2, 3, 4),\n                     labels = c(\"Never\", \"Daily\", \"Weekly\", \"Monthly\", \"Less Often\"))\n\ndata$income &lt;- factor(data$income,\n                      levels = c(1, 2, 3, 4, 5, 6, 7, 9),\n                      labels = c(\"Less than $10,000\", \"$10,000-$19,999\", \"$20,000-$29,999\", \"$30,000-$39,999\", \"$40,000-$49,999\", \"$50,000-$59,999\", \"$60,000 or more\", \"Do not wish to answer\"))\n\ndata$HBP &lt;- factor(data$HBP,\n                   levels = c(1, 2, 3, 4, 9, -1),\n                   labels = c(\"No\", \"Yes\", \"No, based on data trajectory\", \"Yes, based on data trajectory\", \"Insufficient data, may include reported treatment without diagnosis\", \"Improbable Value\"))\n\ndata$DIAB &lt;- factor(data$DIAB,\n                    levels = c(1, 2, 3, 4, 9),\n                    labels = c(\"No\", \"Yes\", \"No, based on data trajectory\", \"Yes, based on data trajectory\", \"Insufficient data\"))\n                      \ndata$LIV34 &lt;- factor(data$LIV34,\n                     levels = c(1, 2, 9),\n                     labels = c(\"No\", \"Yes\", \"Insufficient Data\"))\n\ndata$KID &lt;- factor(data$KID,\n                   levels = c(1, 2, 3, 4, 9),\n                   labels = c(\"No\", \"Yes\", \"No, based on data trajectory\", \"Yes, based on data trajectory\", \"Insufficient data\"))\n\ndata$FRP &lt;- factor(data$FRP,\n                   levels = c(1,2,9),\n                   labels = c(\"No\", \"Yes\", \"Insufficient Data\"))\n\ndata$FP &lt;- factor(data$FP,\n                  levels = c(1,2,9),\n                  labels = c(\"No\", \"Yes\", \"Insufficient Data\"))\n\ndata$DYSLIP &lt;- factor(data$DYSLIP,\n                      levels = c(1, 2, 3, 4, 9),\n                      labels = c(\"No\", \"Yes\", \"No, based on data trajectory\", \"Yes, based on data trajectory\", \"Insufficient data\"))\n\ndata$SMOKE &lt;- factor(data$SMOKE,\n                     levels = c(1, 2, 3),\n                     labels = c(\"Never Smoked\", \"Former Smoker\", \"Current Smoker\"))\n\ndata$DKGRP &lt;- factor(data$DKGRP,\n                     levels = c(0, 1, 2, 3),\n                     labels = c(\"None\", \"1-3 drinks/week\", \"4-13 drinks/week\", \"&gt;13 drinks/week\"))\n\ndata$HEROPIATE &lt;- factor(data$HEROPIATE,\n                         levels = c(1, 2, -9),\n                         labels = c(\"No\", \"Yes\", \"Not Specified\"))\n\ndata$IDU &lt;- factor(data$IDU,\n                   levels = c(1, 2),\n                   labels = c(\"No\", \"Yes\"))\n\ndata$ADH &lt;- factor(data$ADH,\n                   levels = c(1, 2, 3, 4),\n                   labels = c(\"100%\", \"95-99%\", \"75-94%\", \"&lt;75%\"))\n\ndata$RACE &lt;- factor(data$RACE,\n                    levels = c(1, 2, 3, 4, 5, 6, 7),\n                    labels = c(\"White, non-Hispanic\", \"White, Hispanic\", \"Black, non-Hispanic \", \"Black, Hispanic\",  \"American Indian or Alaskan Native\", \"Asian or Pacific Islander\", \"Other Hispanic\"))\n\ndata$EDUCBAS &lt;- factor(data$EDUCBAS,\n                       levels = c(1, 2, 3, 4, 5, 6, 7),\n                       labels = c(\"8th grade or less \", \"9,10, or 11th grade\", \"12th grade\", \"At least one year college but no degree\", \"Four years college or got degree\", \"Some graduate work\", \"Post-graduate degree\"))\n\ndata$hard_drugs &lt;- factor(data$hard_drugs,\n                          levels = c(0, 1),\n                          labels = c(\"No\", \"Yes\"))\n\n# Create labels for variables to make the names of each variable more professional in outputs\nlabel(data$newid) &lt;- \"ID\"\nlabel(data$AGG_MENT) &lt;- \"Aggregate Mental QOL Score\"\nlabel(data$AGG_PHYS) &lt;- \"Aggregate Physical QOL Score\"\nlabel(data$HASHF) &lt;- \"Hash/Marijuana Use Since Last Visit\"\nlabel(data$HASHV) &lt;- \"Frequency of Hash/Marijuana Use\"\nlabel(data$income) &lt;- \"Income\"\nlabel(data$HBP) &lt;- \"High Blood Pressure\"\nlabel(data$DIAB) &lt;- \"Diabetes\"\nlabel(data$LIV34) &lt;- \"Liver Disease Stage 3/4\"\nlabel(data$KID) &lt;-\"Kidney Disease\"\nlabel(data$FRP) &lt;- \"Frailty Related Phenotype\"\nlabel(data$FP) &lt;- \"Fraily Phenotype\"\nlabel(data$BMI) &lt;- \"BMI\"\nlabel(data$TCHOL) &lt;- \"Total Cholesterol\"\nlabel(data$TRIG) &lt;- \"Triglycerides\"\nlabel(data$LDL) &lt;- \"LDL\"\nlabel(data$DYSLIP) &lt; \"Dyslipidemia\"\nlabel(data$SMOKE) &lt; \"Smoking Status\"\nlabel(data$CESD) &lt;- \"CESD Depression Score\"\nlabel(data$SMOKE) &lt; \"Smoking Status\"\nlabel(data$DKGRP) &lt;- \"Drinking Group\"\nlabel(data$HEROPIATE) &lt;- \"Heroin or Opiate Use Since Last Visit\"\nlabel(data$IDU) &lt;- \"Intravenous Drug Usage Since Last Visit\"\nlabel(data$LEU3N) &lt;- \"CD4+ T Cell Count\"\nlabel(data$VLOAD) &lt;- \"Viral Load\"\nlabel(data$ADH) &lt;- \"Adherence to Treatment Regimen\"\nlabel(data$RACE) &lt;- \"Race\"\nlabel(data$EDUCBAS) &lt;- \"Education at Baseline\"\nlabel(data$hivpos) &lt;- \"HIV Serostatus\"\nlabel(data$age) &lt;- \"Age\"\nlabel(data$ART) &lt;- \"Antiretroviral Therapy\"\nlabel(data$years) &lt;- \"Year of Visit\"\nlabel(data$hard_drugs) &lt;- \"Hard Drug Usage\"\n\nLet’s take another look to check that those variables are no longer doubles.\n\n# Examine data\nglimpse(data)\n\nRows: 3,632\nColumns: 33\n$ newid      &lt;dbl&gt; 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4,…\n$ AGG_MENT   &lt;dbl&gt; 44.90710, 58.20754, 59.65136, 56.80657, 46.34190, 48.71791,…\n$ AGG_PHYS   &lt;dbl&gt; 52.52557, 41.29347, 48.54453, 46.73991, 27.92331, 38.03807,…\n$ HASHV      &lt;fct&gt; No, No, No, No, No, Yes, No, Yes, No, Yes, No, No, Yes, No,…\n$ HASHF      &lt;fct&gt; NA, Less Often, Never, Never, Never, Never, Never, Never, N…\n$ income     &lt;fct&gt; \"$30,000-$39,999\", \"$30,000-$39,999\", \"$30,000-$39,999\", \"$…\n$ BMI        &lt;dbl&gt; 24.71756, 26.06801, 27.16421, 25.71786, 26.66936, 25.96576,…\n$ HBP        &lt;fct&gt; \"No\", \"No\", \"No\", \"No\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", …\n$ DIAB       &lt;fct&gt; \"No\", \"No\", \"No\", \"No\", \"Insufficient data\", \"Insufficient …\n$ LIV34      &lt;fct&gt; No, No, No, No, No, No, No, No, No, No, No, No, No, No, Yes…\n$ KID        &lt;fct&gt; \"No\", \"No\", \"No\", \"No\", \"Insufficient data\", \"Insufficient …\n$ FRP        &lt;fct&gt; No, No, No, No, Yes, No, No, No, No, No, No, No, No, No, No…\n$ FP         &lt;fct&gt; No, No, No, No, Insufficient Data, Insufficient Data, No, N…\n$ TCHOL      &lt;dbl&gt; 133, 131, 180, 171, 125, NA, 134, 105, 141, 153, 170, 170, …\n$ TRIG       &lt;dbl&gt; 176, 107, 233, 139, NA, NA, NA, 104, 162, 127, NA, NA, 82, …\n$ LDL        &lt;dbl&gt; 62, 66, 86, 96, NA, NA, NA, 44, 73, 84, NA, NA, 127, NA, NA…\n$ DYSLIP     &lt;fct&gt; \"Yes\", \"No\", \"Yes\", \"No\", \"Yes\", \"Insufficient data\", \"Yes,…\n$ CESD       &lt;dbl&gt; 14, 2, 1, 18, 20, 18, 18, 21, 23, 17, 18, 22, 23, 14, 1, 1,…\n$ SMOKE      &lt;fct&gt; Current Smoker, Current Smoker, Current Smoker, Current Smo…\n$ DKGRP      &lt;fct&gt; None, &gt;13 drinks/week, None, 1-3 drinks/week, None, &gt;13 dri…\n$ HEROPIATE  &lt;fct&gt; No, No, No, No, No, No, No, No, No, No, No, No, No, No, No,…\n$ IDU        &lt;fct&gt; Yes, No, No, No, Yes, No, No, No, No, No, Yes, Yes, Yes, Ye…\n$ LEU3N      &lt;dbl&gt; 104.1659, 262.0061, 345.4010, 292.3271, 257.8278, 459.4562,…\n$ VLOAD      &lt;dbl&gt; 102013.000000, 27.000000, 60.000000, 9.000000, 8121.000000,…\n$ ADH        &lt;fct&gt; NA, 95-99%, 100%, 100%, NA, 100%, 100%, 100%, 100%, 100%, N…\n$ RACE       &lt;fct&gt; \"White, non-Hispanic\", \"White, non-Hispanic\", \"White, non-H…\n$ EDUCBAS    &lt;fct&gt; \"At least one year college but no degree\", \"At least one ye…\n$ hivpos     &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ age        &lt;dbl&gt; 52, 53, 54, 55, 54, 55, 56, 60, 61, 62, 47, 48, 49, 51, 52,…\n$ ART        &lt;dbl&gt; 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1,…\n$ everART    &lt;dbl&gt; 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1,…\n$ years      &lt;dbl&gt; 0, 1, 2, 3, 0, 1, 2, 6, 7, 8, 0, 1, 2, 4, 5, 6, 7, 8, 0, 1,…\n$ hard_drugs &lt;fct&gt; Yes, No, No, No, Yes, No, No, No, No, No, Yes, Yes, Yes, Ye…\n\n\nLooks good."
  },
  {
    "objectID": "Project_2/Project_2_R/Code/Project2.html#filtering-data-set",
    "href": "Project_2/Project_2_R/Code/Project2.html#filtering-data-set",
    "title": "Advanced Data Analysis - Project 2",
    "section": "Filtering Data Set",
    "text": "Filtering Data Set\nNow let’s take a look at the header to get a good feeling for our data.\n\n# Pretty print data header\npretty_print(head(data))\n\n\n\n\nnewid\nAGG_MENT\nAGG_PHYS\nHASHV\nHASHF\nincome\nBMI\nHBP\nDIAB\nLIV34\nKID\nFRP\nFP\nTCHOL\nTRIG\nLDL\nDYSLIP\nCESD\nSMOKE\nDKGRP\nHEROPIATE\nIDU\nLEU3N\nVLOAD\nADH\nRACE\nEDUCBAS\nhivpos\nage\nART\neverART\nyears\nhard_drugs\n\n\n\n\n1\n44.90710\n52.52557\nNo\nNA\n$30,000-$39,999\n24.71756\nNo\nNo\nNo\nNo\nNo\nNo\n133\n176\n62\nYes\n14\nCurrent Smoker\nNone\nNo\nYes\n104.1659\n102013\nNA\nWhite, non-Hispanic\nAt least one year college but no degree\n1\n52\n0\n0\n0\nYes\n\n\n1\n58.20754\n41.29346\nNo\nLess Often\n$30,000-$39,999\n26.06801\nNo\nNo\nNo\nNo\nNo\nNo\n131\n107\n66\nNo\n2\nCurrent Smoker\n&gt;13 drinks/week\nNo\nNo\n262.0061\n27\n95-99%\nWhite, non-Hispanic\nAt least one year college but no degree\n1\n53\n1\n1\n1\nNo\n\n\n1\n59.65136\n48.54453\nNo\nNever\n$30,000-$39,999\n27.16421\nNo\nNo\nNo\nNo\nNo\nNo\n180\n233\n86\nYes\n1\nCurrent Smoker\nNone\nNo\nNo\n345.4010\n60\n100%\nWhite, non-Hispanic\nAt least one year college but no degree\n1\n54\n1\n1\n2\nNo\n\n\n1\n56.80657\n46.73991\nNo\nNever\n$40,000-$49,999\n25.71786\nNo\nNo\nNo\nNo\nNo\nNo\n171\n139\n96\nNo\n18\nCurrent Smoker\n1-3 drinks/week\nNo\nNo\n292.3271\n9\n100%\nWhite, non-Hispanic\nAt least one year college but no degree\n1\n55\n1\n1\n3\nNo\n\n\n2\n46.34190\n27.92331\nNo\nNever\n$10,000-$19,999\n26.66936\nYes\nInsufficient data\nNo\nInsufficient data\nYes\nInsufficient Data\n125\nNA\nNA\nYes\n20\nCurrent Smoker\nNone\nNo\nYes\n257.8278\n8121\nNA\nBlack, non-Hispanic\n9,10, or 11th grade\n1\n54\n0\n0\n0\nYes\n\n\n2\n48.71791\n38.03807\nYes\nNever\nLess than $10,000\n25.96576\nYes\nInsufficient data\nNo\nInsufficient data\nNo\nInsufficient Data\nNA\nNA\nNA\nInsufficient data\n18\nCurrent Smoker\n&gt;13 drinks/week\nNo\nNo\n459.4562\n21\n100%\nBlack, non-Hispanic\n9,10, or 11th grade\n1\n55\n1\n1\n1\nNo\n\n\n\n\n\n\n\nHmm, we have 8 years worth of data points, but the experimenters are only interested in the first 2 years.\nOut of curiosity, let’s look at how many participants they had each year.\n\n# Visualize patient drop off over 8 years of study\nbarplot(table(data$years))\n\n\n\n\n\n\n\n# Check number of patients in each year\npretty_print(table(data$years))\n\n\n\n\nVar1\nFreq\n\n\n\n\n0\n550\n\n\n1\n550\n\n\n2\n550\n\n\n3\n414\n\n\n4\n381\n\n\n5\n338\n\n\n6\n325\n\n\n7\n272\n\n\n8\n252\n\n\n\n\n\n\n\nThis is interesting, we don’t seem to have as drastic a drop off as I expected. The researchers managed to retain all participants for the first 2 years, and 50% by the end of the 8-year study.\nLet’s filter to only include values from the first 2 years, as this is the timeframe the researchers are interested in.\n\n# Filter long form data set to be include only first 2 years\ndata_2 &lt;- data[data$years &lt;= 2,]\n\n# Check how many visits we have in the filtered data set.\ndim(data_2)\n\n[1] 1650   33\n\n\nWe went from 3632 visits in the 8 year data set to 1650 in the 2 year filtered data set.\n\n# Double check if any patients dropped out within the first 2 years\nany(is.na(data_2$years))\n\n[1] FALSE\n\n\nLuckily, no patients dropped out within the first 2 years of the study!"
  },
  {
    "objectID": "Project_2/Project_2_R/Code/Project2.html#transpose-to-wideform",
    "href": "Project_2/Project_2_R/Code/Project2.html#transpose-to-wideform",
    "title": "Advanced Data Analysis - Project 2",
    "section": "Transpose to Wideform",
    "text": "Transpose to Wideform\nWe can also see that the provided data set is in longform. Let’s convert that to wideform.\n\n# Create new wideform data set for first 2 years of study              \ndata_wide_2 &lt;- pivot_wider(data_2, id_cols = newid, names_from = years, values_from = -c(newid, years))\n\nAnd take a look at the header to check that was done correctly.\n\n# Pretty print header of wideform data\npretty_print(head(data_wide_2))\n\n\n\n\nnewid\nAGG_MENT_0\nAGG_MENT_1\nAGG_MENT_2\nAGG_PHYS_0\nAGG_PHYS_1\nAGG_PHYS_2\nHASHV_0\nHASHV_1\nHASHV_2\nHASHF_0\nHASHF_1\nHASHF_2\nincome_0\nincome_1\nincome_2\nBMI_0\nBMI_1\nBMI_2\nHBP_0\nHBP_1\nHBP_2\nDIAB_0\nDIAB_1\nDIAB_2\nLIV34_0\nLIV34_1\nLIV34_2\nKID_0\nKID_1\nKID_2\nFRP_0\nFRP_1\nFRP_2\nFP_0\nFP_1\nFP_2\nTCHOL_0\nTCHOL_1\nTCHOL_2\nTRIG_0\nTRIG_1\nTRIG_2\nLDL_0\nLDL_1\nLDL_2\nDYSLIP_0\nDYSLIP_1\nDYSLIP_2\nCESD_0\nCESD_1\nCESD_2\nSMOKE_0\nSMOKE_1\nSMOKE_2\nDKGRP_0\nDKGRP_1\nDKGRP_2\nHEROPIATE_0\nHEROPIATE_1\nHEROPIATE_2\nIDU_0\nIDU_1\nIDU_2\nLEU3N_0\nLEU3N_1\nLEU3N_2\nVLOAD_0\nVLOAD_1\nVLOAD_2\nADH_0\nADH_1\nADH_2\nRACE_0\nRACE_1\nRACE_2\nEDUCBAS_0\nEDUCBAS_1\nEDUCBAS_2\nhivpos_0\nhivpos_1\nhivpos_2\nage_0\nage_1\nage_2\nART_0\nART_1\nART_2\neverART_0\neverART_1\neverART_2\nhard_drugs_0\nhard_drugs_1\nhard_drugs_2\n\n\n\n\n1\n44.90710\n58.20754\n59.65136\n52.52557\n41.29346\n48.54453\nNo\nNo\nNo\nNA\nLess Often\nNever\n$30,000-$39,999\n$30,000-$39,999\n$30,000-$39,999\n24.71756\n26.06801\n27.16421\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\n133\n131\n180\n176\n107\n233\n62\n66\n86\nYes\nNo\nYes\n14\n2\n1\nCurrent Smoker\nCurrent Smoker\nCurrent Smoker\nNone\n&gt;13 drinks/week\nNone\nNo\nNo\nNo\nYes\nNo\nNo\n104.1659\n262.0061\n345.4010\n102013.000\n27.00000\n60.00000\nNA\n95-99%\n100%\nWhite, non-Hispanic\nWhite, non-Hispanic\nWhite, non-Hispanic\nAt least one year college but no degree\nAt least one year college but no degree\nAt least one year college but no degree\n1\n1\n1\n52\n53\n54\n0\n1\n1\n0\n1\n1\nYes\nNo\nNo\n\n\n2\n46.34190\n48.71791\n45.41483\n27.92331\n38.03807\n37.32204\nNo\nYes\nNo\nNever\nNever\nNever\n$10,000-$19,999\nLess than $10,000\n$10,000-$19,999\n26.66936\n25.96576\n26.96037\nYes\nYes\nYes\nInsufficient data\nInsufficient data\nInsufficient data\nNo\nNo\nNo\nInsufficient data\nInsufficient data\nInsufficient data\nYes\nNo\nNo\nInsufficient Data\nInsufficient Data\nNo\n125\nNA\n134\nNA\nNA\nNA\nNA\nNA\nNA\nYes\nInsufficient data\nYes, based on data trajectory\n20\n18\n18\nCurrent Smoker\nCurrent Smoker\nCurrent Smoker\nNone\n&gt;13 drinks/week\n4-13 drinks/week\nNo\nNo\nNo\nYes\nNo\nNo\n257.8278\n459.4562\n263.0693\n8121.000\n21.00000\n48.00000\nNA\n100%\n100%\nBlack, non-Hispanic\nBlack, non-Hispanic\nBlack, non-Hispanic\n9,10, or 11th grade\n9,10, or 11th grade\n9,10, or 11th grade\n1\n1\n1\n54\n55\n56\n0\n1\n1\n0\n1\n1\nYes\nNo\nNo\n\n\n3\n40.22337\n44.42011\n41.70079\n60.06970\n62.71705\n58.51450\nNo\nNo\nYes\nLess Often\nLess Often\nLess Often\n$50,000-$59,999\n$50,000-$59,999\n$50,000-$59,999\n28.59085\n28.35320\n28.18510\nInsufficient data, may include reported treatment without diagnosis\nInsufficient data, may include reported treatment without diagnosis\nNo\nInsufficient data\nInsufficient data\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\n170\n170\n180\nNA\nNA\n82\nNA\nNA\n127\nYes\nYes\nYes\n18\n22\n23\nFormer Smoker\nFormer Smoker\nFormer Smoker\nNone\nNone\nNone\nNo\nNo\nNo\nYes\nYes\nYes\n563.1223\n488.9100\n405.1816\n4001.556\n2020.00000\n27.50917\nNA\n100%\n100%\nWhite, non-Hispanic\nWhite, non-Hispanic\nWhite, non-Hispanic\nPost-graduate degree\nPost-graduate degree\nPost-graduate degree\n1\n1\n1\n47\n48\n49\n0\n1\n1\n0\n1\n1\nYes\nYes\nYes\n\n\n4\n42.90638\n31.15971\n52.68223\n50.78850\n44.62883\n51.50533\nYes\nNo\nNo\nLess Often\nWeekly\nNever\nLess than $10,000\nLess than $10,000\nNA\n20.36451\n18.21865\n20.28485\nNo\nNo\nNo\nNo\nNo, based on data trajectory\nNo\nNo\nNo\nNo\nInsufficient data\nInsufficient data\nInsufficient data\nNo\nYes\nNo\nNo\nYes\nNo\n214\n197\n251\n97\nNA\n260\n147\nNA\n152\nYes\nYes, based on data trajectory\nYes\n14\n25\n13\nCurrent Smoker\nCurrent Smoker\nCurrent Smoker\n1-3 drinks/week\n4-13 drinks/week\nNone\nYes\nYes\nNo\nNo\nNo\nNo\n110.4218\n159.6297\n179.6409\n740.000\n26.64732\n27.00000\nNA\n75-94%\n95-99%\nWhite, non-Hispanic\nWhite, non-Hispanic\nWhite, non-Hispanic\nFour years college or got degree\nFour years college or got degree\nFour years college or got degree\n1\n1\n1\n44\n45\n46\n0\n1\n1\n0\n1\n1\nYes\nYes\nNo\n\n\n5\n56.42904\n56.21993\n66.50629\n43.75671\n30.47055\n18.82350\nNo\nNA\nYes\nMonthly\nMonthly\nLess Often\n$50,000-$59,999\n$10,000-$19,999\nDo not wish to answer\n22.26986\n24.97865\n20.80193\nNo\nNo\nNo\nInsufficient data\nInsufficient data\nInsufficient data\nNo\nNo\nInsufficient Data\nInsufficient data\nInsufficient data\nInsufficient data\nNo\nNo\nYes\nInsufficient Data\nInsufficient Data\nYes\n196\n204\nNA\n162\n192\nNA\n135\nNA\nNA\nYes\nInsufficient data\nYes, based on data trajectory\n1\n0\n1\nCurrent Smoker\nCurrent Smoker\nCurrent Smoker\nNone\n1-3 drinks/week\n1-3 drinks/week\nNot Specified\nNA\nNo\nYes\nYes\nYes\n252.6634\n92.6634\n59.6219\n62727.039\n30389.00000\n419.50000\nNA\n100%\n100%\nWhite, non-Hispanic\nWhite, non-Hispanic\nWhite, non-Hispanic\nPost-graduate degree\nPost-graduate degree\nPost-graduate degree\n1\n1\n1\n53\n54\n55\n0\n1\n1\n0\n1\n1\nYes\nYes\nYes\n\n\n6\n59.74437\n53.84956\n50.26010\n56.86261\n57.91396\n55.95668\nNo\nNo\nNo\nNA\nNA\nLess Often\n$30,000-$39,999\n$30,000-$39,999\nNA\n23.22166\n23.75318\n22.41001\nNo\nNo\nNo\nInsufficient data\nInsufficient data\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\n216\n216\n151\nNA\nNA\n125\nNA\nNA\n81\nInsufficient data\nInsufficient data\nNo\n3\n3\n4\nNever Smoked\nNever Smoked\nNever Smoked\n1-3 drinks/week\n1-3 drinks/week\n1-3 drinks/week\nNo\nNo\nNo\nNo\nNo\nYes\n634.1246\n745.6517\n893.4328\n15745.000\n7870.00000\n53.50000\nNA\n95-99%\n95-99%\nWhite, non-Hispanic\nWhite, non-Hispanic\nWhite, non-Hispanic\nSome graduate work\nSome graduate work\nSome graduate work\n1\n1\n1\n36\n37\n38\n0\n1\n1\n0\n1\n1\nNo\nNo\nYes\n\n\n\n\n\n\n\nGood. now we have a long and wide form of the data set for the first two years of the study.\nFinally, let’s just clean that wide data set up a bit to drop repeat measures of variables that are constant over time (race, education at baseline, HIV serostatus, everART)\n\n# Clean up the wide data set a bit by deleting multiple observations across time for constant variables such as race\ndata_wide_2 &lt;- data_wide_2 %&gt;% select(-RACE_1, -RACE_2, -EDUCBAS_1, -EDUCBAS_2, -hivpos_1, - hivpos_2, -everART_1, -everART_2)\n\nNow that our data sets are adequately prepared, we can move on to performing our data checks to ensure fidelity of the data set."
  },
  {
    "objectID": "Project_2/Project_2_R/Code/Project2.html#removing-superfluous-variables",
    "href": "Project_2/Project_2_R/Code/Project2.html#removing-superfluous-variables",
    "title": "Advanced Data Analysis - Project 2",
    "section": "Removing Superfluous Variables",
    "text": "Removing Superfluous Variables\nWe previously determined that LDL, TRIG, DIAB, KID, and DYSLIP had excessive missing values (&gt;40%).\nNow that we have seen that they are not strongly related to the outcome variables, we can be assured that we can safely remove them with no need for imputation.\n\n# Drop variables with excessive missing values from the wideform data set\ndata_2 &lt;- data_2 %&gt;%\n  select(-LDL, -TRIG, -DIAB, -KID, -DYSLIP)\n\nFRP and FP are both precision variables for AGG_PHYS, but highly correlated to each other.\nFP has more missing values (22%) than FRP(~0%), and will thus be dropped.\n\n# Drop FP\ndata_2 &lt;- data_2 %&gt;%\n  select(-FP)\n\nEDUCBASE is highly correlated with income, TCHOL, SMOKE, and RACE.\nincome has 27% missing values and thus will be dropped from further analysis.\nTCHOL has 32% missing values and will thus be dropped.\nSMOKE and RACE will be dropped to avoid issues of multicollinearity, and EDUCBASE used as the covariate of choice.\n\n# Drop income, total cholesterol, smoke, and race\ndata_2 &lt;- data_2 %&gt;%\n  select(-income, -TCHOL, -SMOKE, -RACE)\n\nHard_drugs is highly correlated with HEROPIATE and IDU.\nHard_drugs is our main independent variable of interest and thus we drop HEROPIATE and IDU.\n\n# Drop income, total cholesterol, smoke, and race\ndata_2 &lt;- data_2 %&gt;%\n  select(-HEROPIATE, -IDU)\n\nHBP is lightly correlated with age and BMI. Let’s drop it to avoid multicollinearity.\n\n# Drop high blood pressure.\ndata_2 &lt;- data_2 %&gt;%\n  select(-HBP)"
  },
  {
    "objectID": "Project_2/Project_2_R/Code/Project2.html#correlation-matrix-redux",
    "href": "Project_2/Project_2_R/Code/Project2.html#correlation-matrix-redux",
    "title": "Advanced Data Analysis - Project 2",
    "section": "Correlation Matrix Redux",
    "text": "Correlation Matrix Redux\nLet’s take another look at that correlation matrix now that we have cleaned up our data set to remove variables with excessive missing values and issues of multicollinearity.\n\n# Let's clean our output by making a trimmed dataset excluding extraneous variables\ndata_for_matrix &lt;- select(data_wide_2, AGG_MENT_CHANGE, AGG_PHYS_CHANGE, LEU3N_CHANGE, VLOAD_log_CHANGE, BMI_2, FRP_2, CESD_2, DKGRP_2, ADH_2, ADH_HIGHVSLOW, EDUC_COLLEGE, age_2, hard_drugs_grp)\n\n# We factored our variables at the start. To make a correlation matrix we must reconvert those back to numeric\ndata_for_matrix &lt;- data.frame(lapply(data_for_matrix, function(x) if (is.factor(x)) as.numeric(x) else x))\n\n# Make a correlation matrix with all variables of the trimmed data set\ncorrelation_matrix &lt;- cor(data_for_matrix, use = \"complete.obs\")\n\n# Plot the matrix\ncorrplot(correlation_matrix, method = \"circle\")\n\n\n\n\n\n\n\n\nLooking MUCH better! Here we can see some potentially strong relationships emerge.\n\nCESD as mentioned will be included as a precision variable for AGG_MENT\nFRP as mentioned will be included as a precision variable for AGG_PHYS\nEDUCBASE looks like it will be a predictor for all outcome variables except AGG_MENT\nBMI appears to have a weak correlation with all outcome variables and will likely be included in the final models.\nage also looks weakly correlated to all outcome variables except VLOAD_log\n\nLet’s run some individual regression and assess these relationships more closely.\np-values of &lt; 0.1 will be considered for the final models."
  },
  {
    "objectID": "Project_2/Project_2_R/Code/Project2.html#log-viral-load-change-1",
    "href": "Project_2/Project_2_R/Code/Project2.html#log-viral-load-change-1",
    "title": "Advanced Data Analysis - Project 2",
    "section": "Log Viral Load Change",
    "text": "Log Viral Load Change\n\nFull ModelReduced Model - No InteractionConfoundingVisualizing Main Effects\n\n\nAs determined by interactive variable selection, the variables of interest for a model predicting VLOAD_log_CHANGE are hard_drug_grp, ADH_HIGHVSLOW, and EDUC_COLLEGE.\nThe researchers are interested in if differences in treatment response between the drug use groups can be explained by differences in adherence to the HAART regimen, so we will also include an interaction term between hard drug use group and adherence.\nThus the full model is:\n                                     ADD LATEX MATH SYNTAX HERE.\n                                     \n\nAnalysisModel SelectionInterpretationVisualizing the Interaction\n\n\nLet’s run the regression for the full model.\n\n# Relevel reference group\ndata_wide_2$hard_drugs_grp &lt;- relevel(data_wide_2$hard_drugs_grp, ref = \"Never User\")\n\n# Relevel reference group\ndata_wide_2$ADH_HIGHVSLOW &lt;- relevel(data_wide_2$ADH_HIGHVSLOW, ref = \"Low Adherence\")\n\n# Perform the regression on log viral load change with the full model\nmodel_VLOAD_full1 &lt;- lm(VLOAD_log_CHANGE ~ hard_drugs_grp + ADH_HIGHVSLOW + hard_drugs_grp*ADH_HIGHVSLOW + EDUC_COLLEGE, data = data_wide_2)\nlibrary(sjPlot)\n# Examine summary\nsummary(model_VLOAD_full1)\n\n\nCall:\nlm(formula = VLOAD_log_CHANGE ~ hard_drugs_grp + ADH_HIGHVSLOW + \n    hard_drugs_grp * ADH_HIGHVSLOW + EDUC_COLLEGE, data = data_wide_2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.2508 -1.6120 -0.2134  1.1308  9.2569 \n\nCoefficients:\n                                                        Estimate Std. Error\n(Intercept)                                              -4.9327     0.5241\nhard_drugs_grpPrevious User                               5.8083     1.1857\nhard_drugs_grpCurrent User                                3.4758     1.4138\nADH_HIGHVSLOWHigh Adherence                              -0.7876     0.4898\nEDUC_COLLEGECollege                                      -0.8489     0.2955\nhard_drugs_grpPrevious User:ADH_HIGHVSLOWHigh Adherence  -5.8479     1.2773\nhard_drugs_grpCurrent User:ADH_HIGHVSLOWHigh Adherence   -2.7430     1.4659\n                                                        t value\n(Intercept)                                              -9.412\nhard_drugs_grpPrevious User                               4.898\nhard_drugs_grpCurrent User                                2.458\nADH_HIGHVSLOWHigh Adherence                              -1.608\nEDUC_COLLEGECollege                                      -2.872\nhard_drugs_grpPrevious User:ADH_HIGHVSLOWHigh Adherence  -4.578\nhard_drugs_grpCurrent User:ADH_HIGHVSLOWHigh Adherence   -1.871\n                                                                    Pr(&gt;|t|)\n(Intercept)                                             &lt; 0.0000000000000002\nhard_drugs_grpPrevious User                                       0.00000130\nhard_drugs_grpCurrent User                                           0.01428\nADH_HIGHVSLOWHigh Adherence                                          0.10846\nEDUC_COLLEGECollege                                                  0.00424\nhard_drugs_grpPrevious User:ADH_HIGHVSLOWHigh Adherence           0.00000589\nhard_drugs_grpCurrent User:ADH_HIGHVSLOWHigh Adherence               0.06189\n                                                           \n(Intercept)                                             ***\nhard_drugs_grpPrevious User                             ***\nhard_drugs_grpCurrent User                              *  \nADH_HIGHVSLOWHigh Adherence                                \nEDUC_COLLEGECollege                                     ** \nhard_drugs_grpPrevious User:ADH_HIGHVSLOWHigh Adherence ***\nhard_drugs_grpCurrent User:ADH_HIGHVSLOWHigh Adherence  .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.661 on 513 degrees of freedom\n  (30 observations deleted due to missingness)\nMultiple R-squared:  0.1001,    Adjusted R-squared:  0.08956 \nF-statistic:  9.51 on 6 and 513 DF,  p-value: 0.0000000006388\n\ntab_model(model_VLOAD_full1)\n\n\n\n\n\n\n\n\n\n\n \nLog Viral Load Change\nScore\n\n\nPredictors\nEstimates\nCI\np\n\n\n(Intercept)\n-4.93\n-5.96 – -3.90\n&lt;0.001\n\n\nhard_drugs_grp: Previous\nUser\n5.81\n3.48 – 8.14\n&lt;0.001\n\n\nhard_drugs_grp: Current\nUser\n3.48\n0.70 – 6.25\n0.014\n\n\nADH_HIGHVSLOW: High\nAdherence\n-0.79\n-1.75 – 0.17\n0.108\n\n\nCollege Status: College\n-0.85\n-1.43 – -0.27\n0.004\n\n\nhard_drugs_grpPrevious User:ADH_HIGHVSLOWHigh Adherence\n-5.85\n-8.36 – -3.34\n&lt;0.001\n\n\nhard_drugs_grpCurrent User:ADH_HIGHVSLOWHigh Adherence\n-2.74\n-5.62 – 0.14\n0.062\n\n\nObservations\n520\n\n\nR2 / R2 adjusted\n0.100 / 0.090\n\n\n\n\n\n\n\n\n# Print useful model parameters\nmodel_results(model_VLOAD_full1)\n\n\n\n\n\nEstimate\nStd. Error\nt value\nPr(&gt;|t|)\np_adjusted\n2.5 %\n97.5 %\n\n\n\n\n(Intercept)\n-4.9326626\n0.5240675\n-9.412266\n0.0000000\n0.0000000\n-5.9622450\n-3.9030801\n\n\nhard_drugs_grpPrevious User\n5.8083303\n1.1857389\n4.898490\n0.0000013\n0.0000091\n3.4788288\n8.1378317\n\n\nhard_drugs_grpCurrent User\n3.4757907\n1.4138375\n2.458409\n0.0142848\n0.0999936\n0.6981668\n6.2534145\n\n\nADH_HIGHVSLOWHigh Adherence\n-0.7876174\n0.4898289\n-1.607944\n0.1084627\n0.7592392\n-1.7499349\n0.1747001\n\n\nEDUC_COLLEGECollege\n-0.8489132\n0.2955395\n-2.872418\n0.0042421\n0.0296945\n-1.4295299\n-0.2682965\n\n\nhard_drugs_grpPrevious User:ADH_HIGHVSLOWHigh Adherence\n-5.8479420\n1.2773135\n-4.578314\n0.0000059\n0.0000412\n-8.3573508\n-3.3385331\n\n\nhard_drugs_grpCurrent User:ADH_HIGHVSLOWHigh Adherence\n-2.7429503\n1.4659004\n-1.871171\n0.0618904\n0.4332325\n-5.6228568\n0.1369561\n\n\n\n\n\n\n# Get the VIFs\nvif_values &lt;- vif(model_VLOAD_full1)\n\nthere are higher-order terms (interactions) in this model\nconsider setting type = 'predictor'; see ?vif\n\npretty_print(vif_values)\n\n\n\n\n\nGVIF\nDf\nGVIF^(1/(2*Df))\n\n\n\n\nhard_drugs_grp\n121.062588\n2\n3.317054\n\n\nADH_HIGHVSLOW\n1.307856\n1\n1.143615\n\n\nEDUC_COLLEGE\n1.090681\n1\n1.044357\n\n\nhard_drugs_grp:ADH_HIGHVSLOW\n125.887538\n2\n3.349621\n\n\n\n\n\n\n\nThe overall model is highly significant (p &lt; 0.000001) and we’re getting some interesting significant relationships. It looks like all of our main variables and interaction term are significant. Multicollinearity for this model is not a concern.\nFor low adherence, previous and current drug users have a higher increase in log viral load change over 2 years compared to never hard drug users (p &gt; 0.05).\nFor never drug users, there is no difference in log viral load over 2 years in those with high adherence compared to those with low adherence (p = 0.11).\nLet’s keep going.\nChange the reference category to high adherence, to compare the impact of hard drug use groups for those with high adherence to the treatment regiment.\n\n# Relevel reference group\ndata_wide_2$ADH_HIGHVSLOW &lt;- relevel(data_wide_2$ADH_HIGHVSLOW, ref = \"High Adherence\")\n\n# Perform the regression on log viral load change with the full model\nmodel_VLOAD_full2 &lt;- lm(VLOAD_log_CHANGE ~ hard_drugs_grp + ADH_HIGHVSLOW + hard_drugs_grp*ADH_HIGHVSLOW + EDUC_COLLEGE, data = data_wide_2)\n\n# Examine summary\nsummary(model_VLOAD_full2)\n\n\nCall:\nlm(formula = VLOAD_log_CHANGE ~ hard_drugs_grp + ADH_HIGHVSLOW + \n    hard_drugs_grp * ADH_HIGHVSLOW + EDUC_COLLEGE, data = data_wide_2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.2508 -1.6120 -0.2134  1.1308  9.2569 \n\nCoefficients:\n                                                       Estimate Std. Error\n(Intercept)                                            -5.72028    0.27445\nhard_drugs_grpPrevious User                            -0.03961    0.45829\nhard_drugs_grpCurrent User                              0.73284    0.38167\nADH_HIGHVSLOWLow Adherence                              0.78762    0.48983\nEDUC_COLLEGECollege                                    -0.84891    0.29554\nhard_drugs_grpPrevious User:ADH_HIGHVSLOWLow Adherence  5.84794    1.27731\nhard_drugs_grpCurrent User:ADH_HIGHVSLOWLow Adherence   2.74295    1.46590\n                                                       t value\n(Intercept)                                            -20.843\nhard_drugs_grpPrevious User                             -0.086\nhard_drugs_grpCurrent User                               1.920\nADH_HIGHVSLOWLow Adherence                               1.608\nEDUC_COLLEGECollege                                     -2.872\nhard_drugs_grpPrevious User:ADH_HIGHVSLOWLow Adherence   4.578\nhard_drugs_grpCurrent User:ADH_HIGHVSLOWLow Adherence    1.871\n                                                                   Pr(&gt;|t|)    \n(Intercept)                                            &lt; 0.0000000000000002 ***\nhard_drugs_grpPrevious User                                         0.93116    \nhard_drugs_grpCurrent User                                          0.05540 .  \nADH_HIGHVSLOWLow Adherence                                          0.10846    \nEDUC_COLLEGECollege                                                 0.00424 ** \nhard_drugs_grpPrevious User:ADH_HIGHVSLOWLow Adherence           0.00000589 ***\nhard_drugs_grpCurrent User:ADH_HIGHVSLOWLow Adherence               0.06189 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.661 on 513 degrees of freedom\n  (30 observations deleted due to missingness)\nMultiple R-squared:  0.1001,    Adjusted R-squared:  0.08956 \nF-statistic:  9.51 on 6 and 513 DF,  p-value: 0.0000000006388\n\n\n\n# Print useful model parameters\nmodel_results(model_VLOAD_full2)\n\n\n\n\n\nEstimate\nStd. Error\nt value\nPr(&gt;|t|)\np_adjusted\n2.5 %\n97.5 %\n\n\n\n\n(Intercept)\n-5.7202799\n0.2744497\n-20.8427262\n0.0000000\n0.0000000\n-6.2594636\n-5.1810963\n\n\nhard_drugs_grpPrevious User\n-0.0396117\n0.4582879\n-0.0864341\n0.9311551\n1.0000000\n-0.9399637\n0.8607403\n\n\nhard_drugs_grpCurrent User\n0.7328404\n0.3816725\n1.9200766\n0.0554024\n0.3878167\n-0.0169930\n1.4826737\n\n\nADH_HIGHVSLOWLow Adherence\n0.7876174\n0.4898289\n1.6079437\n0.1084627\n0.7592392\n-0.1747001\n1.7499349\n\n\nEDUC_COLLEGECollege\n-0.8489132\n0.2955395\n-2.8724184\n0.0042421\n0.0296945\n-1.4295299\n-0.2682965\n\n\nhard_drugs_grpPrevious User:ADH_HIGHVSLOWLow Adherence\n5.8479420\n1.2773135\n4.5783138\n0.0000059\n0.0000412\n3.3385331\n8.3573508\n\n\nhard_drugs_grpCurrent User:ADH_HIGHVSLOWLow Adherence\n2.7429503\n1.4659004\n1.8711710\n0.0618904\n0.4332325\n-0.1369561\n5.6228568\n\n\n\n\n\n\n\nAt the high adherence levels, there is no difference between previous and current hard drug users and never hard drug users on log viral load change (p &gt; 0.05).\nChange the reference categories to previous users and low adherence to compare the impact of hard drug use groups for those with low adherence to the treatment regiment.\n\n# Relevel reference group\ndata_wide_2$hard_drugs_grp &lt;- relevel(data_wide_2$hard_drugs_grp, ref = \"Previous User\")\n\n# Relevel reference group\ndata_wide_2$ADH_HIGHVSLOW &lt;- relevel(data_wide_2$ADH_HIGHVSLOW, ref = \"Low Adherence\")\n\n# Perform the regression on log viral load change with the full model\nmodel_VLOAD_full3 &lt;- lm(VLOAD_log_CHANGE ~ hard_drugs_grp + ADH_HIGHVSLOW + hard_drugs_grp*ADH_HIGHVSLOW + EDUC_COLLEGE, data = data_wide_2)\n\n\n# Examine summary\nsummary(model_VLOAD_full3)\n\n\nCall:\nlm(formula = VLOAD_log_CHANGE ~ hard_drugs_grp + ADH_HIGHVSLOW + \n    hard_drugs_grp * ADH_HIGHVSLOW + EDUC_COLLEGE, data = data_wide_2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.2508 -1.6120 -0.2134  1.1308  9.2569 \n\nCoefficients:\n                                                       Estimate Std. Error\n(Intercept)                                              0.8757     1.1260\nhard_drugs_grpNever User                                -5.8083     1.1857\nhard_drugs_grpCurrent User                              -2.3325     1.7242\nADH_HIGHVSLOWHigh Adherence                             -6.6356     1.1785\nEDUC_COLLEGECollege                                     -0.8489     0.2955\nhard_drugs_grpNever User:ADH_HIGHVSLOWHigh Adherence     5.8479     1.2773\nhard_drugs_grpCurrent User:ADH_HIGHVSLOWHigh Adherence   3.1050     1.8278\n                                                       t value     Pr(&gt;|t|)    \n(Intercept)                                              0.778      0.43710    \nhard_drugs_grpNever User                                -4.898 0.0000012967 ***\nhard_drugs_grpCurrent User                              -1.353      0.17672    \nADH_HIGHVSLOWHigh Adherence                             -5.630 0.0000000297 ***\nEDUC_COLLEGECollege                                     -2.872      0.00424 ** \nhard_drugs_grpNever User:ADH_HIGHVSLOWHigh Adherence     4.578 0.0000058885 ***\nhard_drugs_grpCurrent User:ADH_HIGHVSLOWHigh Adherence   1.699      0.08997 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.661 on 513 degrees of freedom\n  (30 observations deleted due to missingness)\nMultiple R-squared:  0.1001,    Adjusted R-squared:  0.08956 \nF-statistic:  9.51 on 6 and 513 DF,  p-value: 0.0000000006388\n\n# Print useful model parameters\nmodel_results(model_VLOAD_full3)\n\n\n\n\n\nEstimate\nStd. Error\nt value\nPr(&gt;|t|)\np_adjusted\n2.5 %\n97.5 %\n\n\n\n\n(Intercept)\n0.8756677\n1.1259682\n0.777702\n0.4371032\n1.0000000\n-1.3364083\n3.0877438\n\n\nhard_drugs_grpNever User\n-5.8083303\n1.1857389\n-4.898490\n0.0000013\n0.0000091\n-8.1378317\n-3.4788288\n\n\nhard_drugs_grpCurrent User\n-2.3325396\n1.7242354\n-1.352796\n0.1767166\n1.0000000\n-5.7199709\n1.0548917\n\n\nADH_HIGHVSLOWHigh Adherence\n-6.6355594\n1.1785472\n-5.630287\n0.0000000\n0.0000002\n-8.9509321\n-4.3201866\n\n\nEDUC_COLLEGECollege\n-0.8489132\n0.2955395\n-2.872418\n0.0042421\n0.0296945\n-1.4295299\n-0.2682965\n\n\nhard_drugs_grpNever User:ADH_HIGHVSLOWHigh Adherence\n5.8479420\n1.2773135\n4.578314\n0.0000059\n0.0000412\n3.3385331\n8.3573508\n\n\nhard_drugs_grpCurrent User:ADH_HIGHVSLOWHigh Adherence\n3.1049916\n1.8277854\n1.698773\n0.0899684\n0.6297785\n-0.4858737\n6.6958570\n\n\n\n\n\n\n\nAt low adherence, there is a significant difference between previous and current drug users, with current drug users having a higher increase in log viral load over 2 years (p &lt; 0.0001).\nFor previous drug users, those with high adherence had less of an increase in log viral load over 2 years compared to those with low adherence (p &lt; 0.0001).\nChange the reference group to previous drug users and high adherence.\n\n# Relevel reference group\ndata_wide_2$hard_drugs_grp &lt;- relevel(data_wide_2$hard_drugs_grp, ref = \"Previous User\")\n\n# Relevel reference group\ndata_wide_2$ADH_HIGHVSLOW &lt;- relevel(data_wide_2$ADH_HIGHVSLOW, ref = \"High Adherence\")\n\n# Perform the regression on log viral load change with the full model\nmodel_VLOAD_full4 &lt;- lm(VLOAD_log_CHANGE ~ hard_drugs_grp + ADH_HIGHVSLOW + hard_drugs_grp*ADH_HIGHVSLOW + EDUC_COLLEGE, data = data_wide_2)\n\n# Examine summary\nsummary(model_VLOAD_full4)\n\n\nCall:\nlm(formula = VLOAD_log_CHANGE ~ hard_drugs_grp + ADH_HIGHVSLOW + \n    hard_drugs_grp * ADH_HIGHVSLOW + EDUC_COLLEGE, data = data_wide_2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.2508 -1.6120 -0.2134  1.1308  9.2569 \n\nCoefficients:\n                                                      Estimate Std. Error\n(Intercept)                                           -5.75989    0.43709\nhard_drugs_grpNever User                               0.03961    0.45829\nhard_drugs_grpCurrent User                             0.77245    0.56988\nADH_HIGHVSLOWLow Adherence                             6.63556    1.17855\nEDUC_COLLEGECollege                                   -0.84891    0.29554\nhard_drugs_grpNever User:ADH_HIGHVSLOWLow Adherence   -5.84794    1.27731\nhard_drugs_grpCurrent User:ADH_HIGHVSLOWLow Adherence -3.10499    1.82779\n                                                      t value\n(Intercept)                                           -13.178\nhard_drugs_grpNever User                                0.086\nhard_drugs_grpCurrent User                              1.355\nADH_HIGHVSLOWLow Adherence                              5.630\nEDUC_COLLEGECollege                                    -2.872\nhard_drugs_grpNever User:ADH_HIGHVSLOWLow Adherence    -4.578\nhard_drugs_grpCurrent User:ADH_HIGHVSLOWLow Adherence  -1.699\n                                                                  Pr(&gt;|t|)    \n(Intercept)                                           &lt; 0.0000000000000002 ***\nhard_drugs_grpNever User                                           0.93116    \nhard_drugs_grpCurrent User                                         0.17587    \nADH_HIGHVSLOWLow Adherence                                    0.0000000297 ***\nEDUC_COLLEGECollege                                                0.00424 ** \nhard_drugs_grpNever User:ADH_HIGHVSLOWLow Adherence           0.0000058885 ***\nhard_drugs_grpCurrent User:ADH_HIGHVSLOWLow Adherence              0.08997 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.661 on 513 degrees of freedom\n  (30 observations deleted due to missingness)\nMultiple R-squared:  0.1001,    Adjusted R-squared:  0.08956 \nF-statistic:  9.51 on 6 and 513 DF,  p-value: 0.0000000006388\n\n\n\n# Print useful model parameters\nmodel_results(model_VLOAD_full4)\n\n\n\n\n\nEstimate\nStd. Error\nt value\nPr(&gt;|t|)\np_adjusted\n2.5 %\n97.5 %\n\n\n\n\n(Intercept)\n-5.7598916\n0.4370859\n-13.1779394\n0.0000000\n0.0000000\n-6.6185902\n-4.9011931\n\n\nhard_drugs_grpNever User\n0.0396117\n0.4582879\n0.0864341\n0.9311551\n1.0000000\n-0.8607403\n0.9399637\n\n\nhard_drugs_grpCurrent User\n0.7724521\n0.5698803\n1.3554637\n0.1758663\n1.0000000\n-0.3471342\n1.8920383\n\n\nADH_HIGHVSLOWLow Adherence\n6.6355594\n1.1785472\n5.6302871\n0.0000000\n0.0000002\n4.3201866\n8.9509321\n\n\nEDUC_COLLEGECollege\n-0.8489132\n0.2955395\n-2.8724184\n0.0042421\n0.0296945\n-1.4295299\n-0.2682965\n\n\nhard_drugs_grpNever User:ADH_HIGHVSLOWLow Adherence\n-5.8479420\n1.2773135\n-4.5783138\n0.0000059\n0.0000412\n-8.3573508\n-3.3385331\n\n\nhard_drugs_grpCurrent User:ADH_HIGHVSLOWLow Adherence\n-3.1049916\n1.8277854\n-1.6987726\n0.0899684\n0.6297785\n-6.6958570\n0.4858737\n\n\n\n\n\n\n\nAt the high adherence level, there is no difference in log viral load change between current and previous hard drug users (p &gt; 0.05).\nChange the reference categories to current users and low adherence to compare the impact of hard drug use groups for those with low adherence to the treatment regiment.\n\n# Relevel reference group\ndata_wide_2$hard_drugs_grp &lt;- relevel(data_wide_2$hard_drugs_grp, ref = \"Current User\")\n\n# Relevel reference group\ndata_wide_2$ADH_HIGHVSLOW &lt;- relevel(data_wide_2$ADH_HIGHVSLOW, ref = \"Low Adherence\")\n\n# Perform the regression on log viral load change with the full model\nmodel_VLOAD_full5 &lt;- lm(VLOAD_log_CHANGE ~ hard_drugs_grp + ADH_HIGHVSLOW + hard_drugs_grp*ADH_HIGHVSLOW + EDUC_COLLEGE, data = data_wide_2)\n\n# Examine summary\nsummary(model_VLOAD_full5)\n\n\nCall:\nlm(formula = VLOAD_log_CHANGE ~ hard_drugs_grp + ADH_HIGHVSLOW + \n    hard_drugs_grp * ADH_HIGHVSLOW + EDUC_COLLEGE, data = data_wide_2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.2508 -1.6120 -0.2134  1.1308  9.2569 \n\nCoefficients:\n                                                        Estimate Std. Error\n(Intercept)                                              -1.4569     1.3389\nhard_drugs_grpPrevious User                               2.3325     1.7242\nhard_drugs_grpNever User                                 -3.4758     1.4138\nADH_HIGHVSLOWHigh Adherence                              -3.5306     1.3823\nEDUC_COLLEGECollege                                      -0.8489     0.2955\nhard_drugs_grpPrevious User:ADH_HIGHVSLOWHigh Adherence  -3.1050     1.8278\nhard_drugs_grpNever User:ADH_HIGHVSLOWHigh Adherence      2.7430     1.4659\n                                                        t value Pr(&gt;|t|)   \n(Intercept)                                              -1.088  0.27704   \nhard_drugs_grpPrevious User                               1.353  0.17672   \nhard_drugs_grpNever User                                 -2.458  0.01428 * \nADH_HIGHVSLOWHigh Adherence                              -2.554  0.01093 * \nEDUC_COLLEGECollege                                      -2.872  0.00424 **\nhard_drugs_grpPrevious User:ADH_HIGHVSLOWHigh Adherence  -1.699  0.08997 . \nhard_drugs_grpNever User:ADH_HIGHVSLOWHigh Adherence      1.871  0.06189 . \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.661 on 513 degrees of freedom\n  (30 observations deleted due to missingness)\nMultiple R-squared:  0.1001,    Adjusted R-squared:  0.08956 \nF-statistic:  9.51 on 6 and 513 DF,  p-value: 0.0000000006388\n\n\nFor current hard drug users, those with high adherence have a lower increase in log viral load over 2 years compared to those with low adherence (p = 0.011).\nTop of Tabset\n\n\nWe can see that all variables, including EDUC_COLLEGE, are significant, and thus there is no need to compare the full model to reduced models; this is our final model for VLOAD_log_CHANGE\nWe can double check this with a partial F-test comparing the full model with the reduced model without the interaction term\n\n# Perform full model with interaction term\nmodel_log_VLOAD_full &lt;- lm(VLOAD_log_CHANGE ~ hard_drugs_grp + ADH_HIGHVSLOW + EDUC_COLLEGE + hard_drugs_grp*ADH_HIGHVSLOW, data = data_wide_2)\n\n# Perform reduced model without interaction term\nmodel_log_VLOAD_red &lt;- lm(VLOAD_log_CHANGE ~ hard_drugs_grp + ADH_HIGHVSLOW + EDUC_COLLEGE, data = data_wide_2)\n\n# Perform F-Test\nanova(model_log_VLOAD_red, model_log_VLOAD_full)\n\nAnalysis of Variance Table\n\nModel 1: VLOAD_log_CHANGE ~ hard_drugs_grp + ADH_HIGHVSLOW + EDUC_COLLEGE\nModel 2: VLOAD_log_CHANGE ~ hard_drugs_grp + ADH_HIGHVSLOW + EDUC_COLLEGE + \n    hard_drugs_grp * ADH_HIGHVSLOW\n  Res.Df    RSS Df Sum of Sq      F     Pr(&gt;F)    \n1    515 3794.7                                   \n2    513 3633.5  2    161.24 11.382 0.00001456 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe p-value is significant (p &lt; 0.0001), indicating that the interaction term adds explanatory power to the model and should be included. Thus we move on with interpreting the interaction.\nTop of Tabset\n\n\n\nOverall Model\nThere were significant differences in change in log viral load based on hard drug usage and adherence to the treatment regiment, while controlling for education at baseline (F~(6, 513)~= 9.51), p &lt; 0.0001).\n\n\nHard Drug Use\nThe relationship between hard drug use and change in log viral load over 2 years depended on adherence to the treatment regiment.\n\nHard Drug Use at Low Adherence\nFor those with low adherence to the treatment regiment, current hard drug users had a change in log viral load that was 3.48 log copies/mL (or change in copies/mL that was 32.32 times) higher than never hard drug users (t = 2.458, p-adjusted = 0.04284, 95% CI: 0.70 to 6.25 log copies/mL, or 2.01 to 519.78 times copies/mL). Additionally, for those with low adherence to the treatment regimen, previous hard drug users had a change in log viral load that was 5.81 log copies/mL (or change in copies/mL that was 333.05 times) greater than never hard drug users (t = 4.90, p-adjusted &lt; 0.0001, 95% CI: 3.48 to 8.14 log copies/mL, or 32.42 to 3421.49 times copies/mL).\nFor those with low adherence to the treatment regiment, the difference between previous and current drug users was not statistically significant (t = -1.35, p-adjusted = 0.177, 95% CI: -5.72 to 1.05 log copies/mL, or 0.0033 to 2.87 times copies/mL).\n\n\nHard Drug use at High Adherence\nFor those with high adherence to the protocol, there was no difference in log viral load between current and never hard drug users (t = 1.920, p-adjusted = 0.388, 95% CI: -0.017 to 1.48 log copies/mL), previous and never hard drug users (t = -0.086, p-adjusted = 1.00, 95% CI: -0.86 to 0.94 log copies/mL), or previous and current hard drug users (t = 1.355, p-adjusted = 1.00, 95% CI: -0.35 to 1.89 log copies/mL).\n\n\n\nAdherence\nThe relationship between adherence to the treatment regiment and change in log viral load over 2 years differed by hard drug use.\n\nAdherence for Current Hard Drug Users\nFor current hard drug users, those with high adherence had a change in log viral load that was 3.53 log copies/mL (or 0.029 times) less than that of those with low adherence (t = -2.554, p-adjusted = 0.0999936, 95% CI: -6.25 to -0.70 log copies/mL, or 0.0019 to 0.50 times). However, this relationship was not statistically significant following Bonferroni correction, likely due to the small number of patients who were hard drug users with low protocol adherence (n = 4). This relationship however does appear to be real based on trends in the data, and we recommend increasing the sample size to discover the true differences.\n\n\nAdherence for Previous Hard Drug Users\nFor previous hard drug users, those with high adherence had a change in log viral load that was 6.64 log copies/mL less (or 0.0013 times) than those with low adherence (t = -5.63, p-adjusted &lt; 0.0001, 95% CI: -8.95 to -4.32 log copies/mL, or 0.00013 to 0.013 times).\n\n\nAdherence for Never Hard Drug Users\nFor never hard drug users, the difference in log viral load between those with high adherence and low adherence was not statistically significant (t = -1.61, p-adusted = 0.759, 95% CI: -1.75 to 0.17 log copies/mL, or 0.17 to 1.19 times).\n\n\n\nAdditional Interaction Comparisons\nThere are additional comparisons we can make in the interaction between hard drug usage and adherence, but they are of little clinical relevance (e.g. current hard drug users with low adherence had a higher viral load compared to never hard drug users with high adherence). Basically, any comparison made between a current or previous hard drug user with low adherence was significant.\n\n\nCollege Education at Baseline\nEducation at baseline is a significant predictor for change in log viral load over 2 years, while controlling for hard drug use and adherence to treatment regiment (t = -2.872, p = 0.00424). On average, those with a college education had an 0.85 log copies/mL (or 2.34 times) greater decrease in log viral load than those without a college education (95% CI:-1.43 to -0.27 log copies/mL).\nTop of Tabset\n\n\n\nThe interaction between hard drug use and adherence group on log viral load can be visualized in the graph below.\n\n# Relevel reference group\ndata_wide_2$hard_drugs_grp &lt;- relevel(data_wide_2$hard_drugs_grp, ref = \"Never User\")\n\n# Relevel reference group\ndata_wide_2$ADH_HIGHVSLOW &lt;- relevel(data_wide_2$ADH_HIGHVSLOW, ref = \"Low Adherence\")\n\n# Perform a regression of the full model predicting log viral load change\nmodel_VLOAD_full &lt;- lm(VLOAD_log_CHANGE ~ hard_drugs_grp + ADH_HIGHVSLOW + hard_drugs_grp * ADH_HIGHVSLOW + EDUC_COLLEGE, data = data_wide_2)\n\nlibrary(interactions)\n\n# Use interactions package to plot interaction\ncat_plot(\n  model_VLOAD_full, \n  pred = hard_drugs_grp, \n  modx = ADH_HIGHVSLOW, \n  geom = \"line\", \n  colors = \"Pastel2\",\n  x.label = \"Hard Drugs Group\",\n  y.label = \"Predicted Log Viral Load Change\"\n)\n\n\n\n\n\n\n\n\nThis very clearly illustrates the relationships we saw in our model.\nWe can see the relationship between adherence and change in log viral load based on hard drug use: for previous and current hard drug users, those with high adherence had a reduced viral load compared to those with low adherence.\nSimilarly, we can see the relationship between hard drug use and change in log viral load based on adherence: For those with low adherence, previous and current drug users had a higher viral load compared to those with high adherence.\nInterestingly, viral load did not seem to differ for never users based on adherence, suggesting that the ART treatment is most important for those with current or previous hard drug use history.\nImportantly however, this plot is not replicated exactly when I recreate it in ggplot2.\n\n# Filter out NA values\ndata_no_na &lt;- data_wide_2 %&gt;% filter(!is.na(ADH_HIGHVSLOW))\n\n# Summarize the data (using mean as an example)\ndata_summary &lt;- data_no_na %&gt;%\n  group_by(hard_drugs_grp, ADH_HIGHVSLOW) %&gt;%\n  summarize(mean_VLOAD_log_CHANGE = mean(VLOAD_log_CHANGE, na.rm = TRUE))\n\n`summarise()` has grouped output by 'hard_drugs_grp'. You can override using\nthe `.groups` argument.\n\n# Create the plot with lines\nggplot(data_no_na, aes(x = hard_drugs_grp, y = VLOAD_log_CHANGE, fill = ADH_HIGHVSLOW)) +\n  geom_boxplot(alpha = 0.5) +\n  scale_fill_brewer(palette = \"Pastel2\") +\n  labs(title = \"Interaction between Hard Drugs and Adherence on Log Viral Load Change\",\n       x = \"Hard Drugs Group\",\n       y = \"Log Viral Load Change\") +\n  guides(fill = guide_legend(title = \"Adherence Group\")) +\n   theme_minimal() \n\nWarning: Removed 18 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\n\n\n\n\nWe can see that the boxplot for low adherence previous users (n = 4), and low adherence hard drug users (n = 6) are either really tiny or really large, respectively. This makes sense, because the sample size for both of those categories is so small.\n\n# Create table of hard drugs group by adherence group\npretty_print(table(data_wide_2$hard_drugs_grp, data_wide_2$ADH_HIGHVSLOW))\n\n\n\n\n\nLow Adherence\nHigh Adherence\n\n\n\n\nNever User\n33\n399\n\n\nCurrent User\n4\n56\n\n\nPrevious User\n6\n40\n\n\n\n\n\n\n\nAnd let’s examine how close those 4 values all are to each other.\n\ndata_wide_2 %&gt;%\n  filter(hard_drugs_grp == \"Current User\" & ADH_HIGHVSLOW == \"Low Adherence\") %&gt;%\n  select(newid, VLOAD_log_CHANGE, hard_drugs_grp, ADH_HIGHVSLOW)\n\n# A tibble: 4 × 4\n  newid VLOAD_log_CHANGE hard_drugs_grp ADH_HIGHVSLOW\n  &lt;dbl&gt;            &lt;dbl&gt; &lt;fct&gt;          &lt;fct&gt;        \n1    11             1.21 Current User   Low Adherence\n2    15            -4.96 Current User   Low Adherence\n3   319             1.21 Current User   Low Adherence\n4   356            -4.99 Current User   Low Adherence\n\n\n\ndata_wide_2 %&gt;%\n  filter(hard_drugs_grp == \"Previous User\" & ADH_HIGHVSLOW == \"Low Adherence\") %&gt;%\n  select(newid, VLOAD_log_CHANGE, hard_drugs_grp, ADH_HIGHVSLOW)\n\n# A tibble: 6 × 4\n  newid VLOAD_log_CHANGE hard_drugs_grp ADH_HIGHVSLOW\n  &lt;dbl&gt;            &lt;dbl&gt; &lt;fct&gt;          &lt;fct&gt;        \n1    20           0.0114 Previous User  Low Adherence\n2    36           0.0330 Previous User  Low Adherence\n3    52           0.0335 Previous User  Low Adherence\n4   425           0.0123 Previous User  Low Adherence\n5   504           0.0349 Previous User  Low Adherence\n6   547           0.0354 Previous User  Low Adherence\n\n\nThe values for current users with low adherence have low variance, explaining the tiny boxplot, and the values for previous users with low adherence have high variance, explaining the larger boxplot.\nSo I guess that’s just a testament to the fact that you can’t rely on packages!\n\n\n\n\n\n\nNote\n\n\n\nThis is a very important limitation that will need to be addressed in the discussion!!\nWhile we see this interaction between hard drug use and adherence on the outcome variables in the data, more data must be collected to get a minimum of 30 patients per group and the analyses re-run to confirm these relationships hold!!\n\n\nTop of Tabset\n\n\n\n\n\n\nAnalysisInterpretationComparisons\n\n\nUnderstanding the limitations of an interaction model with only n=4 or n=6 in some categories, we chose to also run the model without the interaction term to acquire the main effects of hard drug use group and adherence on log viral load change, while controlling for baseline education.\n\n# Relevel reference group\ndata_wide_2$hard_drugs_grp &lt;- relevel(data_wide_2$hard_drugs_grp, ref = \"Never User\")\n\n# Perform a regression on log viral load change without interaction term\nmodel_VLOAD_red1 &lt;- lm(VLOAD_log_CHANGE ~ hard_drugs_grp + ADH_HIGHVSLOW + EDUC_COLLEGE, data = data_wide_2)\n\n# Examine summary\nsummary(model_VLOAD_red1)\n\n\nCall:\nlm(formula = VLOAD_log_CHANGE ~ hard_drugs_grp + ADH_HIGHVSLOW + \n    EDUC_COLLEGE, data = data_wide_2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.7941 -1.7418 -0.2149  1.2043  9.3073 \n\nCoefficients:\n                            Estimate Std. Error t value             Pr(&gt;|t|)\n(Intercept)                  -4.0871     0.4910  -8.324 0.000000000000000771\nhard_drugs_grpCurrent User    0.9170     0.3755   2.442               0.0149\nhard_drugs_grpPrevious User   0.7190     0.4336   1.658               0.0978\nADH_HIGHVSLOWHigh Adherence  -1.8429     0.4377  -4.211 0.000030041916341915\nEDUC_COLLEGECollege          -0.6897     0.2970  -2.322               0.0206\n                               \n(Intercept)                 ***\nhard_drugs_grpCurrent User  *  \nhard_drugs_grpPrevious User .  \nADH_HIGHVSLOWHigh Adherence ***\nEDUC_COLLEGECollege         *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.714 on 515 degrees of freedom\n  (30 observations deleted due to missingness)\nMultiple R-squared:  0.06016,   Adjusted R-squared:  0.05286 \nF-statistic: 8.241 on 4 and 515 DF,  p-value: 0.000001901\n\n\n\n# Print useful model parameters\nmodel_results(model_VLOAD_red1)\n\n\n\n\n\nEstimate\nStd. Error\nt value\nPr(&gt;|t|)\np_adjusted\n2.5 %\n97.5 %\n\n\n\n\n(Intercept)\n-4.0870510\n0.4910084\n-8.323790\n0.0000000\n0.0000000\n-5.0516769\n-3.1224251\n\n\nhard_drugs_grpCurrent User\n0.9169947\n0.3754605\n2.442320\n0.0149284\n0.0746420\n0.1793722\n1.6546172\n\n\nhard_drugs_grpPrevious User\n0.7190166\n0.4335575\n1.658411\n0.0978434\n0.4892169\n-0.1327422\n1.5707754\n\n\nADH_HIGHVSLOWHigh Adherence\n-1.8428916\n0.4376635\n-4.210750\n0.0000300\n0.0001502\n-2.7027170\n-0.9830663\n\n\nEDUC_COLLEGECollege\n-0.6896592\n0.2969844\n-2.322207\n0.0206108\n0.1030539\n-1.2731090\n-0.1062094\n\n\n\n\n\n\n\n\n# Relevel reference group\ndata_wide_2$hard_drugs_grp &lt;- relevel(data_wide_2$hard_drugs_grp, ref = \"Previous User\")\n\n# Perform a regression on log viral load change without interaction term\nmodel_VLOAD_red2 &lt;- lm(VLOAD_log_CHANGE ~ hard_drugs_grp + ADH_HIGHVSLOW + EDUC_COLLEGE, data = data_wide_2)\n\n# Examine summary\nsummary(model_VLOAD_red2)\n\n\nCall:\nlm(formula = VLOAD_log_CHANGE ~ hard_drugs_grp + ADH_HIGHVSLOW + \n    EDUC_COLLEGE, data = data_wide_2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.7941 -1.7418 -0.2149  1.2043  9.3073 \n\nCoefficients:\n                            Estimate Std. Error t value      Pr(&gt;|t|)    \n(Intercept)                  -3.3680     0.5718  -5.890 0.00000000697 ***\nhard_drugs_grpNever User     -0.7190     0.4336  -1.658        0.0978 .  \nhard_drugs_grpCurrent User    0.1980     0.5451   0.363        0.7166    \nADH_HIGHVSLOWHigh Adherence  -1.8429     0.4377  -4.211 0.00003004192 ***\nEDUC_COLLEGECollege          -0.6897     0.2970  -2.322        0.0206 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.714 on 515 degrees of freedom\n  (30 observations deleted due to missingness)\nMultiple R-squared:  0.06016,   Adjusted R-squared:  0.05286 \nF-statistic: 8.241 on 4 and 515 DF,  p-value: 0.000001901\n\n\n\n# Print useful model parameters\nmodel_results(model_VLOAD_red2)\n\n\n\n\n\nEstimate\nStd. Error\nt value\nPr(&gt;|t|)\np_adjusted\n2.5 %\n97.5 %\n\n\n\n\n(Intercept)\n-3.3680344\n0.5718018\n-5.8902135\n0.0000000\n0.0000000\n-4.4913853\n-2.2446835\n\n\nhard_drugs_grpNever User\n-0.7190166\n0.4335575\n-1.6584112\n0.0978434\n0.4892169\n-1.5707754\n0.1327422\n\n\nhard_drugs_grpCurrent User\n0.1979781\n0.5451391\n0.3631698\n0.7166270\n1.0000000\n-0.8729918\n1.2689479\n\n\nADH_HIGHVSLOWHigh Adherence\n-1.8428916\n0.4376635\n-4.2107503\n0.0000300\n0.0001502\n-2.7027170\n-0.9830663\n\n\nEDUC_COLLEGECollege\n-0.6896592\n0.2969844\n-2.3222073\n0.0206108\n0.1030539\n-1.2731090\n-0.1062094\n\n\n\n\n\n\n\nThe overall model is significant (F(~4, 515)~= 8.241, p &lt; 0.0001).\nTop of Tabset\n\n\n\nHard Drug Use\nHard drug use is a significant predictor of log viral load change, while controlling for adherence to the treatment regiment and college education. Specifically, current hard drug users had a change in log viral load that was 0.92 log copies/mL (or 2.50 times) greater than never hard drug users (t = 2.442, p-adjusted = 0.0447, 95% CI: 0.18 to 1.65 log copies/mL, or 1.20 to 5.23 times). There was no difference between previous and current hard drug users in change in log viral load over 2 years (t = 0.363, p-adjusted = 1.00, 95% CI: -0.87 to 1.27 log copies/mL, or 0.42 to 4.81 times).\n\n\nAdherence\nAdherence to the treatment regiment is a significant predictor of change in log viral load, while controlling for hard drug usage and college education (t = -4.21, p &lt; 0.0001). Specifically, those with high adherence to the protocol had an average change in log viral load that was 1.84 log copies/mL (or 0.16 times) less than those with low adherence to the protocol.\n\n\nCollege Education at Baseline\nFinally, college education is a significant predictor of log viral load change, while controlling for hard drug usage and adherence to the treatment regiment (t = -2.32, p = 0.0206). On average, those with a college education had a change in log viral load that was 0.69 log copies/mL (or 1.99 times) less than those without a college education (95% CI: -2.70 to -0.98 log copies/mL, or 0.067 to 0.37 times).\nTop of Tabset\n\n\n\nThe reduced model without the interaction has an adjusted R-squared of 0.0529, compared to the full model with the interaction term (R^2-adjusted = 0.090).\nTherefore, dropping the interaction term causes us to lose explanation of 3.71% of the variance in log viral load change, but is also more statistically sound since we no longer have small sample size issues.\nTop of Tabset\n\n\n\n\n\nUnder the classical definition of a confounder, a variable Z is a confounder if:\n\n\nIt is associated with outcome Y\n\n\nIt is associated with PEV X\n\n\nIt is not on the causal pathway (not a mediator)\n\n\nWe know that ADH_HIGHVSLOW is a predictor of VLOAD_log_CHANGE.\nTo assess if ADH_HIGHVSLOW is associated with hard drug use group, we can run a Fisher’s test\n\n# Create a contingency table\ncontingency_table &lt;- table(data_wide_2$hard_drugs_grp, data_wide_2$ADH_HIGHVSLOW)\n\n# We have to run Fisher's Exact test since we have fewer than 5 observations in some categories\nfisher_test &lt;- fisher.test(contingency_table)\n\nfisher_test\n\n\n    Fisher's Exact Test for Count Data\n\ndata:  contingency_table\np-value = 0.4168\nalternative hypothesis: two.sided\n\n\n\n# Create a contingency table\ncontingency_table &lt;- table(data_wide_2$hard_drugs_grp, data_wide_2$EDUC_COLLEGE)\n\n# We have to run Fisher's Exact test since we have fewer than 5 observations in some categories\nfisher_test &lt;- fisher.test(contingency_table)\n\nThey are not associated.\nHowever, it is not necessary for this associaton to be statistically significant for there to be important confounding present.\nWhen we ran drug_use_grp by itself as a predictor on log viral load, the overall model was significant, but after correcting for multiple pairwise comparisons, none of the between-group comparisons was significant (p &gt; 0.05).\nHowever, after including ADH_HIGHVSLOW in the model, the current vs never drug use comparison is significant, even after performing a Bonferroni correction (p-adjusted = 0.0447). In the model without ADH, this same comparison was p-adjusted = 0.1377.\nThus, ADH_HIGHVSLOW is associated with the outcome variable, and changes the relationship between the PEV and outcome variable in a meaningful way when included in the model (changes it from not significant to significant), meeting the definition of a confounder under the classical definition. However, the beta coefficient of the PEV does not change by &gt; 20%, not satisfying the operational definition of a confounder.\nAdherence to the treatment regiment is therefore a maverick variable!\nTop of Tabset\n\n\nHere we will create some simple plots to help with visualizing the main effects of hard drug use and adherence on log viral load change.\n\n# Create boxplots of log viral load change by hard drug use group\nggplot(data_wide_2, aes(x = hard_drugs_grp, y = VLOAD_log_CHANGE, fill = hard_drugs_grp)) +\n  geom_boxplot(alpha = 0.5) +\n  theme_minimal() +\n  scale_fill_brewer(palette=\"Pastel2\") +\n  labs(title = \"Boxplot of Log Viral Load Change by Hard Drug Use Group\",\n       x = \"Hard Drug Use Group\",\n       y = \"Log Viral Load Change\") +\n  guides(fill = guide_legend(title = \"Hard Drug Use Group\"))\n\nWarning: Removed 18 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\n\n\n\n\nWe can see that current and previous drug users had less of a decrease in log viral load over 2 years compared to never hard drug users.\n\n# Create boxplots of log viral load change by adherence group\nggplot(data_no_na, aes(x = ADH_HIGHVSLOW, y = VLOAD_log_CHANGE, fill = ADH_HIGHVSLOW)) +\n  geom_boxplot(alpha = 0.5) +\n  theme_minimal() +\n  scale_fill_brewer(palette=\"Pastel2\") +\n  labs(title = \"Boxplot of Log Viral Load Change by Adherence at Year 2\",\n       x = \"Adherence at year 2\",\n       y = \"Log Viral Load Change\") +\n  guides(fill = guide_legend(title = \"Adherence Group\"))\n\nWarning: Removed 18 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\n\n\n\n\nAnd here we can see those with high adherence had a higher decrease in log viral load over 2 years compared to those with low adherence.\n\n\n\n:::"
  },
  {
    "objectID": "Project_2/Project_2_R/Code/Project2.html#cd4-t-cell-count-change-1",
    "href": "Project_2/Project_2_R/Code/Project2.html#cd4-t-cell-count-change-1",
    "title": "Advanced Data Analysis - Project 2",
    "section": "CD4+ T Cell Count Change",
    "text": "CD4+ T Cell Count Change\nThe second outcome variable of interest was change in CD4+ T Cell Count over the first 2 years of the study.\n\nFull ModelReduced Model - No Interaction\n\n\n\nModel SelectionAnalysisInterpretationVisualizing the Interaction\n\n\nThe candidate variables for inclusion in our model predicting LEU3N_CHANGE were hard_drugs_grp, ADH_HIGHLOW, BMI_2, FRP_2, and EDUC_COLLEGE.\nThe researchers are interested in if differences in treatment response between the drug use groups can be explained by differences in adherence to the HAART regimen, so we will also include an interaction term between hard drug use group and adherence.\nWe begin by examining the full model with all these variables included.\n\n# Relevel to change to the reference group to never user\ndata_wide_2$hard_drugs_grp&lt;- relevel(data_wide_2$hard_drugs_grp, ref = \"Never User\")\n\n# Relevel to change to the reference group to never user\ndata_wide_2$ADH_HIGHVSLOW &lt;- relevel(data_wide_2$ADH_HIGHVSLOW, ref = \"Low Adherence\")\n\n# Perform regression on CD4+ T Cell count change by hard drugs group, include adherence as confounder, and BMI, frailty related phenotype, and college education as precision variables.\nmodel_LEU3N_full1 &lt;- lm(LEU3N_CHANGE ~ hard_drugs_grp + ADH_HIGHVSLOW + BMI_2 + FRP_2 + EDUC_COLLEGE + CESD_2 + hard_drugs_grp*ADH_HIGHVSLOW, data = data_wide_2)\n\n# Exmamine the summary\nsummary(model_LEU3N_full1)\n\n\nCall:\nlm(formula = LEU3N_CHANGE ~ hard_drugs_grp + ADH_HIGHVSLOW + \n    BMI_2 + FRP_2 + EDUC_COLLEGE + CESD_2 + hard_drugs_grp * \n    ADH_HIGHVSLOW, data = data_wide_2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-665.80 -119.44   -3.02  110.80 1080.47 \n\nCoefficients: (1 not defined because of singularities)\n                                                         Estimate Std. Error\n(Intercept)                                              -46.7866    63.8333\nhard_drugs_grpPrevious User                              -92.1062    31.5556\nhard_drugs_grpCurrent User                              -161.2223   132.3190\nADH_HIGHVSLOWHigh Adherence                               78.9814    35.0133\nBMI_2                                                      6.0075     2.1384\nFRP_2Yes                                                -101.6289    36.3579\nEDUC_COLLEGECollege                                       28.7527    21.5453\nCESD_2                                                    -1.0786     0.8141\nhard_drugs_grpPrevious User:ADH_HIGHVSLOWHigh Adherence        NA         NA\nhard_drugs_grpCurrent User:ADH_HIGHVSLOWHigh Adherence    49.4909   134.8433\n                                                        t value Pr(&gt;|t|)   \n(Intercept)                                              -0.733  0.46397   \nhard_drugs_grpPrevious User                              -2.919  0.00369 **\nhard_drugs_grpCurrent User                               -1.218  0.22370   \nADH_HIGHVSLOWHigh Adherence                               2.256  0.02457 * \nBMI_2                                                     2.809  0.00518 **\nFRP_2Yes                                                 -2.795  0.00541 **\nEDUC_COLLEGECollege                                       1.335  0.18271   \nCESD_2                                                   -1.325  0.18588   \nhard_drugs_grpPrevious User:ADH_HIGHVSLOWHigh Adherence      NA       NA   \nhard_drugs_grpCurrent User:ADH_HIGHVSLOWHigh Adherence    0.367  0.71377   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 180.3 on 448 degrees of freedom\n  (93 observations deleted due to missingness)\nMultiple R-squared:  0.1193,    Adjusted R-squared:  0.1036 \nF-statistic: 7.587 on 8 and 448 DF,  p-value: 0.000000001577\n\n\nWe get an NA for the interaction term. This is likely because we dropped the n=6 patients that were previous hard drug users with low adherence, making the interaction impossible to make. Indeed we can see we only have 7, 453 degrees of freedom.\nBMI is significant, but has 10% missing values. Let’s try running the regression without it so we can examine that interaction term, which is the main research question. (it’s also correlated to college education, so we can kind of capture it with that variable).\n\n# Relevel to change to the reference group to never user\ndata_wide_2$hard_drugs_grp&lt;- relevel(data_wide_2$hard_drugs_grp, ref = \"Current User\")\n\n# Relevel to change to the reference group to never user\ndata_wide_2$ADH_HIGHVSLOW &lt;- relevel(data_wide_2$ADH_HIGHVSLOW, ref = \"Low Adherence\")\n\n# Perform regression on CD4+ T Cell count change by hard drugs group, include adherence as confounder, and BMI, frailty related phenotype, and college education as precision variables.\nmodel_LEU3N_red1 &lt;- lm(LEU3N_CHANGE ~ hard_drugs_grp + ADH_HIGHVSLOW + FRP_2 + EDUC_COLLEGE + CESD_2 + hard_drugs_grp*ADH_HIGHVSLOW, data = data_wide_2)\n\n# Exmamine the summary\nsummary(model_LEU3N_red1)\n\n\nCall:\nlm(formula = LEU3N_CHANGE ~ hard_drugs_grp + ADH_HIGHVSLOW + \n    FRP_2 + EDUC_COLLEGE + CESD_2 + hard_drugs_grp * ADH_HIGHVSLOW, \n    data = data_wide_2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-697.61 -112.73   -1.31  106.13 1073.99 \n\nCoefficients:\n                                                         Estimate Std. Error\n(Intercept)                                              -30.8189    92.4188\nhard_drugs_grpNever User                                 126.1630    96.3440\nhard_drugs_grpPrevious User                              355.9657   119.3430\nADH_HIGHVSLOWHigh Adherence                              104.4788    94.2502\nFRP_2Yes                                                -110.1432    35.5533\nEDUC_COLLEGECollege                                       25.0343    20.1854\nCESD_2                                                    -1.0187     0.7723\nhard_drugs_grpNever User:ADH_HIGHVSLOWHigh Adherence     -17.0631    99.8903\nhard_drugs_grpPrevious User:ADH_HIGHVSLOWHigh Adherence -333.1581   126.6465\n                                                        t value Pr(&gt;|t|)   \n(Intercept)                                              -0.333  0.73892   \nhard_drugs_grpNever User                                  1.310  0.19096   \nhard_drugs_grpPrevious User                               2.983  0.00299 **\nADH_HIGHVSLOWHigh Adherence                               1.109  0.26816   \nFRP_2Yes                                                 -3.098  0.00206 **\nEDUC_COLLEGECollege                                       1.240  0.21547   \nCESD_2                                                   -1.319  0.18776   \nhard_drugs_grpNever User:ADH_HIGHVSLOWHigh Adherence     -0.171  0.86443   \nhard_drugs_grpPrevious User:ADH_HIGHVSLOWHigh Adherence  -2.631  0.00878 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 181.3 on 506 degrees of freedom\n  (35 observations deleted due to missingness)\nMultiple R-squared:  0.09847,   Adjusted R-squared:  0.08422 \nF-statistic: 6.908 on 8 and 506 DF,  p-value: 0.000000012\n\n\n\nBackwards Elimination\nWe will perform model selection using backwards elimination and BIC to select the most parsimonious model.\nA delta BIC of &gt;2 indicates a difference in model performance.\n\n# Perform backward elimination regression selection based on BIC \nols_step_backward_sbc(model_LEU3N_red1, include = c(\"hard_drugs_grp\", \"ADH_HIGHVSLOW\"))\n\n\n                                Stepwise Summary                                \n------------------------------------------------------------------------------\nStep    Variable          AIC         SBC         SBIC        R2       Adj. R2 \n------------------------------------------------------------------------------\n 0      Full Model      6828.740    6871.181    5380.046    0.09847    0.08422 \n 1      EDUC_COLLEGE    6828.303    6866.500    5355.906    0.09573    0.08324 \n 2      CESD_2          6828.406    6862.359    5356.186    0.09203    0.08130 \n------------------------------------------------------------------------------\n\nFinal Model Output \n------------------\n\n                           Model Summary                            \n-------------------------------------------------------------------\nR                         0.303       RMSE                 180.377 \nR-Squared                 0.092       MSE                32984.102 \nAdj. R-Squared            0.081       Coef. Var            114.193 \nPred R-Squared            0.076       AIC                 6828.406 \nMAE                     136.282       SBC                 6862.359 \n-------------------------------------------------------------------\n RMSE: Root Mean Square Error \n MSE: Mean Square Error \n MAE: Mean Absolute Error \n AIC: Akaike Information Criteria \n SBC: Schwarz Bayesian Criteria \n\n                                 ANOVA                                   \n------------------------------------------------------------------------\n                    Sum of                                              \n                   Squares         DF    Mean Square      F        Sig. \n------------------------------------------------------------------------\nRegression     1698306.434          6     283051.072    8.581    0.0000 \nResidual      16755923.716        508      32984.102                    \nTotal         18454230.150        514                                   \n------------------------------------------------------------------------\n\n                                                           Parameter Estimates                                                            \n-----------------------------------------------------------------------------------------------------------------------------------------\n                                                  model        Beta    Std. Error    Std. Beta      t        Sig        lower      upper \n-----------------------------------------------------------------------------------------------------------------------------------------\n                                            (Intercept)     -36.638        90.808                 -0.403    0.687    -215.043    141.767 \n                               hard_drugs_grpNever User     136.867        96.322        0.292     1.421    0.156     -52.373    326.106 \n                            hard_drugs_grpPrevious User     341.997       117.232        0.515     2.917    0.004     111.678    572.317 \n                            ADH_HIGHVSLOWHigh Adherence     116.056        94.071        0.168     1.234    0.218     -68.761    300.873 \n                                               FRP_2Yes    -113.844        35.435       -0.136    -3.213    0.001    -183.462    -44.226 \n   hard_drugs_grpNever User:ADH_HIGHVSLOWHigh Adherence     -25.037        99.815       -0.059    -0.251    0.802    -221.138    171.064 \nhard_drugs_grpPrevious User:ADH_HIGHVSLOWHigh Adherence    -330.080       123.130       -0.467    -2.681    0.008    -571.988    -88.173 \n-----------------------------------------------------------------------------------------------------------------------------------------\n\n\nAs predicted, the most parsimonious model is with CESD_2 and EDUC_COLLEGE removed, and only including FRP_2 as a precision variable.\nTop of Tabset\n\n\n\nLet’s investigate the key relationships in the final model for LEU3N_CHANGE as determined through backwards selection.\nFirst let’s look at the impact of hard drug use on CD4+ T Cell change, at low adherence.\n\n# Relevel to change to the reference group to never user\ndata_wide_2$hard_drugs_grp&lt;- relevel(data_wide_2$hard_drugs_grp, ref = \"Never User\")\n\n# Relevel to change to the reference group to never user\ndata_wide_2$ADH_HIGHVSLOW &lt;- relevel(data_wide_2$ADH_HIGHVSLOW, ref = \"Low Adherence\")\n\n# Perform regression on CD4+ T Cell count change by hard drugs group with covariates as determined by backward elimination\nmodel_LEU3N_final1 &lt;- lm(LEU3N_CHANGE ~ hard_drugs_grp + ADH_HIGHVSLOW + FRP_2 + hard_drugs_grp*ADH_HIGHVSLOW, data = data_wide_2)\n\n# Examine the summary\nsummary(model_LEU3N_final1)\n\n\nCall:\nlm(formula = LEU3N_CHANGE ~ hard_drugs_grp + ADH_HIGHVSLOW + \n    FRP_2 + hard_drugs_grp * ADH_HIGHVSLOW, data = data_wide_2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-700.74 -111.21   -1.67  112.23 1077.98 \n\nCoefficients:\n                                                        Estimate Std. Error\n(Intercept)                                               107.46      31.59\nhard_drugs_grpCurrent User                               -144.10      96.02\nhard_drugs_grpPrevious User                               197.90      80.49\nADH_HIGHVSLOWHigh Adherence                                83.09      32.92\nFRP_2Yes                                                 -113.63      35.38\nhard_drugs_grpCurrent User:ADH_HIGHVSLOWHigh Adherence     32.94      99.51\nhard_drugs_grpPrevious User:ADH_HIGHVSLOWHigh Adherence  -297.13      85.95\n                                                        t value Pr(&gt;|t|)    \n(Intercept)                                               3.402 0.000721 ***\nhard_drugs_grpCurrent User                               -1.501 0.134038    \nhard_drugs_grpPrevious User                               2.459 0.014277 *  \nADH_HIGHVSLOWHigh Adherence                               2.524 0.011895 *  \nFRP_2Yes                                                 -3.212 0.001402 ** \nhard_drugs_grpCurrent User:ADH_HIGHVSLOWHigh Adherence    0.331 0.740764    \nhard_drugs_grpPrevious User:ADH_HIGHVSLOWHigh Adherence  -3.457 0.000592 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 181.3 on 512 degrees of freedom\n  (31 observations deleted due to missingness)\nMultiple R-squared:  0.08984,   Adjusted R-squared:  0.07917 \nF-statistic: 8.423 on 6 and 512 DF,  p-value: 0.000000009913\n\n\n\n# Print useful model parameters\nmodel_results(model_LEU3N_final1)\n\n\n\n\n\nEstimate\nStd. Error\nt value\nPr(&gt;|t|)\np_adjusted\n2.5 %\n97.5 %\n\n\n\n\n(Intercept)\n107.46049\n31.58675\n3.4020752\n0.0007211\n0.0050476\n45.40491\n169.51607\n\n\nhard_drugs_grpCurrent User\n-144.09848\n96.01798\n-1.5007447\n0.1340380\n0.9382660\n-332.73618\n44.53922\n\n\nhard_drugs_grpPrevious User\n197.89868\n80.49146\n2.4586294\n0.0142768\n0.0999374\n39.76450\n356.03287\n\n\nADH_HIGHVSLOWHigh Adherence\n83.09406\n32.91813\n2.5242642\n0.0118953\n0.0832670\n18.42283\n147.76529\n\n\nFRP_2Yes\n-113.62595\n35.37733\n-3.2118295\n0.0014019\n0.0098130\n-183.12853\n-44.12336\n\n\nhard_drugs_grpCurrent User:ADH_HIGHVSLOWHigh Adherence\n32.93895\n99.50721\n0.3310208\n0.7407642\n1.0000000\n-162.55372\n228.43162\n\n\nhard_drugs_grpPrevious User:ADH_HIGHVSLOWHigh Adherence\n-297.12895\n85.95111\n-3.4569531\n0.0005918\n0.0041424\n-465.98920\n-128.26871\n\n\n\n\n\n\n\nThe overall model is highly significant (F(6,512)= 8.42, p &lt; 0.00001).\nAt low adherence, previous hard drug users have a higher CD4+ T Cell count than never hard drug users (p-adjusted = 0.043), but current hard drugs users did not differ from never drug users (p-adjusted = 0.40).\nAdditionally, for never hard drug users, those with high adherence have a higher CD4+ T Cell count than those with low adherence (p = 0.011895)\nNow let’s compare hard drug usage for those with high adherence.\n\n# Relevel to change to the reference group to never user\ndata_wide_2$hard_drugs_grp&lt;- relevel(data_wide_2$hard_drugs_grp, ref = \"Never User\")\n\n# Relevel to change to the reference group to never user\ndata_wide_2$ADH_HIGHVSLOW &lt;- relevel(data_wide_2$ADH_HIGHVSLOW, ref = \"High Adherence\")\n\n# Perform regression on CD4+ T Cell count change by hard drugs group with covariates as determined by backward elimination\nmodel_LEU3N_final2 &lt;- lm(LEU3N_CHANGE ~ hard_drugs_grp + ADH_HIGHVSLOW + FRP_2 + hard_drugs_grp*ADH_HIGHVSLOW, data = data_wide_2)\n\n# Examine the summary\nsummary(model_LEU3N_final2)\n\n\nCall:\nlm(formula = LEU3N_CHANGE ~ hard_drugs_grp + ADH_HIGHVSLOW + \n    FRP_2 + hard_drugs_grp * ADH_HIGHVSLOW, data = data_wide_2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-700.74 -111.21   -1.67  112.23 1077.98 \n\nCoefficients:\n                                                       Estimate Std. Error\n(Intercept)                                              190.56       9.47\nhard_drugs_grpCurrent User                              -111.16      26.04\nhard_drugs_grpPrevious User                              -99.23      30.14\nADH_HIGHVSLOWLow Adherence                               -83.09      32.92\nFRP_2Yes                                                -113.63      35.38\nhard_drugs_grpCurrent User:ADH_HIGHVSLOWLow Adherence    -32.94      99.51\nhard_drugs_grpPrevious User:ADH_HIGHVSLOWLow Adherence   297.13      85.95\n                                                       t value\n(Intercept)                                             20.123\nhard_drugs_grpCurrent User                              -4.269\nhard_drugs_grpPrevious User                             -3.292\nADH_HIGHVSLOWLow Adherence                              -2.524\nFRP_2Yes                                                -3.212\nhard_drugs_grpCurrent User:ADH_HIGHVSLOWLow Adherence   -0.331\nhard_drugs_grpPrevious User:ADH_HIGHVSLOWLow Adherence   3.457\n                                                                   Pr(&gt;|t|)    \n(Intercept)                                            &lt; 0.0000000000000002 ***\nhard_drugs_grpCurrent User                                        0.0000234 ***\nhard_drugs_grpPrevious User                                        0.001064 ** \nADH_HIGHVSLOWLow Adherence                                         0.011895 *  \nFRP_2Yes                                                           0.001402 ** \nhard_drugs_grpCurrent User:ADH_HIGHVSLOWLow Adherence              0.740764    \nhard_drugs_grpPrevious User:ADH_HIGHVSLOWLow Adherence             0.000592 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 181.3 on 512 degrees of freedom\n  (31 observations deleted due to missingness)\nMultiple R-squared:  0.08984,   Adjusted R-squared:  0.07917 \nF-statistic: 8.423 on 6 and 512 DF,  p-value: 0.000000009913\n\n\n\n# Print useful model parameters\nmodel_results(model_LEU3N_final2)\n\n\n\n\n\nEstimate\nStd. Error\nt value\nPr(&gt;|t|)\np_adjusted\n2.5 %\n97.5 %\n\n\n\n\n(Intercept)\n190.55455\n9.469605\n20.1227563\n0.0000000\n0.0000000\n171.9505\n209.15861\n\n\nhard_drugs_grpCurrent User\n-111.15953\n26.036468\n-4.2693782\n0.0000234\n0.0001636\n-162.3110\n-60.00807\n\n\nhard_drugs_grpPrevious User\n-99.23027\n30.144942\n-3.2917718\n0.0010643\n0.0074500\n-158.4533\n-40.00727\n\n\nADH_HIGHVSLOWLow Adherence\n-83.09406\n32.918132\n-2.5242642\n0.0118953\n0.0832670\n-147.7653\n-18.42283\n\n\nFRP_2Yes\n-113.62595\n35.377328\n-3.2118295\n0.0014019\n0.0098130\n-183.1285\n-44.12336\n\n\nhard_drugs_grpCurrent User:ADH_HIGHVSLOWLow Adherence\n-32.93895\n99.507208\n-0.3310208\n0.7407642\n1.0000000\n-228.4316\n162.55372\n\n\nhard_drugs_grpPrevious User:ADH_HIGHVSLOWLow Adherence\n297.12895\n85.951110\n3.4569531\n0.0005918\n0.0041424\n128.2687\n465.98920\n\n\n\n\n\n\n\nFor those with high adherence, previous hard drug users had a lower CD4+ T Cell count (p-adjusted = 0.0032), and current hard drug users had a lower CD4+ T Cell (p-adjusted &lt; 0.0001) count compared to never hard users.\nNow we make the same comparisons with previous hard drug users as the baseline.\n\n# Relevel to change to the reference group to never user\ndata_wide_2$hard_drugs_grp&lt;- relevel(data_wide_2$hard_drugs_grp, ref = \"Previous User\")\n\n# Relevel to change to the reference group to never user\ndata_wide_2$ADH_HIGHVSLOW &lt;- relevel(data_wide_2$ADH_HIGHVSLOW, ref = \"Low Adherence\")\n\n# Perform regression on CD4+ T Cell count change by hard drugs group with covariates as determined by backward elimination\nmodel_LEU3N_final2 &lt;- lm(LEU3N_CHANGE ~ hard_drugs_grp + ADH_HIGHVSLOW + FRP_2 + hard_drugs_grp*ADH_HIGHVSLOW, data = data_wide_2)\n\n# Examine the summary\nsummary(model_LEU3N_final2)\n\n\nCall:\nlm(formula = LEU3N_CHANGE ~ hard_drugs_grp + ADH_HIGHVSLOW + \n    FRP_2 + hard_drugs_grp * ADH_HIGHVSLOW, data = data_wide_2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-700.74 -111.21   -1.67  112.23 1077.98 \n\nCoefficients:\n                                                       Estimate Std. Error\n(Intercept)                                              305.36      74.03\nhard_drugs_grpNever User                                -197.90      80.49\nhard_drugs_grpCurrent User                              -342.00     117.06\nADH_HIGHVSLOWHigh Adherence                             -214.03      79.41\nFRP_2Yes                                                -113.63      35.38\nhard_drugs_grpNever User:ADH_HIGHVSLOWHigh Adherence     297.13      85.95\nhard_drugs_grpCurrent User:ADH_HIGHVSLOWHigh Adherence   330.07     122.95\n                                                       t value  Pr(&gt;|t|)    \n(Intercept)                                              4.125 0.0000433 ***\nhard_drugs_grpNever User                                -2.459  0.014277 *  \nhard_drugs_grpCurrent User                              -2.922  0.003636 ** \nADH_HIGHVSLOWHigh Adherence                             -2.695  0.007266 ** \nFRP_2Yes                                                -3.212  0.001402 ** \nhard_drugs_grpNever User:ADH_HIGHVSLOWHigh Adherence     3.457  0.000592 ***\nhard_drugs_grpCurrent User:ADH_HIGHVSLOWHigh Adherence   2.685  0.007497 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 181.3 on 512 degrees of freedom\n  (31 observations deleted due to missingness)\nMultiple R-squared:  0.08984,   Adjusted R-squared:  0.07917 \nF-statistic: 8.423 on 6 and 512 DF,  p-value: 0.000000009913\n\n\n\n# Print useful model parameters\nmodel_results(model_LEU3N_final2)\n\n\n\n\n\nEstimate\nStd. Error\nt value\nPr(&gt;|t|)\np_adjusted\n2.5 %\n97.5 %\n\n\n\n\n(Intercept)\n305.3592\n74.03481\n4.124535\n0.0000433\n0.0003034\n159.90978\n450.80856\n\n\nhard_drugs_grpNever User\n-197.8987\n80.49146\n-2.458629\n0.0142768\n0.0999374\n-356.03287\n-39.76450\n\n\nhard_drugs_grpCurrent User\n-341.9972\n117.05931\n-2.921572\n0.0036365\n0.0254554\n-571.97284\n-112.02148\n\n\nADH_HIGHVSLOWHigh Adherence\n-214.0349\n79.41319\n-2.695206\n0.0072658\n0.0508607\n-370.05069\n-58.01909\n\n\nFRP_2Yes\n-113.6259\n35.37733\n-3.211829\n0.0014019\n0.0098130\n-183.12853\n-44.12336\n\n\nhard_drugs_grpNever User:ADH_HIGHVSLOWHigh Adherence\n297.1290\n85.95111\n3.456953\n0.0005918\n0.0041424\n128.26871\n465.98920\n\n\nhard_drugs_grpCurrent User:ADH_HIGHVSLOWHigh Adherence\n330.0679\n122.94881\n2.684596\n0.0074971\n0.0524799\n88.52168\n571.61413\n\n\n\n\n\n\n\nAt low adherence, current hard drug users had a CD4+ T Cell count compared to previous hard drug users (p-adjusted = 0.011).\nAdditionally, for previous hard drug users, those with high adherence had a greater decrease in CD4+ T Cell count compared to those with low adherence (p = 0.0073).\nNow let’s change the reference level to compare high adherence previous vs current drug users.\n\n# Relevel to change to the reference group to never user\ndata_wide_2$hard_drugs_grp &lt;- relevel(data_wide_2$hard_drugs_grp, ref = \"Previous User\")\n\n# Relevel to change to the reference group to never user\ndata_wide_2$ADH_HIGHVSLOW &lt;- relevel(data_wide_2$ADH_HIGHVSLOW, ref = \"High Adherence\")\n\n# Perform regression on CD4+ T Cell count change by hard drugs group with covariates as determined by backward elimination\nmodel_LEU3N_final3 &lt;- lm(LEU3N_CHANGE ~ hard_drugs_grp + ADH_HIGHVSLOW + FRP_2 + hard_drugs_grp*ADH_HIGHVSLOW, data = data_wide_2)\n\n# Examine the summary\nsummary(model_LEU3N_final3)\n\n\nCall:\nlm(formula = LEU3N_CHANGE ~ hard_drugs_grp + ADH_HIGHVSLOW + \n    FRP_2 + hard_drugs_grp * ADH_HIGHVSLOW, data = data_wide_2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-700.74 -111.21   -1.67  112.23 1077.98 \n\nCoefficients:\n                                                      Estimate Std. Error\n(Intercept)                                              91.32      28.73\nhard_drugs_grpNever User                                 99.23      30.14\nhard_drugs_grpCurrent User                              -11.93      37.60\nADH_HIGHVSLOWLow Adherence                              214.03      79.41\nFRP_2Yes                                               -113.63      35.38\nhard_drugs_grpNever User:ADH_HIGHVSLOWLow Adherence    -297.13      85.95\nhard_drugs_grpCurrent User:ADH_HIGHVSLOWLow Adherence  -330.07     122.95\n                                                      t value Pr(&gt;|t|)    \n(Intercept)                                             3.179 0.001568 ** \nhard_drugs_grpNever User                                3.292 0.001064 ** \nhard_drugs_grpCurrent User                             -0.317 0.751150    \nADH_HIGHVSLOWLow Adherence                              2.695 0.007266 ** \nFRP_2Yes                                               -3.212 0.001402 ** \nhard_drugs_grpNever User:ADH_HIGHVSLOWLow Adherence    -3.457 0.000592 ***\nhard_drugs_grpCurrent User:ADH_HIGHVSLOWLow Adherence  -2.685 0.007497 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 181.3 on 512 degrees of freedom\n  (31 observations deleted due to missingness)\nMultiple R-squared:  0.08984,   Adjusted R-squared:  0.07917 \nF-statistic: 8.423 on 6 and 512 DF,  p-value: 0.000000009913\n\n\n\n# Print useful model parameters\nmodel_results(model_LEU3N_final3)\n\n\n\n\n\nEstimate\nStd. Error\nt value\nPr(&gt;|t|)\np_adjusted\n2.5 %\n97.5 %\n\n\n\n\n(Intercept)\n91.32428\n28.72807\n3.1789217\n0.0015677\n0.0109736\n34.88488\n147.76367\n\n\nhard_drugs_grpNever User\n99.23027\n30.14494\n3.2917718\n0.0010643\n0.0074500\n40.00727\n158.45327\n\n\nhard_drugs_grpCurrent User\n-11.92926\n37.59689\n-0.3172937\n0.7511501\n1.0000000\n-85.79241\n61.93390\n\n\nADH_HIGHVSLOWLow Adherence\n214.03489\n79.41319\n2.6952058\n0.0072658\n0.0508607\n58.01909\n370.05069\n\n\nFRP_2Yes\n-113.62595\n35.37733\n-3.2118295\n0.0014019\n0.0098130\n-183.12853\n-44.12336\n\n\nhard_drugs_grpNever User:ADH_HIGHVSLOWLow Adherence\n-297.12895\n85.95111\n-3.4569531\n0.0005918\n0.0041424\n-465.98920\n-128.26871\n\n\nhard_drugs_grpCurrent User:ADH_HIGHVSLOWLow Adherence\n-330.06790\n122.94881\n-2.6845962\n0.0074971\n0.0524799\n-571.61413\n-88.52168\n\n\n\n\n\n\n\nFor those with high adherence, previous vs current hard drug users did not differ in CD4+ T Cell Count (p-adjusted= 1.00).\nFinally let’s compare the impact of adherence for hard drug users.\n\n# Relevel to change to the reference group to never user\ndata_wide_2$hard_drugs_grp &lt;- relevel(data_wide_2$hard_drugs_grp, ref = \"Current User\")\n\n# Relevel to change to the reference group to never user\ndata_wide_2$ADH_HIGHVSLOW &lt;- relevel(data_wide_2$ADH_HIGHVSLOW, ref = \"Low Adherence\")\n\n# Perform regression on CD4+ T Cell count change by hard drugs group with covariates as determined by backward elimination\nmodel_LEU3N_final4 &lt;- lm(LEU3N_CHANGE ~ hard_drugs_grp + ADH_HIGHVSLOW + FRP_2 + hard_drugs_grp*ADH_HIGHVSLOW, data = data_wide_2)\n\n# Examine the summary\nsummary(model_LEU3N_final4)\n\n\nCall:\nlm(formula = LEU3N_CHANGE ~ hard_drugs_grp + ADH_HIGHVSLOW + \n    FRP_2 + hard_drugs_grp * ADH_HIGHVSLOW, data = data_wide_2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-700.74 -111.21   -1.67  112.23 1077.98 \n\nCoefficients:\n                                                        Estimate Std. Error\n(Intercept)                                               -36.64      90.67\nhard_drugs_grpPrevious User                               342.00     117.06\nhard_drugs_grpNever User                                  144.10      96.02\nADH_HIGHVSLOWHigh Adherence                               116.03      93.93\nFRP_2Yes                                                 -113.63      35.38\nhard_drugs_grpPrevious User:ADH_HIGHVSLOWHigh Adherence  -330.07     122.95\nhard_drugs_grpNever User:ADH_HIGHVSLOWHigh Adherence      -32.94      99.51\n                                                        t value Pr(&gt;|t|)   \n(Intercept)                                              -0.404  0.68633   \nhard_drugs_grpPrevious User                               2.922  0.00364 **\nhard_drugs_grpNever User                                  1.501  0.13404   \nADH_HIGHVSLOWHigh Adherence                               1.235  0.21729   \nFRP_2Yes                                                 -3.212  0.00140 **\nhard_drugs_grpPrevious User:ADH_HIGHVSLOWHigh Adherence  -2.685  0.00750 **\nhard_drugs_grpNever User:ADH_HIGHVSLOWHigh Adherence     -0.331  0.74076   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 181.3 on 512 degrees of freedom\n  (31 observations deleted due to missingness)\nMultiple R-squared:  0.08984,   Adjusted R-squared:  0.07917 \nF-statistic: 8.423 on 6 and 512 DF,  p-value: 0.000000009913\n\n\n\n# Print useful model parameters\nmodel_results(model_LEU3N_final4)\n\n\n\n\n\nEstimate\nStd. Error\nt value\nPr(&gt;|t|)\np_adjusted\n2.5 %\n97.5 %\n\n\n\n\n(Intercept)\n-36.63799\n90.67376\n-0.4040639\n0.6863344\n1.0000000\n-214.77639\n141.50040\n\n\nhard_drugs_grpPrevious User\n341.99716\n117.05931\n2.9215715\n0.0036365\n0.0254554\n112.02148\n571.97284\n\n\nhard_drugs_grpNever User\n144.09848\n96.01798\n1.5007447\n0.1340380\n0.9382660\n-44.53922\n332.73618\n\n\nADH_HIGHVSLOWHigh Adherence\n116.03301\n93.93276\n1.2352773\n0.2172938\n1.0000000\n-68.50805\n300.57408\n\n\nFRP_2Yes\n-113.62595\n35.37733\n-3.2118295\n0.0014019\n0.0098130\n-183.12853\n-44.12336\n\n\nhard_drugs_grpPrevious User:ADH_HIGHVSLOWHigh Adherence\n-330.06790\n122.94881\n-2.6845962\n0.0074971\n0.0524799\n-571.61413\n-88.52168\n\n\nhard_drugs_grpNever User:ADH_HIGHVSLOWHigh Adherence\n-32.93895\n99.50721\n-0.3310208\n0.7407642\n1.0000000\n-228.43162\n162.55372\n\n\n\n\n\n\n\nFor current hard drug users, those with high adherence did not differ significantly compared to those with low adherence (p = 0.22)\nFRP_2 was a significant predictor for LEU3N_CHANGE, while controlling for hard drug use and adherence to the treatment regiment (t = -3.21, p = 0.0014). On average, those with a Frailty Related Phenotype had a decrease in CD4+ T cells that was 113.63 cells greater than those without a Frailty Related Phenotype (95% CI: 44.12 to 183.13 cells).\nTop of Tabset\n\n\n\nOverall Model\nThere were significant differences in change in CD4+ T Cell count over 2 years based on hard drug usage and adherence to the treatment regiment, while controlling for Frailty Related Phenotype (F(6,512) = 8.42, p &lt; 0.00001).\n\nHard Drug Use\nThe relationship between hard drug use and change in CD4+ T Cell count over 2 years depended on adherence to the treatment regiment.\n\nHard Drug Use at Low Adherence\nFor those with low adherence to the treatment regiment, previous hard drug users had on average a 197.90 cells greater increase in CD4+ T Cell count compared to never hard drug users (t = 2.46, p-adjusted = 0.043, 95%: 39.77 to 356.03). Current hard drug users did not differ from never hard drug users (t = -1.50, p-adjusted = 0.40, 95% CI: -332.74 to 44.54). Additionally, current hard drug users had on average a -342.00 cells greater decrease in CD4+ T Cell count compared to previous hard drug users (t = -2.922, p-adjusted = 0.011, 95% CI: -571.97 to -112.02).\n\n\nHard Drug Use at High Adherence\nFor those with high adherence to the treatment regiment, previous hard drug users had on average a 99.23 cells greater decrease (t = -3.29, p-adjusted = 0.0032, 95% CI: -158.4533 to -40.00727), and current hard drug users had on average a 111.16 cells greater decrease (t = -4.27, p-adjusted &lt; 0.0001, 95% CI: -162.31 to -60.00) compared to never hard drug users. in CD4+ T cell count compared never hard drug users. Current hard drug users did not differ from previous hard drug users on change in CD4+ T Cell count over 2 years (t = -0.32, p-adjusted= 1.00, 95% CI: -85.79 to 61.93).\n\n\n\n\nAdherence\nThe relationship between adherence to the treatment regiment and change in CD4+ T Cell count over 2 years differed by hard drug use.\n\nAdherence for Current Hard Drug Users\nFor current hard drug users, those with high adherence did not differ significantly in average change in CD4+ T Cells over two years compared to those with low adherence (t = 1.24, p-adjusted = 0.22, 95% CI: -68.51 to 300.57).\n\n\n\nAdherence for Previous Hard Drug Users\nFor previous hard drug users, those with high adherence had on average a 214.03 cells greater decrease in CD4+ T Cell count compared to those with low adherence (t = -2.70, p = 0.0073, 95% CI: -370.05 to -58.020).\n\n\nAdherence for Never Hard Drug Users\nFor never drug users, those with high adherence had on average a 83.09 cells greater decrease in CD4+ T Cell count compared to those with low adherence (t = 2.52, p = 0.0119, 95% CI: 18.42283 147.76529).\n\n\nAdditional Interaction Comparisons\nThere are additional comparisons we can make in the interaction between hard drug usage and adherence, but they are of little clinical relevance (e.g. comparing current hard drug users with low adherence to never hard drug users with high adherence). ### Frailty Related Phenotype\nFrailty Related Phenotype was a significant predictor for change in CD4+ T Cell count over 2 years, while controlling for hard drug use and adherence to the treatment regiment (t = -3.21, p = 0.0014). On average, those with a Frailty Related Phenotype had a 113.63 cells greater decrease in CD4+ T Cell count than those without a Frailty Related Phenotype (95% CI: 44.12 to 183.13 cells).\nTop of Tabset\n\n\n\nThe below plot is useful in interpreting the interaction\n\n# Use interactions package to plot interaction\ncat_plot(\n  model_LEU3N_final1, \n  pred = hard_drugs_grp, \n  modx = ADH_HIGHVSLOW, \n  geom = \"line\", \n  colors = \"Pastel2\",\n  x.label = \"Hard Drugs Group\",\n  y.label = \"Predicted CD4+ T Cell Count Change\")\n\n\n\n\n\n\n\n# Filter out NA values\ndata_no_na &lt;- data_wide_2 %&gt;% filter(!is.na(ADH_HIGHVSLOW))\n\n# Create the plot with lines\nggplot(data_no_na, aes(x = hard_drugs_grp, y = LEU3N_CHANGE, fill = ADH_HIGHVSLOW)) +\n  geom_boxplot(alpha = 0.5) +\n  scale_fill_brewer(palette = \"Pastel2\") +\n  labs(title = \"Interaction between Hard Drugs and Adherence on LEU3N Change\",\n       x = \"Hard Drugs Group\",\n       y = \"LEU3N Change\") +\n  guides(fill = guide_legend(title = \"Adherence Group\")) +\n   theme_minimal() \n\nWarning: Removed 18 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\n\n\n\n\nHere we can see the key relationships in our model.\nFor current hard drug users, those with high adherence did not differ from those with low adherence. This may be an artifact of the small number of current hard drug users with low adherence, however (n = 6).\nFor previous hard drug users, those with high adherence had a smaller increase in CD4+ T Cell count compared to those with low adherence. This is a counter-inuitive finding and may be an artifact of the small number of previous hard drug users with low adherence, however (n=4). It may be of interest to the clinicians however, and we therefore report it here.\nFor never hard drug users, those with high adherence had a greater increase in CD4+ T Cell count compared to those with low adherence.\nAt low adherence, previous hard drug users had greater increase in CD4+ T Cell count compared to never hard drug users, and also when compared to current hard drug users. Current hard drugs users did not differ crom never hard drug users in change in CD4+ T Cell count at low adherence.\nAt high adherence, never drug users had a greater increase in CD4+ T cell count compared to previous and current hard drug users. Current hard drug users did not differ from previous hard drug users at high adherence.\nTop of Tabset\n\n\n\n\n\n\nAnalysisInterpretationVisualizing Main Effects\n\n\n\n# Relevel to change to the reference group to never user\ndata_wide_2$hard_drugs_grp&lt;- relevel(data_wide_2$hard_drugs_grp, ref = \"Never User\")\n\n# Relevel to change to the reference group to never user\ndata_wide_2$ADH_HIGHVSLOW &lt;- relevel(data_wide_2$ADH_HIGHVSLOW, ref = \"Low Adherence\")\n\n# Perform regression on CD4+ T Cell count change by hard drugs group with covariates as determined by backward elimination\nmodel_LEU3N_final_noX1 &lt;- lm(LEU3N_CHANGE ~ hard_drugs_grp + ADH_HIGHVSLOW + FRP_2, data = data_wide_2)\n\n# Examine the summary\nsummary(model_LEU3N_final_noX1)\n\n\nCall:\nlm(formula = LEU3N_CHANGE ~ hard_drugs_grp + ADH_HIGHVSLOW + \n    FRP_2, data = data_wide_2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-697.88 -115.61   -3.14  114.98 1080.85 \n\nCoefficients:\n                            Estimate Std. Error t value    Pr(&gt;|t|)    \n(Intercept)                   140.88      28.36   4.967 0.000000927 ***\nhard_drugs_grpCurrent User   -112.84      25.38  -4.446 0.000010724 ***\nhard_drugs_grpPrevious User   -62.31      28.51  -2.185     0.02932 *  \nADH_HIGHVSLOWHigh Adherence    46.81      29.24   1.601     0.11000    \nFRP_2Yes                     -114.33      35.72  -3.201     0.00145 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 183.2 on 514 degrees of freedom\n  (31 observations deleted due to missingness)\nMultiple R-squared:  0.06752,   Adjusted R-squared:  0.06026 \nF-statistic: 9.305 on 4 and 514 DF,  p-value: 0.000000289\n\n\n\n# Examine the summary\nmodel_results(model_LEU3N_final_noX1)\n\n\n\n\n\nEstimate\nStd. Error\nt value\nPr(&gt;|t|)\np_adjusted\n2.5 %\n97.5 %\n\n\n\n\n(Intercept)\n140.8774\n28.36272\n4.966993\n0.0000009\n0.0000046\n85.15631\n196.598539\n\n\nhard_drugs_grpCurrent User\n-112.8387\n25.37954\n-4.446049\n0.0000107\n0.0000536\n-162.69908\n-62.978288\n\n\nhard_drugs_grpPrevious User\n-62.3112\n28.51439\n-2.185255\n0.0293209\n0.1466044\n-118.33027\n-6.292117\n\n\nADH_HIGHVSLOWHigh Adherence\n46.8124\n29.24046\n1.600946\n0.1100033\n0.5500164\n-10.63312\n104.257925\n\n\nFRP_2Yes\n-114.3343\n35.72007\n-3.200841\n0.0014550\n0.0072749\n-184.50955\n-44.158969\n\n\n\n\n\n\n\nChange the reference level to previous users and re-run the model\n\n# Relevel to change to the reference group to never user\ndata_wide_2$hard_drugs_grp&lt;- relevel(data_wide_2$hard_drugs_grp, ref = \"Previous User\")\n\n# Relevel to change to the reference group to never user\ndata_wide_2$ADH_HIGHVSLOW &lt;- relevel(data_wide_2$ADH_HIGHVSLOW, ref = \"Low Adherence\")\n\n# Perform regression on CD4+ T Cell count change by hard drugs group with covariates as determined by backward elimination\nmodel_LEU3N_final_noX2 &lt;- lm(LEU3N_CHANGE ~ hard_drugs_grp + ADH_HIGHVSLOW + FRP_2, data = data_wide_2)\n\n# Examine the summary\nsummary(model_LEU3N_final_noX2)\n\n\nCall:\nlm(formula = LEU3N_CHANGE ~ hard_drugs_grp + ADH_HIGHVSLOW + \n    FRP_2, data = data_wide_2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-697.88 -115.61   -3.14  114.98 1080.85 \n\nCoefficients:\n                            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)                    78.57      37.09   2.118  0.03462 * \nhard_drugs_grpNever User       62.31      28.51   2.185  0.02932 * \nhard_drugs_grpCurrent User    -50.53      36.00  -1.403  0.16110   \nADH_HIGHVSLOWHigh Adherence    46.81      29.24   1.601  0.11000   \nFRP_2Yes                     -114.33      35.72  -3.201  0.00145 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 183.2 on 514 degrees of freedom\n  (31 observations deleted due to missingness)\nMultiple R-squared:  0.06752,   Adjusted R-squared:  0.06026 \nF-statistic: 9.305 on 4 and 514 DF,  p-value: 0.000000289\n\n\n\n# Examine the summary\nmodel_results(model_LEU3N_final_noX2)\n\n\n\n\n\nEstimate\nStd. Error\nt value\nPr(&gt;|t|)\np_adjusted\n2.5 %\n97.5 %\n\n\n\n\n(Intercept)\n78.56623\n37.08694\n2.118434\n0.0346175\n0.1730874\n5.705608\n151.42685\n\n\nhard_drugs_grpNever User\n62.31120\n28.51439\n2.185255\n0.0293209\n0.1466044\n6.292117\n118.33027\n\n\nhard_drugs_grpCurrent User\n-50.52749\n36.00333\n-1.403412\n0.1610979\n0.8054896\n-121.259266\n20.20429\n\n\nADH_HIGHVSLOWHigh Adherence\n46.81240\n29.24046\n1.600946\n0.1100033\n0.5500164\n-10.633119\n104.25793\n\n\nFRP_2Yes\n-114.33426\n35.72007\n-3.200841\n0.0014550\n0.0072749\n-184.509547\n-44.15897\n\n\n\n\n\n\n\n\n\n\nOverall Model\nThe overall model is significant (F~(4, 514)~ = 9.31, p &lt; .0001).\n\n\nHard Drug Use\nHard drug use is a significant predictor of change in CD4+ T Cell count over 2 years.\nCurrent hard drug users had an average change in CD4+ T Cell count that was 112.84 cells lower than never drug users, while controlling for adherence to treatment regiment and Frailty Related Phenotype (p-adjusted &lt; 0.0001, 95% CI: -162.70 to -62.98).\nPrevious hard drug users did not differ from never hard drugs users (p-adjusted = 0.088, 95% CI:-118.33 to -6.29), or current hard drug users (p-adjusted = 0.483, 95% CI: -121.26 to 20.20).\n\n\nAdherence\nAdherence was not a significant predictor for change in CD4+ T Cell count over 2 years, after controlling for hard drug use and Frailty Related Phenotype (t = 1.60, p = 0.110).\n\n\nFrailty Related Phenotype\nFrailty related phenotype is a significant predictor for change in CD4+ T Cell count over 2 years, while controlling for hard drug use and adherence to the treatment regiment (t = -3.20, p = 0.00145). On average, those with a frailty related phenotype had a change in CD4+ T Cell count that was 114.33 cells lower than those without a frailty related phenotype (95% CI: -184.51 to -44.16).\nTop of Tabset\n\n\n\n\nHard Drug Use\nHere we will create some simple plots to help with visualizing the main effects of hard drug use and adherence on change in CD4+ T Cell count.\n\n# Relevel to change the reference cateory to Never User\ndata_wide_2$hard_drugs_grp &lt;- relevel(data_wide_2$hard_drugs_grp, ref = \"Never User\")\n\n# Create boxplots of CD4+ T Cell count change by hard drug use group\nggplot(data_wide_2, aes(x = hard_drugs_grp, y = LEU3N_CHANGE, fill = hard_drugs_grp)) +\n  geom_boxplot(alpha = 0.5) +\n  theme_minimal() +\n  scale_fill_brewer(palette=\"Pastel2\") +\n  labs(title = \"Boxplot of CD4+ T Cell Count Change by Hard Drug Use Group\",\n       x = \"Hard Drug Use Group\",\n       y = \"CD4+ T Cell Count Change\") +\n  guides(fill = guide_legend(title = \"Hard Drug Use Group\"))\n\nWarning: Removed 18 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\n\n\n\n\nCurrent hard drug users had less of an increase in CD4+ T Cells over 2 years compared to never hard drug users. All other between-group comparisons were not significant.\n\n\nAdherence\n\n# Create boxplots of CD4+ T Cell count change by adherence group\nggplot(data_no_na, aes(x = ADH_HIGHVSLOW, y = LEU3N_CHANGE, fill = ADH_HIGHVSLOW)) +\n  geom_boxplot(alpha = 0.5) +\n  theme_minimal() +\n  scale_fill_brewer(palette = \"Pastel2\") +\n  labs(title = \"Boxplot of CD4+ T Cell Count Change by Adherence at Year 2\",\n       x = \"Adherence Level\",\n       y = \"CD4+ T Cell Count Change\") +\n  guides(fill = guide_legend(title = \"Adherence Group\"))\n\nWarning: Removed 18 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\n\n\n\n\nChange in CD4+ T Cell count did not differ based on adherence.\n\n\nFrailty Related Phenotype\n\n# Filter out NA values\ndata_no_na &lt;- data_wide_2 %&gt;% filter(!is.na(FRP_2))\n\n# Create boxplots of CD4+ T Cell count change by frailty related phenotype at year 2\nggplot(data_no_na, aes(x = FRP_2, y = LEU3N_CHANGE, fill = FRP_2)) +\n  geom_boxplot(alpha = 0.5) +\n  theme_minimal() +\n  scale_fill_brewer(palette = \"Pastel2\") +\n  labs(title = \"Boxplot of CD4+ T Cell Count Change by Frailty Related Phenotype\",\n       x = \"Frailty Related Phenotype\",\n       y = \"CD4+ T Cell Count Change\") +\n  guides(fill = guide_legend(title = \"Frailty Related Phenotype\"))\n\nWarning: Removed 18 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\n\n\n\n\nThose with a Frailty Related Phenotype had less of an increase in CD4+ T Cell count compared to those without."
  },
  {
    "objectID": "Project_2/Project_2_R/Code/Project2.html#mental-qol-change",
    "href": "Project_2/Project_2_R/Code/Project2.html#mental-qol-change",
    "title": "Advanced Data Analysis - Project 2",
    "section": "Mental QOL Change",
    "text": "Mental QOL Change\nThe candidate variables for mental QOL change as determined by interactive variable selection are hard_drug_grp, ADH_HIGHLOW, CESD_2.\n\nModel ExplorationModel 1 - Main Research QuestionModel 2 - Depression Interaction TermConclusionBonus - Predicting Depression\n\n\n\nAnalysesDepression as a ConfounderIncluding Depression Interaction Term\n\n\nLet’s begin by running the full model as determined by interactive model selection.\n\n# Relevel to change to the reference group to never user\ndata_wide_2$hard_drugs_grp&lt;- relevel(data_wide_2$hard_drugs_grp, ref = \"Never User\")\n\n# Relevel to change to the reference group to never user\ndata_wide_2$ADH_HIGHVSLOW &lt;- relevel(data_wide_2$ADH_HIGHVSLOW, ref = \"Low Adherence\")\n\n# Perform regression on mental QOL change by hard drugs group, full model\nmodel_MENT_full1 &lt;- lm(AGG_MENT_CHANGE ~ hard_drugs_grp + ADH_HIGHVSLOW +  CESD_2 + hard_drugs_grp*ADH_HIGHVSLOW, data = data_wide_2)\n\n# Examine the summary\nsummary(model_MENT_full1)\n\n\nCall:\nlm(formula = AGG_MENT_CHANGE ~ hard_drugs_grp + ADH_HIGHVSLOW + \n    CESD_2 + hard_drugs_grp * ADH_HIGHVSLOW, data = data_wide_2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-32.537  -6.126  -1.540   5.582  38.662 \n\nCoefficients:\n                                                         Estimate Std. Error\n(Intercept)                                               3.42934    2.16906\nhard_drugs_grpPrevious User                              -6.80279    5.39588\nhard_drugs_grpCurrent User                                9.20460    6.18696\nADH_HIGHVSLOWHigh Adherence                               2.68386    2.14723\nCESD_2                                                   -0.30407    0.04914\nhard_drugs_grpPrevious User:ADH_HIGHVSLOWHigh Adherence  10.70651    5.68836\nhard_drugs_grpCurrent User:ADH_HIGHVSLOWHigh Adherence  -10.79007    6.40420\n                                                        t value      Pr(&gt;|t|)\n(Intercept)                                               1.581        0.1145\nhard_drugs_grpPrevious User                              -1.261        0.2080\nhard_drugs_grpCurrent User                                1.488        0.1374\nADH_HIGHVSLOWHigh Adherence                               1.250        0.2119\nCESD_2                                                   -6.188 0.00000000123\nhard_drugs_grpPrevious User:ADH_HIGHVSLOWHigh Adherence   1.882        0.0604\nhard_drugs_grpCurrent User:ADH_HIGHVSLOWHigh Adherence   -1.685        0.0926\n                                                           \n(Intercept)                                                \nhard_drugs_grpPrevious User                                \nhard_drugs_grpCurrent User                                 \nADH_HIGHVSLOWHigh Adherence                                \nCESD_2                                                  ***\nhard_drugs_grpPrevious User:ADH_HIGHVSLOWHigh Adherence .  \nhard_drugs_grpCurrent User:ADH_HIGHVSLOWHigh Adherence  .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 11.66 on 521 degrees of freedom\n  (22 observations deleted due to missingness)\nMultiple R-squared:  0.1089,    Adjusted R-squared:  0.09867 \nF-statistic: 10.62 on 6 and 521 DF,  p-value: 0.00000000003878\n\n\nCompare hard drug use groups at high adherence\n\n# Relevel to change to the reference group to never user\ndata_wide_2$hard_drugs_grp&lt;- relevel(data_wide_2$hard_drugs_grp, ref = \"Never User\")\n\n# Relevel to change to the reference group to never user\ndata_wide_2$ADH_HIGHVSLOW &lt;- relevel(data_wide_2$ADH_HIGHVSLOW, ref = \"High Adherence\")\n\n# Perform regression on mental QOL change by hard drugs group, full model\nmodel_MENT_full2 &lt;- lm(AGG_MENT_CHANGE ~ hard_drugs_grp + ADH_HIGHVSLOW +  CESD_2 + hard_drugs_grp*ADH_HIGHVSLOW, data = data_wide_2)\n\n# Examine the summary\nsummary(model_MENT_full2)\n\n\nCall:\nlm(formula = AGG_MENT_CHANGE ~ hard_drugs_grp + ADH_HIGHVSLOW + \n    CESD_2 + hard_drugs_grp * ADH_HIGHVSLOW, data = data_wide_2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-32.537  -6.126  -1.540   5.582  38.662 \n\nCoefficients:\n                                                        Estimate Std. Error\n(Intercept)                                              6.11320    0.81275\nhard_drugs_grpPrevious User                              3.90372    1.94422\nhard_drugs_grpCurrent User                              -1.58547    1.68651\nADH_HIGHVSLOWLow Adherence                              -2.68386    2.14723\nCESD_2                                                  -0.30407    0.04914\nhard_drugs_grpPrevious User:ADH_HIGHVSLOWLow Adherence -10.70651    5.68836\nhard_drugs_grpCurrent User:ADH_HIGHVSLOWLow Adherence   10.79007    6.40420\n                                                       t value\n(Intercept)                                              7.522\nhard_drugs_grpPrevious User                              2.008\nhard_drugs_grpCurrent User                              -0.940\nADH_HIGHVSLOWLow Adherence                              -1.250\nCESD_2                                                  -6.188\nhard_drugs_grpPrevious User:ADH_HIGHVSLOWLow Adherence  -1.882\nhard_drugs_grpCurrent User:ADH_HIGHVSLOWLow Adherence    1.685\n                                                                Pr(&gt;|t|)    \n(Intercept)                                            0.000000000000239 ***\nhard_drugs_grpPrevious User                                       0.0452 *  \nhard_drugs_grpCurrent User                                        0.3476    \nADH_HIGHVSLOWLow Adherence                                        0.2119    \nCESD_2                                                 0.000000001234665 ***\nhard_drugs_grpPrevious User:ADH_HIGHVSLOWLow Adherence            0.0604 .  \nhard_drugs_grpCurrent User:ADH_HIGHVSLOWLow Adherence             0.0926 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 11.66 on 521 degrees of freedom\n  (22 observations deleted due to missingness)\nMultiple R-squared:  0.1089,    Adjusted R-squared:  0.09867 \nF-statistic: 10.62 on 6 and 521 DF,  p-value: 0.00000000003878\n\n\nCompare hard drug use groups at low adherence\n\n# Relevel to change to the reference group to never user\ndata_wide_2$hard_drugs_grp&lt;- relevel(data_wide_2$hard_drugs_grp, ref = \"Previous User\")\n\n# Relevel to change to the reference group to never user\ndata_wide_2$ADH_HIGHVSLOW &lt;- relevel(data_wide_2$ADH_HIGHVSLOW, ref = \"Low Adherence\")\n\n# Perform regression on mental QOL change by hard drugs group, full model\nmodel_MENT_full3 &lt;- lm(AGG_MENT_CHANGE ~ hard_drugs_grp + ADH_HIGHVSLOW +  CESD_2 + hard_drugs_grp*ADH_HIGHVSLOW, data = data_wide_2)\n\n# Examine the summary\nsummary(model_MENT_full3)\n\n\nCall:\nlm(formula = AGG_MENT_CHANGE ~ hard_drugs_grp + ADH_HIGHVSLOW + \n    CESD_2 + hard_drugs_grp * ADH_HIGHVSLOW, data = data_wide_2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-32.537  -6.126  -1.540   5.582  38.662 \n\nCoefficients:\n                                                        Estimate Std. Error\n(Intercept)                                             -3.37346    5.22804\nhard_drugs_grpNever User                                 6.80279    5.39588\nhard_drugs_grpCurrent User                              16.00739    7.63391\nADH_HIGHVSLOWHigh Adherence                             13.39037    5.29906\nCESD_2                                                  -0.30407    0.04914\nhard_drugs_grpNever User:ADH_HIGHVSLOWHigh Adherence   -10.70651    5.68836\nhard_drugs_grpCurrent User:ADH_HIGHVSLOWHigh Adherence -21.49658    8.01958\n                                                       t value      Pr(&gt;|t|)\n(Intercept)                                             -0.645       0.51904\nhard_drugs_grpNever User                                 1.261       0.20797\nhard_drugs_grpCurrent User                               2.097       0.03649\nADH_HIGHVSLOWHigh Adherence                              2.527       0.01180\nCESD_2                                                  -6.188 0.00000000123\nhard_drugs_grpNever User:ADH_HIGHVSLOWHigh Adherence    -1.882       0.06037\nhard_drugs_grpCurrent User:ADH_HIGHVSLOWHigh Adherence  -2.681       0.00758\n                                                          \n(Intercept)                                               \nhard_drugs_grpNever User                                  \nhard_drugs_grpCurrent User                             *  \nADH_HIGHVSLOWHigh Adherence                            *  \nCESD_2                                                 ***\nhard_drugs_grpNever User:ADH_HIGHVSLOWHigh Adherence   .  \nhard_drugs_grpCurrent User:ADH_HIGHVSLOWHigh Adherence ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 11.66 on 521 degrees of freedom\n  (22 observations deleted due to missingness)\nMultiple R-squared:  0.1089,    Adjusted R-squared:  0.09867 \nF-statistic: 10.62 on 6 and 521 DF,  p-value: 0.00000000003878\n\n\n\n# Relevel to change to the reference group to never user\ndata_wide_2$hard_drugs_grp&lt;- relevel(data_wide_2$hard_drugs_grp, ref = \"Previous User\")\n\n# Relevel to change to the reference group to never user\ndata_wide_2$ADH_HIGHVSLOW &lt;- relevel(data_wide_2$ADH_HIGHVSLOW, ref = \"High Adherence\")\n\n# Perform regression on mental QOL change by hard drugs group, full model\nmodel_MENT_full4 &lt;- lm(AGG_MENT_CHANGE ~ hard_drugs_grp + ADH_HIGHVSLOW +  CESD_2 + hard_drugs_grp*ADH_HIGHVSLOW, data = data_wide_2)\n\n# Examine the summary\nsummary(model_MENT_full4)\n\n\nCall:\nlm(formula = AGG_MENT_CHANGE ~ hard_drugs_grp + ADH_HIGHVSLOW + \n    CESD_2 + hard_drugs_grp * ADH_HIGHVSLOW, data = data_wide_2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-32.537  -6.126  -1.540   5.582  38.662 \n\nCoefficients:\n                                                       Estimate Std. Error\n(Intercept)                                            10.01691    1.98635\nhard_drugs_grpNever User                               -3.90372    1.94422\nhard_drugs_grpCurrent User                             -5.48919    2.41509\nADH_HIGHVSLOWLow Adherence                            -13.39037    5.29906\nCESD_2                                                 -0.30407    0.04914\nhard_drugs_grpNever User:ADH_HIGHVSLOWLow Adherence    10.70651    5.68836\nhard_drugs_grpCurrent User:ADH_HIGHVSLOWLow Adherence  21.49658    8.01958\n                                                      t value      Pr(&gt;|t|)    \n(Intercept)                                             5.043 0.00000063408 ***\nhard_drugs_grpNever User                               -2.008       0.04517 *  \nhard_drugs_grpCurrent User                             -2.273       0.02344 *  \nADH_HIGHVSLOWLow Adherence                             -2.527       0.01180 *  \nCESD_2                                                 -6.188 0.00000000123 ***\nhard_drugs_grpNever User:ADH_HIGHVSLOWLow Adherence     1.882       0.06037 .  \nhard_drugs_grpCurrent User:ADH_HIGHVSLOWLow Adherence   2.681       0.00758 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 11.66 on 521 degrees of freedom\n  (22 observations deleted due to missingness)\nMultiple R-squared:  0.1089,    Adjusted R-squared:  0.09867 \nF-statistic: 10.62 on 6 and 521 DF,  p-value: 0.00000000003878\n\n\n\n# Relevel to change to the reference group to never user\ndata_wide_2$hard_drugs_grp&lt;- relevel(data_wide_2$hard_drugs_grp, ref = \"Current User\")\n\n# Relevel to change to the reference group to never user\ndata_wide_2$ADH_HIGHVSLOW &lt;- relevel(data_wide_2$ADH_HIGHVSLOW, ref = \"High Adherence\")\n\n# Perform regression on mental QOL change by hard drugs group, full model\nmodel_MENT_full5 &lt;- lm(AGG_MENT_CHANGE ~ hard_drugs_grp + ADH_HIGHVSLOW +  CESD_2 + hard_drugs_grp*ADH_HIGHVSLOW, data = data_wide_2)\n\n# Examine the summary\nsummary(model_MENT_full5)\n\n\nCall:\nlm(formula = AGG_MENT_CHANGE ~ hard_drugs_grp + ADH_HIGHVSLOW + \n    CESD_2 + hard_drugs_grp * ADH_HIGHVSLOW, data = data_wide_2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-32.537  -6.126  -1.540   5.582  38.662 \n\nCoefficients:\n                                                        Estimate Std. Error\n(Intercept)                                              4.52773    1.76048\nhard_drugs_grpPrevious User                              5.48919    2.41509\nhard_drugs_grpNever User                                 1.58547    1.68651\nADH_HIGHVSLOWLow Adherence                               8.10621    6.03476\nCESD_2                                                  -0.30407    0.04914\nhard_drugs_grpPrevious User:ADH_HIGHVSLOWLow Adherence -21.49658    8.01958\nhard_drugs_grpNever User:ADH_HIGHVSLOWLow Adherence    -10.79007    6.40420\n                                                       t value      Pr(&gt;|t|)\n(Intercept)                                              2.572       0.01039\nhard_drugs_grpPrevious User                              2.273       0.02344\nhard_drugs_grpNever User                                 0.940       0.34761\nADH_HIGHVSLOWLow Adherence                               1.343       0.17978\nCESD_2                                                  -6.188 0.00000000123\nhard_drugs_grpPrevious User:ADH_HIGHVSLOWLow Adherence  -2.681       0.00758\nhard_drugs_grpNever User:ADH_HIGHVSLOWLow Adherence     -1.685       0.09262\n                                                          \n(Intercept)                                            *  \nhard_drugs_grpPrevious User                            *  \nhard_drugs_grpNever User                                  \nADH_HIGHVSLOWLow Adherence                                \nCESD_2                                                 ***\nhard_drugs_grpPrevious User:ADH_HIGHVSLOWLow Adherence ** \nhard_drugs_grpNever User:ADH_HIGHVSLOWLow Adherence    .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 11.66 on 521 degrees of freedom\n  (22 observations deleted due to missingness)\nMultiple R-squared:  0.1089,    Adjusted R-squared:  0.09867 \nF-statistic: 10.62 on 6 and 521 DF,  p-value: 0.00000000003878\n\n\n\n# Relevel to change to the reference group to never user\ndata_wide_2$hard_drugs_grp&lt;- relevel(data_wide_2$hard_drugs_grp, ref = \"Current User\")\n\n# Relevel to change to the reference group to never user\ndata_wide_2$ADH_HIGHVSLOW &lt;- relevel(data_wide_2$ADH_HIGHVSLOW, ref = \"Low Adherence\")\n\n# Perform regression on mental QOL change by hard drugs group, full model\nmodel_MENT_full6 &lt;- lm(AGG_MENT_CHANGE ~ hard_drugs_grp + ADH_HIGHVSLOW +  CESD_2 + hard_drugs_grp*ADH_HIGHVSLOW, data = data_wide_2)\n\n# Examine the summary\nsummary(model_MENT_full6)\n\n\nCall:\nlm(formula = AGG_MENT_CHANGE ~ hard_drugs_grp + ADH_HIGHVSLOW + \n    CESD_2 + hard_drugs_grp * ADH_HIGHVSLOW, data = data_wide_2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-32.537  -6.126  -1.540   5.582  38.662 \n\nCoefficients:\n                                                         Estimate Std. Error\n(Intercept)                                              12.63393    5.89652\nhard_drugs_grpPrevious User                             -16.00739    7.63391\nhard_drugs_grpNever User                                 -9.20460    6.18696\nADH_HIGHVSLOWHigh Adherence                              -8.10621    6.03476\nCESD_2                                                   -0.30407    0.04914\nhard_drugs_grpPrevious User:ADH_HIGHVSLOWHigh Adherence  21.49658    8.01958\nhard_drugs_grpNever User:ADH_HIGHVSLOWHigh Adherence     10.79007    6.40420\n                                                        t value      Pr(&gt;|t|)\n(Intercept)                                               2.143       0.03261\nhard_drugs_grpPrevious User                              -2.097       0.03649\nhard_drugs_grpNever User                                 -1.488       0.13742\nADH_HIGHVSLOWHigh Adherence                              -1.343       0.17978\nCESD_2                                                   -6.188 0.00000000123\nhard_drugs_grpPrevious User:ADH_HIGHVSLOWHigh Adherence   2.681       0.00758\nhard_drugs_grpNever User:ADH_HIGHVSLOWHigh Adherence      1.685       0.09262\n                                                           \n(Intercept)                                             *  \nhard_drugs_grpPrevious User                             *  \nhard_drugs_grpNever User                                   \nADH_HIGHVSLOWHigh Adherence                                \nCESD_2                                                  ***\nhard_drugs_grpPrevious User:ADH_HIGHVSLOWHigh Adherence ** \nhard_drugs_grpNever User:ADH_HIGHVSLOWHigh Adherence    .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 11.66 on 521 degrees of freedom\n  (22 observations deleted due to missingness)\nMultiple R-squared:  0.1089,    Adjusted R-squared:  0.09867 \nF-statistic: 10.62 on 6 and 521 DF,  p-value: 0.00000000003878\n\n\nInterestingly, we have lost a lot of the significant relationships we were seeing before with our main predictors of hard drug use and adherence.\nIt seems that the main driver of this model, and best predictor of change in mental QOL score is depression score.\nWithin adherence levels, none of the hard drug use groups differed from each other on QOL (p-adjusted &gt; 0.05).\nThe only significant difference in mental QOL change is for previous drug users. For previous drug users, those with low adherence have a mental QOL change that is on average 13.39 points lower than those with high adherence (p= 0.0118). Depending on how we handle multiple pairwise comparisons, this may not be significant!\nSomething fishy is going on here…\nLet’s examine this in a simplified model with no interactions.\n\n# Relevel to change to the reference group to never user\ndata_wide_2$hard_drugs_grp&lt;- relevel(data_wide_2$hard_drugs_grp, ref = \"Previous User\")\n\n# Relevel to change to the reference group to never user\ndata_wide_2$ADH_HIGHVSLOW &lt;- relevel(data_wide_2$ADH_HIGHVSLOW, ref = \"Low Adherence\")\n\n# Perform regression on mental QOL change by hard drugs group, full model\nmodel_MENT_simple &lt;- lm(AGG_MENT_CHANGE ~ hard_drugs_grp + ADH_HIGHVSLOW +  CESD_2, data = data_wide_2)\n\n# Examine the summary\nsummary(model_MENT_simple)\n\n\nCall:\nlm(formula = AGG_MENT_CHANGE ~ hard_drugs_grp + ADH_HIGHVSLOW + \n    CESD_2, data = data_wide_2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-32.274  -6.421  -1.574   5.343  39.297 \n\nCoefficients:\n                            Estimate Std. Error t value        Pr(&gt;|t|)    \n(Intercept)                  6.17713    2.64623   2.334           0.020 *  \nhard_drugs_grpCurrent User  -3.44081    2.30106  -1.495           0.135    \nhard_drugs_grpNever User    -2.70142    1.85281  -1.458           0.145    \nADH_HIGHVSLOWHigh Adherence  2.94279    1.90898   1.542           0.124    \nCESD_2                      -0.32881    0.04803  -6.845 0.0000000000214 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 11.72 on 523 degrees of freedom\n  (22 observations deleted due to missingness)\nMultiple R-squared:  0.09648,   Adjusted R-squared:  0.08957 \nF-statistic: 13.96 on 4 and 523 DF,  p-value: 0.00000000007883\n\n\nIndeed, we can see that, when including depression in the model, hard drug use and adherence are no longer significant predictors of change in mental QOL!\nThis suggests that depression score is either a mediator, a moderator, or a confounder for the relationship between hard drug use and adherence on mental QOL.\nLet’s explore.\nTop of Tabset\n\n\n\nConfounder is related to PEV\nWe saw earlier that hard drug use was a significant predictor for depression. Specifically:\nHard drug use is a significant predictor of depression score (F~(2, 542)~= 14.31). On average, current hard drug users had depression scores that were 5.23 points higher than never hard drug users (p &lt; 0.0001), and previous hard drug users had depression scores that were 7.29 points higher than those who never used hard drugs (p &lt; 0.0001).\nThus, depression meets criteria 1 of the classical definition for a confounder: It is related to the PEV.\n\n\nConfounder is related to the Outcome Measure\nWe also saw in the interactive variable selection that depression was significantly related change in mental QOL change.\nThus depression meets criteria 2 of the classical definition for a confounder: it is related to the main outcome.\n\n\nConfounder is not a Mediator\nIt is possible that depression is on the causal pathway for mental QOL (hard drug use -&gt; depression -&gt; mental QOL), but it is likewise possible that mental QOL is on the causal pathway for depression (hard drug use -&gt; mental QOL -&gt; depression). Thus we cannot conclude that depression is a mediator.\n\n\nConfounder Meaningfully Changes the Relationship of PEV on Outcome\nWe also saw that including depression in the model changed hard drug use from being significant to not significant. Thus depression score changes the relationship between the PEV and outcome measure in a very meaningful way.\nI therefore feel justified in exploring depression as a confounder, and analyzing whether an interaction term between hard drug use group and depression will add explanatory power to our model.\nTop of Tabset\n\n\n\nLet’s run a new full model, with the included interaction term between hard drug use group and depression. In essence we are running a three way interaction. It will need to be emphasized in the discussion that this was exploratory data analysis.\n\n# Relevel to change to the reference group\ndata_wide_2$hard_drugs_grp&lt;- relevel(data_wide_2$hard_drugs_grp, ref = \"Current User\")\n\n# Relevel to change to the reference group\ndata_wide_2$ADH_HIGHVSLOW &lt;- relevel(data_wide_2$ADH_HIGHVSLOW, ref = \"Low Adherence\")\n\n# Perform regression on mental QOL change by hard drugs group, full model\nmodel_MENT_newfull &lt;- lm(AGG_MENT_CHANGE ~ hard_drugs_grp + ADH_HIGHVSLOW +  CESD_2 + hard_drugs_grp*ADH_HIGHVSLOW + hard_drugs_grp*CESD_2, data = data_wide_2)\n\n# Examine the summary\nsummary(model_MENT_newfull)\n\n\nCall:\nlm(formula = AGG_MENT_CHANGE ~ hard_drugs_grp + ADH_HIGHVSLOW + \n    CESD_2 + hard_drugs_grp * ADH_HIGHVSLOW + hard_drugs_grp * \n    CESD_2, data = data_wide_2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-33.459  -6.016  -1.375   5.512  37.513 \n\nCoefficients:\n                                                        Estimate Std. Error\n(Intercept)                                              9.51665    6.16622\nhard_drugs_grpPrevious User                             30.31096   10.51019\nhard_drugs_grpNever User                                -7.17735    6.52016\nADH_HIGHVSLOWHigh Adherence                             -7.87736    5.82799\nCESD_2                                                  -0.13089    0.14003\nhard_drugs_grpPrevious User:ADH_HIGHVSLOWHigh Adherence -7.15661    8.97026\nhard_drugs_grpNever User:ADH_HIGHVSLOWHigh Adherence    10.75016    6.18586\nhard_drugs_grpPrevious User:CESD_2                      -1.15503    0.21476\nhard_drugs_grpNever User:CESD_2                         -0.09391    0.14973\n                                                        t value    Pr(&gt;|t|)    \n(Intercept)                                               1.543     0.12336    \nhard_drugs_grpPrevious User                               2.884     0.00409 ** \nhard_drugs_grpNever User                                 -1.101     0.27150    \nADH_HIGHVSLOWHigh Adherence                              -1.352     0.17708    \nCESD_2                                                   -0.935     0.35038    \nhard_drugs_grpPrevious User:ADH_HIGHVSLOWHigh Adherence  -0.798     0.42534    \nhard_drugs_grpNever User:ADH_HIGHVSLOWHigh Adherence      1.738     0.08283 .  \nhard_drugs_grpPrevious User:CESD_2                       -5.378 0.000000114 ***\nhard_drugs_grpNever User:CESD_2                          -0.627     0.53083    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 11.26 on 519 degrees of freedom\n  (22 observations deleted due to missingness)\nMultiple R-squared:  0.1729,    Adjusted R-squared:  0.1601 \nF-statistic: 13.56 on 8 and 519 DF,  p-value: &lt; 0.00000000000000022\n\n\nWe get a highly significant p-value for the interaction between hard drug use group and depression (p &lt; 0.0001)\nOur adjusted R-squared also shot up from 0.090 to 0.16. We’re on to something here.\nLet’s run backwards model selection and examine the BIC to see if we need to include the hard drug use * depression interaction score.\n\nBackwards Elimination\nWe are going to force inclusion of our main variables of interest, hard_drugs_grp and ADH_HIGHVSLOW, and the interaction between them, since those were the main questions the researchers posed.\n\n# Relevel to change to the reference group to never user\ndata_wide_2$hard_drugs_grp&lt;- relevel(data_wide_2$hard_drugs_grp, ref = \"Never User\")\n\n# Relevel to change to the reference group to never user\ndata_wide_2$ADH_HIGHVSLOW &lt;- relevel(data_wide_2$ADH_HIGHVSLOW, ref = \"Low Adherence\")\n\n# Perform regression on mental QOL change by hard drugs group, full model\nmodel_MENT_newfull &lt;- lm(AGG_MENT_CHANGE ~ hard_drugs_grp + ADH_HIGHVSLOW +  CESD_2 + hard_drugs_grp*ADH_HIGHVSLOW + hard_drugs_grp*CESD_2, data = data_wide_2)\n\n# Perform backward elimination regression selection based on BIC \nols_step_backward_sbc(model_MENT_newfull, include = c(\"hard_drugs_grp\", \"ADH_HIGHVSLOW\", \"hard_drugs_grp:ADH_HIGHVSLOW\"))\n\n[1] \"No variables have been removed from the model.\"\n\n\nNo variable was removed from the model, indicating that the interaction between hard drug use group and depression adds significant explanatory power and should be included in the model.\nThis puts us in a tricky position, it is clear that there is a relationship between hard drug use and mental quality of life, and that this differs based on both adherence and depression. However we don’t necessarily have the sample size to run a three-way interaction.\n\n\nNext Steps\nThus, we will proceed in two ways from here, and run separate models for:\n\n\nDoes the relationship between hard drug use and mental QOL differ based on adherence (irrespective of depression)?\n\n\nDoes the relationship between hard drug use and mental QOL differ based on depression (irrespective of adherence)?\n\n\nRemoving the confounder of depression in model 1 allows us to answer the main research question for this project. Model 2) will be presented as an exploratory data analysis and allows us to explore how depression is related to hard drug use, and how that is affecting our main model.\nTop of Tabset\n\n\n\n\n\n\nThis is the model predicting AGG_MENT_CHANGE, including only hard_drugs_grp and ADH_HIGHVSLOW, and their interaction term.\nThis addresses the main research question of how mental QOL is impacted by ART, based on hard drug use and adherence to the treatment regimen.\n\nAnalysisInterpretationVisualizing the Interaction\n\n\nLet’s run the regression as specified.\n\n# Relevel to change to the reference group to never user\ndata_wide_2$hard_drugs_grp&lt;- relevel(data_wide_2$hard_drugs_grp, ref = \"Never User\")\n\n# Relevel to change to the reference group to never user\ndata_wide_2$ADH_HIGHVSLOW &lt;- relevel(data_wide_2$ADH_HIGHVSLOW, ref = \"Low Adherence\")\n\n# Perform regression on mental QOL change by hard drugs group, full model\nmodel_MENT_main1 &lt;- lm(AGG_MENT_CHANGE ~ hard_drugs_grp + ADH_HIGHVSLOW+ hard_drugs_grp*ADH_HIGHVSLOW, data = data_wide_2)\n\n# Examine the summary\nsummary(model_MENT_main1)\n\n\nCall:\nlm(formula = AGG_MENT_CHANGE ~ hard_drugs_grp + ADH_HIGHVSLOW + \n    hard_drugs_grp * ADH_HIGHVSLOW, data = data_wide_2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-37.392  -5.850  -0.839   5.089  38.498 \n\nCoefficients:\n                                                        Estimate Std. Error\n(Intercept)                                              -0.8003     2.0950\nhard_drugs_grpCurrent User                                7.9610     6.3718\nhard_drugs_grpPrevious User                             -15.9522     5.3413\nADH_HIGHVSLOWHigh Adherence                               3.4343     2.1812\nhard_drugs_grpCurrent User:ADH_HIGHVSLOWHigh Adherence  -11.1387     6.5996\nhard_drugs_grpPrevious User:ADH_HIGHVSLOWHigh Adherence  18.7589     5.7025\n                                                        t value Pr(&gt;|t|)   \n(Intercept)                                              -0.382  0.70261   \nhard_drugs_grpCurrent User                                1.249  0.21207   \nhard_drugs_grpPrevious User                              -2.987  0.00295 **\nADH_HIGHVSLOWHigh Adherence                               1.574  0.11598   \nhard_drugs_grpCurrent User:ADH_HIGHVSLOWHigh Adherence   -1.688  0.09204 . \nhard_drugs_grpPrevious User:ADH_HIGHVSLOWHigh Adherence   3.290  0.00107 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 12.03 on 526 degrees of freedom\n  (18 observations deleted due to missingness)\nMultiple R-squared:  0.04347,   Adjusted R-squared:  0.03438 \nF-statistic: 4.781 on 5 and 526 DF,  p-value: 0.0002777\n\n\n\n# Print useful model parameters\nmodel_results(model_MENT_main1)\n\n\n\n\n\nEstimate\nStd. Error\nt value\nPr(&gt;|t|)\np_adjusted\n2.5 %\n97.5 %\n\n\n\n\n(Intercept)\n-0.8003171\n2.095019\n-0.3820094\n0.7026087\n1.0000000\n-4.9159498\n3.315316\n\n\nhard_drugs_grpCurrent User\n7.9609869\n6.371753\n1.2494186\n0.2120677\n1.0000000\n-4.5562212\n20.478195\n\n\nhard_drugs_grpPrevious User\n-15.9522320\n5.341272\n-2.9865976\n0.0029525\n0.0177150\n-26.4450776\n-5.459386\n\n\nADH_HIGHVSLOWHigh Adherence\n3.4342726\n2.181205\n1.5744837\n0.1159769\n0.6958614\n-0.8506712\n7.719216\n\n\nhard_drugs_grpCurrent User:ADH_HIGHVSLOWHigh Adherence\n-11.1386718\n6.599562\n-1.6877895\n0.0920444\n0.5522667\n-24.1034081\n1.826064\n\n\nhard_drugs_grpPrevious User:ADH_HIGHVSLOWHigh Adherence\n18.7589336\n5.702522\n3.2895855\n0.0010705\n0.0064233\n7.5564192\n29.961448\n\n\n\n\n\n\n# Get the VIFs\nvif_values &lt;- vif(model_MENT_main1)\n\nthere are higher-order terms (interactions) in this model\nconsider setting type = 'predictor'; see ?vif\n\npretty_print(vif_values)\n\n\n\n\n\nGVIF\nDf\nGVIF^(1/(2*Df))\n\n\n\n\nhard_drugs_grp\n119.991456\n2\n3.309692\n\n\nADH_HIGHVSLOW\n1.298281\n1\n1.139422\n\n\nhard_drugs_grp:ADH_HIGHVSLOW\n121.957959\n2\n3.323170\n\n\n\n\n\n\n\nThe overall model is significant (F(5,526) = 4.78, p = 0.000278, and we are getting significant interactions.\nAt the same time, this is a weaker model than when we included depression. Our R-adjusted is now 0.034, meaning we now only explain 3.4% of the variance in mental QOL (compared to 16% in the full model).\nAt low adherence, the previous group has a greater decrease in mental QOL compared to never users.\nFor never users, there is no difference in mental QOL based on high or low adherence (t = 1.5744837, p-adjusted = 0.116, 95% CI: -0.85 to 7.72).\nChange the reference group to compare drug use group at high adherence\n\n# Relevel to change to the reference group to never user\ndata_wide_2$hard_drugs_grp&lt;- relevel(data_wide_2$hard_drugs_grp, ref = \"Never User\")\n\n# Relevel to change to the reference group to never user\ndata_wide_2$ADH_HIGHVSLOW &lt;- relevel(data_wide_2$ADH_HIGHVSLOW, ref = \"High Adherence\")\n\n# Perform regression on mental QOL change by hard drugs group, full model\nmodel_MENT_main2 &lt;- lm(AGG_MENT_CHANGE ~ hard_drugs_grp + ADH_HIGHVSLOW+ hard_drugs_grp*ADH_HIGHVSLOW, data = data_wide_2)\n\n# Examine the summary\nsummary(model_MENT_main2)\n\n\nCall:\nlm(formula = AGG_MENT_CHANGE ~ hard_drugs_grp + ADH_HIGHVSLOW + \n    hard_drugs_grp * ADH_HIGHVSLOW, data = data_wide_2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-37.392  -5.850  -0.839   5.089  38.498 \n\nCoefficients:\n                                                       Estimate Std. Error\n(Intercept)                                              2.6340     0.6071\nhard_drugs_grpCurrent User                              -3.1777     1.7190\nhard_drugs_grpPrevious User                              2.8067     1.9974\nADH_HIGHVSLOWLow Adherence                              -3.4343     2.1812\nhard_drugs_grpCurrent User:ADH_HIGHVSLOWLow Adherence   11.1387     6.5996\nhard_drugs_grpPrevious User:ADH_HIGHVSLOWLow Adherence -18.7589     5.7025\n                                                       t value  Pr(&gt;|t|)    \n(Intercept)                                              4.339 0.0000172 ***\nhard_drugs_grpCurrent User                              -1.849   0.06508 .  \nhard_drugs_grpPrevious User                              1.405   0.16056    \nADH_HIGHVSLOWLow Adherence                              -1.574   0.11598    \nhard_drugs_grpCurrent User:ADH_HIGHVSLOWLow Adherence    1.688   0.09204 .  \nhard_drugs_grpPrevious User:ADH_HIGHVSLOWLow Adherence  -3.290   0.00107 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 12.03 on 526 degrees of freedom\n  (18 observations deleted due to missingness)\nMultiple R-squared:  0.04347,   Adjusted R-squared:  0.03438 \nF-statistic: 4.781 on 5 and 526 DF,  p-value: 0.0002777\n\n\n\n# Print useful model parameters\nmodel_results(model_MENT_main2)\n\n\n\n\n\nEstimate\nStd. Error\nt value\nPr(&gt;|t|)\np_adjusted\n2.5 %\n97.5 %\n\n\n\n\n(Intercept)\n2.633955\n0.607084\n4.338700\n0.0000172\n0.0001031\n1.441349\n3.8265623\n\n\nhard_drugs_grpCurrent User\n-3.177685\n1.719008\n-1.848557\n0.0650828\n0.3904967\n-6.554649\n0.1992793\n\n\nhard_drugs_grpPrevious User\n2.806702\n1.997389\n1.405185\n0.1605566\n0.9633394\n-1.117138\n6.7305412\n\n\nADH_HIGHVSLOWLow Adherence\n-3.434273\n2.181205\n-1.574484\n0.1159769\n0.6958614\n-7.719216\n0.8506712\n\n\nhard_drugs_grpCurrent User:ADH_HIGHVSLOWLow Adherence\n11.138672\n6.599562\n1.687790\n0.0920444\n0.5522667\n-1.826064\n24.1034081\n\n\nhard_drugs_grpPrevious User:ADH_HIGHVSLOWLow Adherence\n-18.758934\n5.702522\n-3.289585\n0.0010705\n0.0064233\n-29.961448\n-7.5564192\n\n\n\n\n\n\n\nAt high adherence, there is no difference in mental QOL between previous and never hard drug users (t = 1.41, p-adjusted = 0.48, 95% CI: -1.12 to 6.73), or between current and never hard drug users (t = -1.85, p-adjusted = 0.195, 95% CI: -6.55 to 0.20).\nChange reference level to compare previous hard drug users at low adherence.\n\n# Relevel to change to the reference group to never user\ndata_wide_2$hard_drugs_grp&lt;- relevel(data_wide_2$hard_drugs_grp, ref = \"Previous User\")\n\n# Relevel to change to the reference group to never user\ndata_wide_2$ADH_HIGHVSLOW &lt;- relevel(data_wide_2$ADH_HIGHVSLOW, ref = \"Low Adherence\")\n\n# Perform regression on mental QOL change by hard drugs group, full model\nmodel_MENT_main3 &lt;- lm(AGG_MENT_CHANGE ~ hard_drugs_grp + ADH_HIGHVSLOW+ hard_drugs_grp*ADH_HIGHVSLOW, data = data_wide_2)\n\n# Examine the summary\nsummary(model_MENT_main3)\n\n\nCall:\nlm(formula = AGG_MENT_CHANGE ~ hard_drugs_grp + ADH_HIGHVSLOW + \n    hard_drugs_grp * ADH_HIGHVSLOW, data = data_wide_2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-37.392  -5.850  -0.839   5.089  38.498 \n\nCoefficients:\n                                                       Estimate Std. Error\n(Intercept)                                             -16.753      4.913\nhard_drugs_grpNever User                                 15.952      5.341\nhard_drugs_grpCurrent User                               23.913      7.769\nADH_HIGHVSLOWHigh Adherence                              22.193      5.269\nhard_drugs_grpNever User:ADH_HIGHVSLOWHigh Adherence    -18.759      5.703\nhard_drugs_grpCurrent User:ADH_HIGHVSLOWHigh Adherence  -29.898      8.158\n                                                       t value  Pr(&gt;|t|)    \n(Intercept)                                             -3.410  0.000700 ***\nhard_drugs_grpNever User                                 2.987  0.002953 ** \nhard_drugs_grpCurrent User                               3.078  0.002191 ** \nADH_HIGHVSLOWHigh Adherence                              4.212 0.0000298 ***\nhard_drugs_grpNever User:ADH_HIGHVSLOWHigh Adherence    -3.290  0.001071 ** \nhard_drugs_grpCurrent User:ADH_HIGHVSLOWHigh Adherence  -3.665  0.000273 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 12.03 on 526 degrees of freedom\n  (18 observations deleted due to missingness)\nMultiple R-squared:  0.04347,   Adjusted R-squared:  0.03438 \nF-statistic: 4.781 on 5 and 526 DF,  p-value: 0.0002777\n\n\n\n# Print useful model parameters\nmodel_results(model_MENT_main3)\n\n\n\n\n\nEstimate\nStd. Error\nt value\nPr(&gt;|t|)\np_adjusted\n2.5 %\n97.5 %\n\n\n\n\n(Intercept)\n-16.75255\n4.913256\n-3.409663\n0.0007004\n0.0042022\n-26.404563\n-7.100535\n\n\nhard_drugs_grpNever User\n15.95223\n5.341272\n2.986598\n0.0029525\n0.0177150\n5.459386\n26.445078\n\n\nhard_drugs_grpCurrent User\n23.91322\n7.768540\n3.078213\n0.0021911\n0.0131465\n8.652045\n39.174393\n\n\nADH_HIGHVSLOWHigh Adherence\n22.19321\n5.268880\n4.212130\n0.0000298\n0.0001786\n11.842574\n32.543838\n\n\nhard_drugs_grpNever User:ADH_HIGHVSLOWHigh Adherence\n-18.75893\n5.702522\n-3.289585\n0.0010705\n0.0064233\n-29.961448\n-7.556419\n\n\nhard_drugs_grpCurrent User:ADH_HIGHVSLOWHigh Adherence\n-29.89761\n8.158288\n-3.664691\n0.0002728\n0.0016365\n-45.924434\n-13.870777\n\n\n\n\n\n\n\nHere we can see that all our p-values are now very low (all have at least 2 decimal places). This gives me confidence that this is a meaningful and strong model. All the most important comparisons are with previous drug users at low adherence!\nAt low adherence, current hard drug users have a higher mental QOL (p-adjusted = 0.00657), and never hard drug users have a higher mental QOL (p-adjusted = 0.00886) compared to previous hard drug users.\nAdditionally, for previous hard drug users, those with high adherence had a higher increase in mental QOL compared to those with low adherence (p &lt; 0.0001).\nChange reference category to compare high adherence previous hard drug users.\n\n# Relevel to change to the reference group to never user\ndata_wide_2$hard_drugs_grp &lt;- relevel(data_wide_2$hard_drugs_grp, ref = \"Previous User\")\n\n# Relevel to change to the reference group to never user\ndata_wide_2$ADH_HIGHVSLOW &lt;- relevel(data_wide_2$ADH_HIGHVSLOW, ref = \"High Adherence\")\n\n# Perform regression on mental QOL change by hard drugs group, full model\nmodel_MENT_main4 &lt;- lm(AGG_MENT_CHANGE ~ hard_drugs_grp + ADH_HIGHVSLOW + hard_drugs_grp*ADH_HIGHVSLOW, data = data_wide_2)\n\n# Examine the summary\nsummary(model_MENT_main4)\n\n\nCall:\nlm(formula = AGG_MENT_CHANGE ~ hard_drugs_grp + ADH_HIGHVSLOW + \n    hard_drugs_grp * ADH_HIGHVSLOW, data = data_wide_2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-37.392  -5.850  -0.839   5.089  38.498 \n\nCoefficients:\n                                                      Estimate Std. Error\n(Intercept)                                              5.441      1.903\nhard_drugs_grpNever User                                -2.807      1.997\nhard_drugs_grpCurrent User                              -5.984      2.491\nADH_HIGHVSLOWLow Adherence                             -22.193      5.269\nhard_drugs_grpNever User:ADH_HIGHVSLOWLow Adherence     18.759      5.703\nhard_drugs_grpCurrent User:ADH_HIGHVSLOWLow Adherence   29.898      8.158\n                                                      t value  Pr(&gt;|t|)    \n(Intercept)                                             2.859  0.004416 ** \nhard_drugs_grpNever User                               -1.405  0.160557    \nhard_drugs_grpCurrent User                             -2.402  0.016654 *  \nADH_HIGHVSLOWLow Adherence                             -4.212 0.0000298 ***\nhard_drugs_grpNever User:ADH_HIGHVSLOWLow Adherence     3.290  0.001071 ** \nhard_drugs_grpCurrent User:ADH_HIGHVSLOWLow Adherence   3.665  0.000273 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 12.03 on 526 degrees of freedom\n  (18 observations deleted due to missingness)\nMultiple R-squared:  0.04347,   Adjusted R-squared:  0.03438 \nF-statistic: 4.781 on 5 and 526 DF,  p-value: 0.0002777\n\n\n\n# Print useful model parameters\nmodel_results(model_MENT_main4)\n\n\n\n\n\nEstimate\nStd. Error\nt value\nPr(&gt;|t|)\np_adjusted\n2.5 %\n97.5 %\n\n\n\n\n(Intercept)\n5.440657\n1.902896\n2.859146\n0.0044164\n0.0264985\n1.702448\n9.178866\n\n\nhard_drugs_grpNever User\n-2.806702\n1.997389\n-1.405185\n0.1605566\n0.9633394\n-6.730541\n1.117138\n\n\nhard_drugs_grpCurrent User\n-5.984387\n2.491476\n-2.401945\n0.0166539\n0.0999236\n-10.878851\n-1.089922\n\n\nADH_HIGHVSLOWLow Adherence\n-22.193206\n5.268880\n-4.212130\n0.0000298\n0.0001786\n-32.543838\n-11.842574\n\n\nhard_drugs_grpNever User:ADH_HIGHVSLOWLow Adherence\n18.758934\n5.702522\n3.289585\n0.0010705\n0.0064233\n7.556419\n29.961448\n\n\nhard_drugs_grpCurrent User:ADH_HIGHVSLOWLow Adherence\n29.897605\n8.158288\n3.664691\n0.0002728\n0.0016365\n13.870777\n45.924434\n\n\n\n\n\n\n\nAt high adherence, there is a borderline significant difference between previous and current drug users (p-adjusted = 0.050). Considering the small sample size in both categories, this could become a non significant difference with a larger N.\nChange the reference level to compare current hard drug users\n\n# Relevel to change to the reference group to never user\ndata_wide_2$hard_drugs_grp &lt;- relevel(data_wide_2$hard_drugs_grp, ref = \"Current User\")\n\n# Relevel to change to the reference group to never user\ndata_wide_2$ADH_HIGHVSLOW &lt;- relevel(data_wide_2$ADH_HIGHVSLOW, ref = \"Low Adherence\")\n\n# Perform regression on mental QOL change by hard drugs group, full model\nmodel_MENT_main5 &lt;- lm(AGG_MENT_CHANGE ~ hard_drugs_grp + ADH_HIGHVSLOW + hard_drugs_grp*ADH_HIGHVSLOW, data = data_wide_2)\n\n# Examine the summary\nsummary(model_MENT_main5)\n\n\nCall:\nlm(formula = AGG_MENT_CHANGE ~ hard_drugs_grp + ADH_HIGHVSLOW + \n    hard_drugs_grp * ADH_HIGHVSLOW, data = data_wide_2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-37.392  -5.850  -0.839   5.089  38.498 \n\nCoefficients:\n                                                        Estimate Std. Error\n(Intercept)                                                7.161      6.017\nhard_drugs_grpPrevious User                              -23.913      7.769\nhard_drugs_grpNever User                                  -7.961      6.372\nADH_HIGHVSLOWHigh Adherence                               -7.704      6.229\nhard_drugs_grpPrevious User:ADH_HIGHVSLOWHigh Adherence   29.898      8.158\nhard_drugs_grpNever User:ADH_HIGHVSLOWHigh Adherence      11.139      6.600\n                                                        t value Pr(&gt;|t|)    \n(Intercept)                                               1.190 0.234592    \nhard_drugs_grpPrevious User                              -3.078 0.002191 ** \nhard_drugs_grpNever User                                 -1.249 0.212068    \nADH_HIGHVSLOWHigh Adherence                              -1.237 0.216668    \nhard_drugs_grpPrevious User:ADH_HIGHVSLOWHigh Adherence   3.665 0.000273 ***\nhard_drugs_grpNever User:ADH_HIGHVSLOWHigh Adherence      1.688 0.092044 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 12.03 on 526 degrees of freedom\n  (18 observations deleted due to missingness)\nMultiple R-squared:  0.04347,   Adjusted R-squared:  0.03438 \nF-statistic: 4.781 on 5 and 526 DF,  p-value: 0.0002777\n\n\n\n# Print useful model parameters\nmodel_results(model_MENT_main5)\n\n\n\n\n\nEstimate\nStd. Error\nt value\nPr(&gt;|t|)\np_adjusted\n2.5 %\n97.5 %\n\n\n\n\n(Intercept)\n7.160670\n6.017485\n1.189977\n0.2345922\n1.0000000\n-4.660585\n18.981925\n\n\nhard_drugs_grpPrevious User\n-23.913219\n7.768540\n-3.078213\n0.0021911\n0.0131465\n-39.174393\n-8.652045\n\n\nhard_drugs_grpNever User\n-7.960987\n6.371753\n-1.249419\n0.2120677\n1.0000000\n-20.478195\n4.556221\n\n\nADH_HIGHVSLOWHigh Adherence\n-7.704399\n6.228689\n-1.236922\n0.2166682\n1.0000000\n-19.940561\n4.531762\n\n\nhard_drugs_grpPrevious User:ADH_HIGHVSLOWHigh Adherence\n29.897605\n8.158288\n3.664691\n0.0002728\n0.0016365\n13.870777\n45.924434\n\n\nhard_drugs_grpNever User:ADH_HIGHVSLOWHigh Adherence\n11.138672\n6.599562\n1.687790\n0.0920444\n0.5522667\n-1.826064\n24.103408\n\n\n\n\n\n\n\nFor current hard drug users, there was no difference in mental QOL based on adherence (t = -1.24, p = 0.227, 95% CI: -19.94 to 4.53).\nTop of Tabset\n\n\n\nOverall model\nThere were significant differences in change mental QOL over 2 years based on hard drug usage and adherence to the treatment regiment (F(5,526) = 4.78, p = 0.000278).\n\n\nHard Drug Use\nThe relationship between hard drug use and change in mental QOL over 2 years depended on adherence to the treatment regiment.\n\nHard Drug Use at Low Adherence\nFor those with low adherence to the treatment regiment, previous hard drug users had a 15.95 point decrease in mental QOL compared to never hard drug users (t = -2.99, p-adjusted = 0.00886, 95% CI: -26.46 to -5.50), and a 23.91 point decrease compared to current hard drug users (t = 3.078, p-adjusted = 0.00657, 95% CI: 8.65 to 39.17). There was no difference in mental QOL between current and never hard drug users at low adherence (t = 1.25, p-adjusted = 0.636, 95% CI: -4.56 to 20.48).\n\n\nHard Drug use at High Adherence\nFor those with low adherence to the treatment regiment, there was no difference in mental QOL between previous and never hard drug users (t = 1.41, p-adjusted = 0.48, 95% CI: -1.12 to 6.73), or between current and never hard drug users (t = -1.85, p-adjusted = 0.195, 95% CI: -6.55 to 0.20). The difference in mental QOL for current and previous users was borderline significant (t= -2.40, p-adjusted = 0.050, 95% CI: -10.88 to -1.09), and with a larger sample size may become non-significant.\n\n\n\nAdherence\nThe relationship between adherence to the treatment regiment and change in mental QOL over 2 years differed by hard drug use.\n\nAdherence for Current Hard Drug Users\nFor current hard drug users,there was no difference in mental QOL change over 2 years based on adherence (t = -1.24, p = 0.227, 95% CI: -19.94 to 4.53).\n\n\nAdherence for Previous Hard Drug Users\nFor previous hard drug users, there was a statistically significant difference in change in mental QOL over 2 years based on adherence (t = 4.21, p-adjusted = 0.000536). On average, previous hard drug users with high adherence had a 22.19 point increase in mental QOL over 2 years compared to those with low adherence (95% CI: 11.84 to 32.54).\n\n\nAdherence for Never Hard Drug Users\nFor never hard drug users, there was no difference in change in mental QOL over 2 years based on high or low adherence (t = 1.5744837, p-adjusted = 0.116, 95% CI: -0.85 to 7.72).\nTop of Tabset\n\n\n\n\nThe below plot is useful in interpreting the interaction\n\n# Relevel to change to the reference group to never user\ndata_wide_2$hard_drugs_grp&lt;- relevel(data_wide_2$hard_drugs_grp, ref = \"Never User\")\n\n# Relevel to change to the reference group to never user\ndata_wide_2$ADH_HIGHVSLOW &lt;- relevel(data_wide_2$ADH_HIGHVSLOW, ref = \"Low Adherence\")\n\n# Use interactions package to plot interaction\ncat_plot(\n  model_MENT_main1, \n  pred = hard_drugs_grp, \n  modx = ADH_HIGHVSLOW, \n  geom = \"line\", \n  colors = \"Pastel2\",\n  x.label = \"Hard Drugs Group\",\n  y.label = \"Predicted Mental QOL Change\")\n\n\n\n\n\n\n\n\nHere we see the main interaction finding: for previous users, those with low adherence had a decrease in mental QOL over 2 years compared to those with high adherence who had an slight increase.\nAdditionally, for those with low adherence, previous hard drug users had a decrease in mental QOL over 2 years compared to current hard drug users and never hard drug users.\nWe interpret this to mean that previous hard drug users (those in either their first or second year of sobriety) are struggling to deal with withdrawals, addiction, or the adverse effects of the treatment. While current users are in the midst of using hard drugs as a coping mechanism, and thus are protected from the negative impact of the treatment on mental QOL.\nThus, previous hard drug users are a vulnerable population, and high adherence to the treatment regiment might buffer them against the adverse effects of the HAART treatment.\nTop of Tabset\n\n\n\n\n\nThis is the model predicting AGG_MENT_CHANGE, including only hard_drugs_grp and CESD_2, and their interaction term.\nThis addresses the confounding of depression on the relationship between hard drug use and change in mental QOL.\n\nAnalysisInterpretationVisualizing the Interaction\n\n\nCompare slopes for never users.\n\n# Relevel to change to the reference group to never user\ndata_wide_2$hard_drugs_grp&lt;- relevel(data_wide_2$hard_drugs_grp, ref = \"Never User\")\n\n# Perform regression on mental QOL change by hard drugs group, full model\nmodel_MENT_sub1 &lt;- lm(AGG_MENT_CHANGE ~ hard_drugs_grp + CESD_2 + hard_drugs_grp*CESD_2, data = data_wide_2)\n\n# Examine the summary\nsummary(model_MENT_sub1)\n\n\nCall:\nlm(formula = AGG_MENT_CHANGE ~ hard_drugs_grp + CESD_2 + hard_drugs_grp * \n    CESD_2, data = data_wide_2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-34.141  -6.071  -1.545   5.758  37.779 \n\nCoefficients:\n                                   Estimate Std. Error t value       Pr(&gt;|t|)\n(Intercept)                         5.10534    0.81668   6.251 0.000000000836\nhard_drugs_grpCurrent User         -3.04165    2.90324  -1.048          0.295\nhard_drugs_grpPrevious User        16.97744    2.91062   5.833 0.000000009463\nCESD_2                             -0.23480    0.05313  -4.419 0.000011998960\nhard_drugs_grpCurrent User:CESD_2   0.10992    0.15093   0.728          0.467\nhard_drugs_grpPrevious User:CESD_2 -0.80296    0.13010  -6.172 0.000000001341\n                                      \n(Intercept)                        ***\nhard_drugs_grpCurrent User            \nhard_drugs_grpPrevious User        ***\nCESD_2                             ***\nhard_drugs_grpCurrent User:CESD_2     \nhard_drugs_grpPrevious User:CESD_2 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 11.36 on 532 degrees of freedom\n  (12 observations deleted due to missingness)\nMultiple R-squared:  0.1571,    Adjusted R-squared:  0.1492 \nF-statistic: 19.84 on 5 and 532 DF,  p-value: &lt; 0.00000000000000022\n\n# Get the VIFs\nvif_values &lt;- vif(model_MENT_sub1)\n\nthere are higher-order terms (interactions) in this model\nconsider setting type = 'predictor'; see ?vif\n\npretty_print(vif_values)\n\n\n\n\n\nGVIF\nDf\nGVIF^(1/(2*Df))\n\n\n\n\nhard_drugs_grp\n9.440857\n2\n1.752883\n\n\nCESD_2\n1.416421\n1\n1.190135\n\n\nhard_drugs_grp:CESD_2\n11.642359\n2\n1.847184\n\n\n\n\n\n\n\n\n# Print useful model parameters\nmodel_results(model_MENT_sub1)\n\n\n\n\n\nEstimate\nStd. Error\nt value\nPr(&gt;|t|)\np_adjusted\n2.5 %\n97.5 %\n\n\n\n\n(Intercept)\n5.1053388\n0.8166797\n6.2513354\n0.0000000\n0.0000000\n3.5010261\n6.7096514\n\n\nhard_drugs_grpCurrent User\n-3.0416518\n2.9032412\n-1.0476745\n0.2952643\n1.0000000\n-8.7448750\n2.6615715\n\n\nhard_drugs_grpPrevious User\n16.9774420\n2.9106247\n5.8329203\n0.0000000\n0.0000001\n11.2597144\n22.6951696\n\n\nCESD_2\n-0.2347979\n0.0531285\n-4.4194338\n0.0000120\n0.0000720\n-0.3391653\n-0.1304305\n\n\nhard_drugs_grpCurrent User:CESD_2\n0.1099198\n0.1509341\n0.7282635\n0.4667728\n1.0000000\n-0.1865801\n0.4064197\n\n\nhard_drugs_grpPrevious User:CESD_2\n-0.8029584\n0.1301049\n-6.1716247\n0.0000000\n0.0000000\n-1.0585407\n-0.5473761\n\n\n\n\n\n\n\nThe overall model is highly significant (F~(5, 532)~ = 19.84, p &lt; 0.0001, adjusted R-squared = 0.149).\nFor never hard drug users, there was a significant relationship between depression and mental QOL (t = -4.42, p &lt; 0.0001). Specifically, for never hard drug users, a 1 point increase in depression score was associated with 0.23 point decrease in mental QOL (95% CI: -0.34 to -0.13).\nCurrent hard drug users did not differ significantly from never hard drug users while controlling for the other variables in the model (t = -1.05, p-adjusted = 0.89, 95% CI: -8.74 to 2.66). Previous hard drug users had on average a 16.98 higher increase in mental QOL compared to never hard drug users (at low depression scores?) (t = 2.91062, p-adjusted &lt; 0.0001.\nThe slope for depression on mental QOL did not differ between current and never hard drug users (t = 0.73, p-adjusted = 1.00, 95% CI: -0.19 to 0.41). However, the slope for depression on mental QOL did differ between previous and never hard drug users (t = -6.17, p-adjusted &lt; 0.0001). For each point increase in depression, the mental QOL score for previous hard drug users decreases an additional 0.80 points compared to never hard drug users (95% CI: -1.06 to -0.55).\nPrevious hard drug users have an increase of approximately 16.98 units in mental QOL, holding all other variables constant. This difference is statistically significant, suggesting that prior hard drug use is associated with a considerably higher outcome measure in this model. (It’s significant because we’re not account for the interaction with this estimate.)\nCompare slopes for previous users.\n\n# Relevel to change to the reference group to never user\ndata_wide_2$hard_drugs_grp&lt;- relevel(data_wide_2$hard_drugs_grp, ref = \"Previous User\")\n\n# Perform regression on mental QOL change by hard drugs group, full model\nmodel_MENT_sub2 &lt;- lm(AGG_MENT_CHANGE ~ hard_drugs_grp + CESD_2 + hard_drugs_grp*CESD_2, data = data_wide_2)\n\n# Examine the summary\nsummary(model_MENT_sub2)\n\n\nCall:\nlm(formula = AGG_MENT_CHANGE ~ hard_drugs_grp + CESD_2 + hard_drugs_grp * \n    CESD_2, data = data_wide_2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-34.141  -6.071  -1.545   5.758  37.779 \n\nCoefficients:\n                                  Estimate Std. Error t value\n(Intercept)                        22.0828     2.7937   7.904\nhard_drugs_grpNever User          -16.9774     2.9106  -5.833\nhard_drugs_grpCurrent User        -20.0191     3.9455  -5.074\nCESD_2                             -1.0378     0.1188  -8.738\nhard_drugs_grpNever User:CESD_2     0.8030     0.1301   6.172\nhard_drugs_grpCurrent User:CESD_2   0.9129     0.1846   4.946\n                                              Pr(&gt;|t|)    \n(Intercept)                         0.0000000000000156 ***\nhard_drugs_grpNever User            0.0000000094634100 ***\nhard_drugs_grpCurrent User          0.0000005392622284 ***\nCESD_2                            &lt; 0.0000000000000002 ***\nhard_drugs_grpNever User:CESD_2     0.0000000013409353 ***\nhard_drugs_grpCurrent User:CESD_2   0.0000010165357517 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 11.36 on 532 degrees of freedom\n  (12 observations deleted due to missingness)\nMultiple R-squared:  0.1571,    Adjusted R-squared:  0.1492 \nF-statistic: 19.84 on 5 and 532 DF,  p-value: &lt; 0.00000000000000022\n\n\n\n# Print useful model parameters\nmodel_results(model_MENT_sub2)\n\n\n\n\n\nEstimate\nStd. Error\nt value\nPr(&gt;|t|)\np_adjusted\n2.5 %\n97.5 %\n\n\n\n\n(Intercept)\n22.0827808\n2.7937019\n7.904487\n0.0000000\n0.0000000\n16.5947401\n27.5708214\n\n\nhard_drugs_grpNever User\n-16.9774420\n2.9106247\n-5.832920\n0.0000000\n0.0000001\n-22.6951696\n-11.2597144\n\n\nhard_drugs_grpCurrent User\n-20.0190937\n3.9454549\n-5.073963\n0.0000005\n0.0000032\n-27.7696761\n-12.2685114\n\n\nCESD_2\n-1.0377563\n0.1187629\n-8.738048\n0.0000000\n0.0000000\n-1.2710581\n-0.8044544\n\n\nhard_drugs_grpNever User:CESD_2\n0.8029584\n0.1301049\n6.171625\n0.0000000\n0.0000000\n0.5473761\n1.0585407\n\n\nhard_drugs_grpCurrent User:CESD_2\n0.9128781\n0.1845619\n4.946190\n0.0000010\n0.0000061\n0.5503186\n1.2754377\n\n\n\n\n\n\n\nFor previous hard drug users, there was a significant relationship between depression and mental QOL (t = -8.74, p &lt; 0.0001). Specifically, for previous hard drug users, a 1 point increase in depression score was associated with a 1.04 point decrease in mental QOL (95% CI: -1.27 to -0.80). That is nearly 5 times more of a decrease compared to never users!\nCompare slopes for current users.\n\n# Relevel to change to the reference group to never user\ndata_wide_2$hard_drugs_grp&lt;- relevel(data_wide_2$hard_drugs_grp, ref = \"Current User\")\n\n# Perform regression on mental QOL change by hard drugs group, full model\nmodel_MENT_sub3 &lt;- lm(AGG_MENT_CHANGE ~ hard_drugs_grp + CESD_2 + hard_drugs_grp*CESD_2, data = data_wide_2)\n\n# Examine the summary\nsummary(model_MENT_sub3)\n\n\nCall:\nlm(formula = AGG_MENT_CHANGE ~ hard_drugs_grp + CESD_2 + hard_drugs_grp * \n    CESD_2, data = data_wide_2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-34.141  -6.071  -1.545   5.758  37.779 \n\nCoefficients:\n                                   Estimate Std. Error t value    Pr(&gt;|t|)    \n(Intercept)                          2.0637     2.7860   0.741       0.459    \nhard_drugs_grpPrevious User         20.0191     3.9455   5.074 0.000000539 ***\nhard_drugs_grpNever User             3.0417     2.9032   1.048       0.295    \nCESD_2                              -0.1249     0.1413  -0.884       0.377    \nhard_drugs_grpPrevious User:CESD_2  -0.9129     0.1846  -4.946 0.000001017 ***\nhard_drugs_grpNever User:CESD_2     -0.1099     0.1509  -0.728       0.467    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 11.36 on 532 degrees of freedom\n  (12 observations deleted due to missingness)\nMultiple R-squared:  0.1571,    Adjusted R-squared:  0.1492 \nF-statistic: 19.84 on 5 and 532 DF,  p-value: &lt; 0.00000000000000022\n\n\n\n# Print useful model parameters\nmodel_results(model_MENT_sub3)\n\n\n\n\n\nEstimate\nStd. Error\nt value\nPr(&gt;|t|)\np_adjusted\n2.5 %\n97.5 %\n\n\n\n\n(Intercept)\n2.0636870\n2.7860086\n0.7407325\n0.4591825\n1.0000000\n-3.4092406\n7.5366146\n\n\nhard_drugs_grpPrevious User\n20.0190937\n3.9454549\n5.0739634\n0.0000005\n0.0000032\n12.2685114\n27.7696761\n\n\nhard_drugs_grpNever User\n3.0416518\n2.9032412\n1.0476745\n0.2952643\n1.0000000\n-2.6615715\n8.7448750\n\n\nCESD_2\n-0.1248781\n0.1412744\n-0.8839402\n0.3771278\n1.0000000\n-0.4024022\n0.1526460\n\n\nhard_drugs_grpPrevious User:CESD_2\n-0.9128781\n0.1845619\n-4.9461896\n0.0000010\n0.0000061\n-1.2754377\n-0.5503186\n\n\nhard_drugs_grpNever User:CESD_2\n-0.1099198\n0.1509341\n-0.7282635\n0.4667728\n1.0000000\n-0.4064197\n0.1865801\n\n\n\n\n\n\n\nFor current users, the relationship between depression and mental QOL is not significant (t = 0.88, p = 0.377, 95% CI: -0.40 to 0.15).\nThe slope for depression on mental QOL depended on hard drug use (t = -4.946, p-adjusted &lt; 0.0001). For each point increase in depression, the mental QOL score for previous hard drug users decreased by an additional 0.91 points compared to current hard drug users (95%: -1.28 to -0.55).\nTop of Tabset\n\n\n\nOverall Model\nThe overall model is highly significant (F~(5, 532)~ = 19.84, p &lt; 0.0001, adjusted R-squared = 0.149).\nThe relationship between depression and mental QOL depended on hard drug use.\n\n\nDepression Slopes within Hard Drug Use Group\n\nDepression for Current Hard Drug Users\nFor current hard drug users, the relationship between depression and mental QOL is not significant (t = 0.88, p = 0.377, 95% CI: -0.40 to 0.15).\n\n\nDepression for Previous Hard Drug Users\nFor previous hard drug users, there was a significant relationship between depression and mental QOL (t = -8.74, p &lt; 0.0001). Specifically, a 1 point increase in depression score was associated with a 1.04 point decrease in mental QOL for previous hard drug users (95% CI: -1.27 to -0.80). That is nearly 5 times more of a decrease compared to never users!\n\n\nDepression for Never Hard Drug Users\nFor never hard drug users, there was a significant relationship between depression and mental QOL (t = -4.42, p &lt; 0.0001). Specifically, a 1 point increase in depression score was associated with 0.23 point decrease in mental QOL for never hard drug users (95% CI: -0.34 to -0.13).\n\n\n\nDepression Slopes between Hard Drug use Group\nThe slopes for the relationship between depression and QOL differed based on hard drug use.\nFor each point increase in depression, the mental QOL score for previous hard drug users decreased an additional 0.80 points compared to never hard drug users (t = -6.17, p-adjusted &lt; 0.0001, 95% CI: -1.06 to -0.55).\nAdditionally, for each point increase in depression, the mental QOL score for previous hard drug users decreased by an additional 0.91 points compared to current hard drug users (t = -4.946, p-adjusted &lt; 0.0001, 95%: -1.28 to -0.55).\nThe slope for depression on mental QOL did not differ between current and never hard drug users (t = 0.73, p-adjusted = 1.00, 95% CI: -0.19 to 0.41).\nTop of Tabset\n\n\n\nThe plot below is helpful in interpreting the interaction between hard drug use and depression on mental QOL.\n\n# Create a scatterplot of mental QOLchange by depression score, colored by hard drug use group.\nggplot(data_wide_2, aes(x = CESD_2, y = AGG_MENT_CHANGE, color = hard_drugs_grp)) + \n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  theme_minimal() +\n  labs(title = \"Mental QOL Change by Depression at 2 years, Colored by Hard Drug Use Group\",\n       y = \"Mental QOL Change\",\n       x = \"Depression Score\") + \n  scale_fill_brewer(palette = \"Pastel2\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 12 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 12 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nHere we can see the key relationships found in the model.\nThere is a negative relationship between depression and mental QOL scores at each level of drug usage. However, this relationship is much stronger for previous hard drug users, where every 1 point increase in depression scores is associated with an almost 5x greater decrease in mental QOL score over 2 years (~1 point for previous users compared to ~0.2 points for current and never user).\nAnd most importantly the slopes differed significantly by hard drug use group, with previous hard drug users having an additional 0.8 or 0.9 greater decrease in mental QOL score compared to never and current hard drug users, respectively.\nThus, we used this relationship as further evidence that previous hard drug users are a vulnerable group. They are susceptible to losses in mental QOL if they have low adherence to the treatment regiment, and depression has a greater impact on the mental QOL of previous hard drug users compared to current and never hard drug users.\nTop of Tabset\n\n\n\n\n\nWe found a dual interaction where the impact of hard drug use on mental QOL depended on both adherence to the treatment regimen and depression.\nFor adherence, those with previous hard drug use had a decrease in mental QOL if and only if they had low adherence to the treatment regiment.\nFor depression, previous hard drug users had a larger decrease in mental QOL compared to never and current hard drug users.\nThus, we used both of these relationships as strong evidence that previous hard drug users are a vulnerable population. They are susceptible to losses in mental QOL if they have low adherence to the treatment regiment, and depression has a greater impact on their mental QOL compared to current and never hard drug users.\nThis finding has great clinical relevance for the treatment of HIV in hard drug users. Specifically, extra efforts should be made to help previous hard drug users in two ways: One, adherence appears to serve as a buffer for previous hard drug users in the relationship between hard drug use and mental QOL. Efforts to increase adherence should therefore be made to help patients receive this ameliorative effect. Two, previous hard drug users do not have the coping mechanism of heroin or opiates available to them to buffer against the impact of adverse effects from ART, and thus extra aid perhaps in the form of increased therapy should be provided to these patients to provide them with alternate coping strategies.\nTop of Tabset\n\n\nJust for completion’s sake, we can run a regression predicting depression score based on the interaction between hard drug use and adherence.\n\n# Relevel to change to the reference group to never user\ndata_wide_2$hard_drugs_grp&lt;- relevel(data_wide_2$hard_drugs_grp, ref = \"Never User\")\n\n# Relevel to change to the reference group to never user\ndata_wide_2$ADH_HIGHVSLOW &lt;- relevel(data_wide_2$ADH_HIGHVSLOW, ref = \"Low Adherence\")\n\n# Perform regression on mental QOL change by hard drugs group, full model\nmodel_MENT_full1 &lt;- lm(CESD_2 ~ hard_drugs_grp + ADH_HIGHVSLOW+ hard_drugs_grp*ADH_HIGHVSLOW, data = data_wide_2)\n\n# Examine the summary\nsummary(model_MENT_full1)\n\n\nCall:\nlm(formula = CESD_2 ~ hard_drugs_grp + ADH_HIGHVSLOW + hard_drugs_grp * \n    ADH_HIGHVSLOW, data = data_wide_2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.679  -8.508  -2.508   6.492  38.492 \n\nCoefficients:\n                                                        Estimate Std. Error\n(Intercept)                                               13.750      1.848\nhard_drugs_grpCurrent User                                 4.250      5.544\nhard_drugs_grpPrevious User                               30.250      4.651\nADH_HIGHVSLOWHigh Adherence                               -2.242      1.921\nhard_drugs_grpCurrent User:ADH_HIGHVSLOWHigh Adherence     0.921      5.742\nhard_drugs_grpPrevious User:ADH_HIGHVSLOWHigh Adherence  -26.708      4.964\n                                                        t value\n(Intercept)                                               7.440\nhard_drugs_grpCurrent User                                0.767\nhard_drugs_grpPrevious User                               6.504\nADH_HIGHVSLOWHigh Adherence                              -1.167\nhard_drugs_grpCurrent User:ADH_HIGHVSLOWHigh Adherence    0.160\nhard_drugs_grpPrevious User:ADH_HIGHVSLOWHigh Adherence  -5.380\n                                                                 Pr(&gt;|t|)    \n(Intercept)                                             0.000000000000411 ***\nhard_drugs_grpCurrent User                                          0.444    \nhard_drugs_grpPrevious User                             0.000000000181686 ***\nADH_HIGHVSLOWHigh Adherence                                         0.244    \nhard_drugs_grpCurrent User:ADH_HIGHVSLOWHigh Adherence              0.873    \nhard_drugs_grpPrevious User:ADH_HIGHVSLOWHigh Adherence 0.000000111914533 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 10.45 on 528 degrees of freedom\n  (16 observations deleted due to missingness)\nMultiple R-squared:  0.1175,    Adjusted R-squared:  0.1091 \nF-statistic: 14.06 on 5 and 528 DF,  p-value: 0.0000000000006409\n\n\n\n# Use interactions package to plot interaction\ncat_plot(model_MENT_full1, pred = hard_drugs_grp, modx = ADH_HIGHVSLOW, geom = \"line\", colors = \"Pastel2\")\n\n\n\n\n\n\n\n\nThe interaction is significant, and we can see that previous hard drug users with low adherence had higher depression scores at year 2, and that those with high adherance were buffered from this effect.\nThe fact that depression and mental QOL both have this relationship, and are similar constructs, lends credence to the likelihood that this is a real relationship, and a strong one at that."
  },
  {
    "objectID": "Project_2/Project_2_R/Code/Project2.html#Phys",
    "href": "Project_2/Project_2_R/Code/Project2.html#Phys",
    "title": "Advanced Data Analysis - Project 2",
    "section": "Physical QOL Score",
    "text": "Physical QOL Score\nThe fourth outcome variable of interest was change physcal QOL over the first 2 years of the study.\n\nFull Model\n\n\n\nModel SelectionAnalysisInterpretationVisualizing the Interaction\n\n\nThe candidate variables for inclusion in our model predicting AGG_PHYS_CHANGE were hard_drugs_grp, ADH_HIGHVSLOW, FRP_2, CESD_2, BMI, and DKGRP_2\nWe will remove BMI based on the missing values issue identified in the mental QOL model.\nLet’s run the full model including all those variables and the interaction between hard drug use group and adherence high vs low.\n\n# Relevel to change to the reference group to never user\ndata_wide_2$hard_drugs_grp&lt;- relevel(data_wide_2$hard_drugs_grp, ref = \"Never User\")\n\n\n# Relevel to change to the reference group to never user\ndata_wide_2$ADH_HIGHVSLOW &lt;- relevel(data_wide_2$ADH_HIGHVSLOW, ref = \"Low Adherence\")\n\n# Perform regression on Physical QOL change\nmodel_PHYS_full1 &lt;- lm(AGG_PHYS_CHANGE ~ hard_drugs_grp + ADH_HIGHVSLOW + FRP_2 + CESD_2 + DKGRP_2 + hard_drugs_grp*ADH_HIGHVSLOW, data = data_wide_2)\n\n# Examine the model\nsummary(model_PHYS_full1)\n\n\nCall:\nlm(formula = AGG_PHYS_CHANGE ~ hard_drugs_grp + ADH_HIGHVSLOW + \n    FRP_2 + CESD_2 + DKGRP_2 + hard_drugs_grp * ADH_HIGHVSLOW, \n    data = data_wide_2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-32.215  -3.939   0.013   3.673  31.134 \n\nCoefficients:\n                                                          Estimate Std. Error\n(Intercept)                                              -4.584386   1.612457\nhard_drugs_grpCurrent User                              -10.266222   4.130948\nhard_drugs_grpPrevious User                             -11.067396   3.633010\nADH_HIGHVSLOWHigh Adherence                               3.209555   1.451052\nFRP_2Yes                                                -12.656122   1.481490\nCESD_2                                                    0.007245   0.033055\nDKGRP_21-3 drinks/week                                    0.651656   0.906606\nDKGRP_24-13 drinks/week                                   2.333646   1.035604\nDKGRP_2&gt;13 drinks/week                                   -0.398119   1.636008\nhard_drugs_grpCurrent User:ADH_HIGHVSLOWHigh Adherence    6.472409   4.284786\nhard_drugs_grpPrevious User:ADH_HIGHVSLOWHigh Adherence  12.329569   3.847471\n                                                        t value\n(Intercept)                                              -2.843\nhard_drugs_grpCurrent User                               -2.485\nhard_drugs_grpPrevious User                              -3.046\nADH_HIGHVSLOWHigh Adherence                               2.212\nFRP_2Yes                                                 -8.543\nCESD_2                                                    0.219\nDKGRP_21-3 drinks/week                                    0.719\nDKGRP_24-13 drinks/week                                   2.253\nDKGRP_2&gt;13 drinks/week                                   -0.243\nhard_drugs_grpCurrent User:ADH_HIGHVSLOWHigh Adherence    1.511\nhard_drugs_grpPrevious User:ADH_HIGHVSLOWHigh Adherence   3.205\n                                                                    Pr(&gt;|t|)\n(Intercept)                                                          0.00465\nhard_drugs_grpCurrent User                                           0.01327\nhard_drugs_grpPrevious User                                          0.00244\nADH_HIGHVSLOWHigh Adherence                                          0.02742\nFRP_2Yes                                                &lt; 0.0000000000000002\nCESD_2                                                               0.82659\nDKGRP_21-3 drinks/week                                               0.47260\nDKGRP_24-13 drinks/week                                              0.02466\nDKGRP_2&gt;13 drinks/week                                               0.80783\nhard_drugs_grpCurrent User:ADH_HIGHVSLOWHigh Adherence               0.13152\nhard_drugs_grpPrevious User:ADH_HIGHVSLOWHigh Adherence              0.00144\n                                                           \n(Intercept)                                             ** \nhard_drugs_grpCurrent User                              *  \nhard_drugs_grpPrevious User                             ** \nADH_HIGHVSLOWHigh Adherence                             *  \nFRP_2Yes                                                ***\nCESD_2                                                     \nDKGRP_21-3 drinks/week                                     \nDKGRP_24-13 drinks/week                                 *  \nDKGRP_2&gt;13 drinks/week                                     \nhard_drugs_grpCurrent User:ADH_HIGHVSLOWHigh Adherence     \nhard_drugs_grpPrevious User:ADH_HIGHVSLOWHigh Adherence ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.757 on 510 degrees of freedom\n  (29 observations deleted due to missingness)\nMultiple R-squared:  0.2022,    Adjusted R-squared:  0.1865 \nF-statistic: 12.92 on 10 and 510 DF,  p-value: &lt; 0.00000000000000022\n\n\n\n# Get the VIFs\nvif_values &lt;- vif(model_PHYS_full1)\n\nthere are higher-order terms (interactions) in this model\nconsider setting type = 'predictor'; see ?vif\n\npretty_print(vif_values)\n\n\n\n\n\nGVIF\nDf\nGVIF^(1/(2*Df))\n\n\n\n\nhard_drugs_grp\n125.173912\n2\n3.344864\n\n\nADH_HIGHVSLOW\n1.321733\n1\n1.149667\n\n\nFRP_2\n1.031224\n1\n1.015492\n\n\nCESD_2\n1.157182\n1\n1.075724\n\n\nDKGRP_2\n1.052687\n3\n1.008594\n\n\nhard_drugs_grp:ADH_HIGHVSLOW\n124.834338\n2\n3.342593\n\n\n\n\n\n\n\nThe overall model is highly significant (F(10,510) = 12.92, p &lt; 0.0001, Adjusted R-squared = 0.187).\nWe have some nice significant covariates, but it looks like FRP_2 is the main driver for this model.\nWe might have another double interaction situation like we did for the mental QOL model. Let’s create an interaction term between hard_drugs_grp and FRP_2 and run it through backwards elimination to select our final model based on BIC.\n\nBackwards Elimination\nWe will perform model selection using backwards elimination and BIC to select the most parsimonious model.\nA delta BIC of &gt;2 indicates a difference in model performance.\n\n# Relevel to change to the reference group to never user\ndata_wide_2$hard_drugs_grp&lt;- relevel(data_wide_2$hard_drugs_grp, ref = \"Never User\")\n\n# Relevel to change to the reference group to never user\ndata_wide_2$ADH_HIGHVSLOW &lt;- relevel(data_wide_2$ADH_HIGHVSLOW, ref = \"Low Adherence\")\n\n# Perform regression on physical QOL change\nmodel_PHYS_full1 &lt;- lm(AGG_PHYS_CHANGE ~ hard_drugs_grp + ADH_HIGHVSLOW + FRP_2 + CESD_2 + DKGRP_2 + hard_drugs_grp*ADH_HIGHVSLOW + hard_drugs_grp*FRP_2, data = data_wide_2)\n\n# Perform backward elimination regression selection based on BIC \nols_step_backward_sbc(model_PHYS_full1, include = c(\"hard_drugs_grp\", \"ADH_HIGHVSLOW\", \"hard_drugs_grp:ADH_HIGHVSLOW\"))\n\n\n                                    Stepwise Summary                                    \n--------------------------------------------------------------------------------------\nStep    Variable                  AIC         SBC         SBIC        R2       Adj. R2 \n--------------------------------------------------------------------------------------\n 0      Full Model              3622.820    3682.401    2164.416    0.21322    0.19463 \n 1      DKGRP_2                 3623.753    3670.567    2159.814    0.20268    0.18864 \n 2      CESD_2                  3621.873    3664.430    2147.887    0.20250    0.19004 \n 3      hard_drugs_grp:FRP_2    3625.025    3659.071    2143.951    0.19147    0.18204 \n--------------------------------------------------------------------------------------\n\nFinal Model Output \n------------------\n\n                         Model Summary                           \n----------------------------------------------------------------\nR                       0.438       RMSE                  7.726 \nR-Squared               0.191       MSE                  60.507 \nAdj. R-Squared          0.182       Coef. Var          -408.677 \nPred R-Squared          0.174       AIC                3625.025 \nMAE                     5.577       SBC                3659.071 \n----------------------------------------------------------------\n RMSE: Root Mean Square Error \n MSE: Mean Square Error \n MAE: Mean Absolute Error \n AIC: Akaike Information Criteria \n SBC: Schwarz Bayesian Criteria \n\n                                ANOVA                                  \n----------------------------------------------------------------------\n                 Sum of                                               \n                Squares         DF    Mean Square      F         Sig. \n----------------------------------------------------------------------\nRegression     7365.193          6       1227.532    20.287    0.0000 \nResidual      31100.658        514         60.507                     \nTotal         38465.851        520                                    \n----------------------------------------------------------------------\n\n                                                         Parameter Estimates                                                           \n--------------------------------------------------------------------------------------------------------------------------------------\n                                                  model       Beta    Std. Error    Std. Beta      t        Sig       lower     upper \n--------------------------------------------------------------------------------------------------------------------------------------\n                                            (Intercept)     -3.611         1.398                 -2.583    0.010     -6.357    -0.864 \n                             hard_drugs_grpCurrent User     -9.617         4.133       -0.352    -2.327    0.020    -17.736    -1.497 \n                            hard_drugs_grpPrevious User    -10.860         3.470       -0.351    -3.130    0.002    -17.676    -4.043 \n                            ADH_HIGHVSLOWHigh Adherence      3.202         1.452        0.100     2.205    0.028      0.349     6.055 \n                                               FRP_2Yes    -12.737         1.468       -0.345    -8.674    0.000    -15.622    -9.852 \n hard_drugs_grpCurrent User:ADH_HIGHVSLOWHigh Adherence      5.711         4.286        0.203     1.332    0.183     -2.710    14.132 \nhard_drugs_grpPrevious User:ADH_HIGHVSLOWHigh Adherence     12.158         3.713        0.368     3.274    0.001      4.863    19.453 \n--------------------------------------------------------------------------------------------------------------------------------------\n\n\nThe final model selected for AGG_PHYS_CHANGE includes the PEVs of hard_drugs_grp and ADH_HIGHVSLOW and their interaction term, and FRP_2 as a precision variable. Luckily we only have to interpret one interaction for this model!\n\n# Relevel to change to the reference group to never user\ndata_wide_2$hard_drugs_grp&lt;- relevel(data_wide_2$hard_drugs_grp, ref = \"Never User\")\n\n\n# Relevel to change to the reference group to never user\ndata_wide_2$ADH_HIGHVSLOW &lt;- relevel(data_wide_2$ADH_HIGHVSLOW, ref = \"Low Adherence\")\n\n# Perform regression on Physical QOL change\nmodel_PHYS_full1 &lt;- lm(AGG_PHYS_CHANGE ~ hard_drugs_grp + ADH_HIGHVSLOW + FRP_2 + CESD_2 + DKGRP_2 + hard_drugs_grp*ADH_HIGHVSLOW, data = data_wide_2)\n\n# Examine the model\nsummary(model_PHYS_full1)\n\n\nCall:\nlm(formula = AGG_PHYS_CHANGE ~ hard_drugs_grp + ADH_HIGHVSLOW + \n    FRP_2 + CESD_2 + DKGRP_2 + hard_drugs_grp * ADH_HIGHVSLOW, \n    data = data_wide_2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-32.215  -3.939   0.013   3.673  31.134 \n\nCoefficients:\n                                                          Estimate Std. Error\n(Intercept)                                              -4.584386   1.612457\nhard_drugs_grpCurrent User                              -10.266222   4.130948\nhard_drugs_grpPrevious User                             -11.067396   3.633010\nADH_HIGHVSLOWHigh Adherence                               3.209555   1.451052\nFRP_2Yes                                                -12.656122   1.481490\nCESD_2                                                    0.007245   0.033055\nDKGRP_21-3 drinks/week                                    0.651656   0.906606\nDKGRP_24-13 drinks/week                                   2.333646   1.035604\nDKGRP_2&gt;13 drinks/week                                   -0.398119   1.636008\nhard_drugs_grpCurrent User:ADH_HIGHVSLOWHigh Adherence    6.472409   4.284786\nhard_drugs_grpPrevious User:ADH_HIGHVSLOWHigh Adherence  12.329569   3.847471\n                                                        t value\n(Intercept)                                              -2.843\nhard_drugs_grpCurrent User                               -2.485\nhard_drugs_grpPrevious User                              -3.046\nADH_HIGHVSLOWHigh Adherence                               2.212\nFRP_2Yes                                                 -8.543\nCESD_2                                                    0.219\nDKGRP_21-3 drinks/week                                    0.719\nDKGRP_24-13 drinks/week                                   2.253\nDKGRP_2&gt;13 drinks/week                                   -0.243\nhard_drugs_grpCurrent User:ADH_HIGHVSLOWHigh Adherence    1.511\nhard_drugs_grpPrevious User:ADH_HIGHVSLOWHigh Adherence   3.205\n                                                                    Pr(&gt;|t|)\n(Intercept)                                                          0.00465\nhard_drugs_grpCurrent User                                           0.01327\nhard_drugs_grpPrevious User                                          0.00244\nADH_HIGHVSLOWHigh Adherence                                          0.02742\nFRP_2Yes                                                &lt; 0.0000000000000002\nCESD_2                                                               0.82659\nDKGRP_21-3 drinks/week                                               0.47260\nDKGRP_24-13 drinks/week                                              0.02466\nDKGRP_2&gt;13 drinks/week                                               0.80783\nhard_drugs_grpCurrent User:ADH_HIGHVSLOWHigh Adherence               0.13152\nhard_drugs_grpPrevious User:ADH_HIGHVSLOWHigh Adherence              0.00144\n                                                           \n(Intercept)                                             ** \nhard_drugs_grpCurrent User                              *  \nhard_drugs_grpPrevious User                             ** \nADH_HIGHVSLOWHigh Adherence                             *  \nFRP_2Yes                                                ***\nCESD_2                                                     \nDKGRP_21-3 drinks/week                                     \nDKGRP_24-13 drinks/week                                 *  \nDKGRP_2&gt;13 drinks/week                                     \nhard_drugs_grpCurrent User:ADH_HIGHVSLOWHigh Adherence     \nhard_drugs_grpPrevious User:ADH_HIGHVSLOWHigh Adherence ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.757 on 510 degrees of freedom\n  (29 observations deleted due to missingness)\nMultiple R-squared:  0.2022,    Adjusted R-squared:  0.1865 \nF-statistic: 12.92 on 10 and 510 DF,  p-value: &lt; 0.00000000000000022\n\n\n\n# Relevel to change to the reference group to never user\ndata_wide_2$ADH_HIGHVSLOW &lt;- relevel(data_wide_2$ADH_HIGHVSLOW, ref = \"Low Adherence\")\n\n# Perform regression on Physical QOL change by hard drugs group, full model\nmodel_PHYS_full &lt;- lm(AGG_PHYS_CHANGE ~ hard_drugs_grp + ADH_HIGHVSLOW + hard_drugs_grp*ADH_HIGHVSLOW, data = data_wide_2)\n\n# Exmamine the summary\nsummary(model_PHYS_full)\n\n\nCall:\nlm(formula = AGG_PHYS_CHANGE ~ hard_drugs_grp + ADH_HIGHVSLOW + \n    hard_drugs_grp * ADH_HIGHVSLOW, data = data_wide_2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-36.596  -3.832   0.574   4.155  33.175 \n\nCoefficients:\n                                                        Estimate Std. Error\n(Intercept)                                               -3.964      1.443\nhard_drugs_grpCurrent User                                -9.263      4.389\nhard_drugs_grpPrevious User                              -10.506      3.679\nADH_HIGHVSLOWHigh Adherence                                2.890      1.503\nhard_drugs_grpCurrent User:ADH_HIGHVSLOWHigh Adherence     4.642      4.546\nhard_drugs_grpPrevious User:ADH_HIGHVSLOWHigh Adherence   11.967      3.928\n                                                        t value Pr(&gt;|t|)   \n(Intercept)                                              -2.747  0.00622 **\nhard_drugs_grpCurrent User                               -2.110  0.03529 * \nhard_drugs_grpPrevious User                              -2.856  0.00447 **\nADH_HIGHVSLOWHigh Adherence                               1.923  0.05498 . \nhard_drugs_grpCurrent User:ADH_HIGHVSLOWHigh Adherence    1.021  0.30771   \nhard_drugs_grpPrevious User:ADH_HIGHVSLOWHigh Adherence   3.046  0.00243 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.29 on 526 degrees of freedom\n  (18 observations deleted due to missingness)\nMultiple R-squared:  0.07391,   Adjusted R-squared:  0.06511 \nF-statistic: 8.396 on 5 and 526 DF,  p-value: 0.0000001181\n\n# Use interactions package to plot interaction\ncat_plot(model_PHYS_full, pred = hard_drugs_grp, modx = ADH_HIGHVSLOW, geom = \"line\", colors = \"Pastel2\")\n\n\n\n\n\n\n\n\nTop of Tabset\n\n\n\nLet’s investigate the key relationships in the final model for AGG_PHYS_CHANGE as determined through backwards selection.\nFirst let’s look at the impact of hard drug use on physical QOL at low adherence.\n\n# Relevel to change to the reference group to never user\ndata_wide_2$hard_drugs_grp&lt;- relevel(data_wide_2$hard_drugs_grp, ref = \"Never User\")\n\n# Relevel to change to the reference group to never user\ndata_wide_2$ADH_HIGHVSLOW &lt;- relevel(data_wide_2$ADH_HIGHVSLOW, ref = \"Low Adherence\")\n\n# Perform regression on phys QOL\nmodel_PHYS_final1 &lt;- lm(AGG_PHYS_CHANGE ~ hard_drugs_grp + ADH_HIGHVSLOW + FRP_2 + hard_drugs_grp*ADH_HIGHVSLOW, data = data_wide_2)\n\n# Examine the summary\nsummary(model_PHYS_final1)\n\n\nCall:\nlm(formula = AGG_PHYS_CHANGE ~ hard_drugs_grp + ADH_HIGHVSLOW + \n    FRP_2 + hard_drugs_grp * ADH_HIGHVSLOW, data = data_wide_2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-33.074  -4.227  -0.026   3.800  32.493 \n\nCoefficients:\n                                                        Estimate Std. Error\n(Intercept)                                               -3.578      1.351\nhard_drugs_grpCurrent User                                -9.650      4.106\nhard_drugs_grpPrevious User                              -10.893      3.442\nADH_HIGHVSLOWHigh Adherence                                3.185      1.406\nFRP_2Yes                                                 -12.756      1.463\nhard_drugs_grpCurrent User:ADH_HIGHVSLOWHigh Adherence     5.713      4.255\nhard_drugs_grpPrevious User:ADH_HIGHVSLOWHigh Adherence   12.309      3.675\n                                                        t value\n(Intercept)                                              -2.648\nhard_drugs_grpCurrent User                               -2.350\nhard_drugs_grpPrevious User                              -3.164\nADH_HIGHVSLOWHigh Adherence                               2.265\nFRP_2Yes                                                 -8.719\nhard_drugs_grpCurrent User:ADH_HIGHVSLOWHigh Adherence    1.343\nhard_drugs_grpPrevious User:ADH_HIGHVSLOWHigh Adherence   3.349\n                                                                    Pr(&gt;|t|)\n(Intercept)                                                         0.008328\nhard_drugs_grpCurrent User                                          0.019144\nhard_drugs_grpPrevious User                                         0.001644\nADH_HIGHVSLOWHigh Adherence                                         0.023912\nFRP_2Yes                                                &lt; 0.0000000000000002\nhard_drugs_grpCurrent User:ADH_HIGHVSLOWHigh Adherence              0.179911\nhard_drugs_grpPrevious User:ADH_HIGHVSLOWHigh Adherence             0.000868\n                                                           \n(Intercept)                                             ** \nhard_drugs_grpCurrent User                              *  \nhard_drugs_grpPrevious User                             ** \nADH_HIGHVSLOWHigh Adherence                             *  \nFRP_2Yes                                                ***\nhard_drugs_grpCurrent User:ADH_HIGHVSLOWHigh Adherence     \nhard_drugs_grpPrevious User:ADH_HIGHVSLOWHigh Adherence ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.756 on 525 degrees of freedom\n  (18 observations deleted due to missingness)\nMultiple R-squared:  0.1911,    Adjusted R-squared:  0.1818 \nF-statistic: 20.67 on 6 and 525 DF,  p-value: &lt; 0.00000000000000022\n\n\n\n# Print useful model parameters\nmodel_results(model_PHYS_final1)\n\n\n\n\n\nEstimate\nStd. Error\nt value\nPr(&gt;|t|)\np_adjusted\n2.5 %\n97.5 %\n\n\n\n\n(Intercept)\n-3.577634\n1.350818\n-2.648494\n0.0083285\n0.0582993\n-6.2313069\n-0.9239609\n\n\nhard_drugs_grpCurrent User\n-9.649910\n4.106380\n-2.349980\n0.0191437\n0.1340061\n-17.7168637\n-1.5829561\n\n\nhard_drugs_grpPrevious User\n-10.892966\n3.442355\n-3.164394\n0.0016441\n0.0115089\n-17.6554473\n-4.1304844\n\n\nADH_HIGHVSLOWHigh Adherence\n3.184855\n1.406039\n2.265126\n0.0239116\n0.1673814\n0.4227021\n5.9470082\n\n\nFRP_2Yes\n-12.755668\n1.462953\n-8.719122\n0.0000000\n0.0000000\n-15.6296286\n-9.8817067\n\n\nhard_drugs_grpCurrent User:ADH_HIGHVSLOWHigh Adherence\n5.713316\n4.254723\n1.342817\n0.1799114\n1.0000000\n-2.6450571\n14.0716887\n\n\nhard_drugs_grpPrevious User:ADH_HIGHVSLOWHigh Adherence\n12.309256\n3.675079\n3.349385\n0.0008681\n0.0060767\n5.0895895\n19.5289225\n\n\n\n\n\n\n\nThe overall model is highly significant (F~(6, 525)~ = 20.67, p &lt; 0.0001, adjusted R-squared = 0.1818).\nAt low adherence, previous hard drug users had on average a 10.89 point decrease in physical QOL compared to never hard drug users (t = -3.16, p-adjusted = 0.00493, 95% CI: -17.66 to -4.13). Current hard drug users did not differ significantly from never hard drug users at low adherence (t = -2.35, p-adjusted =0.0574, 95% CI: -17.72 to -1.58), although with a larger sample size that could become significant.\nFor never hard drug users, those with high adherence had a 3.19 point increase in physical QOL compared to those with low adherence (t = 2.27, p = 0.0239).\nCompare hard drug use groups at high adherence\n\n# Relevel to change to the reference group to never user\ndata_wide_2$hard_drugs_grp&lt;- relevel(data_wide_2$hard_drugs_grp, ref = \"Never User\")\n\n# Relevel to change to the reference group to never user\ndata_wide_2$ADH_HIGHVSLOW &lt;- relevel(data_wide_2$ADH_HIGHVSLOW, ref = \"High Adherence\")\n\n# Perform regression on phys QOL\nmodel_PHYS_final2 &lt;- lm(AGG_PHYS_CHANGE ~ hard_drugs_grp + ADH_HIGHVSLOW + FRP_2 + hard_drugs_grp*ADH_HIGHVSLOW, data = data_wide_2)\n\n# Examine the summary\nsummary(model_PHYS_final2)\n\n\nCall:\nlm(formula = AGG_PHYS_CHANGE ~ hard_drugs_grp + ADH_HIGHVSLOW + \n    FRP_2 + hard_drugs_grp * ADH_HIGHVSLOW, data = data_wide_2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-33.074  -4.227  -0.026   3.800  32.493 \n\nCoefficients:\n                                                       Estimate Std. Error\n(Intercept)                                             -0.3928     0.3990\nhard_drugs_grpCurrent User                              -3.9366     1.1106\nhard_drugs_grpPrevious User                              1.4163     1.2872\nADH_HIGHVSLOWLow Adherence                              -3.1849     1.4060\nFRP_2Yes                                               -12.7557     1.4630\nhard_drugs_grpCurrent User:ADH_HIGHVSLOWLow Adherence   -5.7133     4.2547\nhard_drugs_grpPrevious User:ADH_HIGHVSLOWLow Adherence -12.3093     3.6751\n                                                       t value\n(Intercept)                                             -0.985\nhard_drugs_grpCurrent User                              -3.545\nhard_drugs_grpPrevious User                              1.100\nADH_HIGHVSLOWLow Adherence                              -2.265\nFRP_2Yes                                                -8.719\nhard_drugs_grpCurrent User:ADH_HIGHVSLOWLow Adherence   -1.343\nhard_drugs_grpPrevious User:ADH_HIGHVSLOWLow Adherence  -3.349\n                                                                   Pr(&gt;|t|)    \n(Intercept)                                                        0.325315    \nhard_drugs_grpCurrent User                                         0.000428 ***\nhard_drugs_grpPrevious User                                        0.271705    \nADH_HIGHVSLOWLow Adherence                                         0.023912 *  \nFRP_2Yes                                               &lt; 0.0000000000000002 ***\nhard_drugs_grpCurrent User:ADH_HIGHVSLOWLow Adherence              0.179911    \nhard_drugs_grpPrevious User:ADH_HIGHVSLOWLow Adherence             0.000868 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.756 on 525 degrees of freedom\n  (18 observations deleted due to missingness)\nMultiple R-squared:  0.1911,    Adjusted R-squared:  0.1818 \nF-statistic: 20.67 on 6 and 525 DF,  p-value: &lt; 0.00000000000000022\n\n\n\n# Print useful model parameters\nmodel_results(model_PHYS_final2)\n\n\n\n\n\nEstimate\nStd. Error\nt value\nPr(&gt;|t|)\np_adjusted\n2.5 %\n97.5 %\n\n\n\n\n(Intercept)\n-0.3927788\n0.3989561\n-0.9845163\n0.3253152\n1.0000000\n-1.176525\n0.3909676\n\n\nhard_drugs_grpCurrent User\n-3.9365941\n1.1105611\n-3.5446893\n0.0004282\n0.0029973\n-6.118283\n-1.7549047\n\n\nhard_drugs_grpPrevious User\n1.4162901\n1.2871848\n1.1003005\n0.2717054\n1.0000000\n-1.112375\n3.9449554\n\n\nADH_HIGHVSLOWLow Adherence\n-3.1848551\n1.4060388\n-2.2651261\n0.0239116\n0.1673814\n-5.947008\n-0.4227021\n\n\nFRP_2Yes\n-12.7556677\n1.4629531\n-8.7191225\n0.0000000\n0.0000000\n-15.629629\n-9.8817067\n\n\nhard_drugs_grpCurrent User:ADH_HIGHVSLOWLow Adherence\n-5.7133158\n4.2547230\n-1.3428173\n0.1799114\n1.0000000\n-14.071689\n2.6450571\n\n\nhard_drugs_grpPrevious User:ADH_HIGHVSLOWLow Adherence\n-12.3092560\n3.6750790\n-3.3493854\n0.0008681\n0.0060767\n-19.528923\n-5.0895895\n\n\n\n\n\n\n\nAt high adherence, current hard drug users had on average a 3.94 point decrease in physical QOL compared to never hard drug users (t = -3.545, p-adjusted = 0.00128, 95% CI: -6.12 to -1.75). Additionally, previous hard drug users did not differ significantly from never hard drug users at high adherence (t = 1.100, p-adjusted = 0.815, 95% CI: -1.11 to 3.94).\nChange reference level to compare previous users\n\n# Relevel to change to the reference group to never user\ndata_wide_2$hard_drugs_grp&lt;- relevel(data_wide_2$hard_drugs_grp, ref = \"Previous User\")\n\n# Relevel to change to the reference group to never user\ndata_wide_2$ADH_HIGHVSLOW &lt;- relevel(data_wide_2$ADH_HIGHVSLOW, ref = \"Low Adherence\")\n\n# Perform regression on phys QOL\nmodel_PHYS_final3 &lt;- lm(AGG_PHYS_CHANGE ~ hard_drugs_grp + ADH_HIGHVSLOW + FRP_2 + hard_drugs_grp*ADH_HIGHVSLOW, data = data_wide_2)\n\n# Examine the summary\nsummary(model_PHYS_final3)\n\n\nCall:\nlm(formula = AGG_PHYS_CHANGE ~ hard_drugs_grp + ADH_HIGHVSLOW + \n    FRP_2 + hard_drugs_grp * ADH_HIGHVSLOW, data = data_wide_2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-33.074  -4.227  -0.026   3.800  32.493 \n\nCoefficients:\n                                                       Estimate Std. Error\n(Intercept)                                             -14.471      3.166\nhard_drugs_grpNever User                                 10.893      3.442\nhard_drugs_grpCurrent User                                1.243      5.006\nADH_HIGHVSLOWHigh Adherence                              15.494      3.396\nFRP_2Yes                                                -12.756      1.463\nhard_drugs_grpNever User:ADH_HIGHVSLOWHigh Adherence    -12.309      3.675\nhard_drugs_grpCurrent User:ADH_HIGHVSLOWHigh Adherence   -6.596      5.258\n                                                       t value\n(Intercept)                                             -4.570\nhard_drugs_grpNever User                                 3.164\nhard_drugs_grpCurrent User                               0.248\nADH_HIGHVSLOWHigh Adherence                              4.562\nFRP_2Yes                                                -8.719\nhard_drugs_grpNever User:ADH_HIGHVSLOWHigh Adherence    -3.349\nhard_drugs_grpCurrent User:ADH_HIGHVSLOWHigh Adherence  -1.254\n                                                                   Pr(&gt;|t|)    \n(Intercept)                                                      0.00000608 ***\nhard_drugs_grpNever User                                           0.001644 ** \nhard_drugs_grpCurrent User                                         0.804000    \nADH_HIGHVSLOWHigh Adherence                                      0.00000631 ***\nFRP_2Yes                                               &lt; 0.0000000000000002 ***\nhard_drugs_grpNever User:ADH_HIGHVSLOWHigh Adherence               0.000868 ***\nhard_drugs_grpCurrent User:ADH_HIGHVSLOWHigh Adherence             0.210242    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.756 on 525 degrees of freedom\n  (18 observations deleted due to missingness)\nMultiple R-squared:  0.1911,    Adjusted R-squared:  0.1818 \nF-statistic: 20.67 on 6 and 525 DF,  p-value: &lt; 0.00000000000000022\n\n\n\n# Print useful model parameters\nmodel_results(model_PHYS_final3)\n\n\n\n\n\nEstimate\nStd. Error\nt value\nPr(&gt;|t|)\np_adjusted\n2.5 %\n97.5 %\n\n\n\n\n(Intercept)\n-14.470600\n3.166243\n-4.5702741\n0.0000061\n0.0000426\n-20.690662\n-8.250537\n\n\nhard_drugs_grpNever User\n10.892966\n3.442355\n3.1643937\n0.0016441\n0.0115089\n4.130484\n17.655447\n\n\nhard_drugs_grpCurrent User\n1.243056\n5.006270\n0.2482998\n0.8039995\n1.0000000\n-8.591726\n11.077838\n\n\nADH_HIGHVSLOWHigh Adherence\n15.494111\n3.396206\n4.5621830\n0.0000063\n0.0000442\n8.822290\n22.165933\n\n\nFRP_2Yes\n-12.755668\n1.462953\n-8.7191225\n0.0000000\n0.0000000\n-15.629629\n-9.881707\n\n\nhard_drugs_grpNever User:ADH_HIGHVSLOWHigh Adherence\n-12.309256\n3.675079\n-3.3493854\n0.0008681\n0.0060767\n-19.528923\n-5.089589\n\n\nhard_drugs_grpCurrent User:ADH_HIGHVSLOWHigh Adherence\n-6.595940\n5.258100\n-1.2544342\n0.2102424\n1.0000000\n-16.925439\n3.733559\n\n\n\n\n\n\n\nAt low adherence, previous hard drug users did not differ significantly on change in physical QOL compared to current hard drug users (t = 0.25, p-adjusted = 1.00, 95% CI: -8.59 to 11.08).\nFor previous hard drug users, those with high adherence to the treatment regiment had on average a 15.49 point increase in physical QOL compared to those with low adherence (t = 4.56, p-adjusted &lt; 0.0001, 95% CI: 8.82 to 22.17).\nChange the reference level to current hard drug users.\n\n# Relevel to change to the reference group to never user\ndata_wide_2$hard_drugs_grp&lt;- relevel(data_wide_2$hard_drugs_grp, ref = \"Current User\")\n\n# Relevel to change to the reference group to never user\ndata_wide_2$ADH_HIGHVSLOW &lt;- relevel(data_wide_2$ADH_HIGHVSLOW, ref = \"Low Adherence\")\n\n# Perform regression on phys QOL\nmodel_PHYS_final4 &lt;- lm(AGG_PHYS_CHANGE ~ hard_drugs_grp + ADH_HIGHVSLOW + FRP_2 + hard_drugs_grp*ADH_HIGHVSLOW, data = data_wide_2)\n\n# Examine the summary\nsummary(model_PHYS_final4)\n\n\nCall:\nlm(formula = AGG_PHYS_CHANGE ~ hard_drugs_grp + ADH_HIGHVSLOW + \n    FRP_2 + hard_drugs_grp * ADH_HIGHVSLOW, data = data_wide_2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-33.074  -4.227  -0.026   3.800  32.493 \n\nCoefficients:\n                                                        Estimate Std. Error\n(Intercept)                                              -13.228      3.878\nhard_drugs_grpPrevious User                               -1.243      5.006\nhard_drugs_grpNever User                                   9.650      4.106\nADH_HIGHVSLOWHigh Adherence                                8.898      4.017\nFRP_2Yes                                                 -12.756      1.463\nhard_drugs_grpPrevious User:ADH_HIGHVSLOWHigh Adherence    6.596      5.258\nhard_drugs_grpNever User:ADH_HIGHVSLOWHigh Adherence      -5.713      4.255\n                                                        t value\n(Intercept)                                              -3.411\nhard_drugs_grpPrevious User                              -0.248\nhard_drugs_grpNever User                                  2.350\nADH_HIGHVSLOWHigh Adherence                               2.215\nFRP_2Yes                                                 -8.719\nhard_drugs_grpPrevious User:ADH_HIGHVSLOWHigh Adherence   1.254\nhard_drugs_grpNever User:ADH_HIGHVSLOWHigh Adherence     -1.343\n                                                                    Pr(&gt;|t|)\n(Intercept)                                                         0.000697\nhard_drugs_grpPrevious User                                         0.804000\nhard_drugs_grpNever User                                            0.019144\nADH_HIGHVSLOWHigh Adherence                                         0.027180\nFRP_2Yes                                                &lt; 0.0000000000000002\nhard_drugs_grpPrevious User:ADH_HIGHVSLOWHigh Adherence             0.210242\nhard_drugs_grpNever User:ADH_HIGHVSLOWHigh Adherence                0.179911\n                                                           \n(Intercept)                                             ***\nhard_drugs_grpPrevious User                                \nhard_drugs_grpNever User                                *  \nADH_HIGHVSLOWHigh Adherence                             *  \nFRP_2Yes                                                ***\nhard_drugs_grpPrevious User:ADH_HIGHVSLOWHigh Adherence    \nhard_drugs_grpNever User:ADH_HIGHVSLOWHigh Adherence       \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.756 on 525 degrees of freedom\n  (18 observations deleted due to missingness)\nMultiple R-squared:  0.1911,    Adjusted R-squared:  0.1818 \nF-statistic: 20.67 on 6 and 525 DF,  p-value: &lt; 0.00000000000000022\n\n\n\n# Print useful model parameters\nmodel_results(model_PHYS_final4)\n\n\n\n\n\nEstimate\nStd. Error\nt value\nPr(&gt;|t|)\np_adjusted\n2.5 %\n97.5 %\n\n\n\n\n(Intercept)\n-13.227544\n3.877840\n-3.4110595\n0.0006970\n0.0048787\n-20.845533\n-5.609554\n\n\nhard_drugs_grpPrevious User\n-1.243056\n5.006270\n-0.2482998\n0.8039995\n1.0000000\n-11.077838\n8.591726\n\n\nhard_drugs_grpNever User\n9.649910\n4.106380\n2.3499799\n0.0191437\n0.1340061\n1.582956\n17.716864\n\n\nADH_HIGHVSLOWHigh Adherence\n8.898171\n4.017005\n2.2151255\n0.0271799\n0.1902593\n1.006793\n16.789549\n\n\nFRP_2Yes\n-12.755668\n1.462953\n-8.7191225\n0.0000000\n0.0000000\n-15.629629\n-9.881707\n\n\nhard_drugs_grpPrevious User:ADH_HIGHVSLOWHigh Adherence\n6.595940\n5.258100\n1.2544342\n0.2102424\n1.0000000\n-3.733559\n16.925439\n\n\nhard_drugs_grpNever User:ADH_HIGHVSLOWHigh Adherence\n-5.713316\n4.254723\n-1.3428173\n0.1799114\n1.0000000\n-14.071689\n2.645057\n\n\n\n\n\n\n\nFor current hard drug users, those with high adherence had on average an 8.90 point increase in physical QOL compared to those with low adherence (t = 2.22, p = 0.027, 95% CI: 1.00 to 16.79).\nChange reference level to high adherence to compare last group.\n\n# Relevel to change to the reference group to never user\ndata_wide_2$hard_drugs_grp&lt;- relevel(data_wide_2$hard_drugs_grp, ref = \"Current User\")\n\n# Relevel to change to the reference group to never user\ndata_wide_2$ADH_HIGHVSLOW &lt;- relevel(data_wide_2$ADH_HIGHVSLOW, ref = \"High Adherence\")\n\n# Perform regression on phys QOL\nmodel_PHYS_final5 &lt;- lm(AGG_PHYS_CHANGE ~ hard_drugs_grp + ADH_HIGHVSLOW + FRP_2 + hard_drugs_grp*ADH_HIGHVSLOW, data = data_wide_2)\n\n# Examine the summary\nsummary(model_PHYS_final5)\n\n\nCall:\nlm(formula = AGG_PHYS_CHANGE ~ hard_drugs_grp + ADH_HIGHVSLOW + \n    FRP_2 + hard_drugs_grp * ADH_HIGHVSLOW, data = data_wide_2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-33.074  -4.227  -0.026   3.800  32.493 \n\nCoefficients:\n                                                       Estimate Std. Error\n(Intercept)                                              -4.329      1.048\nhard_drugs_grpPrevious User                               5.353      1.608\nhard_drugs_grpNever User                                  3.937      1.111\nADH_HIGHVSLOWLow Adherence                               -8.898      4.017\nFRP_2Yes                                                -12.756      1.463\nhard_drugs_grpPrevious User:ADH_HIGHVSLOWLow Adherence   -6.596      5.258\nhard_drugs_grpNever User:ADH_HIGHVSLOWLow Adherence       5.713      4.255\n                                                       t value\n(Intercept)                                             -4.130\nhard_drugs_grpPrevious User                              3.329\nhard_drugs_grpNever User                                 3.545\nADH_HIGHVSLOWLow Adherence                              -2.215\nFRP_2Yes                                                -8.719\nhard_drugs_grpPrevious User:ADH_HIGHVSLOWLow Adherence  -1.254\nhard_drugs_grpNever User:ADH_HIGHVSLOWLow Adherence      1.343\n                                                                   Pr(&gt;|t|)    \n(Intercept)                                                       0.0000421 ***\nhard_drugs_grpPrevious User                                        0.000931 ***\nhard_drugs_grpNever User                                           0.000428 ***\nADH_HIGHVSLOWLow Adherence                                         0.027180 *  \nFRP_2Yes                                               &lt; 0.0000000000000002 ***\nhard_drugs_grpPrevious User:ADH_HIGHVSLOWLow Adherence             0.210242    \nhard_drugs_grpNever User:ADH_HIGHVSLOWLow Adherence                0.179911    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.756 on 525 degrees of freedom\n  (18 observations deleted due to missingness)\nMultiple R-squared:  0.1911,    Adjusted R-squared:  0.1818 \nF-statistic: 20.67 on 6 and 525 DF,  p-value: &lt; 0.00000000000000022\n\n\n\n# Print useful model parameters\nmodel_results(model_PHYS_final5)\n\n\n\n\n\nEstimate\nStd. Error\nt value\nPr(&gt;|t|)\np_adjusted\n2.5 %\n97.5 %\n\n\n\n\n(Intercept)\n-4.329373\n1.048183\n-4.130362\n0.0000421\n0.0002950\n-6.388520\n-2.270226\n\n\nhard_drugs_grpPrevious User\n5.352884\n1.607753\n3.329419\n0.0009314\n0.0065198\n2.194464\n8.511304\n\n\nhard_drugs_grpNever User\n3.936594\n1.110561\n3.544689\n0.0004282\n0.0029973\n1.754905\n6.118283\n\n\nADH_HIGHVSLOWLow Adherence\n-8.898171\n4.017005\n-2.215126\n0.0271799\n0.1902593\n-16.789549\n-1.006793\n\n\nFRP_2Yes\n-12.755668\n1.462953\n-8.719122\n0.0000000\n0.0000000\n-15.629629\n-9.881707\n\n\nhard_drugs_grpPrevious User:ADH_HIGHVSLOWLow Adherence\n-6.595940\n5.258100\n-1.254434\n0.2102424\n1.0000000\n-16.925439\n3.733559\n\n\nhard_drugs_grpNever User:ADH_HIGHVSLOWLow Adherence\n5.713316\n4.254723\n1.342817\n0.1799114\n1.0000000\n-2.645057\n14.071689\n\n\n\n\n\n\n\nAt high adherence, current hard drug users had on average a change in physical QOL that was 5.35 points less than previous hard drug users (t = 3.33, p-adjusted = 0.00279, 95% CI: 2.19 to 8.51).\nTop of Tabset\n\n\n\nOverall Model\nThere were significant differences in change in physical QOL over 2 years based on hard drug usage and adherence to the treatment regiment, while controlling for Frailty Related Phenotype (F~(6, 525)~ = 20.67, p &lt; 0.0001, adjusted R-squared = 0.1818).\n\nHard Drug Use\nThe relationship between hard drug use and change in physical QOL over 2 years depended on adherence to the treatment regiment.\n\n\nHard Drug Use at Low Adherence\nAt low adherence, previous hard drug users had on average a 10.89 point decrease in physical QOL compared to never hard drug users (t = -3.16, p-adjusted = 0.00493, 95% CI: -17.66 to -4.13), but did not differ significantly on change in physical QOL compared to current hard drug users (t = 0.25, p-adjusted = 1.00, 95% CI: -8.59 to 11.08). . Current hard drug users did not differ significantly from never hard drug users at low adherence (t = -2.35, p-adjusted =0.0574, 95% CI: -17.72 to -1.58), although with a larger sample size that could become significant.\n\n\nHard Drug Use at High Adherence\nAt high adherence, current hard drug users had on average a 3.94 point decrease in physical QOL compared to never hard drug users (t = -3.545, p-adjusted = 0.00128, 95% CI: -6.12 to -1.75), and 5.35 points decrease compared to previous hard drug users (t = 3.33, p-adjusted = 0.00279, 95% CI: 2.19 to 8.51). Additionally, previous hard drug users did not differ significantly from never hard drug users at high adherence (t = 1.100, p-adjusted = 0.815, 95% CI: -1.11 to 3.94).\n\n\n\nAdherence\nThe relationship between adherence and change in physical QOL over 2 years depended on hard drug use.\n\nAdherence for Current Hard Drug Users\nFor current hard drug users, those with high adherence had on average an 8.90 point increase in physical QOL compared to those with low adherence (t = 2.22, p = 0.027, 95% CI: 1.00 to 16.79).\n\n\nAdherence for Previous Hard Drug Users\nFor previous hard drug users, those with high adherence to the treatment regiment had on average a 15.49 point increase in physical QOL compared to those with low adherence (t = 4.56, p-adjusted &lt; 0.0001, 95% CI: 8.82 to 22.17).\n\n\nAdherence for Never Hard Drug Users\nFor never hard drug users, those with high adherence had a 3.19 point increase in physical QOL compared to those with low adherence (t = 2.27, p = 0.0239).\n\n\n\nFrailty Related Phenotype\nFrailty related phenotype was a significant predictor of change in physical QOL over 2 years (t = -8.72, p &lt; 0.0001). On average, those with a frailty related phenotype had a 12.76 decrease in physical QOL compared to those without a frailty related phenotype (95% CI: -15.63 to -9.88).\nTop of Tabset\n\n\n\nThe below plot is useful in interpreting the interaction.\n\n# Use interactions package to plot interaction\ncat_plot(\n  model_PHYS_final1, \n  pred = hard_drugs_grp, \n  modx = ADH_HIGHVSLOW, \n  geom = \"line\", \n  colors = \"Pastel2\",\n  x.label = \"Hard Drugs Group\",\n  y.label = \"Predicted Physical QOL Change\")\n\n\n\n\n\n\n\n\nFor those with low adherence, previous users had a greater decrease in physical QOL compared to never users. The difference between current and never hard drug users at low adherence is borderline significant, and may be so with a larger sample size.\nFor those with high adherence, the current hard drug use group had a greater decrease in physical QOL compared to both never and previous hard drug users.\nAnd most importantly, for previous hard drug users, those with low adherence had a decrease in physical QOL when compared to those with high adherence.\nWe take this as further evidence that previous hard drug users are at a vulnerable population. High adherence serves as a buffer and could protect patients that have quit heroin or opiates within the past 2 years.\nAdditionally, we have evidence that even at high adherence, current hard drug users have worse overall physical QOL."
  },
  {
    "objectID": "Project_2/Project_2_R/Code/Project2.html#log-viral-load-1",
    "href": "Project_2/Project_2_R/Code/Project2.html#log-viral-load-1",
    "title": "Advanced Data Analysis - Project 2",
    "section": "Log Viral Load",
    "text": "Log Viral Load\n\nLinearityIndependenceNormalityEqual Variances (Homoscedasticity)Residuals Centered Around ZeroSummary\n\n\nSince the mains IVs are categorical, we do not need to evaluate linearity.\nTop of Tabset\n\n\nIndependence can be assessed in part by the study design and how the data was collected. Based on the information provided by the PI, I will assume subjects are independent from each other.\nAdditionally, we can examine a scatter plot of the model’s residuals against any time point variable (such as ID).\n\ndata_VLOAD &lt;- data_wide_2 %&gt;%\n  dplyr::select(newid, VLOAD_log_CHANGE, hard_drugs_grp, ADH_HIGHVSLOW, EDUC_COLLEGE) %&gt;%\n  filter(\n    !is.na(newid) & \n    !is.na(VLOAD_log_CHANGE) &\n    !is.na(hard_drugs_grp) & \n    !is.na(ADH_HIGHVSLOW) & \n    !is.na(EDUC_COLLEGE) \n  )\n\n# Create a scatterplot of jackknife residuals vs ID to assess independence \nggplot(data_VLOAD, aes(x = newid, y = jackknife_residuals_VLOAD)) + \n  geom_point() +\n  geom_hline(yintercept = 0) +\n  labs(title = \"Scatterplot of Jackknife Residuals vs ID for Log Viral Load\",\n       x = \"ID\",\n       y = \"Jackknife Residuals\")\n\n\n\n\n\n\n\n\nWe may have some non-independence as seen by the further spread above 0.\nTop of Tabset\n\n\nHere we will asses that, for any fixed value of X, Y has a normal distribution. We will do this using Q-Q plots and histograms of the residuals.\n\n# Make the Q-Q plots using the jackknife residuals\nqqnorm(jackknife_residuals_VLOAD, main = \"Q-Q plots of Jackknife Residuals for Log Viral Load\")\nqqline(jackknife_residuals_VLOAD, col = \"black\")\n\n\n\n\n\n\n\n# Create histogram of jackknife residuals\nhist(jackknife_residuals_VLOAD, main = \"Histogram of Jackknife Residuals\",\n     breaks = 24,\n     xlab = \"Jackknife Residuals\",\n     col = \"lightblue\",\n     border = \"black\")\n\n\n\n\n\n\n\n\nWe can also include the Shapiro-Wilk test of normality, which provides a p-value and allows us to numerically establish that the assumption of normality is met. If the p-value is &lt; 0.05 you conclude that the assumption of normality is not met.\n\nshapiro.test(jackknife_residuals_VLOAD)\n\n\n    Shapiro-Wilk normality test\n\ndata:  jackknife_residuals_VLOAD\nW = 0.97025, p-value = 0.000000008927\n\n\nWe have some non-normality as seen by the higher end of the Q-Q Plot.\nLooking at the histograms, the pattern is slightly bimodal, and may have some outliers at the high range.\nWe also fail Shapiro-Wilk’s test, indicating non-normality.\nTop of Tabset\n\n\nTo assess homoscedasticity we examine the residual scatterplots by treatment group. The warning sign to look for here is if the variance differs greatly across groups.\n\n# Make the residual scatterplot using the jackknife residuals\nggplot(data_VLOAD, aes(x = hard_drugs_grp, y = jackknife_residuals_VLOAD)) +\n  geom_boxplot() + \n  labs(title = \"Jackknife Residuals vs Hard Drug Use for Log Viral Load\",\n       x = \"Hard Drug Use Group\",\n       y = \"JackKnife Residuals\")\n\n\n\n\n\n\n\n\n\n# Make the residual scatterplot using the jackknife residuals\nggplot(data_VLOAD, aes(x = ADH_HIGHVSLOW, y = jackknife_residuals_VLOAD)) +\n  geom_boxplot() + \n  labs(title = \"Jackknife Residuals vs Adherence for Log Viral Load\",\n       x = \"Adherence\",\n       y = \"JackKnife Residuals\")\n\n\n\n\n\n\n\n\nIt appears that our hard drug use groups have unequal variances.\nAdditionally, while it is not recommended to perform a statistical test to assess for equality of variances (because formal tests of equality of variance are not very powerful), we can still do this using Bartlett’s test. The null hypothesis of the Bartlett test is that the variances are equal. Thus failing to reject the null (p &gt; 0.05) indicates that the data are consistent with the equal variance assumption.\n\n# Perform Bartlett's test to check for homogeneity of variances\nbartlett.test(VLOAD_log_CHANGE ~ hard_drugs_grp, data = data_VLOAD)\n\n\n    Bartlett test of homogeneity of variances\n\ndata:  VLOAD_log_CHANGE by hard_drugs_grp\nBartlett's K-squared = 23.563, df = 2, p-value = 0.000007644\n\n\nWe also do not meet the assumption for equality of variances (p &lt; 0.05).\nThis is likely because our patients were not randomly assigned into these groups!\nTop of Tabset\n\n\nTo start, we can simply check that the mean of our residuals is close to 0.\n\n# Generate the mean of the jackknife residuals. Should be close to 0.\nmean(jackknife_residuals_VLOAD)\n\n[1] 0.0004681325\n\n# Generate vectors containing the fitted values vs jackknife residuals so we can plot them \nfitted_values_VLOAD &lt;- fitted(model_VLOAD_full1)\n\nggplot(data_VLOAD, aes(x = fitted_values_VLOAD, y = jackknife_residuals_VLOAD)) + \n  geom_point() +\n  geom_smooth(method = \"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nWe definitely have outliers present.\nThe mean is close to 0.\nA more sophisticated approach is to plot the fitted values vs jackknife residuals. We can then compare the trend between groups to see if they are random. The fitted line should gravitate around 0 with no obvious trends.\nTop of Tabset\n\n\nTop of Tabset"
  },
  {
    "objectID": "Project_2/Project_2_R/Code/Project2.html#cd4-t-cell-count-3",
    "href": "Project_2/Project_2_R/Code/Project2.html#cd4-t-cell-count-3",
    "title": "Advanced Data Analysis - Project 2",
    "section": "CD4+ T Cell Count",
    "text": "CD4+ T Cell Count\n\nLinearityIndependenceNormalityEqual Variances (Homoscedasticity)Residuals Centered Around ZeroSummary\n\n\nWe do not need to assess linearity since our main PEVs are categorical.\nTop of Tabset\n\n\nIndependence can be assessed in part by the study design and how the data was collected. Based on the information provided by the PI, I will assume subjects are independent from each other.\nAdditionally, we can examine a scatter plot of the model’s residuals against any time point variable (such as ID).\n\n# Filter to create same data set as in the final analysis\ndata_LEU3N &lt;- data_wide_2 %&gt;%\n  dplyr::select(newid, LEU3N_CHANGE, hard_drugs_grp, ADH_HIGHVSLOW, FRP_2) %&gt;%\n  filter(\n    !is.na(newid) & \n    !is.na(LEU3N_CHANGE) &\n    !is.na(hard_drugs_grp) & \n    !is.na(ADH_HIGHVSLOW) & \n    !is.na(FRP_2) \n  )\n\n# Create a scatterplot of jackknife residuals vs ID to assess independence \nggplot(data_LEU3N, aes(x = newid, y = jackknife_residuals_LEU3N)) + \n  geom_point() +\n  geom_hline(yintercept = 0) +\n  labs(title = \"Scatterplot of Jackknife Residuals vs ID for CD4+ T Cells\",\n       x = \"ID\",\n       y = \"Jackknife Residuals\")\n\n\n\n\n\n\n\n\nLooks like we have 4 outlier points beyound 3 residuals from 0, but otherwise looking good.\nTop of Tabset\n\n\nHere we will asses that, for any fixed value of X, Y has a normal distribution. We will do this using Q-Q plots and histograms of the residuals.\n\n# Make the Q-Q plots using the jackknife residuals\nqqnorm(jackknife_residuals_LEU3N, main = \"Q-Q plots of Jackknife Residuals for Log Viral Load\")\nqqline(jackknife_residuals_LEU3N, col = \"black\")\n\n\n\n\n\n\n\n# Create histogram of jackknife residuals\nhist(jackknife_residuals_LEU3N, main = \"Histogram of Jackknife Residuals\",\n     breaks = 24,\n     xlab = \"Jackknife Residuals\",\n     col = \"lightblue\",\n     border = \"black\")\n\n\n\n\n\n\n\n\nWe look very normal here, sans some obvious outliers.\n\nshapiro.test(jackknife_residuals_LEU3N)\n\n\n    Shapiro-Wilk normality test\n\ndata:  jackknife_residuals_LEU3N\nW = 0.97052, p-value = 0.00000001042\n\n\nWe have non-normality, but that is due to outliers.\nTop of Tabset\n\n\nTo assess homoscedasticity we examine the residual scatterplots by treatment group. The warning sign to look for here is if the variance differs greatly across groups.\n\n# Make the residual scatterplot using the jackknife residuals\nggplot(data_LEU3N, aes(x = hard_drugs_grp, y = jackknife_residuals_LEU3N)) +\n  geom_boxplot() + \n  labs(title = \"Jackknife Residuals vs Hard Drug Use for CD4+ T Cell Count Change\",\n       x = \"Hard Drug Use Group\",\n       y = \"JackKnife Residuals\")\n\n\n\n\n\n\n\n# Make the residual scatterplot using the jackknife residuals\nggplot(data_LEU3N, aes(x = ADH_HIGHVSLOW, y = jackknife_residuals_LEU3N)) +\n  geom_boxplot() + \n  labs(title = \"Jackknife Residuals vs Adherence for CD4+ T Cell Count Change\",\n       x = \"Hard Drug Use Group\",\n       y = \"JackKnife Residuals\")\n\n\n\n\n\n\n\n\n\n# Perform Bartlett's test to check for homogeneity of variances\nbartlett.test(LEU3N_CHANGE ~ hard_drugs_grp, data = data_LEU3N)\n\n\n    Bartlett test of homogeneity of variances\n\ndata:  LEU3N_CHANGE by hard_drugs_grp\nBartlett's K-squared = 5.8811, df = 2, p-value = 0.05284\n\n\nWe can actually conclude that we have equality of variances! This will look even better once we get rid of outliers.\nTop of Tabset\n\n\nto start, we can simply check that the mean of our residuals is close to 0.\n\n# Generate the mean of the jackknife residuals. Should be close to 0.\nmean(jackknife_residuals_LEU3N)\n\n[1] 0.0004433429\n\n# Generate vectors containing the fitted values vs jackknife residuals so we can plot them \nfitted_values_LEU3N &lt;- fitted(model_LEU3N_final1)\n\nggplot(data_LEU3N, aes(x = fitted_values_LEU3N, y = jackknife_residuals_LEU3N)) + \n  geom_point() +\n  geom_smooth(method = \"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nThe mean is close to 0. The residuals look centered around 0 excepting the outliers.\nTop of Tabset\n\n\nTop of Tabset"
  },
  {
    "objectID": "Project_2/Project_2_R/Code/Project2.html#mental-quality-of-life",
    "href": "Project_2/Project_2_R/Code/Project2.html#mental-quality-of-life",
    "title": "Advanced Data Analysis - Project 2",
    "section": "Mental Quality of Life",
    "text": "Mental Quality of Life\n\nLinearityIndependenceNormalityEqual Variances (Homoscedasticity)Residuals Centered Around ZeroSummary\n\n\nWe do not need to assess linearity since our main PEVs are categorical.\nTop of Tabset\n\n\nIndependence can be assessed in part by the study design and how the data was collected. Based on the information provided by the PI, I will assume subjects are independent from each other.\nAdditionally, we can examine a scatter plot of the model’s residuals against any time point variable (such as ID).\n\n# Filter to create same data set as in the final analysis\ndata_MENT_main &lt;- data_wide_2 %&gt;%\n  dplyr::select(newid, AGG_MENT_CHANGE, hard_drugs_grp, ADH_HIGHVSLOW) %&gt;%\n  filter(\n    !is.na(newid) & \n    !is.na(AGG_MENT_CHANGE) &\n    !is.na(hard_drugs_grp) & \n    !is.na(ADH_HIGHVSLOW) \n  )\n\n# Create a scatterplot of jackknife residuals vs ID to assess independence \nggplot(data_MENT_main, aes(x = newid, y = jackknife_residuals_MENT_main)) + \n  geom_point() +\n  geom_hline(yintercept = 0) +\n  labs(title = \"Scatterplot of Jackknife Residuals vs ID for Mental QOL Change\",\n       x = \"ID\",\n       y = \"Jackknife Residuals\")\n\n\n\n\n\n\n\n\nNo clear pattern, we meet the assumption of independence.\nTop of Tabset\n\n\nHere we will asses that, for any fixed value of X, Y has a normal distribution. We will do this using Q-Q plots and histograms of the residuals.\n\n# Make the Q-Q plots using the jackknife residuals\nqqnorm(jackknife_residuals_MENT_main, main = \"Q-Q plots of Jackknife Residuals for Mental QOL Change\")\nqqline(jackknife_residuals_MENT_main, col = \"black\")\n\n\n\n\n\n\n\n# Create histogram of jackknife residuals\nhist(jackknife_residuals_MENT_main, main = \"Histogram of Jackknife Residuals\",\n     breaks = 24,\n     xlab = \"Jackknife Residuals\",\n     col = \"lightblue\",\n     border = \"black\")\n\n\n\n\n\n\n\n\n\nshapiro.test(jackknife_residuals_MENT_main)\n\n\n    Shapiro-Wilk normality test\n\ndata:  jackknife_residuals_MENT_main\nW = 0.96193, p-value = 0.0000000001675\n\n\nWe have non-normality. It does not look like outliers are driving this.\nTop of Tabset\n\n\nTo assess homoscedasticity we examine the residual scatterplots by treatment group. The warning sign to look for here is if the variance differs greatly across groups.\n\n# Make the residual scatterplot using the jackknife residuals\nggplot(data_MENT_main, aes(x = hard_drugs_grp, y = jackknife_residuals_MENT_main)) +\n  geom_boxplot() + \n  labs(title = \"Jackknife Residuals vs Hard Drug Use for Mental QOL Change\",\n       x = \"Hard Drug Use Group\",\n       y = \"JackKnife Residuals\")\n\n\n\n\n\n\n\n\n\n# Make the residual scatterplot using the jackknife residuals\nggplot(data_MENT_main, aes(x = ADH_HIGHVSLOW, y = jackknife_residuals_MENT_main)) +\n  geom_boxplot() + \n  labs(title = \"Jackknife Residuals vs Adherence for Mental QOL Change\",\n       x = \"Hard Drug Use Group\",\n       y = \"JackKnife Residuals\")\n\n\n\n\n\n\n\n\n\n# Perform Bartlett's test to check for homogeneity of variances\nbartlett.test(AGG_MENT_CHANGE ~ hard_drugs_grp, data = data_MENT_main)\n\n\n    Bartlett test of homogeneity of variances\n\ndata:  AGG_MENT_CHANGE by hard_drugs_grp\nBartlett's K-squared = 19.522, df = 2, p-value = 0.00005765\n\n\nThe test is significant, and we conclude we do not have homogeneity of variances.\nTop of Tabset\n\n\n\n# Generate the mean of the jackknife residuals. Should be close to 0.\nmean(jackknife_residuals_MENT_main)\n\n[1] 0.0001715436\n\n# Generate vectors containing the fitted values vs jackknife residuals so we can plot them \nfitted_values_MENT_main &lt;- fitted(model_MENT_main1)\n\nggplot(data_MENT_main, aes(x = fitted_values_MENT_main, y = jackknife_residuals_MENT_main)) + \n  geom_point() +\n  geom_smooth(method = \"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nOur residuals look centered around 0, sans a few outlier points.\nTop of Tabset\n\n\nWe have non-normality and non homogeneity of variances.\nTop of Tabset"
  },
  {
    "objectID": "Project_2/Project_2_R/Code/Project2.html#physical-quality-of-life",
    "href": "Project_2/Project_2_R/Code/Project2.html#physical-quality-of-life",
    "title": "Advanced Data Analysis - Project 2",
    "section": "Physical Quality of Life",
    "text": "Physical Quality of Life\n\nLinearityIndependenceNormalityEqual Variances (Homoscedasticity)Residuals Centered Around ZeroSummaryResiduals Centered Around ZeroSummary\n\n\nWe do not need to assess linearity since our main PEVs are categorical.\nTop of Tabset\n\n\nIndependence can be assessed in part by the study design and how the data was collected. Based on the information provided by the PI, I will assume subjects are independent from each other.\nAdditionally, we can examine a scatter plot of the model’s residuals against any time point variable (such as ID).\n\n# Filter to create same data set as in the final analysis\ndata_PHYS &lt;- data_wide_2 %&gt;%\n  dplyr::select(newid, AGG_PHYS_CHANGE, hard_drugs_grp, ADH_HIGHVSLOW, FRP_2) %&gt;%\n  filter(\n    !is.na(newid) & \n    !is.na(AGG_PHYS_CHANGE) &\n    !is.na(hard_drugs_grp) & \n    !is.na(ADH_HIGHVSLOW) &\n    !is.na(FRP_2)\n  )\n\n# Create a scatterplot of jackknife residuals vs ID to assess independence \nggplot(data_PHYS, aes(x = newid, y = jackknife_residuals_PHYS)) + \n  geom_point() +\n  geom_hline(yintercept = 0) +\n  labs(title = \"Scatterplot of Jackknife Residuals vs ID for Physical QOL Change\",\n       x = \"ID\",\n       y = \"Jackknife Residuals\")\n\n\n\n\n\n\n\n\nIndependence looks pretty good.\nTop of Tabset\n\n\nHere we will asses that, for any fixed value of X, Y has a normal distribution. We will do this using Q-Q plots and histograms of the residuals.\n\n# Make the Q-Q plots using the jackknife residuals\nqqnorm(jackknife_residuals_PHYS, main = \"Q-Q plots of Jackknife Residuals for Mental QOL Change\")\nqqline(jackknife_residuals_PHYS, col = \"black\")\n\n\n\n\n\n\n\n# Create histogram of jackknife residuals\nhist(jackknife_residuals_PHYS, main = \"Histogram of Jackknife Residuals\",\n     breaks = 24,\n     xlab = \"Jackknife Residuals\",\n     col = \"lightblue\",\n     border = \"black\")\n\n\n\n\n\n\n\nshapiro.test(jackknife_residuals_PHYS)\n\n\n    Shapiro-Wilk normality test\n\ndata:  jackknife_residuals_PHYS\nW = 0.9682, p-value = 0.000000002488\n\n\nPhysical QOL looks mostly normal. There may be 1 outier at the upper and lower bound.\nShapiro Wilk’s does say we fail normality however.\nTop of Tabset\n\n\nTo assess homoscedasticity we examine the residual scatterplots by treatment group. The warning sign to look for here is if the variance differs greatly across groups.\n\n# Make the residual scatterplot using the jackknife residuals\nggplot(data_PHYS, aes(x = hard_drugs_grp, y = jackknife_residuals_PHYS)) +\n  geom_boxplot() + \n  labs(title = \"Jackknife Residuals vs Hard Drug Use for Physical QOL Change\",\n       x = \"Hard Drug Use Group\",\n       y = \"JackKnife Residuals\")\n\n\n\n\n\n\n\n# Perform Bartlett's test to check for homogeneity of variances\nbartlett.test(AGG_PHYS_CHANGE ~ hard_drugs_grp, data = data_PHYS)\n\n\n    Bartlett test of homogeneity of variances\n\ndata:  AGG_PHYS_CHANGE by hard_drugs_grp\nBartlett's K-squared = 4.2398, df = 2, p-value = 0.12\n\n\nWe do have homogeneity of variances for physical QOL.\nTop of Tabset\n\n\n\n# Generate the mean of the jackknife residuals. Should be close to 0.\nmean(jackknife_residuals_PHYS)\n\n[1] 0.0002469251\n\n# Generate vectors containing the fitted values vs jackknife residuals so we can plot them \nfitted_values_PHYS &lt;- fitted(model_PHYS_final1)\n\nggplot(data_PHYS, aes(x = fitted_values_PHYS, y = jackknife_residuals_PHYS)) + \n  geom_point() +\n  geom_smooth(method = \"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nTop of Tabset\n\n\nThe assumptions for CD4+ T cell count are almost met. Likely once removing those 4 or so outliers they would be perfect.\nTop of Tabset\nTo assess homoscedasticity we examine the residual scatterplots by treatment group. The warning sign to look for here is if the variance differs greatly across groups.\n\n# Make the residual scatterplot using the jackknife residuals\nggplot(data_PHYS, aes(x = hard_drugs_grp, y = jackknife_residuals_PHYS)) +\n  geom_boxplot() + \n  labs(title = \"Jackknife Residuals vs Hard Drug Use for Physical QOL Change\",\n       x = \"Hard Drug Use Group\",\n       y = \"JackKnife Residuals\")\n\n\n\n\n\n\n\n\n\n# Make the residual scatterplot using the jackknife residuals\nggplot(data_PHYS, aes(x = ADH_HIGHVSLOW, y = jackknife_residuals_PHYS)) +\n  geom_boxplot() + \n  labs(title = \"Jackknife Residuals vs Adherence for Physical QOL Change\",\n       x = \"Hard Drug Use Group\",\n       y = \"JackKnife Residuals\")\n\n\n\n\n\n\n\n\n\n# Perform Bartlett's test to check for homogeneity of variances\nbartlett.test(AGG_PHYS_CHANGE ~ hard_drugs_grp, data = data_PHYS)\n\n\n    Bartlett test of homogeneity of variances\n\ndata:  AGG_PHYS_CHANGE by hard_drugs_grp\nBartlett's K-squared = 4.2398, df = 2, p-value = 0.12\n\n\nWe do have homogeneity of variances.\n\n\n\n# Generate the mean of the jackknife residuals. Should be close to 0.\nmean(jackknife_residuals_PHYS)\n\n[1] 0.0002469251\n\n# Generate vectors containing the fitted values vs jackknife residuals so we can plot them \nfitted_values_PHYS &lt;- fitted(model_PHYS_final1)\n\nggplot(data_PHYS, aes(x = fitted_values_PHYS, y = jackknife_residuals_PHYS)) + \n  geom_point() +\n  geom_smooth(method = \"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nThe residuals appear centered around zero, for the most part.\n\n\nWe do not meet the assumption of normality for physical QOL."
  },
  {
    "objectID": "Project_2/Project_2_R/Code/Project2.html#plotting-outcome-variables-across-full-8-years",
    "href": "Project_2/Project_2_R/Code/Project2.html#plotting-outcome-variables-across-full-8-years",
    "title": "Advanced Data Analysis - Project 2",
    "section": "Plotting Outcome Variables Across Full 8 Years",
    "text": "Plotting Outcome Variables Across Full 8 Years\nThe below plots were used as justification and evidence for separating adherence from four levels to two levels, as this seemed to be whether the most meaninfgul and true split was.\n\nMental QOL\n\n# Get rid of that value of 1 for adherence at baseline for patient 426\ndata$ADH[data$years == 0] &lt;- NA\n\n# Get means for each year\nsummary_data &lt;- data %&gt;%\n  group_by(ADH, years) %&gt;%\n  summarize(Average_QOL = mean(AGG_MENT, na.rm = TRUE))\n\n`summarise()` has grouped output by 'ADH'. You can override using the `.groups`\nargument.\n\n# Create plot\np &lt;- ggplot(summary_data, aes(x = years, y = Average_QOL, color = ADH)) +\n  geom_line(size = 1) +   # Line plot\n  geom_point(size = 2) +  # Add points for clarity\n  labs(title = \"Average Mental QOL Score by Adherence Group Over 8 Years\",\n       x = \"Year\",\n       y = \"Average Mental QOL Score\",\n       color = \"Adherence Group\") +\n  theme_minimal() +       # Clean theme\n  scale_color_brewer(palette = \"Pastel2\")  # Nice color palette for groups\n\nggplotly(p)\n\n\n\n\n\nSuccess! This is very interesting. It appears that the 100% adherence group has the highest average mental QOL score across the 8 year study, followed by the 95-99% group, followed by the &lt;75% group and followed by (with some variation) the &lt;75% and 75-94% groups.\n\n\nPhysical QOL\nLet’s do the same thing but for AGG_PHYS.\n\n# Get means for each year\nsummary_data &lt;- data %&gt;%\n  group_by(ADH, years) %&gt;%\n  summarize(Average_QOL = mean(AGG_PHYS, na.rm = TRUE))\n\n`summarise()` has grouped output by 'ADH'. You can override using the `.groups`\nargument.\n\n# Create plot\nggplot(summary_data, aes(x = years, y = Average_QOL, color = ADH)) +\n  geom_line(size = 1) +   # Line plot\n  geom_point(size = 2) +  # Add points for clarity\n  labs(title = \"Average Physical QOL Score by Adherence Group Over 8 Years\",\n       x = \"Year\",\n       y = \"Average Physical QOL Score\",\n       color = \"Adherence Group\") +\n  theme_minimal() +       # Clean theme\n  scale_color_brewer(palette = \"Pastel2\")  # Nice color palette for groups\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_line()`).\n\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nWe see a similiar relationship here, though there’s more variation. For some reason that &lt;75% group likes to spike in their QOL at year 7. Interesting.\n\n\nLog Viral Load\n\n# Get means for each year\nsummary_data &lt;- data %&gt;%\n  group_by(ADH, years) %&gt;%\n  summarize(Average_LEU3N = mean(LEU3N, na.rm = TRUE))\n\n`summarise()` has grouped output by 'ADH'. You can override using the `.groups`\nargument.\n\n# Create plot\np &lt;- ggplot(summary_data, aes(x = years, y = Average_LEU3N, color = ADH)) +\n  geom_line(size = 1) +   # Line plot\n  geom_point(size = 2) +  # Add points for clarity\n  labs(title = \"Average LEU3N  by Adherence Group Over 8 Years\",\n       x = \"Year\",\n       y = \"Average LEU3N\",\n       color = \"Adherence Group\") +\n  theme_minimal() +       # Clean theme\n  scale_color_brewer(palette = \"Pastel2\")  # Nice color palette for groups\n\nggplotly(p)\n\n\n\n\n\n\n\nCD4+ T Cell Count\n\n# Create log vload variable for original 8 year data set.\ndata$VLOAD_log &lt;- log(data$VLOAD)\n\n# Get means for each year\nsummary_data &lt;- data %&gt;%\n  group_by(ADH, years) %&gt;%\n  summarize(Average_VLOAD_log = mean(VLOAD_log, na.rm = TRUE))\n\n`summarise()` has grouped output by 'ADH'. You can override using the `.groups`\nargument.\n\n# Create plot\np &lt;- ggplot(summary_data, aes(x = years, y = Average_VLOAD_log, color = ADH)) +\n  geom_line(size = 1) +   # Line plot\n  geom_point(size = 2) +  # Add points for clarity\n  labs(title = \"Average VLOAD log by Adherence Group Over 8 Years\",\n       x = \"Year\",\n       y = \"Average VLOAD log\",\n       color = \"Adherence Group\") +\n  theme_minimal() +       # Clean theme\n  scale_color_brewer(palette = \"Pastel2\")  # Nice color palette for groups\n\nggplotly(p)\n\n\n\n\n\nThat’s very interesting. The groups with the highest vload log had the least adherence. There is an inverse relationship here.\nThis is strong evidence that the treatment is efficacious and the ADH should be considered as a confounder.\nOhhhhh, that’s super cool. So there is an inverse relationship between LEU3N and VLOAD_log, such that spikes in one should be dips in the other. We can see that in this data set! It appears that for the adherence group of &lt;75%, they had viral load spikes at years 5 and 7, which correspond with leukocyte dips at years 5 and 7! They weren’t adhering to the protocol, which resulted in them getting infected (?) more and having more of the virus present compared to the other groups.\n\n\nComparing High Vs Low Adherence Groups\n\ndata$ADH_HIGHLOW &lt;- ifelse(data$ADH == \"&lt;75%\" | data$ADH == \"75-94%\", 0, 1)\n\ndata$ADH_HIGHLOW &lt;- factor(data$ADH_HIGHLOW,\n                           levels = c(0,1),\n                           labels = c(\"Low Adherence\", \"High Adherence\"))\n\n#### tRY WITH HIGH LOW ADH\n# Create log vload variable for original 8 year data set.\ndata$VLOAD_log &lt;- log(data$VLOAD)\n\n# Get means for each year\nsummary_data &lt;- data %&gt;%\n  group_by(ADH_HIGHLOW, years) %&gt;%\n  summarize(Average_VLOAD_log = mean(VLOAD_log, na.rm = TRUE))\n\n`summarise()` has grouped output by 'ADH_HIGHLOW'. You can override using the\n`.groups` argument.\n\n# Create plot\np &lt;- ggplot(summary_data, aes(x = years, y = Average_VLOAD_log, color = ADH_HIGHLOW)) +\n  geom_line(size = 1) +   # Line plot\n  geom_point(size = 2) +  # Add points for clarity\n  labs(title = \"Average VLOAD log by Adherence Group Over 8 Years\",\n       x = \"Year\",\n       y = \"Average VLOAD log\",\n       color = \"Adherence Group\") +\n  theme_minimal() +       # Clean theme\n  scale_color_brewer(palette = \"Pastel2\")  # Nice color palette for groups\n\nggplotly(p)\n\n\n\n\n\nHigh adherence patients have lower viral load then low adherence\nSame for leu3n\n\n#### tRY WITH HIGH LOW ADH\n\n# Get means for each year\nsummary_data &lt;- data %&gt;%\n  group_by(ADH_HIGHLOW, years) %&gt;%\n  summarize(Average_LEU3N = mean(LEU3N, na.rm = TRUE))\n\n`summarise()` has grouped output by 'ADH_HIGHLOW'. You can override using the\n`.groups` argument.\n\n# Create plot\np &lt;- ggplot(summary_data, aes(x = years, y = Average_LEU3N, color = ADH_HIGHLOW)) +\n  geom_line(size = 1) +   # Line plot\n  geom_point(size = 2) +  # Add points for clarity\n  labs(title = \"Average LEU3N by Adherence Group Over 8 Years\",\n       x = \"Year\",\n       y = \"Average LEU3N\",\n       color = \"Adherence Group\") +\n  theme_minimal() +       # Clean theme\n  scale_color_brewer(palette = \"Pastel2\")  # Nice color palette for groups\n\nggplotly(p)\n\n\n\n\n\nHigh adherence users have higher CD4+ T Cells, for the most part.\n\n\nOther literature\nhttps://pmc.ncbi.nlm.nih.gov/articles/PMC9234842/\nThis one shows that poor adherence is related to anxiety and depression, leading credence to why I want to examine the impact of hard drugs and adherence while controlling for depression."
  },
  {
    "objectID": "Project_1_Regression/Project_1_Regression_R/Project_1_Regression.html",
    "href": "Project_1_Regression/Project_1_Regression_R/Project_1_Regression.html",
    "title": "Advanced Data Analysis - Project 1",
    "section": "",
    "text": "The aim of the current study is to assess whether a new gel treatment for gum disease results in lower whole-mouth average pocket depth and attachment loss after one year. Average pocket depth and attachment loss were taken at baseline, and participants were assigned into one of five groups (1 = placebo, 2 = no treatment, 3 = low concentration, 4 = medium concentration, 5 = high concentration) and instructed to apply the gel to their gums twice a day. After 1-year, average pocket depth and attachment loss were recorded again. Data was received as a .csv file containing treatment level, average pocket depth and attachment loss at baseline and at one-year, demographic information, gender, age, number of sites measured, and smoking status. The clinical hypothesis is that average pocket depth and attachment loss in participants who applied the gel will be lower compared to participants who did not. Gender, age, ethnicity, and smoking status will be investigated as potential covariates.\nThe project description provided by the PI is available below:\n\n\n\nFirst we begin by loading in the necessary packages\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(tidyverse)\nlibrary(knitr)\nlibrary(kableExtra) # This lets me pretty print the tables\nlibrary(naniar) # Used to visualize missing data\nlibrary(glue) # This is used for f-strings in R\nlibrary(purrr) # Used for summary tables\nlibrary(corrplot) # Used to make the correlation matrix\nlibrary(corrtable) # used to make the table for the correlation matrix\nlibrary(xtable) # Used to make the table for the correlation matrix\nlibrary(htmlTable) # Used to make Table 1\nlibrary(boot) # Used to make Table 1\nlibrary(table1) # Used to make Table 1\nlibrary(sjPlot) # Used to generate publication quality tables for regressions\nlibrary(mice) # Used for multiple imputation\nlibrary(car) # Used for outlier diagnostics with jackknife residuals\n\nThen we will import the dataset\n\n# Import dataset\ndata &lt;- read.csv(\"C:\\\\Users\\\\sviea\\\\Documents\\\\Advanced Data Analysis\\\\Project 1 MLR with Covariates\\\\Project1_data.csv\")\nnames(data)[1] &lt;- \"id\" # Renaming the weird character out of this colname\n\nAnd visualize the dataset\n\n# Create a nicely formatted table, code adapted from ChatGPT\nkable(head(data), format = \"html\") %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\"))\n\n\n\n\nid\ntrtgroup\ngender\nrace\nage\nsmoker\nsites\nattachbase\nattach1year\npdbase\npd1year\n\n\n\n\n101\n4\n2\n5\n44.57221\n1\n162\n2.432099\n2.577640\n3.246914\n3.407407\n\n\n102\n5\n2\n5\n35.57290\n1\n162\n2.543210\nNA\n3.006173\nNA\n\n\n103\n2\n2\n5\n47.94524\n1\n144\n2.881944\n3.076389\n3.118056\n3.125000\n\n\n104\n3\n2\n5\n55.17864\n1\n138\n4.956522\n5.304348\n5.217391\n4.891304\n\n\n105\n1\n2\n2\n43.79740\n1\n168\n1.773810\n1.452381\n3.363095\n2.898810\n\n\n106\n4\n2\n5\n42.14921\n1\n168\n2.369048\n1.922619\n3.910714\n3.083333\n\n\n\n\n\n\n# Acquire number of variables and observations of the data set\ndim(data)\n\n[1] 130  11\n\n\nOur data set is comprised of 130 observations, with 11 variables\n\n\nThe variables in the provided data set are as follows:\n\nid: Patient ID\ntrtgroup: Treatment group (1 = placebo, 2 = no treatment control, 3 = low concentration, 4 = medium concentration, 5 = high concentration gel)\ngender: Gender (1 = male, 2 = female)\nrace: Race, (1 = Native American, 2 = African American, 3 = Not used, 4 = Asian, 5 = White)\nage: Age in years\nsmoker: Smoking status (0 = No, 1 = Yes)\nsites: Number of sites gum measurements were averaged from\nattachbase: Whole-mouth average attachment loss taken at base timepoint\nattach1year: Whole-mouth average attachment loss after 1 year\npdbase: Whole-mouth average pocket loss at base timepoint\npd1year: Whole-mouth average pocket loss after 1 year\n\n\n\n\n\n\n\nNote\n\n\n\nPocket depth and attachment loss are both measurements of how far the gums have pulled away from the teeth, hence smaller values are better"
  },
  {
    "objectID": "Project_1_Regression/Project_1_Regression_R/Project_1_Regression.html#data-preparation",
    "href": "Project_1_Regression/Project_1_Regression_R/Project_1_Regression.html#data-preparation",
    "title": "Advanced Data Analysis - Project 1",
    "section": "",
    "text": "First we begin by loading in the necessary packages\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(tidyverse)\nlibrary(knitr)\nlibrary(kableExtra) # This lets me pretty print the tables\nlibrary(naniar) # Used to visualize missing data\nlibrary(glue) # This is used for f-strings in R\nlibrary(purrr) # Used for summary tables\nlibrary(corrplot) # Used to make the correlation matrix\nlibrary(corrtable) # used to make the table for the correlation matrix\nlibrary(xtable) # Used to make the table for the correlation matrix\nlibrary(htmlTable) # Used to make Table 1\nlibrary(boot) # Used to make Table 1\nlibrary(table1) # Used to make Table 1\nlibrary(sjPlot) # Used to generate publication quality tables for regressions\nlibrary(mice) # Used for multiple imputation\nlibrary(car) # Used for outlier diagnostics with jackknife residuals\n\nThen we will import the dataset\n\n# Import dataset\ndata &lt;- read.csv(\"C:\\\\Users\\\\sviea\\\\Documents\\\\Advanced Data Analysis\\\\Project 1 MLR with Covariates\\\\Project1_data.csv\")\nnames(data)[1] &lt;- \"id\" # Renaming the weird character out of this colname\n\nAnd visualize the dataset\n\n# Create a nicely formatted table, code adapted from ChatGPT\nkable(head(data), format = \"html\") %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\"))\n\n\n\n\nid\ntrtgroup\ngender\nrace\nage\nsmoker\nsites\nattachbase\nattach1year\npdbase\npd1year\n\n\n\n\n101\n4\n2\n5\n44.57221\n1\n162\n2.432099\n2.577640\n3.246914\n3.407407\n\n\n102\n5\n2\n5\n35.57290\n1\n162\n2.543210\nNA\n3.006173\nNA\n\n\n103\n2\n2\n5\n47.94524\n1\n144\n2.881944\n3.076389\n3.118056\n3.125000\n\n\n104\n3\n2\n5\n55.17864\n1\n138\n4.956522\n5.304348\n5.217391\n4.891304\n\n\n105\n1\n2\n2\n43.79740\n1\n168\n1.773810\n1.452381\n3.363095\n2.898810\n\n\n106\n4\n2\n5\n42.14921\n1\n168\n2.369048\n1.922619\n3.910714\n3.083333\n\n\n\n\n\n\n# Acquire number of variables and observations of the data set\ndim(data)\n\n[1] 130  11\n\n\nOur data set is comprised of 130 observations, with 11 variables\n\n\nThe variables in the provided data set are as follows:\n\nid: Patient ID\ntrtgroup: Treatment group (1 = placebo, 2 = no treatment control, 3 = low concentration, 4 = medium concentration, 5 = high concentration gel)\ngender: Gender (1 = male, 2 = female)\nrace: Race, (1 = Native American, 2 = African American, 3 = Not used, 4 = Asian, 5 = White)\nage: Age in years\nsmoker: Smoking status (0 = No, 1 = Yes)\nsites: Number of sites gum measurements were averaged from\nattachbase: Whole-mouth average attachment loss taken at base timepoint\nattach1year: Whole-mouth average attachment loss after 1 year\npdbase: Whole-mouth average pocket loss at base timepoint\npd1year: Whole-mouth average pocket loss after 1 year\n\n\n\n\n\n\n\nNote\n\n\n\nPocket depth and attachment loss are both measurements of how far the gums have pulled away from the teeth, hence smaller values are better"
  },
  {
    "objectID": "Project_1_Regression/Project_1_Regression_R/Project_1_Regression.html#Descriptives",
    "href": "Project_1_Regression/Project_1_Regression_R/Project_1_Regression.html#Descriptives",
    "title": "Advanced Data Analysis - Project 1",
    "section": "Descriptive Statistics",
    "text": "Descriptive Statistics\nHere we will acquire the descriptive statistics of our data set and create Table 1. Code modified from cran.r-project.org\n\n# Duplicate the dataset so we are not modifying the original\ndata2 &lt;- data\n\n# Factor the basic variables that we're interested in\ndata2$trtgroup &lt;- factor(data2$trtgroup,\n                                levels = c(1,2,3,4,5),\n                                labels = c(\"Placebo\", \"Control\", \"Low\", \"Medium\", \"High\"))\n\ndata2$gender &lt;- factor(data2$gender,\n                              levels = c(1,2),\n                              labels = c(\"Male\", \"Female\"))\n\ndata2$race &lt;- factor(data2$race,\n                             levels = c(1,2,4,5),\n                             labels = c(\"Native American\", \"African American\", \"White\", \"Asian\"))\n                             \ndata2$smoker &lt;- factor(data2$smoker,\n                               levels = c(0,1),\n                               labels = c(\"Non-Smoker\", \"Smoker\"))\n\n# Create labels to make the names of each variable more professional\nlabel(data2$gender) &lt;- \"Gender\"\nlabel(data2$race) &lt;- \"Race\"\nlabel(data2$age) &lt;- \"Age (Years)\"\nlabel(data2$smoker) &lt;- \"Smoking Status\"\nlabel(data2$sites) &lt;- \"Sites\"\nlabel(data2$attachbase) &lt;- \"Attachment Loss at Baseline\"\nlabel(data2$attach1year) &lt;- \"Attachment Loss at 1 Year\"\nlabel(data2$pdbase) &lt;- \"Pocket Depth at Baseline\"\nlabel(data2$pd1year) &lt;- \"Pocket Depth at 1 Year\"\nlabel(data2$attachchange) &lt;- \"Attachment Loss Change\"\nlabel(data2$pdchange) &lt;- \"Pocket Depth Change\"\n\n\n# Create table 1\ntable1 &lt;- table1(~ gender + race + age + smoker + sites + attachbase + attach1year + pdbase + pd1year + attachchange + pdchange| trtgroup,  data = data2, caption = \"Descriptive Statistics\", overall = c(left=\"Total\"))\ntable1\n\n\n\nDescriptive Statistics\n\n\n\n\n\n\n\n\n\n\n\n\nTotal\n(N=130)\nPlacebo\n(N=26)\nControl\n(N=26)\nLow\n(N=26)\nMedium\n(N=26)\nHigh\n(N=26)\n\n\n\n\nGender\n\n\n\n\n\n\n\n\nMale\n54 (41.5%)\n11 (42.3%)\n10 (38.5%)\n11 (42.3%)\n11 (42.3%)\n11 (42.3%)\n\n\nFemale\n76 (58.5%)\n15 (57.7%)\n16 (61.5%)\n15 (57.7%)\n15 (57.7%)\n15 (57.7%)\n\n\nRace\n\n\n\n\n\n\n\n\nNative American\n4 (3.1%)\n0 (0%)\n1 (3.8%)\n1 (3.8%)\n0 (0%)\n2 (7.7%)\n\n\nAfrican American\n9 (6.9%)\n2 (7.7%)\n1 (3.8%)\n5 (19.2%)\n0 (0%)\n1 (3.8%)\n\n\nWhite\n3 (2.3%)\n1 (3.8%)\n1 (3.8%)\n0 (0%)\n1 (3.8%)\n0 (0%)\n\n\nAsian\n114 (87.7%)\n23 (88.5%)\n23 (88.5%)\n20 (76.9%)\n25 (96.2%)\n23 (88.5%)\n\n\nAge (Years)\n\n\n\n\n\n\n\n\nMean (SD)\n49.9 (10.0)\n47.1 (8.61)\n50.7 (9.90)\n51.9 (10.8)\n49.0 (9.49)\n50.8 (11.2)\n\n\nMedian [Min, Max]\n48.6 [28.6, 74.5]\n44.7 [30.4, 67.1]\n49.2 [36.1, 73.3]\n51.5 [36.9, 71.9]\n48.1 [28.6, 70.9]\n49.9 [34.1, 74.5]\n\n\nMissing\n1 (0.8%)\n1 (3.8%)\n0 (0%)\n0 (0%)\n0 (0%)\n0 (0%)\n\n\nSmoking Status\n\n\n\n\n\n\n\n\nNon-Smoker\n81 (62.3%)\n15 (57.7%)\n17 (65.4%)\n18 (69.2%)\n14 (53.8%)\n17 (65.4%)\n\n\nSmoker\n48 (36.9%)\n11 (42.3%)\n9 (34.6%)\n8 (30.8%)\n11 (42.3%)\n9 (34.6%)\n\n\nMissing\n1 (0.8%)\n0 (0%)\n0 (0%)\n0 (0%)\n1 (3.8%)\n0 (0%)\n\n\nSites\n\n\n\n\n\n\n\n\nMean (SD)\n158 (11.3)\n160 (10.1)\n154 (10.9)\n161 (8.54)\n155 (15.7)\n157 (9.65)\n\n\nMedian [Min, Max]\n162 [114, 168]\n162 [138, 168]\n159 [126, 168]\n162 [138, 168]\n162 [114, 168]\n159 [138, 168]\n\n\nAttachment Loss at Baseline\n\n\n\n\n\n\n\n\nMean (SD)\n2.15 (0.797)\n1.79 (0.646)\n2.46 (0.687)\n2.07 (0.987)\n2.17 (0.656)\n2.24 (0.858)\n\n\nMedian [Min, Max]\n2.03 [0.895, 5.09]\n1.71 [0.899, 3.64]\n2.48 [1.22, 4.39]\n1.77 [0.895, 4.96]\n2.12 [1.02, 4.01]\n1.97 [1.26, 5.09]\n\n\nAttachment Loss at 1 Year\n\n\n\n\n\n\n\n\nMean (SD)\n2.10 (0.772)\n1.74 (0.542)\n2.33 (0.551)\n2.08 (1.06)\n2.24 (0.652)\n2.15 (0.915)\n\n\nMedian [Min, Max]\n1.98 [0.865, 5.30]\n1.64 [0.964, 3.10]\n2.23 [1.46, 3.49]\n1.74 [0.865, 5.30]\n2.25 [1.35, 3.83]\n1.71 [1.22, 4.04]\n\n\nMissing\n27 (20.8%)\n3 (11.5%)\n3 (11.5%)\n5 (19.2%)\n6 (23.1%)\n10 (38.5%)\n\n\nPocket Depth at Baseline\n\n\n\n\n\n\n\n\nMean (SD)\n3.14 (0.437)\n3.09 (0.372)\n3.28 (0.473)\n3.17 (0.593)\n3.05 (0.402)\n3.11 (0.273)\n\n\nMedian [Min, Max]\n3.10 [2.26, 5.22]\n3.11 [2.47, 4.08]\n3.11 [2.65, 4.77]\n3.07 [2.26, 5.22]\n3.09 [2.42, 3.91]\n3.14 [2.62, 3.60]\n\n\nPocket Depth at 1 Year\n\n\n\n\n\n\n\n\nMean (SD)\n2.88 (0.488)\n2.75 (0.482)\n2.95 (0.455)\n3.02 (0.578)\n2.84 (0.469)\n2.80 (0.423)\n\n\nMedian [Min, Max]\n2.90 [1.96, 4.89]\n2.70 [1.96, 3.75]\n2.90 [2.24, 4.07]\n2.97 [2.16, 4.89]\n2.90 [2.05, 3.78]\n2.87 [2.04, 3.40]\n\n\nMissing\n27 (20.8%)\n3 (11.5%)\n3 (11.5%)\n5 (19.2%)\n6 (23.1%)\n10 (38.5%)\n\n\nAttachment Loss Change\n\n\n\n\n\n\n\n\nMean (SD)\n-0.0995 (0.276)\n-0.0871 (0.242)\n-0.222 (0.280)\n-0.0178 (0.266)\n-0.00656 (0.231)\n-0.165 (0.326)\n\n\nMedian [Min, Max]\n-0.0679 [-1.05, 0.452]\n-0.0247 [-0.599, 0.452]\n-0.123 [-0.901, 0.194]\n0.0298 [-0.705, 0.348]\n-0.0160 [-0.446, 0.339]\n-0.0579 [-1.05, 0.199]\n\n\nMissing\n27 (20.8%)\n3 (11.5%)\n3 (11.5%)\n5 (19.2%)\n6 (23.1%)\n10 (38.5%)\n\n\nPocket Depth Change\n\n\n\n\n\n\n\n\nMean (SD)\n-0.294 (0.268)\n-0.350 (0.277)\n-0.338 (0.232)\n-0.206 (0.279)\n-0.203 (0.272)\n-0.382 (0.245)\n\n\nMedian [Min, Max]\n-0.284 [-0.858, 0.455]\n-0.383 [-0.858, 0.161]\n-0.367 [-0.759, 0.0145]\n-0.244 [-0.661, 0.455]\n-0.200 [-0.827, 0.175]\n-0.347 [-0.845, 0.0536]\n\n\nMissing\n27 (20.8%)\n3 (11.5%)\n3 (11.5%)\n5 (19.2%)\n6 (23.1%)\n10 (38.5%)"
  },
  {
    "objectID": "Project_1_Regression/Project_1_Regression_R/Project_1_Regression.html#Preliminary",
    "href": "Project_1_Regression/Project_1_Regression_R/Project_1_Regression.html#Preliminary",
    "title": "Advanced Data Analysis - Project 1",
    "section": "Preliminary Evaluation of Assumptions",
    "text": "Preliminary Evaluation of Assumptions\nBefore we begin digging into the data, let’s take a closer look at the relationships between our variables.\nWe will first run a correlation matrix to assess the correlations between our IVs. Then we will explore any high correlations that which could affect our analysis.\nFinally we will make histograms of attachment loss and pocket depth change to gauge whether they will be normally distributed or if we need to run some transformations.\n\nCorrelation MatrixGender and MissingnessNormality of Dependent Variables\n\n\nFirst we will begin by making a correlation matrix to assess whether any of our IVs are related to each other (multicollinearity). This will inform which variables to incorporate into the final model.\n\n# Since we made dummy codes, we will get spurious correlations that will obfuscate the main relationships we are interested in (e.g. between 'medium' and 'trtgroup'. So we will first make a separate dataset excluding the dummy coded variables\ndata_for_matrix &lt;- select(data_missing, -placebo, -control, -low, -medium,  -high, -trt, -trt3groups)\n\n# Make a correlation matrix with all variables of the trimmed data set\ncorrelation_matrix &lt;- cor(data_for_matrix, use = \"complete.obs\")\n\n# Plot the matrix\ncorrplot(correlation_matrix, method = \"circle\")\n\n\n\n\n\n\n\n# Trim the matrix\ncorrelation_matrix[upper.tri(correlation_matrix)] &lt;- NA\n\n# Save the matrix as a LaTex file for paper\ncor_table &lt;- xtable(correlation_matrix, caption = \"Correlation Matrix\", label = \"tab:correlation\")\nprint(cor_table, type = \"latex\", file = \"correlation_matrix999.tex\")\n\nkable(correlation_matrix, format = \"html\", table.attr = \"class='table table-bordered'\")\n\n\n\n\n\nid\ntrtgroup\ngender\nrace\nage\nsmoker\nsites\nattachbase\nattach1year\npdbase\npd1year\nattachchange\npdchange\n\n\n\n\nid\n1.0000000\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\ntrtgroup\n-0.1452850\n1.0000000\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\ngender\n-0.8306185\n0.1428866\n1.0000000\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nrace\n0.1319767\n-0.0314771\n-0.0955531\n1.0000000\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nage\n0.0593032\n0.1100051\n-0.0453862\n0.1787040\n1.0000000\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nsmoker\n-0.2742888\n-0.0495029\n0.0206456\n0.0377211\n-0.2229620\n1.0000000\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nsites\n0.0520821\n-0.0393742\n-0.1291127\n0.0505164\n-0.0274695\n-0.0226381\n1.0000000\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nattachbase\n-0.1641513\n0.1440999\n0.2416242\n0.0313438\n0.1354434\n0.1456234\n-0.4237813\n1.0000000\nNA\nNA\nNA\nNA\nNA\n\n\nattach1year\n-0.1440182\n0.1671499\n0.2021284\n0.0671713\n0.0850851\n0.1987327\n-0.4042743\n0.9450189\n1.0000000\nNA\nNA\nNA\nNA\n\n\npdbase\n-0.2010672\n-0.0224838\n0.2645908\n0.0182289\n-0.0884687\n0.2614919\n-0.1805543\n0.6047132\n0.6093346\n1.0000000\nNA\nNA\nNA\n\n\npd1year\n-0.0685783\n0.0126731\n0.1362804\n0.0606735\n-0.1246106\n0.2447030\n-0.1901940\n0.5601052\n0.6685457\n0.8436653\n1.0000000\nNA\nNA\n\n\nattachchange\n0.0964391\n0.0296043\n-0.1696216\n0.0928969\n-0.1742586\n0.1135740\n0.1578683\n-0.3976360\n-0.0757224\n-0.1342018\n0.1679506\n1.0000000\nNA\n\n\npdchange\n0.2251487\n0.0622762\n-0.2123192\n0.0789059\n-0.0731704\n-0.0091784\n-0.0323859\n-0.0317726\n0.1579534\n-0.2031295\n0.3543035\n0.5400668\n1\n\n\n\n\n\n\n\nVisually, we can see a cluster of high positive correlations between all of attachment loss and pocket depth at baseline and 1 year. This makes sense, as all measurements were taken from the same sites in the gums. This will be explored later (See Exploratory Data Analysis - Dependent Variables)\nJust for sanity (and practice), let’s make a table of our correlation coefficients.\n\n# Convert the matrix to a dataframe for better formatting\ncorrelation_df &lt;- as.data.frame(correlation_matrix)\n\n# Use Kable to pretty print the table\nkable(correlation_df, caption = \"Correlation Matrix\", format = \"html\") %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\"))\n\n\nCorrelation Matrix\n\n\n\nid\ntrtgroup\ngender\nrace\nage\nsmoker\nsites\nattachbase\nattach1year\npdbase\npd1year\nattachchange\npdchange\n\n\n\n\nid\n1.0000000\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\ntrtgroup\n-0.1452850\n1.0000000\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\ngender\n-0.8306185\n0.1428866\n1.0000000\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nrace\n0.1319767\n-0.0314771\n-0.0955531\n1.0000000\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nage\n0.0593032\n0.1100051\n-0.0453862\n0.1787040\n1.0000000\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nsmoker\n-0.2742888\n-0.0495029\n0.0206456\n0.0377211\n-0.2229620\n1.0000000\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nsites\n0.0520821\n-0.0393742\n-0.1291127\n0.0505164\n-0.0274695\n-0.0226381\n1.0000000\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nattachbase\n-0.1641513\n0.1440999\n0.2416242\n0.0313438\n0.1354434\n0.1456234\n-0.4237813\n1.0000000\nNA\nNA\nNA\nNA\nNA\n\n\nattach1year\n-0.1440182\n0.1671499\n0.2021284\n0.0671713\n0.0850851\n0.1987327\n-0.4042743\n0.9450189\n1.0000000\nNA\nNA\nNA\nNA\n\n\npdbase\n-0.2010672\n-0.0224838\n0.2645908\n0.0182289\n-0.0884687\n0.2614919\n-0.1805543\n0.6047132\n0.6093346\n1.0000000\nNA\nNA\nNA\n\n\npd1year\n-0.0685783\n0.0126731\n0.1362804\n0.0606735\n-0.1246106\n0.2447030\n-0.1901940\n0.5601052\n0.6685457\n0.8436653\n1.0000000\nNA\nNA\n\n\nattachchange\n0.0964391\n0.0296043\n-0.1696216\n0.0928969\n-0.1742586\n0.1135740\n0.1578683\n-0.3976360\n-0.0757224\n-0.1342018\n0.1679506\n1.0000000\nNA\n\n\npdchange\n0.2251487\n0.0622762\n-0.2123192\n0.0789059\n-0.0731704\n-0.0091784\n-0.0323859\n-0.0317726\n0.1579534\n-0.2031295\n0.3543035\n0.5400668\n1\n\n\n\n\n\n\n\nOther than that, there is a correlation between gender and ID which seems spurious. Let’s investigate that in the next tab.\nBack to top of tabset\n\n\nThere is a correlation between gender and ID. Let’s make a simple plot to investigate.\n\n# Creating a simple plot of id and gender\nggplot(data_missing, aes(x = factor(gender), y = id)) + \n  geom_point() + \n  labs(title = \"ID by Gender\")\n\n\n\n\n\n\n\n\nInterestingly, it appears that the experimenters assigned ID based on gender. That is, females received ID’s starting at 101, and males received ID’s starting at 201 (for some reason there’s a few females with ID’s &gt; 200).\nIt will be important to double check with the PI’s how they assigned participants to treatment group to ensure it was in fact random.\nLet’s make a contingency table to see what the breakdown between gender and treatment group is.\n\n# First make a contingency table of both variables\ncontingency_table &lt;- table(data_missing$gender, data_missing$trtgroup)\n\n# Set the row and column names\ndimnames(contingency_table) &lt;- list(\n  \"Gender\" = c(\"Male\", \"Female\"),\n  \"Treatment Condition\" = c(\"Placebo\", \"Control\", \"Low\", \"Medium\", \"High\"))\n\n# Convert the table to a dataframe for better formatting (from ChatGPT)\ncontingency_df &lt;- as.data.frame.matrix(contingency_table)\n\n# Pretty print the table using kable\nkable(contingency_df, format = \"html\") %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\"))\n\n\n\n\n\nPlacebo\nControl\nLow\nMedium\nHigh\n\n\n\n\nMale\n10\n7\n9\n7\n3\n\n\nFemale\n13\n16\n12\n13\n13\n\n\n\n\n\n\n\nThat’s not good! It looks like males were less likely to be in the high treatment condition compared to females.\nThis could be because males were more likely to drop out then females. Let’s make a quick table using the original data set before we dropped the missing variables.\n\n# First make a contingency table of both variables\ncontingency_table_clean &lt;- table(data$gender, data$trtgroup)\n\n# Set the row and column names\ndimnames(contingency_table_clean) &lt;- list(\n  \"Gender\" = c(\"Male\", \"Female\"),\n  \"Treatment Condition\" = c(\"Placebo\", \"Control\", \"Low\", \"Medium\", \"High\"))\n\n# Convert the table to a dataframe for better formatting (from ChatGPT)\ncontingency_df_clean &lt;- as.data.frame.matrix(contingency_table_clean)\n\n# Pretty print the table using kable\nkable(contingency_df_clean, format = \"html\") %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\"))\n\n\n\n\n\nPlacebo\nControl\nLow\nMedium\nHigh\n\n\n\n\nMale\n11\n10\n11\n11\n11\n\n\nFemale\n15\n16\n15\n15\n15\n\n\n\n\n\n\n\nIt looks more balanced before I took out participants with missing data.\nChi-square is known to be unsuitable if a cell has &lt; 5 counts, which we have in this case (3 males in high concentration condition). So I will run Fisher’s test to see if that difference is statistically significant.\n\nfisher_test &lt;- fisher.test(contingency_table)\nfisher_test\n\n\n    Fisher's Exact Test for Count Data\n\ndata:  contingency_table\np-value = 0.506\nalternative hypothesis: two.sided\n\n\nThe p-value is not signficant (p = 0.506).\nWe know from visualizing the missing data that there were 27 missing data points for the gum measurement DVs, and these all belong to the same people. Furthermore, it appears that males in the treatment groups were more likely to have missing values than in the placebo (and maybe control) group. Is it possible that the gel was having an adverse effect on these participants? Does the gel have an adverse effect only on males and not females for some reason? Let’s explore.\nFirst, I want to investigate if males were more likely to have missing data points. It’s possible if their gums were hurting they simply rejected or avoided having these measurements taken.\nLet’s repeat this process and make a contingency table of gender and missing variables.\n\n# First let's add a new dummy code for if a participant is missing any data points\ndata$missing &lt;- ifelse(apply(data, 1, function(row) any(is.na(row))), 1, 0)\n\n# First make a contingency table of both variables\ncontingency_table_missing &lt;- table(data$gender, data$missing)\n\n# Set the row and column names\ndimnames(contingency_table_missing) &lt;- list(\n  \"Gender\" = c(\"Male\", \"Female\"),\n  \"Missing\" = c(\"Not Missing\", \"Missing\"))\n\n# Convert the table to a dataframe for better formatting (from ChatGPT)\ncontingency_df_missing &lt;- as.data.frame.matrix(contingency_table_missing)\n\n# Pretty print the table using kable\nkable(contingency_df_missing, format = \"html\") %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\"))\n\n\n\n\n\nNot Missing\nMissing\n\n\n\n\nMale\n35\n19\n\n\nFemale\n66\n10\n\n\n\n\n\n\n\nProportionally, it appears that males may be more likely to have missing variables than females. Let’s run a chi-square to check.\n\nchi_square_test &lt;- chisq.test(contingency_df_missing)\nchi_square_test\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  contingency_df_missing\nX-squared = 7.6127, df = 1, p-value = 0.005796\n\n\nSuccess! Males were more likely to have missing values compared to females (p = 0.005796). This could be a problem (counfound) if something was causing males to avoid having their gums measured compared to females (such as adverse reactions from the gel)\nLet’s do a quick chi square test to check if there is a relationship between missing values and treatment condition.\nWe start off the same way by making a contingency table and running a chi-square test.\n\n# First make a contingency table of both variables\ncontingency_table_missing2 &lt;- table(data$missing, data$trtgroup)\n\n# Set the row and column names\ndimnames(contingency_table_missing2) &lt;- list(\"Missing\" = c(\"Not Missing\", \"Missing\"),\n                                             \"Treatmtent Condition\" = c(\"Placebo\", \"Control\", \"Low\", \"Medium\", \"High\"))\n\n# Convert the table to a dataframe for better formatting (from ChatGPT)\ncontingency_df_missing2 &lt;- as.data.frame.matrix(contingency_table_missing2)\n\n# Pretty print the table using kable\nkable(contingency_df_missing2, format = \"html\") %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\"))\n\n\n\n\n\nPlacebo\nControl\nLow\nMedium\nHigh\n\n\n\n\nNot Missing\n22\n23\n21\n19\n16\n\n\nMissing\n4\n3\n5\n7\n10\n\n\n\n\n\n\n\nIt does appear that there are more missing variables in the high concentration condition. Is it statistically significant?\n\n# Run a Fisher's Exact Test (since we have &lt; 5 observations in cells)\nfisher_test &lt;- fisher.test(contingency_df_missing2)\nfisher_test\n\n\n    Fisher's Exact Test for Count Data\n\ndata:  contingency_df_missing2\np-value = 0.1675\nalternative hypothesis: two.sided\n\n\nNot significant (p = 0.1675). So we can conclude that there is no difference in gender or missing values based on treatment condition (i.e., participants in all treatment conditions were equally likely to be male or female, or have missing values)\nHowever, across the board, males were more likely to have missing values than females. This will be important to note as a caveat during interpretation of the final results.\nBack to top of tabset\n\n\nHere we will simply plot the histograms of attachment loss and pocket depth changes scores, to assess if they appear normally distributed or if we will have to perform a transformation of some kind.\n\n# Plot simple histogram of attachment loss change score\nhist(data_missing$attachchange,\n     main  = \"Histogram of Attachment Loss Change\",\n     xlab = \"Attachment Loss Change\",\n     ylab = \"Frequency\")\n\n\n\n\n\n\n\n# Plot simple histogram of pocket depth change score\nhist(data_missing$pdchange,\n     main  = \"Histogram of Attachment Loss Change\",\n     xlab = \"Pocket Depth Change\",\n     ylab = \"Frequency\")\n\n\n\n\n\n\n\n\nBoth attachment loss and pocket depth change appear to be normally distributed and will not need to be transformed. Attachment loss change is slightly left-tailed but this could be due to outliers, or may just not impact the analysis."
  },
  {
    "objectID": "Project_1_Regression/Project_1_Regression_R/Project_1_Regression.html#Exploratory",
    "href": "Project_1_Regression/Project_1_Regression_R/Project_1_Regression.html#Exploratory",
    "title": "Advanced Data Analysis - Project 1",
    "section": "Exploratory Data Analysis",
    "text": "Exploratory Data Analysis\nHere I will plot the data and perform a number of simple linear regressions to examine relationships between variables in order to determine which covariates to include in the model.\n\nPrimary Explanatory VariableCovariatesDependent VariablesSummary\n\n\n\n5 Treatment Groups3 Treatment Groups\n\n\nLet’s examine if there appears to be a difference in the average attachment loss and pocket depth change according to treatment level\n\nggplot(data_missing, aes(x = factor(trtgroup), y = attachchange)) + \n  geom_boxplot() +\n  labs(title = \"Boxplot of Attachment Loss change by Treatment\")\n\n\n\n\n\n\n\nggplot(data_missing, aes(x = factor(trtgroup), y = pdchange)) + \n  geom_boxplot()  +\n  labs(title = \"Boxplot of Pocket Depth Change by treatment\")\n\n\n\n\n\n\n\n\nAttachment Loss Change\nVisually, there does not appear to be a difference between the treatment group levels (low, medium, high concentration gel) compared to each other. Additionally, there does not seem to be a difference between the treatment groups and the placebo for attachment loss change.\nPocket Depth Change\nInterestingly, the high concentration condition appears to have had a negative effect. Additionally, there seems to be a difference in the treatment groups compared to the no treatment control, but NOT when compared to the placebo. The exception is the low concentration condition compared to the placebo when looking at pocket depth change. Further analysis will reveal whether these differences are statistically significant or not.\nBack to top of tabset\n\n\nThe relationship between low vs medium vs high gel concentration does not look very strong. Furthermore, we technically do not have a large enough sample size in the high concentration condition to include it.\nFor those reasons, let’s make the same comparisons but while combining all treatment levels into one group called treatment.\n\nggplot(data_missing, aes(x = factor(trt3groups), y = attachchange)) + \n  geom_boxplot()\n\n\n\n\n\n\n\nggplot(data_missing, aes(x = factor(trt3groups), y = attachchange)) +\n  geom_boxplot() +\n  labs(title = \"Boxplot of Attachment Loss Change by Treatment\")\n\n\n\n\n\n\n\nggplot(data_missing, aes(x = factor(trt3groups), y = pdchange)) + \n  geom_boxplot() +\n  labs(title = \"Boxplot of Pocket Depth Change by Treatment\")\n\n\n\n\n\n\n\n\nBy eye, it appears as if there is no difference between the placebo and collapsed treatment groups in attachment loss change or pocket depth change. However, both placebo and any treatment conditions appear to have decreased (?) attachment loss and pocket depth. It may be that this study has null results, unless including one of the covariates changes the results. We will see come the analysis section.\nBack to top of tabset\n\n\n\n\n\nNow we will exlore the relationships between our potential covariates and attachment loss and pocket depth change\n\nAgeSitesGenderRaceSmoking Status\n\n\n\n# Plot age vs attachment loss change\nggplot(data_missing, aes(x = age, y = attachchange)) + \n  geom_point() + \n  labs(title = \"Scatterplot of Age vs Attachment Loss Change\")\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n# Plot age vs pocket depth change\nggplot(data_missing, aes(x = age, y = pdchange)) +\n  geom_point() + \n  labs(title = \"Scatterplot of Age vs Pocket Depth Change\")\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nThere does not appear to be any kind of linear relationship between age and attachment loss change or pocket depth change. I will run a model where I include age, but it will likely be removed in the final model.\nBack to top of tabset\n\n\nIs there any relationship between the number of sites measured from and attachment loss change or pocket depth change (e.g. a lower number of sites could lead to less accurate readings)? If so this could be something to include in our data analysis\n\n# Plot sites vs attachment change loss\nggplot(data_missing, aes(x = sites, y = attachchange)) + \n  geom_point() +\n  labs(title = \"Scatterplot of Attachment Loss Change vs Sites\")\n\n\n\n\n\n\n\n# Plot sites vs pocket depth chagne\nggplot(data_missing, aes(x = sites, y = pdchange)) + \n  geom_point() +\n  labs(title = \"Scatterplot of Pocket Depth Change by Sites\")\n\n\n\n\n\n\n\n\nThere does not seem to be a dramatic difference in attachment loss or pocket depth change based on the number of sites measured from. It’s a bit hard to tell with those two low site subjects, but the points are similar enough to the rest of the dataset that I do not think we have to worry about site number in our analysis.\nBack to top of tabset\n\n\nIt is conceivable that there are sex differences in regards to gum health. Let’s examine if there is a difference in attachment loss or pocket depth change based on gender\n\n# Plot gender vs attachment loss change\nggplot(data_missing, aes(x = factor(gender), y = attachchange)) +\n  geom_boxplot() + \n  labs(title = \"Boxplot of Attachment Loss Change by Gender\",\n      x = \"Gender\")\n\n\n\n\n\n\n\n# Plot gender vs pocket depth change\nggplot(data_missing, aes(x = factor(gender), y = pdchange)) +\n  geom_boxplot() + \n  labs(title = \"Boxplot of Pocket Depth Change by Gender\",\n      x = \"Gender\")\n\n\n\n\n\n\n\n\nBy eye, it appears that males may have more pocket depth loss compared with females. This will be a good variable to include as a covariate.\nLet’s try a t-test to see if there is any difference in attachment loss or pocket depth change based on gender\n\n# Running a t-test on attachment loss change by gender\nmale &lt;- data_missing$attachchange[data_missing$gender == 1]\nfemale &lt;- data_missing$attachchange[data_missing$gender == 2]\nt_test_result &lt;- t.test(male, female)\nt_test_result\n\n\n    Welch Two Sample t-test\n\ndata:  male and female\nt = 2.1969, df = 100.8, p-value = 0.03032\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n 0.01003279 0.19682842\nsample estimates:\n  mean of x   mean of y \n-0.03217094 -0.13560154 \n\n\nThere is a significant difference in attachment loss change score based on gender (t = 2.0502, p = 0.04299)\nLet’s repeat for pocket depth change\n\n# Running a t-test on pocket depth change by gender\nmale &lt;- data_missing$pdchange[data_missing$gender == 1]\nfemale &lt;- data_missing$pdchange[data_missing$gender == 2]\nt_test_result &lt;- t.test(male, female)\nt_test_result\n\n\n    Welch Two Sample t-test\n\ndata:  male and female\nt = 2.3534, df = 81.683, p-value = 0.02101\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n 0.01884469 0.22488042\nsample estimates:\n mean of x  mean of y \n-0.2150844 -0.3369470 \n\n\nThere is also a statistically significant difference in pocket depth change based on gender (t = 2.2626, p = 0.02641).\nTherefore I will include gender as a covariate in the analysis!\nBack to top of tabset\n\n\n\n# Plot race vs attachment loss change\nggplot(data_missing, aes(x = factor(race), y = attachchange)) +\n  geom_boxplot() +\n  labs(title = \"Boxplot of Attachment Change by Race\",\n       x = \"Race\")\n\n\n\n\n\n\n\nggplot(data_missing, aes(x = factor(race), y = pdchange)) + \n  geom_boxplot() +\n  labs(title = \"Boxplot of Pocket Depth Change by Race\",\n  x = \"Race\")\n\n\n\n\n\n\n\n\nThere does not seem to be much of a difference in attachment loss or pocket depth based on race. MAYBE African Americans (2) have more pocket depth loss compared to Asians(4), but the sample size was very small for both races (88.1% of the sample identified as White)\nBack to top of tabset\n\n\nWhether the participant is a smoker or not likely has a dramatic effect on gum health. Let’s assess\n\n# Plot smoking status vs attachment loss change\nggplot(data_missing, aes(x = factor(smoker), y = attachchange)) +\n  geom_boxplot() +\n  labs(title = \"Attachment loss by smoking status\",\n       x = \"Smoking Status\")\n\n\n\n\n\n\n\n# Plot smoking status vs pocket depth change\nggplot(data_missing, aes(x = factor(smoker), y = pdchange)) +\n  geom_boxplot() +\n  labs(title = \"Attachment loss by smoking status\",\n       x = \"Smoking Status\")\n\n\n\n\n\n\n\n\nThere does not appear to be any difference in attachment loss or pocket depth change based on smoking status. I will test a model with smoking status included, but it will likely be dropped in the final model.\nBack to top of tabset\n\n\n\n\n\n\nBaseline vs 1 YearAttachment Loss vs Pocket Depth\n\n\nI am curious what the relationship between base line and 1 year measurements are. The study is an RCT so even if baseline measurements affect 1 year measurements, participants should have been randomly assigned to groups so it is essentially controlled for in the study design. It will still be important to assess this relationship as a moderator however. Maybe treatment only worked for those with high attachment loss or pocket depth at the beginning?\nFirst let’s make a simple plot of attachment loss at baseline and at 1 year\n\n# Creating a scatter plot of attachment loss at baseline and 1 year\nggplot(data_missing, aes(x = attachbase, y = attach1year)) + \n  geom_point() +\n  labs(title = \"Scatterplot of Attachment Loss at Baseline and 1 Year\")\n\n\n\n\n\n\n\n\nLet’s check the correlation coefficient.\n\n# Run a correlation test between attachment at base and 1 year\ncorrelation &lt;- cor.test(data_missing$attachbase, data_missing$attach1year)\ncorrelation\n\n\n    Pearson's product-moment correlation\n\ndata:  data_missing$attachbase and data_missing$attach1year\nt = 29.198, df = 101, p-value &lt; 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.9204657 0.9628839\nsample estimates:\n      cor \n0.9455558 \n\n\nThose are highly correlated (R = 0.946, p &lt;.0001)! Let’s run a simple linear regression\n\n# Run a regression with attachment loss at 1 year as the DV and attachment at base as the IV\nmodel &lt;- lm(attach1year ~ attachbase, data = data_missing)\nsummary(model)\n\n\nCall:\nlm(formula = attach1year ~ attachbase, data = data_missing)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.55683 -0.15641 -0.01841  0.15202  0.82062 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.19872    0.06975   2.849  0.00531 ** \nattachbase   0.86452    0.02961  29.198  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2525 on 101 degrees of freedom\nMultiple R-squared:  0.8941,    Adjusted R-squared:  0.893 \nF-statistic: 852.5 on 1 and 101 DF,  p-value: &lt; 2.2e-16\n\n# Plot the model\nggplot(data_missing, aes(x = attachbase, y = attach1year)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", col = \"blue\") +\n  labs(title = \"Simple Linear Regression of Attachment Loss at Baseline and 1 Year\",\n      x = \"Attachment Loss at Baseline\",\n      y = \"Attachment Loss at 1 Year\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nAttachment loss at baseline is a significant predictor of attachment loss at 1 year (t = 29.20, p &lt;.0001). This is important! We should account for attachment loss at baseline by including it as a covariate in our final model!\nLet’s do the same process of pocket depth at baseline and 1 year\n\n# Create a scatterplot of pocket depth at base vs 1 year\nggplot(data_missing, aes(x = pdbase, y = pd1year)) +\n  geom_point() +\n  labs(title = \"Scatterplot of Pocket Depth at Baseline and 1 Year\")\n\n\n\n\n\n\n\n\nSimilar to attachment loss, we see a relationship between pocket depth at baseline and 1 year. Let’s run the correlation.\n\n# Run a correlation between pocket depth at baseline and 1 year\ncorrelation &lt;- cor.test(data_missing$pdbase, data_missing$pd1year)\ncorrelation\n\n\n    Pearson's product-moment correlation\n\ndata:  data_missing$pdbase and data_missing$pd1year\nt = 15.767, df = 101, p-value &lt; 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.7764571 0.8913340\nsample estimates:\n      cor \n0.8432691 \n\n\nWhile not as strong as attachment loss, there is still a strong relationship between pocket depth at baseline and 1 year (R = 0.84, p &lt;.0001). Let’s run the SLR.\n\n# Run an SLR with pocket depth at 1 year as the DV and pocket depth at baseline as the DV\nmodel &lt;- lm(pd1year ~ pdbase, data = data_missing)\nsummary(model)\n\n\nCall:\nlm(formula = pd1year ~ pdbase, data = data_missing)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.55105 -0.17484  0.01996  0.19627  0.64381 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.07504    0.17948   0.418    0.677    \npdbase       0.88346    0.05603  15.767   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2634 on 101 degrees of freedom\nMultiple R-squared:  0.7111,    Adjusted R-squared:  0.7082 \nF-statistic: 248.6 on 1 and 101 DF,  p-value: &lt; 2.2e-16\n\n# Plot the model\nggplot(data_missing, aes(x = pdbase, y = pd1year)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", col = \"blue\") +\n  labs(title = \"Simple Linear Regression of Pocket Depth at Baseline and 1 Year\",\n       x = \"Pocket Depth at Baseline\",\n       y = \"Pocket Depth at 1 year\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nPocket depth at baseline is also a significant predictor of pocket depth at 1 year (t = 15.78, p = &lt;.0001). We should also therefore include pocket depth at baseline as a covariate in our model to control for it!\nBack to top of tabset\n\n\nI am interested in how attachment loss and pocket depth change are related. Since they are both measurements taken from the same sites in the gums, they are likely to be highly correlated. This could have implications on how we perform the analysis and interpret the results.\nFirst, we plot attachment loss change against pocket depth change\n\n# Creating a scatter plot of attachment loss change against pocket depth change \nggplot(data_missing, aes(x = attachchange, y = pdchange)) + \n  geom_point() +\n  labs(title = \"Scatterplot of Attachment Loss Change and Pocket Depth Change\")\n\n\n\n\n\n\n\n\nThat looks like a linear relationship! Let’s run a correlation and an SLR.\n\n# Running the correlation\ncorrelation &lt;- cor.test(data_missing$attachchange, data_missing$pdchange)\ncorrelation\n\n\n    Pearson's product-moment correlation\n\ndata:  data_missing$attachchange and data_missing$pdchange\nt = 6.3717, df = 101, p-value = 5.621e-09\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.3814636 0.6605362\nsample estimates:\n      cor \n0.5354593 \n\n# Running the regression\nmodel &lt;- lm(attachchange ~ pdchange, data = data_missing)\nsummary(model)\n\n\nCall:\nlm(formula = attachchange ~ pdchange, data = data_missing)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.83130 -0.12683  0.00563  0.14913  0.46480 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.06312    0.03441   1.835   0.0695 .  \npdchange     0.55231    0.08668   6.372 5.62e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2343 on 101 degrees of freedom\nMultiple R-squared:  0.2867,    Adjusted R-squared:  0.2797 \nF-statistic:  40.6 on 1 and 101 DF,  p-value: 5.621e-09\n\n# Creating the plot\nggplot(data_missing, aes(x = attachchange, y = pdchange)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", col = \"blue\") +\n  labs(title = \"Simple Linear Regression of Attachment Loss and Pocket Depth Change\",\n       x = \"Pocket Depth Change\",\n       y = \"Attachment Loss Change\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nThe correlation shows that attachment loss and pocket depth change are very correlated (R = 0.54, p &lt; .0001). The simple linear regression shows that pocket depth change significantly predicts attachment loss change (t = 6.37, p &lt;.0001).\nHowever, multicollinearity is only an issue when IVs are correlated with each other. We can still run a multivariate multiple linear regression even though the DVs are correlated. In fact this is often the case, and is one of the justifications for using a multivariate MLR in the first place! Back to top of tabset\n\n\n\n\n\nTreatment Condition\nCollapsing the low, medium, and high concentration gel groups into 1 group does not seem to improve the relationship between treatment and attachment loss or pocket depth change. Only models including all 5 treatment groups will therefore be considered from here on out to keep in alignment with the original study design.\nGender\nGender is related to attachment loss and pocket depth change, and as such will be included in the final model.\nSupressor / Counfound Variables\nA model will be tested with all covariates to assess for possible suppressor / confound variables. But the other potential covariates of race, age, sites, and smoking status were not related to attachment loss or pocket depth change by themselves.\nBaseline vs 1 Year Measurements\nThe baseline measurements of attachment loss and pocket depth were significant predictors of attachment loss and pocket depth at 1 year, respectively. While the RCT nature of the study should ensure that participants were randomly assigned into treatment condition regardless of their baseline measurements, it will still be good practice to include baseline attachment loss and pocket depth into the final model.\nAttachment Loss vs Pocket Depth Change Scores\nAttachment loss and pocket depth change are highly related to each other, but this should not impact the analysis. PI’s will need to be consulted to interpret the clinical significance of findings, and to help fully understand the implications of any possible differences that may arise between attachment loss and pocket depth change in the analysis.\nBack to top of tabset"
  },
  {
    "objectID": "Project_1_Regression/Project_1_Regression_R/Project_1_Regression.html#Model_1",
    "href": "Project_1_Regression/Project_1_Regression_R/Project_1_Regression.html#Model_1",
    "title": "Advanced Data Analysis - Project 1",
    "section": "Running the Model",
    "text": "Running the Model\nFor ease of interpretation I will be performing this analysis as two separate linear regressions, one for each outcome variable of either attachment loss or pocket depth change. I will begin with a simple linear regression only including the PEV and either attachment loss change or pocket depth change.\n\nAttachment Loss ChangePocket Depth ChangePost-Hoc Analysis\n\n\nWe start by constructing a model with attachment loss change as the dependent variable, and treatment group as the independent variable.\n\n# Running a SLR with attachment loss change as the DV, treatment group as the IV with no treatment as the reference group\nmodel_attach1 &lt;- lm(attachchange ~ placebo + low +  medium + high, data = data_missing)\nsummary(model_attach1)\n\n\nCall:\nlm(formula = attachchange ~ placebo + low + medium + high, data = data_missing)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.88283 -0.14813  0.05115  0.17174  0.53945 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.22169    0.05590  -3.966 0.000139 ***\nplacebo      0.13462    0.07906   1.703 0.091771 .  \nlow          0.20388    0.08092   2.520 0.013365 *  \nmedium       0.21514    0.08197   2.625 0.010063 *  \nhigh         0.05690    0.08728   0.652 0.515950    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2681 on 98 degrees of freedom\nMultiple R-squared:  0.09368,   Adjusted R-squared:  0.05669 \nF-statistic: 2.532 on 4 and 98 DF,  p-value: 0.0451\n\n# Apply Bonferroni correction\np_values &lt;- summary(model_attach1)$coefficients[,4] # This line selects the fourth column of the resulting coefficients table from summary(model_attach), which is the p-values\np_adjusted &lt;- p.adjust(p_values, method = \"bonferroni\")\n\n# Compare adjusted p-values to unadjusted p-values\np_comparison &lt;- cbind(p_values, p_adjusted)\np_comparison\n\n                p_values   p_adjusted\n(Intercept) 0.0001392148 0.0006960742\nplacebo     0.0917709125 0.4588545624\nlow         0.0133650749 0.0668253747\nmedium      0.0100625287 0.0503126435\nhigh        0.5159498143 1.0000000000\n\n# Get confidence intervals\nconf_intervals &lt;- confint(model_attach1)\nconf_intervals\n\n                  2.5 %     97.5 %\n(Intercept) -0.33262557 -0.1107577\nplacebo     -0.02226569  0.2915028\nlow          0.04330175  0.3644540\nmedium       0.05247446  0.3777966\nhigh        -0.11629489  0.2300962\n\n\nThe overall model is significant (F(4,98) = 2.53, p = 0.0451). Looking at the individual t-statistics, we see that - following adjustment for multiple pairwise comparisons - the placebo group (p = 0.549), low concentration group (p = 0.0668), and high concentration group (p = 1.000) are not statistically different from the control group. The medium concentration group is approaching significance (p = 0.0503).\nThat was with the no treatment control group as the reference. Let’s see if any of our treatment conditions were different from the placebo group.\n\n# Running a SLR with attachment loss change as the DV, treatment group as the IV with placebo as the reference group\nmodel_attach2 &lt;- lm(attachchange ~ control + low +  medium + high, data = data_missing)\nsummary(model_attach2)\n\n\nCall:\nlm(formula = attachchange ~ control + low + medium + high, data = data_missing)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.88283 -0.14813  0.05115  0.17174  0.53945 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept) -0.08707    0.05590  -1.558   0.1225  \ncontrol     -0.13462    0.07906  -1.703   0.0918 .\nlow          0.06926    0.08092   0.856   0.3941  \nmedium       0.08052    0.08197   0.982   0.3284  \nhigh        -0.07772    0.08728  -0.890   0.3754  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2681 on 98 degrees of freedom\nMultiple R-squared:  0.09368,   Adjusted R-squared:  0.05669 \nF-statistic: 2.532 on 4 and 98 DF,  p-value: 0.0451\n\n# Apply Bonferroni correction\np_values &lt;- summary(model_attach2)$coefficients[,4] # This line selects the fourth column of the resulting coefficients table from summary(model_attach), which is the p-values\np_adjusted &lt;- p.adjust(p_values, method = \"bonferroni\")\n\n# Compare adjusted p-values to unadjusted p-values\np_comparison &lt;- cbind(p_values, p_adjusted)\np_comparison\n\n              p_values p_adjusted\n(Intercept) 0.12254523  0.6127262\ncontrol     0.09177091  0.4588546\nlow         0.39412117  1.0000000\nmedium      0.32836700  1.0000000\nhigh        0.37538435  1.0000000\n\n# Get confidence intervals\nconf_intervals &lt;- confint(model_attach2)\nconf_intervals\n\n                  2.5 %     97.5 %\n(Intercept) -0.19800701 0.02386082\ncontrol     -0.29150281 0.02226569\nlow         -0.09131681 0.22983549\nmedium      -0.08214410 0.24317801\nhigh        -0.25091345 0.09547760\n\n\nAfter applying a bonferroni correction, none of the groups are significantly different from the placebo group (p &gt; .05).\nConclusion\nWhile the overall model was significant, the only groups that were statistically different from the no treatment control were the low and medium concentration gel groups (p &lt; 0.05). However, since this was a placebo-controlled RCT, and none of the treatment groups were significantly different from the placebo group, we can conclude that we fail to reject the null hypothesis that the average attachment loss over 1 year is the same between all groups.\nTables\n\n\nNote: p-values are unadjusted.\nBack to top of tabset\n\n\nNow we will create our model with pocket depth change as the dependent variable and treatment group as the independent variable.\n\n# Running the regression with pocket depth change as the DV, treatment group as the IV with no treatment as the reference group\nmodel_pd &lt;- lm(pdchange ~ placebo + low + medium + high, data = data_missing)\nsummary(model_pd)\n\n\nCall:\nlm(formula = pdchange ~ placebo + low + medium + high, data = data_missing)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.62483 -0.14595 -0.01768  0.16029  0.66130 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.33817    0.05466  -6.187 1.43e-08 ***\nplacebo     -0.01152    0.07730  -0.149   0.8818    \nlow          0.13200    0.07912   1.668   0.0984 .  \nmedium       0.13562    0.08015   1.692   0.0938 .  \nhigh        -0.04413    0.08534  -0.517   0.6063    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2621 on 98 degrees of freedom\nMultiple R-squared:  0.07806,   Adjusted R-squared:  0.04043 \nF-statistic: 2.074 on 4 and 98 DF,  p-value: 0.08994\n\n\nThe overall model is not statistically significant (F(4,98)= 2.074, p = 0.0899). Based on this model, it appears that the gel treatment has no effect on pocket depth change after 1 year.\nConclusion\nNo groups were significantly different from each other in pocket depth change after 1 year (F(4,98)= 2.074, p = 0.0899), and we fail to reject the null hypothesis that the average pocket depth change over 1 year is the same between all groups.\nTables\n\nNote: p-values are unadjusted\nBack to top of tabset\n\n\nWe did not find a main effect based on treatment group. However I am still interested if including any of the covariates changes the results.\n\n# Run a SLR with attachment loss change as the DV and gender as a covariate\nmodel2_attach &lt;- lm(attachchange ~ placebo + low + medium + high + gender, data = data_missing) \nsummary(model2_attach)\n\n\nCall:\nlm(formula = attachchange ~ placebo + low + medium + high + gender, \n    data = data_missing)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.86656 -0.17533  0.02334  0.16395  0.57717 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept) -0.07460    0.10988  -0.679   0.4988  \nplacebo      0.12330    0.07883   1.564   0.1210  \nlow          0.19310    0.08064   2.395   0.0186 *\nmedium       0.21118    0.08143   2.593   0.0110 *\nhigh         0.06704    0.08690   0.771   0.4423  \ngender      -0.08675    0.05593  -1.551   0.1242  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2662 on 97 degrees of freedom\nMultiple R-squared:  0.1156,    Adjusted R-squared:  0.07003 \nF-statistic: 2.536 on 5 and 97 DF,  p-value: 0.03346\n\n# Apply Bonferroni correction\np_values &lt;- summary(model2_attach)$coefficients[,4] # This line selects the fourth column of the resulting coefficients table from summary(model_attach), which is the p-values\np_adjusted &lt;- p.adjust(p_values, method = \"bonferroni\")\n\n# Compare adjusted p-values to unadjusted p-values\np_comparison &lt;- cbind(p_values, p_adjusted)\np_comparison\n\n              p_values p_adjusted\n(Intercept) 0.49883151 1.00000000\nplacebo     0.12104977 0.72629859\nlow         0.01856222 0.11137333\nmedium      0.01097151 0.06582903\nhigh        0.44234244 1.00000000\ngender      0.12415264 0.74491582\n\n# Get confidence intervals\nconf_intervals &lt;- confint(model2_attach)\nconf_intervals\n\n                  2.5 %     97.5 %\n(Intercept) -0.29268916 0.14349242\nplacebo     -0.03315881 0.27976619\nlow          0.03304940 0.35315426\nmedium       0.04956814 0.37278247\nhigh        -0.10544029 0.23951403\ngender      -0.19775102 0.02425639\n\n\nA model including gender does not seem to help anything. What about with all covariates (just for fun)?\n\n# Run a SLR with attachment loss change as the DV and gender as a covariate\nmodel3_attach &lt;- lm(attachchange ~ placebo + low + medium + high + gender + race + age + smoker + sites + attachbase + pdbase , data = data_missing) \nsummary(model3_attach)\n\n\nCall:\nlm(formula = attachchange ~ placebo + low + medium + high + gender + \n    race + age + smoker + sites + attachbase + pdbase, data = data_missing)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.60415 -0.16360  0.02131  0.16645  0.57481 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.160630   0.480855   0.334 0.739127    \nplacebo      0.017898   0.079113   0.226 0.821541    \nlow          0.151477   0.077598   1.952 0.054074 .  \nmedium       0.172196   0.079066   2.178 0.032060 *  \nhigh         0.045948   0.081336   0.565 0.573557    \ngender      -0.050360   0.054953  -0.916 0.361924    \nrace         0.032325   0.026871   1.203 0.232181    \nage         -0.002472   0.002709  -0.912 0.364067    \nsmoker       0.069384   0.055052   1.260 0.210844    \nsites       -0.001501   0.002539  -0.591 0.555885    \nattachbase  -0.165154   0.043233  -3.820 0.000246 ***\npdbase       0.094444   0.072226   1.308 0.194370    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2456 on 89 degrees of freedom\n  (2 observations deleted due to missingness)\nMultiple R-squared:  0.2908,    Adjusted R-squared:  0.2032 \nF-statistic: 3.318 on 11 and 89 DF,  p-value: 0.0007435\n\n# Apply Bonferroni correction\np_values &lt;- summary(model3_attach)$coefficients[,4] # This line selects the fourth column of the resulting coefficients table from summary(model_attach), which is the p-values\np_adjusted &lt;- p.adjust(p_values, method = \"bonferroni\")\n\n# Compare adjusted p-values to unadjusted p-values\np_comparison &lt;- cbind(p_values, p_adjusted)\np_comparison\n\n               p_values  p_adjusted\n(Intercept) 0.739127379 1.000000000\nplacebo     0.821540779 1.000000000\nlow         0.054074263 0.648891153\nmedium      0.032060232 0.384722790\nhigh        0.573556835 1.000000000\ngender      0.361924208 1.000000000\nrace        0.232180658 1.000000000\nage         0.364067210 1.000000000\nsmoker      0.210844428 1.000000000\nsites       0.555885079 1.000000000\nattachbase  0.000246493 0.002957916\npdbase      0.194369803 1.000000000\n\n# Get confidence intervals\nconf_intervals &lt;- confint(model3_attach)\nconf_intervals\n\n                   2.5 %       97.5 %\n(Intercept) -0.794818756  1.116078333\nplacebo     -0.139298371  0.175094025\nlow         -0.002709329  0.305662938\nmedium       0.015093048  0.329298305\nhigh        -0.115665991  0.207561289\ngender      -0.159551369  0.058830891\nrace        -0.021067104  0.085716311\nage         -0.007855313  0.002911679\nsmoker      -0.040003508  0.178771331\nsites       -0.006546011  0.003543891\nattachbase  -0.251056725 -0.079251471\npdbase      -0.049067286  0.237955593\n\n\nNope. Interestingly the best predictor of attachment loss at 1 year is attachment loss at baseline. The results for pocket depth are likely the same.\nI also want to assess if controlling for baseline attachment loss or pocket depth change affects things, since those were strong predictors of each measurement at 1 year (still need to add those SLRs).\n\nmodel4_attach &lt;- lm(attachchange ~ placebo + low + medium + high + attachbase, data = data_missing)\nsummary(model4_attach)\n\n\nCall:\nlm(formula = attachchange ~ placebo + low + medium + high + attachbase, \n    data = data_missing)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.52469 -0.16400  0.02084  0.15231  0.73422 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.10707    0.09304   1.151   0.2527    \nplacebo      0.04202    0.07616   0.552   0.5824    \nlow          0.14605    0.07592   1.924   0.0573 .  \nmedium       0.17586    0.07622   2.307   0.0232 *  \nhigh         0.02665    0.08087   0.330   0.7425    \nattachbase  -0.12902    0.03039  -4.246 4.99e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2475 on 97 degrees of freedom\nMultiple R-squared:  0.2357,    Adjusted R-squared:  0.1963 \nF-statistic: 5.984 on 5 and 97 DF,  p-value: 7.236e-05\n\n# Apply Bonferroni correction\np_values &lt;- summary(model4_attach)$coefficients[,4] # This line selects the fourth column of the resulting coefficients table from summary(model_attach), which is the p-values\np_adjusted &lt;- p.adjust(p_values, method = \"bonferroni\")\n\n# Compare adjusted p-values to unadjusted p-values\np_comparison &lt;- cbind(p_values, p_adjusted)\np_comparison\n\n                p_values   p_adjusted\n(Intercept) 2.526878e-01 1.0000000000\nplacebo     5.824313e-01 1.0000000000\nlow         5.731590e-02 0.3438953943\nmedium      2.317045e-02 0.1390227138\nhigh        7.424826e-01 1.0000000000\nattachbase  4.989769e-05 0.0002993861\n\n\nAfter controlling for treatment group, the only significant predictor of attachment loss change is baseline attachment loss scores (padj = 0.001)\nThese models were just for exploration, fun, and practice. The final models selected are the SLR’s with treatment group as the IV and either attachment loss change or pocket depth change as the DV. Back to top of tabset"
  },
  {
    "objectID": "Project_1_Regression/Project_1_Regression_R/Project_1_Regression.html#Assumptions",
    "href": "Project_1_Regression/Project_1_Regression_R/Project_1_Regression.html#Assumptions",
    "title": "Advanced Data Analysis - Project 1",
    "section": "Evaluating Assumptions",
    "text": "Evaluating Assumptions\nIn order to evaluate the assumptions of our models, we will first gather the residuals of the model predicting attachment loss change score and the model predicting pocket depth change score.\n\n# Calculate the jackknife residuals of model_attach\njackknife_residuals_attach &lt;- rstudent(model_attach1)\n\n# Calculate the jackknife residuals of model_pd\njackknife_residuals_pd &lt;- rstudent(model_pd)\n\nNow that we have our residuals, we can take a closer look at the assumptions.\n\nLinearityIndependenceNormalityEqual Variances (Homoscedasticity)Residuals Centered Around ZeroSummary\n\n\nSince the IV is categorical, we do not need to assess linearity.\nBack to top of tabset\n\n\nIndependence can be assessed in part by the study design and how the data was collected. Based on the information provided by the PI, I will assume subjects are independent from each other (e.g. not siblings).\nAdditionally, we can examine a scatter plot of the model’s residuals against any time point variable (such as ID). Let’s do that.\n\n# Create a scatterplot of jackknife residuals vs ID to assess independence for pocket depth change\nggplot(data_missing, aes(x = id, y = jackknife_residuals_attach)) + \n  geom_point() +\n  geom_hline(yintercept = 0) +\n  labs(title = \"Scatterplot of Jackknife Residuals vs ID for Attachment Loss Change\",\n       x = \"ID\",\n       y = \"Jackknife Residuals\")\n\n\n\n\n\n\n\n# Create a scatterplot of jackknife residuals vs ID to assess independence for pocket depth change\nggplot(data_missing, aes(x = id, y = jackknife_residuals_pd)) + \n  geom_point() +\n  geom_hline(yintercept = 0) +\n  labs(title = \"Scatterplot of Residuals vs ID for Pocket Depth Change\",\n       x = \"ID\",\n       y = \"Jackknife Residuals\")\n\n\n\n\n\n\n\n\nThe pattern appears random, suggesting independence.\nNote: The gap between ID’s 170 and 200 looks odd, but is an artifact from how the experimenters assigned ID, with females starting at 101, and males starting at 201.\nBack to top of tabset\n\n\nHere we will asses that, for any fixed value of X, Y has a normal distribution. We will do this using Q-Q plots and histograms of the residuals.\n\n# Make the Q-Q plots using the jackknife residuals for attachment loss change\nqqnorm(jackknife_residuals_attach, main = \"Q-Q plots of Jackknife Residuals for model_attach\")\nqqline(jackknife_residuals_attach, col = \"black\")\n\n\n\n\n\n\n\n# Create histogram of jackknife residuals for model_attach\nhist(jackknife_residuals_attach, main = \"Histogram of Jackknife Residuals\",\n     xlab = \"Jackknife Residuals\",\n     col = \"lightblue\",\n     border = \"black\")\n\n\n\n\n\n\n\n# Make the Q-Q plots using the jackknife residuals for pocket depth change\nqqnorm(jackknife_residuals_pd, main = \"Q-Q plots of Jackknife Residuals for model_pd\")\nqqline(jackknife_residuals_pd, col = \"black\")\n\n\n\n\n\n\n\n# Create histogram of jackknife residuals for model_pd\nhist(jackknife_residuals_pd, main = \"Histogram of Jackknife Residuals\",\n     xlab = \"Jackknife Residuals\",\n     col = \"lightblue\",\n     border = \"black\")\n\n\n\n\n\n\n\n\nThe histogram for attachment loss change is a little left-tailed. This could be from an outlier. Comparatively, the Q-Q plot and histogram of the residuals for pocket depth change are normally distributed.\nWe can also include the Shapiro-Wilk test of normality, which provides a p-value and allows us to numerically establish that the assumption of normality is met. If the p-value is &lt; 0.05 you conclude that the assumption of normality is not met.\n\nshapiro.test(jackknife_residuals_attach)\n\n\n    Shapiro-Wilk normality test\n\ndata:  jackknife_residuals_attach\nW = 0.9604, p-value = 0.003601\n\nshapiro.test(jackknife_residuals_pd)\n\n\n    Shapiro-Wilk normality test\n\ndata:  jackknife_residuals_pd\nW = 0.99059, p-value = 0.6933\n\n\nThe assumption of normality is violated for the attachment loss change model (p = 0.0036), but not for the pocket depth change model (p = 0.6933). Looking at the histogram of the residuals for attachment loss change, this is likely due to an outlier.\nSummary\nFor attachment loss change, we have a slight violation of normality, which could be due to the presence of an outlier. However, regressions are robust to violations of assumptions and this may not actually be an issue. Outliers in the model will be assessed using jackknife residuals to confirm that these points do not have an excessive amount of influence on the model.\nBased on the Q-Q plots and histograms of the residuals for pocket depth change, we can conclude that we satisfy the assumption of normality.\nBack to top of tabset\n\n\nUsing the scale-location plot will allow us to evaluate the constant variance assumption. This will allow us to see whether the variability of the residuals is roughly constant between each group (Source).\nTo assess homoscedasticity we examine the residual scatterplots by treatment group. The warning sign to look for here is if the variance differs greatly across groups.\n\n# Make the residual scatterplot using the jackknife residuals for model_attach\nggplot(data_missing, aes(x = trtgroup, y = jackknife_residuals_attach)) +\n  geom_point() + \n  labs(title = \"Jackknife Residuals vs Treatment Condition for Attachment Loss Change\",\n       x = \"Treatment Condition\",\n       y = \"JackKnife Residuals\")\n\n\n\n\n\n\n\n# Make the residual scatterplot using the jackknife residuals for model_pd\nggplot(data_missing, aes(x = trtgroup, y = jackknife_residuals_pd)) +\n  geom_point() + \n  labs(title = \"Jackknife Residuals vs Treatment Condition for Pocket Depth Change\",\n       x = \"Treatment Condition\",\n       y = \"JackKnife Residuals\")\n\n\n\n\n\n\n\n\nIt appears that our variances in all groups are equal!\nAdditionally, while it is not recommended to perform a statistical test to assess for equality of variances (because formal tests of equality of variance are not very powerful), we can still do this using Bartlett’s test.\n\nbartlett.test(attachchange ~ trtgroup, data = data_missing)\n\n\n    Bartlett test of homogeneity of variances\n\ndata:  attachchange by trtgroup\nBartlett's K-squared = 2.5462, df = 4, p-value = 0.6364\n\n\nThe null hypothesis of the Bartlett test is that the variances are equal. Thus failing to reject the null (p &gt; 0.05) indicates that the data are consistent with the equal variance assumption.\nSo we meet the assumption of equality of variances, looking good!\nBack to top of tabset\n\n\nTo start, we can simply check that the mean of our residuals is close to 0.\n\n#### For attachment loss change score\n# Generate the mean of the jackknife residuals for attachment loss change. Should be close to 0.\nmean(jackknife_residuals_attach)\n\n[1] -0.004132486\n\n\nWe are looking good for attachment loss change score. What about for pocket depth change?\n\n#### For pocket depth change score\n# Generate the mean of the jackknife residuals for attachment loss change. Should be close to 0.\nmean(jackknife_residuals_pd)\n\n[1] -6.695748e-06\n\n\nA more sophisticated approach is to plot the fitted values vs jackknife residuals. We can then compare the trend between groups to see if they are random. The fitted line should gravitate around 0 with no obvious trends.\n\n# Generate vectors containing the fitted values vs jackknife residuals so we can plot them \nfitted_values_pd &lt;- fitted(model_pd)\n\nggplot(data_missing, aes(x = fitted_values_pd, y = jackknife_residuals_pd)) + \n  geom_point() +\n  geom_smooth(method = \"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n  labs(title = \"Jackknife Residuals vs Fitted Values for Attachment Loss Change\",\n       x = \"Fitted Values\",\n       y = \"Jackknife Residuals\")\n\n$x\n[1] \"Fitted Values\"\n\n$y\n[1] \"Jackknife Residuals\"\n\n$title\n[1] \"Jackknife Residuals vs Fitted Values for Attachment Loss Change\"\n\nattr(,\"class\")\n[1] \"labels\"\n\n\nThe pattern looks random and we can conclude we meet this assumption.\nBack to top of tabset\n\n\nWe meet the assumptions of independence, equal variances, and errors centered around zero required for this analysis. There is a slight violation of normality for the attachment loss change model, but this could be due to an outlier."
  },
  {
    "objectID": "Project_1_Regression/Project_1_Regression_R/Project_1_Regression.html#Multiple_imputation",
    "href": "Project_1_Regression/Project_1_Regression_R/Project_1_Regression.html#Multiple_imputation",
    "title": "Advanced Data Analysis - Project 1",
    "section": "Multiple Imputation",
    "text": "Multiple Imputation\n\nBackgroundCoding ExamplePerforming the Multiple Imputation\n\n\nNote: This information is taken from Biostats II (BIOS 6612) slides, week 13 lecture 20, and from Flexible Imputation of Missing Data 2nd edition.\nMissing Data\nTo review, there are several assumptions for missing data. The first is MCAR (Missing Completely at Random). This is when the probability of having a missing value is the same for everyone, and is rarely ever the case with real data.\nThe second is MAR (missing at random). This is when the probability of having a missing value is the same within groups defined by the observed data. That is, if we know that second variable by which the data is NOT MCAR, in addition to the measurements, we can assume MCAR within that second variable. For instance, if we know the missing data is not equal between the sexes, then we can assume MCAR within groups defined by sex. MAR is often observed in situations such as males missing more data than females, or older participants missing more data than younger ones, etc. MAR is more general and more realistic than MCAR. Modern missing data assumptions typically start from the MAR assumption.\nMNAR (Missing Not At Random) is the final category of missingness, and is when the the probability of having a missing value varies for reasons that are unknown to you. For example, in public opinion research, those with weaker opinions may respond less than those with stronger opinions, and you may not know this ahead of time. MNAR is the most complex and if you have it, you have your work cut out for you. In that situation, you can find more data about the causes for the missingness, or run lots and lots of sensitivity analyses.\nAnd as they say, the best way to handle missing data is not to have it.\nMultiple Imputation\nIn the current experiment, we know that data is not MCAR (males are missing more data than females), and thus cannot simply throw out subjects with missing values. If the data are not MCAR, listwise deletion (deleting any subject with a missing value) can severely bias estimates of means, regression coefficients and correlations.\nMultiple Imputation is a commonly used method to handle missing data when data is not MCAR, but is MAR. It is a process by which you use observed data to “predict” missing data, then use those “imputed” values in further analyses. Multiple imputation builds upon and pools together “single” imputation approaches.\nFor example, there exists very simple ways of imputing data, such as plugging in the average or median values in place of missing values. This is not sophisticated and kind of sketchy.\nThen there exists regression imputation, where for each variable you fit a regression model of it to the observed data, and then use that model to predict missing values, and use those predictions in place of the missing values. However, this injects bias into the estimate for the correlation between X and Y (since the values fall perfectly in line with the hypothesis that X and Y have a non zero correlation. (and are in a perfect line)).\n\nThen there is stochastic regression imputation, which is like regression imputation but adds noise back into the imputations based on the variance of their residuals. This helps account for variance in the results due to missing data.\n\nMultiple imputation works by fitting multiple stochastic imputation models (can be 100’s or 1000’s), analyzing each dataset separately to produce estimates of interest, and finally pooling together these statistics of interest. If done properly, the pooled statistics are unbiased under MAR, and the SE’s will be correct!\n\nMultiple Imputation is “simple, elegant and powerful. It is simple because it fills the holes in the data with plausible values. It is elegant because the uncertainty about the unknown data is coded in the data itself. And it is powerful because it can solve “other” problems that are actually missing data problems in disguise.” - Stef van Buuren\nBack to top of tabset\n\n\nFirst we will begin with an example of multiple imputation using the built in airquality dataset in R.\n\n# Examining data set and missingness\nhead(airquality)\n\n  Ozone Solar.R Wind Temp Month Day\n1    41     190  7.4   67     5   1\n2    36     118  8.0   72     5   2\n3    12     149 12.6   74     5   3\n4    18     313 11.5   62     5   4\n5    NA      NA 14.3   56     5   5\n6    28      NA 14.9   66     5   6\n\ncolSums(is.na(airquality))\n\n  Ozone Solar.R    Wind    Temp   Month     Day \n     37       7       0       0       0       0 \n\nvis_miss(airquality)\n\n\n\n\n\n\n\n\nWe’re missing 37 values in the Ozone column (24% of values!). We will have to address this missing data somehow.\nMean Imputation\nWe could quickly fill in missing data with mean imputation…\n\n# Here's how you do mean imputation in mice\nimp &lt;- mice(airquality, method = \"mean\", m = 1, maxit = 1)\n\n\n iter imp variable\n  1   1  Ozone  Solar.R\n\n\nBut mean imputation will “underestimate the variance, disturb the relations between variables, bias almost any estimate other than the mean and bias the estimate of the mean when data are not MCAR.” You can use mean imputation as a quick fix when there’s a handful of missing values, but it should be avoided in general.\nRegression Imputation\nWe could alternatively do regression imputation, where we fit a model and then use that model to predict the missing values. In regression imputation, you are essentially using the observed data to predict the missing data.\n\n# Performing regression imputation using the mice package\ndata_example &lt;- airquality[, c(\"Ozone\", \"Solar.R\")]\nimp &lt;- mice(data_example, method = \"norm.predict\", seed = 1,\n           m = 1, print = FALSE)\n\n# Plotting missing vs observed values\nxyplot(imp, Ozone ~ Solar.R,\n       main = \"Missing vs Observed values for Ozone ~ Solar.R\",\n       auto.key = list(corner = c(0,1),\n                  points = TRUE, \n                  text = c(\"Observed\", \"Missing\"), col = c(\"steelblue\", \"red3\")))\n\n\n\n\n\n\n\n\nYou predict the values of the missing datapoints with the regression equation resulting from your model. That is, the imputed values correspond to the most likely values under that model. However, the imputed values (red) vary less than the observed values (blue). So each value is the best under the model, but it is very unlikely that the real values would have had this distribution.\nRegression imputation yields unbiased estimates of the means and of the regression weights of the model under MCAR. It also does so under MAR, provided that the factors that influence missinginess are part of the model. However, correlations are biased to be greater/higher.\n“Regression imputation, as well as its modern incarnations in machine learning is probably the most dangerous of all methods described here. We may be led to believe that we’re to do a good job by preserving the relations between the variables. In reality however, regression imputation artificially strengthens the relations in the data. Correlations are biased upwards. Variability is underestimated. Imputations are too good to be true. Regression imputation is a recipe for false positive and spurious relations.”\nStochastic regression imputation\nStochastic regression imputation is a refinement of regression imputation that attempts to address correlation bias by adding noise back into the predictions of the missing values.\n\n# Impute Ozone from Solar.R by stochastic regression imputation using the mice package.\ndata_example &lt;- airquality[, c(\"Ozone\", \"Solar.R\")]\n\n# Perform stochastic regression imputation\nimp &lt;- mice(data_example, method = \"norm.nob\", m = 1, maxit = 1, seed = 1, print = FALSE)\n\n# Plotting missing vs observed values\nxyplot(imp, Ozone ~ Solar.R,\n       main = \"Missing vs Observed values for Ozone ~ Solar.R\",\n       auto.key = list(corner = c(0,1),\n                  points = TRUE, \n                  text = c(\"Observed\", \"Missing\"), col = c(\"steelblue\", \"red3\")))\n\n\n\n\n\n\n\n\n“The method = norm.nob argument requests a plain, non-Bayesian, stochastic regression method. This method first estimates the intercept, slope and residual variance under the linear model, then calculates the predicted value for each missing value, and adds a random draw from the residual to the prediction”.\nThis can create issues though, such as we have one imputed value that is negative! Which might not be plausible (such as in this case, there is no such thing as a negative ozone level).\nA more convenient solution is multiple imputation\nMultiple Imputation\nMultiple imputation creates m &gt; 1 complete data sets. The m results are then pooled into a final point estimate plus standard error. So each data set is identical in observed values, but differs in imputed values.\nWe then estimate the parameters of interest from each dataset. This is done by applying the analytic method you would have used if the dataset was complete in the first place (here, a regression). The results of the model on each dataset will differ because the data is different. These differences are caused by the uncertainty of what value to impute.\nThe last step is that these parameter estimates are then pooled together into a single value, and its variance estimated. The variance is assessed by combining the “within-imputation variance with the extra variance caused by the missing data (between-imputation variance)”. So under the appropriate conditions the pooled estimates are unbiased. MI solves the problem of ‘too small’ SEs of other imputation methods we just covered.\nNow let’s perform the multiple imputation on the airquality data set.\n\n# Perform multiple imputation on the airquality data set\nimp &lt;- mice(airquality, seed = 1, m = 20, print = FALSE) # This line imputes the missing data 20 times\n\n# Fit a linear regression \nmodel_imp &lt;- with(imp, lm(Ozone ~ Wind + Temp + Solar.R)) # You have to run the regression with \"with(imp, lm(etc))\n                  \nsummary(model_imp) # This runs a regression on each imputed dataset (so 20 different regressions)\n\n# A tibble: 80 × 6\n   term        estimate std.error statistic  p.value  nobs\n   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;int&gt;\n 1 (Intercept) -60.7      17.9        -3.40 8.78e- 4   153\n 2 Wind         -2.95      0.514      -5.75 4.96e- 8   153\n 3 Temp          1.53      0.197       7.74 1.42e-12   153\n 4 Solar.R       0.0671    0.0184      3.64 3.70e- 4   153\n 5 (Intercept) -56.8      18.5        -3.07 2.56e- 3   153\n 6 Wind         -3.33      0.532      -6.25 4.04e- 9   153\n 7 Temp          1.56      0.205       7.58 3.49e-12   153\n 8 Solar.R       0.0554    0.0191      2.89 4.37e- 3   153\n 9 (Intercept) -74.0      18.7        -3.95 1.19e- 4   153\n10 Wind         -2.69      0.540      -4.99 1.67e- 6   153\n# ℹ 70 more rows\n\nsummary(pool(model_imp)) # This pools together the parameters of all 20 regressions (statistic is Wald's test) into a single model.\n\n         term     estimate   std.error statistic       df      p.value\n1 (Intercept) -65.87829658 23.09377412 -2.852643 69.97033 5.696702e-03\n2        Wind  -3.01897171  0.66252377 -4.556775 70.51194 2.125022e-05\n3        Temp   1.63483547  0.25107557  6.511328 75.99913 7.203792e-09\n4     Solar.R   0.05861581  0.02267832  2.584662 90.10797 1.135441e-02\n\n\n\n# mids workflow using pipes\nest3 &lt;- airquality %&gt;%\n  mice(seed = 1, print = FALSE, m = 20) %&gt;%\n  with(lm(formula = Ozone ~ Wind + Temp + Solar.R)) %&gt;%\n  pool()\nsummary(est3)\n\n         term     estimate   std.error statistic       df      p.value\n1 (Intercept) -65.87829658 23.09377412 -2.852643 69.97033 5.696702e-03\n2        Wind  -3.01897171  0.66252377 -4.556775 70.51194 2.125022e-05\n3        Temp   1.63483547  0.25107557  6.511328 75.99913 7.203792e-09\n4     Solar.R   0.05861581  0.02267832  2.584662 90.10797 1.135441e-02\n\n\n\n# Run a regression on the imputed airquality data set. \nimp &lt;- mice(airquality, seed = 1, print = FALSE)\n\nfit &lt;- with(imp, lm(Ozone ~ Solar.R))\n\n# Print out the estimates for the first and second data set\ncoef(fit$analyses[[1]])\n\n(Intercept)     Solar.R \n 22.2963201   0.1057437 \n\ncoef(fit$analyses[[2]])\n\n(Intercept)     Solar.R \n 20.7891622   0.1140081 \n\n\nNotice that the paremter estimates differ because of the uncertainty created by the missing data.\nApplying the standard pooling rules is done with\n\nest &lt;- pool(fit)\nsummary(est)\n\n         term   estimate  std.error statistic       df      p.value\n1 (Intercept) 21.5321395 5.67991951  3.790923 136.3544 0.0002245626\n2     Solar.R  0.1091546 0.02846503  3.834693 102.7169 0.0002170873\n\n\nAny R expression produced by expression() can be used on the multiply imputed data…\n\n# Extract residuals and fitted values\n\n# This gets you ALL the residuals for ALL the models/dataset\nimp_residuals &lt;- sapply(model_imp[[4]], residuals)\n\n# Same thing but for the fitted\nimp_fitted &lt;- sapply(model_imp[[4]],fitted)\n\n# Don't know where to go from here. Apparently you can take the average of all of these to get the average residuals, but it's not published anywhere.\n\nWe can compare the results of our multiple imputation model with the listwise deletion model to see how different they came out.\n\nmodel_non_imp &lt;- lm(Ozone ~ Wind + Temp + Solar.R, data = airquality, na.action = na.omit)\nsummary(model_non_imp)\n\n\nCall:\nlm(formula = Ozone ~ Wind + Temp + Solar.R, data = airquality, \n    na.action = na.omit)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-40.485 -14.219  -3.551  10.097  95.619 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -64.34208   23.05472  -2.791  0.00623 ** \nWind         -3.33359    0.65441  -5.094 1.52e-06 ***\nTemp          1.65209    0.25353   6.516 2.42e-09 ***\nSolar.R       0.05982    0.02319   2.580  0.01124 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 21.18 on 107 degrees of freedom\n  (42 observations deleted due to missingness)\nMultiple R-squared:  0.6059,    Adjusted R-squared:  0.5948 \nF-statistic: 54.83 on 3 and 107 DF,  p-value: &lt; 2.2e-16\n\nsummary(pool(model_imp))\n\n         term     estimate   std.error statistic       df      p.value\n1 (Intercept) -65.87829658 23.09377412 -2.852643 69.97033 5.696702e-03\n2        Wind  -3.01897171  0.66252377 -4.556775 70.51194 2.125022e-05\n3        Temp   1.63483547  0.25107557  6.511328 75.99913 7.203792e-09\n4     Solar.R   0.05861581  0.02267832  2.584662 90.10797 1.135441e-02\n\n\nOur regression using MI and the complete case (i.e. listwise deletion) data set are comparable!\nNote: what we are NOT doing in multiple imputation is taking an average of the imputed data and running a model on that as if its a single, complete data set. “Researchers are often tempted to average the multiply imputed data, and analyze the averaged data as if it were complete. This method yields incorrect standard errors, confidence intervals and p-values, and thus should not be used if any form of statistical testing or uncertainty analysis is to be done on the imputed data. The reason is that the procedure ignores the between-imputation variability, and hence shares all the drawbacks of single imputation”.\nNote: It’s recommended to impute then transform, because if you create variables based on other variables, there are relationships between those variables that MI doesn’t account for (it might create combinations of variables that are unrealistic)\nYou can do this as\n\n# Example of impute then transform method (i.e. how to add variables to an imputed dataset)\ndata_ex &lt;- boys[, c(\"age\", \"hgt\", \"wgt\", \"hc\", \"reg\")]\nimp &lt;- mice(data_ex, print = FALSE, seed = 1)\n \n# put the data in long format\nlong &lt;- mice::complete(imp, \"long\", include = TRUE)\n\nlong$new_var &lt;- with(long, 100 * wgt / hgt)\n\nimp.itt &lt;- as.mids(long)\n\nBack to top of tabset\n\n\nNow that we have reviewed the background and gone through coding examples of multiple imputation, we are ready to perform it for the dataset for Project 1.\n\n# Pool the parameters of interest for a regression on the imputed values for attach change control as reference\nmodel_imp &lt;- data %&gt;%\n  mice(seed = 1, print = FALSE, m = 20) %&gt;%\n  with(lm(attachchange ~ placebo + low + medium + high)) %&gt;%\n  pool()\n\nWarning: Number of logged events: 1158\n\n\nNone of the p-values for the pooled regressions on the MI data are significant (p &gt; 0.05).\nHow do those parameter estimates compare to the complete case analysis model?\n\n# Compare coefficients to complete case analysis for attachment loss with control as reference category\nMI &lt;- summary(model_imp)\nMI\n\n         term   estimate  std.error  statistic       df     p.value\n1 (Intercept) -0.1853493 0.06920536 -2.6782512 65.79196 0.009337056\n2     placebo  0.1158204 0.09076258  1.2760813 91.37003 0.205161055\n3         low  0.1608502 0.10027097  1.6041549 59.48514 0.113976747\n4      medium  0.1612700 0.11393844  1.4154130 37.61151 0.165174849\n5        high  0.0930271 0.15435067  0.6026997 18.41109 0.554060413\n\ncomplete_case &lt;- summary(model_attach1)\ncomplete_case$coefficients\n\n               Estimate Std. Error    t value     Pr(&gt;|t|)\n(Intercept) -0.22169165 0.05590110 -3.9657832 0.0001392148\nplacebo      0.13461856 0.07905610  1.7028232 0.0917709125\nlow          0.20387790 0.08091650  2.5196086 0.0133650749\nmedium       0.21513551 0.08196711  2.6246567 0.0100625287\nhigh         0.05690063 0.08727557  0.6519652 0.5159498143\n\n\nThey are drastically different, although still not significant.\nWe can also plot the imputed values for a sample of m dataset (not too many or it’s hard to see!) in order to visually assess that our imputed values are comparabe to the observed values.\n\n# Plot imputed values for m = 5 data set\nimpy &lt;- mice(data, seed = 1, print = FALSE, m = 5)\n\nWarning: Number of logged events: 285\n\nmodel_impy &lt;- with(imp, attachchange ~ trtgroup)\n\n# This is cool, here's how I can plot the imputed values for attach change \nxyplot(impy, attachchange ~ trtgroup,\n       main = \"Example of Imputed Values for m = 5 Dataset\",\n       auto.key = list(space = \"right\",\n        points = TRUE, \n        text = c(\"Observed\", \"Missing\"), col = c(\"blue4\", \"red3\")))\n\n\n\n\n\n\n\n\nWhat about for the same model but with placebo as the reference group?\n\n# Pool the parameters of interest for a regression on the imputed values for attach change placebo as reference\nmodel_imp &lt;- data %&gt;%\n  mice(seed = 1, print = FALSE, m = 20) %&gt;%\n  with(lm(attachchange ~ control + low + medium + high)) %&gt;%\n  pool()\n\nWarning: Number of logged events: 1158\n\n# Compare coefficients for complete case analysis with attachment with placebo as reference category\nsummary(model_imp)\n\n         term    estimate  std.error  statistic       df   p.value\n1 (Intercept) -0.06952890 0.06697153 -1.0381859 75.81298 0.3024834\n2     control -0.11582043 0.09076258 -1.2760813 91.37003 0.2051611\n3         low  0.04502973 0.11027780  0.4083299 41.79227 0.6851161\n4      medium  0.04544952 0.11870540  0.3828766 33.28878 0.7042453\n5        high -0.02279333 0.16350701 -0.1394028 16.62753 0.8908064\n\nsummary(model_attach2)$coefficients\n\n               Estimate Std. Error    t value   Pr(&gt;|t|)\n(Intercept) -0.08707309 0.05590110 -1.5576275 0.12254523\ncontrol     -0.13461856 0.07905610 -1.7028232 0.09177091\nlow          0.06925934 0.08091650  0.8559360 0.39412117\nmedium       0.08051695 0.08196711  0.9823081 0.32836700\nhigh        -0.07771793 0.08727557 -0.8904889 0.37538435\n\n\nThe estimates are more comparable, although still pretty different. Still not significant with both methods.\nAnd finally for pocket depth change.\n\n# Pool the parameters of interest for a regression on the imputed values for pocket depth  change placebo as reference\nmodel_imp &lt;- data %&gt;%\n  mice(seed = 1, print = FALSE, m = 20) %&gt;%\n  with(lm(pdchange ~ placebo + low + medium + high)) %&gt;%\n  pool()\n\nWarning: Number of logged events: 1158\n\n# Compare coefficients with complete case analysis for pocket depth change\nMI &lt;- summary(model_imp)\nMI\n\n         term    estimate  std.error  statistic       df      p.value\n1 (Intercept) -0.33597389 0.06144440 -5.4679330 66.87925 7.296278e-07\n2     placebo -0.00982096 0.08061544 -0.1218248 92.74234 9.033013e-01\n3         low  0.11415425 0.07995226  1.4277802 96.05307 1.565984e-01\n4      medium  0.11260463 0.08900896  1.2650932 60.47912 2.106907e-01\n5        high -0.02704497 0.10936047 -0.2473011 30.38938 8.063384e-01\n\n\nHere the models are a LOT more comparable, although still not significant. It seems we can conclude that missing values impact the attachment loss measurements more than pocket depth change.\nLet’s finish by plotting the imputed values for pocket depth change\n\n# Plot imputed values for m = 5 data set\nimpy &lt;- mice(data, seed = 1, print = FALSE, m = 5)\n\nWarning: Number of logged events: 285\n\nmodel_impy &lt;- with(imp, pdchange ~ trtgroup)\n\n# This is cool, here's how I can plot the imputed values for attach change \nxyplot(impy, pdchange ~ trtgroup,\n       main = \"Example of Imputed Values for m = 5 Dataset\",\n       auto.key = list(space = \"right\",\n        points = TRUE, \n        text = c(\"Observed\", \"Missing\"), col = c(\"blue4\", \"red3\")))\n\n\n\n\n\n\n\n\nLooks pretty good.\nBack to top of tabset"
  },
  {
    "objectID": "Project_1_Regression/Project_1_Regression_R/Project_1_Regression.html#Jackknife",
    "href": "Project_1_Regression/Project_1_Regression_R/Project_1_Regression.html#Jackknife",
    "title": "Advanced Data Analysis - Project 1",
    "section": "Identify Outliers using Jackknife Residuals",
    "text": "Identify Outliers using Jackknife Residuals\nIn this section we will explore how our analysis would have changed if we identified outliers using the jackknife residuals.\n\nBackgroundIdentifying OutliersRerunning the Model with Outliers Removed\n\n\n*Note: This information is from BIOS 6602 Week 6 Lecture 10\nJackknife residuals are a type of residual used in regressions that allows us to detect outliers that have a significant impact on the regression coefficients of the model.\nJackknife residuals are calculated by systematically leaving out one observation (i) at a time from a dataset, fitting the regression model to the remaining dataset, and predicting the left out observation. The residual for each observation is then computed based on that prediction.\nThis process follows three step:\n\nFit the regression model to the data, excluding observation (i)\nUse the model to predict the left-out observation (i)\nCalculate the jackknife residual as the difference between the actual value and that fitted value\n\nJackknife residuals are similar to standardized residuals. Standardized residuals are standardized by the standard error of the regression, and are primarily used for identifying outliers. Jackknife residuals on the other hand allow you to assess the leverage of individuals data points, to see how much they actually influence the model (Source)\n\nJackknife residuals follow exactly a t(n-p-2) distribution, where approximately 5% of residuals are expected to exceed 1.96 in absolute value. Jackknife residuals make suspicious values more obvious compared to other residuals.\nDefinitions vary, but we generally consider a residual to be an outlier if the jackknife residual is +- 3.\nHowever, a potential outlier value may not actually have that dramatic of an impact on the model (which is what we are concerned that outliers will do). That is why we use jackknife residuals to investigate leverage and influence.\n\nExtreme X values can have high leverage.\nExtreme X values can have high influence\nA high-leverage point becomes an influential point if its Y value doesn’t follow the pattern of the rest of the data (i.e. is too low or too high)\n\n\nAn observation is influential if removing it substantially changes the estimate of the coefficients for that model. We use five measurements to assess influence: Jackknife residuals, Leverage, Cook’s Distance (Cook’s D), DFFITS, and DFBETAS.\n\nLeverage: Measures how far a measurement deviates from the mean. You want to examine values greater than 2(p+1)/n\nCook’s D: Measures the influence of an observation on regression predictions. You want to examine observations with d_i &gt; 1.0\nDFFITS: Measures the influence of an observation on regression predictions (related to Cook’s D). You want to examine observations outside the range of +-2(sqrt(p+10n)). If h_i is near zero, then that observation has little effect.\nDDFBETAS: Measure the influence of an observation on *individual* coefficient estimates. You want to examine estimates outside the range of +- 2/sqrt(n). A large DFBETA for variable k indicates that the i-th observation has a sizeable impact on the k-th regression coefficient.\n\nNote: p = the number of variables in the model\n\nBack to top of tabset\n\n\nThis website was used for coding information for this section, alongside the notes from BIOS 6602\nNow that we have covered the background for jackknife residuals, we can apply that knowledge to assess for outliers in our dataset for this project!\n\nAttachment LossPocket Depth\n\n\n\nSet upJackknife ResidualsLeverageCook’s DDFFITSDFBETASAdditional PlotsFinal Look and Conclusion\n\n\nRecall that we computed the jackknife residuals earlier.\n\n# Calculate the jackknife residuals of model_attach\njackknife_residuals_attach &lt;- rstudent(model_attach1)\n\n# Calculate the jackknife residuals of model_pd\njackknife_residuals_pd &lt;- rstudent(model_pd)\n\nLet’s generate a table so we can handily compare: ID, jackknife residual, leverage (diagonal hat), DFFITS, and DFBETAs for each participant.\n\n# Get leverage values (hat values)\nhat_values_attach &lt;- hatvalues(model_attach1)\n\n# Get Cook's D values\ncooks_d_attach &lt;- cooks.distance(model_attach1)\n\n# Get the DFFITS values\ndffits_attach &lt;- dffits(model_attach1)\n\n# Get the DFBETAS\ndfbetas_attach &lt;- dfbetas(model_attach1)\n\n# Make a table with ID and all diagnostic values\ndiagnostics_attach &lt;- data.frame(id = data_missing$id, jackknife_residuals = jackknife_residuals_attach, leverage = hat_values_attach, cooks_D = cooks_d_attach, dffits = dffits_attach, dfbetas = dfbetas_attach)\n\nkable(head(diagnostics_attach), caption = \"Diagnostics for Attachment Loss Change Model\", format = \"html\") %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\"))\n\n\nDiagnostics for Attachment Loss Change Model\n\n\nid\njackknife_residuals\nleverage\ncooks_D\ndffits\ndfbetas..Intercept.\ndfbetas.placebo\ndfbetas.low\ndfbetas.medium\ndfbetas.high\n\n\n\n\n101\n0.5800959\n0.0500000\n0.0035664\n0.1330831\n0.0000000\n0.0000000\n0.0000000\n0.0973313\n0.0000000\n\n\n103\n1.5996714\n0.0434783\n0.0228989\n0.3410511\n0.3410511\n-0.2411595\n-0.2356149\n-0.2325949\n-0.2184475\n\n\n104\n1.4044559\n0.0476190\n0.0195311\n0.3140459\n0.0000000\n0.0000000\n0.2270548\n0.0000000\n0.0000000\n\n\n105\n-0.8928811\n0.0434783\n0.0072626\n-0.1903629\n0.0000000\n-0.1346069\n0.0000000\n0.0000000\n0.0000000\n\n\n106\n-1.6995151\n0.0500000\n0.0298289\n-0.3898955\n0.0000000\n0.0000000\n0.0000000\n-0.2851530\n0.0000000\n\n\n107\n-2.7107885\n0.0476190\n0.0690131\n-0.6061507\n0.0000000\n0.0000000\n-0.4382463\n0.0000000\n0.0000000\n\n\n\n\n\n\n\nBack to top of tabset\n\n\nWe can start by making a plot of the jackknife residuals to assess if there are outliers &lt; -3 or &gt; 3 SDs).\n\n# Plot jackknife outiers and label any values that are &gt; 3 or &lt; -3.\nggplot(data_missing, aes(x = id, y = jackknife_residuals_attach)) + geom_point() + \n  geom_hline(yintercept = c(-3,0,3)) + ylim(-4,4) + \n  labs(y = \"Jackknife Residuals\",\n       title = \"Jackknife Residuals vs ID\") + \n  geom_text(aes(label = ifelse(jackknife_residuals_attach &gt; 3 | jackknife_residuals_attach &lt; -3, id, '')), \n            vjust = -1)\n\n\n\n\n\n\n\n\nParticipant 168 is a potential outlier based on the residual value.\nLet’s look at this participant more closely.\n\n# Examine participant 168 to assess what values could make them an outlier\ndata_missing[data_missing$id == 168,]\n\n    id trtgroup gender race      age smoker sites attachbase attach1year\n58 168        5      2    5 54.20397      0   168   5.089286    4.041667\n     pdbase  pd1year attachchange   pdchange placebo control low medium high\n58 3.410714 2.904762    -1.047619 -0.5059524       0       0   0      0    1\n   trt trt3groups\n58   1          3\n\n# Sort our dataset by descending order of attachment at baseline to see if participant 168 is the highest\n\nkable(head(data_missing[order(-data_missing$attachbase),]), caption = \"Diagnostics for Attachment Loss Change Model\", format = \"html\") %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\"))\n\n\nDiagnostics for Attachment Loss Change Model\n\n\n\nid\ntrtgroup\ngender\nrace\nage\nsmoker\nsites\nattachbase\nattach1year\npdbase\npd1year\nattachchange\npdchange\nplacebo\ncontrol\nlow\nmedium\nhigh\ntrt\ntrt3groups\n\n\n\n\n58\n168\n5\n2\n5\n54.20397\n0\n168\n5.089286\n4.041667\n3.410714\n2.904762\n-1.0476190\n-0.5059524\n0\n0\n0\n0\n1\n1\n3\n\n\n3\n104\n3\n2\n5\n55.17864\n1\n138\n4.956522\n5.304348\n5.217391\n4.891304\n0.3478261\n-0.3260870\n0\n0\n1\n0\n0\n1\n3\n\n\n40\n144\n2\n2\n2\n49.07598\n0\n126\n4.388889\n3.488000\n3.666667\n3.230159\n-0.9008889\n-0.4365079\n0\n1\n0\n0\n0\n0\n2\n\n\n102\n269\n3\n1\n5\n64.90075\n0\n162\n4.080247\n3.685185\n3.370370\n3.265432\n-0.3950617\n-0.1049383\n0\n0\n1\n0\n0\n1\n3\n\n\n18\n121\n4\n2\n5\n54.49692\n1\n138\n4.014493\n3.825000\n3.608696\n3.783333\n-0.1894928\n0.1746377\n0\n0\n0\n1\n0\n1\n3\n\n\n17\n120\n3\n2\n5\n41.83710\n1\n150\n3.886667\n3.920000\n4.200000\n3.880000\n0.0333333\n-0.3200000\n0\n0\n1\n0\n0\n1\n3\n\n\n\n\n\n\n\nWe have confirmed that participant 168 has the largest values for attachment loss at baseline (5.09). If that’s the case however, participant 3 is not far behind them (4.96) (who incidentally has the highest baseline pocket depth measurement), followed by participant 40 a large amount lower (4.39).\nWe can also (apparently) run a test statistic to test if we have an outlier using the ‘car’ package\n\noutlierTest(model_attach1)\n\nNo Studentized residuals with Bonferroni p &lt; 0.05\nLargest |rstudent|:\n    rstudent unadjusted p-value Bonferroni p\n58 -3.602896         0.00049863     0.051359\n\n\nThis is a test of a hypothesis that we do not have an outlier. We reject that hypothesis (p &lt; 0.05) so we have an outlier (I think).\nBack to top of tabset\n\n\nLeverage is a measure of geometric distance of an observation’s predictor point from the center point of the predictor space. In other words, leverage is a measurement of how far that observation deviates from the mean of that variable. High leverage observations have the potential to be very influential, but they are not necessarily influential. It’s possible for a high leverage point to not be influential, but very difficult for a low leverage point to be influential.\nLeverage is calculated as\n&lt;img src=“Media/leverage.png” width=“90%&gt;\nIf X _i is close to Xbar, then h_i is small (i.e. the i-th point has low leverage).\n\n# Caculate cutoff for leverage\ncutoff_leverage &lt;- 2*((4+1)/103)\ncutoff_leverage\n\n[1] 0.09708738\n\n# Make leverage plot in ggplot. Set a horizontal line at the cutoff point, and label ID's for participants that pass that line\nggplot(diagnostics_attach, aes(x = id, y = leverage)) +\n  geom_point() + \n  geom_hline(yintercept = cutoff_leverage) +\n  labs(title = \"Leverage Plot with Cutoff Line\",\n       x = \"ID\",\n       y = \"Hat Values\") + \n  geom_text(aes(label = ifelse(leverage &gt; cutoff_leverage, id, \"\")), vjust = -0.5)\n\n\n\n\n\n\n\n\nNone of our data points pass the cutoff point for leverage/hat values. I think this may be because the IV is categorical which changes the pattern and interpretation. Will check with a professor.\nBack to top of tabset\n\n\nCook’s D combines information about the residuals and leverage into a single value. A higher Cook’s D value signifies that the data point woud greatly change the regression coefficients, and is therefore influential and may impact the model’s accuracy.\nApparently we actually want to look at a cutoff of Cook’s D as 4/(n-p-1). Let’s calculate that and store it.\n\n# Calculate cutoff for Cook's D\ncutoff_d &lt;- 4/((nrow(data_missing) - length(model_attach1$coefficients) - 1))\ncutoff_d\n\n[1] 0.04123711\n\n# Make Cook's D plot in ggplot. Set a horizontal line at the cutoff point, and label ID's for participants that pass that line\nggplot(diagnostics_attach, aes(x = id, y = cooks_D)) +\n  geom_point() + \n  geom_hline(yintercept = cutoff_d) +\n  labs(title = \"Cook's D with Cutoff Line\",\n       x = \"ID\",\n       y = \"Cook's D\") + \n  geom_text(aes(label = ifelse(cooks_D &gt; cutoff_d, id, \"\")), vjust = -0.5)\n\n\n\n\n\n\n\n\nWe see that participants 107, 144, 146 and 168 are past the cutoff for Cook’s D. As when looking at the residuals plot, participant 168 is drastically different compared to the other potential outliers.\nBack to top of tabset\n\n\nNow we will examine DFFITS (Difference in fits). DFFITS is a measure used to identify influential data points in a regression analysis. It quantifes how much the predicted values (Y) of a model change when that particular observation is left out from the analysis. So in this case it is the difference in attachment loss change if we take out observation i from the analysis.\nLet’s make a plot where we label the values that are past the cutoff.\n\n# Calculate the cutoff for DFFITS\ncutoff_dffits &lt;- 2 * sqrt((4+1)/103)\ncutoff_dffits\n\n[1] 0.4406526\n\n# Make DFFITS plot in ggplot. Set a horizontal line at the cutoff points, and label ID's for participants that pass those lines.\nggplot(diagnostics_attach, aes(x = id, y = dffits)) + \n  geom_point() +\n  geom_hline(yintercept = c(cutoff_dffits, -cutoff_dffits)) + \n  geom_text(aes(label = ifelse(dffits &lt; -cutoff_dffits | dffits &gt; cutoff_dffits, id, \"\")), vjust = -0.5) +\n  labs(title = \"DFFITS by ID\",\n       x = \"ID\",\n       y = \"DFFITS\")\n\n\n\n\n\n\n\n\nAgain we are flagging participants 107, 113, 144, 168. The new addition is 113, who is right on the cutoff line. Again participant 168 is the most egregious, and the other potential outliers are a lot closer to the cutoff.\nBack to top of tabset\n\n\nNow we can look at the DFBETAs (Difference in Betas). DFBETAS quantify how much the beta coefficients for each variable in the model change when you exclude observation i from the analysis.\n\n# Calculate the cutoff for DFBETAS\ncutoff_dfbetas &lt;- 2/sqrt(103)\ncutoff_dfbetas\n\n[1] 0.1970659\n\n# Make DFBETAS plot in ggplot. Set a horizontal line at the cutoff points, and label ID's for participants that pass those lines.\n\n# For placebo group\nggplot(diagnostics_attach, aes(x = id, y = dfbetas.placebo)) + \n  geom_point() +\n  geom_hline(yintercept = c(cutoff_dfbetas, -cutoff_dfbetas)) + \n  geom_text(aes(label = ifelse(dfbetas.placebo &lt; -cutoff_dfbetas | dfbetas.placebo &gt; cutoff_dfbetas, id, \"\")), vjust = -0.5) +\n  labs(title = \"DFBETAS by ID for Placebo\",\n       x = \"ID\",\n       y = \"DFBETA\")\n\n\n\n\n\n\n\n# For low dose group\nggplot(diagnostics_attach, aes(x = id, y = dfbetas.low)) + \n  geom_point() +\n  geom_hline(yintercept = c(cutoff_dfbetas, -cutoff_dfbetas)) + \n  geom_text(aes(label = ifelse(dfbetas.low &lt; -cutoff_dfbetas | dfbetas.low &gt; cutoff_dfbetas, id, \"\")), vjust = -0.5) +\n  labs(title = \"DFBETAS by ID for Low Dose\",\n       x = \"ID\",\n       y = \"DFBETA\")\n\n\n\n\n\n\n\n# For medium dose group\nggplot(diagnostics_attach, aes(x = id, y = dfbetas.medium)) + \n  geom_point() +\n  geom_hline(yintercept = c(cutoff_dfbetas, -cutoff_dfbetas)) + \n  geom_text(aes(label = ifelse(dfbetas.medium &lt; -cutoff_dfbetas | dfbetas.medium &gt; cutoff_dfbetas, id, \"\")), vjust = -0.5) +\n  labs(title = \"DFBETAS by ID for Medium Dose\",\n       x = \"ID\",\n       y = \"DFBETA\")\n\n\n\n\n\n\n\n# For high does group\nggplot(diagnostics_attach, aes(x = id, y = dfbetas.high)) + \n  geom_point() +\n  geom_hline(yintercept = c(cutoff_dfbetas, -cutoff_dfbetas)) + \n  geom_text(aes(label = ifelse(dfbetas.high &lt; -cutoff_dfbetas | dfbetas.high &gt; cutoff_dfbetas, id, \"\")), vjust = -0.5) +\n  labs(title = \"DFBETAS by ID for High Dose\",\n       x = \"ID\",\n       y = \"DFBETA\")\n\n\n\n\n\n\n\n\nThat’s a lot of points that are past the cutoff! I think for these plots it might matter less if we’re pass that point. I think the idea here is we get a fine tune look at, for instance, how participant 168 drastically changes the beta for high, compared to how much any other point changes the betas. Other possible values of concern are 144 and 146. All the other data points are still roughly around the cutoff of .20, but 168 is around .7!\nBack to top of tabset\n\n\nThe car package in R has some handy plots that we can use to help us with diagnosing outliers.\nThe first is the influence plot, which essentially combines the jackknife residuals plot with the Cook’s D plot.\n\n# Influence plot\ninfluencePlot(model_attach1, main = \"Influence Plot\",\n              sub = \"Circle size is proportional to Cook's Distance\")\n\n\n\n\n\n\n\n\n      StudRes        Hat       CookD\n6  -2.7107885 0.04761905 0.069013121\n8  -0.5238664 0.06250000 0.003686439\n12  1.4073671 0.06250000 0.026147438\n58 -3.6028957 0.06250000 0.154223694\n\n\nUsing this plot, we can see that obseration 58 (ID 168) has the largest residual value and Cook’s D value by a large margin!\nAdditionally, a single line of code provides us with some useful diagnostic plots.\n\n# infIndexPlot gives us a series of plots that we need to investigate influence points\ninfIndexPlot(model_attach1)\n\n\n\n\n\n\n\n\nThe first plot is the Cook’s D plot, the second plot is the studentized residuals plot, and the leverage/hat values plot, all of which we plotted before. New is the third plot which is the Bonferroni P-value plot.\nBased on these plots we can see pretty readily that participant 168 is a true outlier. That is, they are a point that has high leverage AND influence in this model.\nBack to top of tabset\n\n\nWe saw that participant 168 is the most egregious offender, and participants 107, 144, 146, and MAYBE 113 are flagging past our different cutoff points.\nLet’s take one last look at the table of these participants\n\n# Pretty print diagnostics table of potential outliers\nkable(diagnostics_attach[diagnostics_attach$id %in% c(107,113,144,146,168),], caption = \"Correlation Matrix\", format = \"html\") %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\"))\n\n\nCorrelation Matrix\n\n\n\nid\njackknife_residuals\nleverage\ncooks_D\ndffits\ndfbetas..Intercept.\ndfbetas.placebo\ndfbetas.low\ndfbetas.medium\ndfbetas.high\n\n\n\n\n6\n107\n-2.710789\n0.0476190\n0.0690131\n-0.6061507\n0.0000000\n0.0000000\n-0.4382463\n0.0000000\n0.0000000\n\n\n11\n113\n2.092587\n0.0434783\n0.0384816\n0.4461411\n0.0000000\n0.3154694\n0.0000000\n0.0000000\n0.0000000\n\n\n40\n144\n-2.670168\n0.0434783\n0.0610008\n-0.5692818\n-0.5692818\n0.4025431\n0.3932880\n0.3882470\n0.3646322\n\n\n42\n146\n-2.291912\n0.0434783\n0.0457672\n-0.4886373\n-0.4886373\n0.3455188\n0.3375748\n0.3332479\n0.3129784\n\n\n58\n168\n-3.602896\n0.0625000\n0.1542237\n-0.9302637\n0.0000000\n0.0000000\n0.0000000\n0.0000000\n-0.7143938\n\n\n\n\n\n\n\nAcross the board we can see that participant 168 has worse values for almost everything, in particular the jackknife residual, Cook’s D, and DFFITS values. I can confidently conclude that we should remove participant 168 as an outlier, and feel justified in keeping participants 107, 113, 144, and 146 based on the closeness to the rest of the data points on the previous plots.\n\n\n\n\n\n\nSet UpJackknife ResidualsLeverageCook’s DDFFITSDFBETASAdditional PlotsFinal Look and Conclusion\n\n\nLet’s generate a table so we can handily compare: ID, jackknife residual, leverage (diagonal hat), DFFITS, and DFBETAs for each participant.\n\n# Get leverage values (hat values)\nhat_values_pd &lt;- hatvalues(model_pd)\n\n# Get Cook's D values\ncooks_d_pd &lt;- cooks.distance(model_pd)\n\n# Get the DFFITS values\ndffits_pd &lt;- dffits(model_pd)\n\n# Get the DFBETAS\ndfbetas_pd &lt;- dfbetas(model_pd)\n\n# Make a table with ID and all diagnostic values\ndiagnostics_pd &lt;- data.frame(id = data_missing$id, jackknife_residuals = jackknife_residuals_pd, leverage = hat_values_pd, cooks_D = cooks_d_pd, dffits = dffits_pd, dfbetas = dfbetas_pd)\n\nkable(head(diagnostics_pd), caption = \"Diagnostics for Attachment Loss Change Model\", format = \"html\") %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\"))\n\n\nDiagnostics for Attachment Loss Change Model\n\n\nid\njackknife_residuals\nleverage\ncooks_D\ndffits\ndfbetas..Intercept.\ndfbetas.placebo\ndfbetas.low\ndfbetas.medium\ndfbetas.high\n\n\n\n\n101\n1.4284091\n0.0500000\n0.0212518\n0.3276995\n0.0000000\n0.0000000\n0.0000000\n0.2396655\n0.0000000\n\n\n103\n1.3517762\n0.0434783\n0.0164727\n0.2881997\n0.2881997\n-0.2037879\n-0.1991025\n-0.1965505\n-0.1845955\n\n\n104\n-0.4668576\n0.0476190\n0.0021971\n-0.1043925\n0.0000000\n0.0000000\n-0.0754757\n0.0000000\n0.0000000\n\n\n105\n-0.4451251\n0.0434783\n0.0018161\n-0.0949010\n0.0000000\n-0.0671051\n0.0000000\n0.0000000\n0.0000000\n\n\n106\n-2.5107398\n0.0500000\n0.0629491\n-0.5760032\n0.0000000\n0.0000000\n0.0000000\n-0.4212642\n0.0000000\n\n\n107\n-1.7949702\n0.0476190\n0.0315049\n-0.4013675\n0.0000000\n0.0000000\n-0.2901882\n0.0000000\n0.0000000\n\n\n\n\n\n\n\nBack to top of tabset\n\n\nWe can start by making a plot of the jackknife residuals to assess if there are outliers &lt; -3 or &gt; 3 SDs).\n\n# Plot jackknife outiers and label any values that are &gt; 3 or &lt; -3.\nggplot(data_missing, aes(x = id, y = jackknife_residuals_pd)) + geom_point() + \n  geom_hline(yintercept = c(-3,0,3)) + ylim(-4,4) + \n  labs(y = \"Jackknife Residuals\",\n       title = \"Jackknife Residuals vs ID\") + \n  geom_text(aes(label = ifelse(jackknife_residuals_pd &gt; 3 | jackknife_residuals_pd &lt; -3, id, '')), \n            vjust = -1)\n\n\n\n\n\n\n\n\nWe have no jackknife residuals +- 3. That’s a good sign!\nLet’s sort our dataset and see who has the highest pocket depth at baseline.\n\n# Sort our dataset by descending order of pocket depth at baseline to see who is the highest\nkable(head(data_missing[order(-data_missing$pdbase),]), caption = \"Diagnostics for Attachment Loss Change Model\", format = \"html\") %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\"))\n\n\nDiagnostics for Attachment Loss Change Model\n\n\n\nid\ntrtgroup\ngender\nrace\nage\nsmoker\nsites\nattachbase\nattach1year\npdbase\npd1year\nattachchange\npdchange\nplacebo\ncontrol\nlow\nmedium\nhigh\ntrt\ntrt3groups\n\n\n\n\n3\n104\n3\n2\n5\n55.17864\n1\n138\n4.956522\n5.304348\n5.217391\n4.891304\n0.3478261\n-0.3260870\n0\n0\n1\n0\n0\n1\n3\n\n\n20\n124\n2\n2\n5\n41.01574\n1\n162\n2.901235\n2.808642\n4.771605\n4.067901\n-0.0925926\n-0.7037037\n0\n1\n0\n0\n0\n0\n2\n\n\n17\n120\n3\n2\n5\n41.83710\n1\n150\n3.886667\n3.920000\n4.200000\n3.880000\n0.0333333\n-0.3200000\n0\n0\n1\n0\n0\n1\n3\n\n\n77\n234\n1\n2\n5\n51.12115\n0\n144\n2.756944\n2.527778\n4.083333\n3.631944\n-0.2291667\n-0.4513889\n1\n0\n0\n0\n0\n0\n1\n\n\n5\n106\n4\n2\n5\n42.14921\n1\n168\n2.369048\n1.922619\n3.910714\n3.083333\n-0.4464286\n-0.8273810\n0\n0\n0\n1\n0\n1\n3\n\n\n6\n107\n3\n2\n5\n37.15811\n1\n156\n3.544872\n2.839744\n3.897436\n3.237179\n-0.7051282\n-0.6602564\n0\n0\n1\n0\n0\n1\n3\n\n\n\n\n\n\n\nParticipant 104 has the highest baseline pocket depth. Interestingly it is not 168, who was our outlier for attachment loss.\nLet’s run that test we did earlier to see if there’s an outlier in the pocket depth model.\n\noutlierTest(model_pd)\n\nNo Studentized residuals with Bonferroni p &lt; 0.05\nLargest |rstudent|:\n   rstudent unadjusted p-value Bonferroni p\n82 2.664155          0.0090393      0.93105\n\n\nThis is a test of a hypothesis that we do not have an outlier. We fail to reject the null and thus have more evidence that we do not have outliers for the pocket depth model.\nBack to top of tabset\n\n\nLet’s plot leverage for our model on pocket depth change.\n\n# Caculate cutoff for leverage\ncutoff_leverage &lt;- 2*((4+1)/103)\ncutoff_leverage\n\n[1] 0.09708738\n\n# Make leverage plot in ggplot. Set a horizontal line at the cutoff point, and label ID's for participants that pass that line\nggplot(diagnostics_pd, aes(x = id, y = leverage)) +\n  geom_point() + \n  geom_hline(yintercept = cutoff_leverage) +\n  labs(title = \"Leverage Plot with Cutoff Line\",\n       x = \"ID\",\n       y = \"Hat Values\") + \n  geom_text(aes(label = ifelse(leverage &gt; cutoff_leverage, id, \"\")), vjust = -0.5)\n\n\n\n\n\n\n\n\nAgain we are good on leverage.\nBack to top of tabset\n\n\nLet’s make our plot for Cook’s D for pocket depth change.\n\n# Calculate cutoff for Cook's D\ncutoff_d &lt;- 4/((nrow(data_missing) - length(model_attach1$coefficients) - 1))\ncutoff_d\n\n[1] 0.04123711\n\n# Make Cook's D plot in ggplot. Set a horizontal line at the cutoff point, and label ID's for participants that pass that line\nggplot(diagnostics_pd, aes(x = id, y = cooks_D)) +\n  geom_point() + \n  geom_hline(yintercept = cutoff_d) +\n  labs(title = \"Cook's D with Cutoff Line\",\n       x = \"ID\",\n       y = \"Cook's D\") + \n  geom_text(aes(label = ifelse(cooks_D &gt; cutoff_d, id, \"\")), vjust = -0.5)\n\n\n\n\n\n\n\n\nWe see that participants 106, 118, and 239 have Cook’s D values past the cutoff. Interestingly participant 104 who had the highest baseline pocket depth did not flag here.\nBack to top of tabset\n\n\nLet’s make the DFFITS plot for pocket depth change.\n\n# Calculate the cutoff for DFFITS\ncutoff_dffits &lt;- 2 * sqrt((4+1)/103)\ncutoff_dffits\n\n[1] 0.4406526\n\n# Make DFFITS plot in ggplot. Set a horizontal line at the cutoff points, and label ID's for participants that pass those lines.\nggplot(diagnostics_pd, aes(x = id, y = dffits)) + \n  geom_point() +\n  geom_hline(yintercept = c(cutoff_dffits, -cutoff_dffits)) + \n  geom_text(aes(label = ifelse(dffits &lt; -cutoff_dffits | dffits &gt; cutoff_dffits, id, \"\")), vjust = -0.5) +\n  labs(title = \"DFFITS by ID\",\n       x = \"ID\",\n       y = \"DFFITS\")\n\n\n\n\n\n\n\n\nParticipants 106, 118, and 239 are flagging here again. In addition we have 137 and 233, which are near enough to the cutoff that we can ignore them.\nBack to top of tabset\n\n\nNow we can look at the DFBETAS plot for pocket depth change.\n\n# Calculate the cutoff for DFBETAS\ncutoff_dfbetas &lt;- 2/sqrt(103)\ncutoff_dfbetas\n\n[1] 0.1970659\n\n# Make DFBETAS plot in ggplot. Set a horizontal line at the cutoff points, and label ID's for participants that pass those lines.\n\n# For placebo group\nggplot(diagnostics_pd, aes(x = id, y = dfbetas.placebo)) + \n  geom_point() +\n  geom_hline(yintercept = c(cutoff_dfbetas, -cutoff_dfbetas)) + \n  geom_text(aes(label = ifelse(dfbetas.placebo &lt; -cutoff_dfbetas | dfbetas.placebo &gt; cutoff_dfbetas, id, \"\")), vjust = -0.5) +\n  labs(title = \"DFBETAS by ID for Placebo\",\n       x = \"ID\",\n       y = \"DFBETA\")\n\n\n\n\n\n\n\n# For low dose group\nggplot(diagnostics_pd, aes(x = id, y = dfbetas.low)) + \n  geom_point() +\n  geom_hline(yintercept = c(cutoff_dfbetas, -cutoff_dfbetas)) + \n  geom_text(aes(label = ifelse(dfbetas.low &lt; -cutoff_dfbetas | dfbetas.low &gt; cutoff_dfbetas, id, \"\")), vjust = -0.5) +\n  labs(title = \"DFBETAS by ID for Low Dose\",\n       x = \"ID\",\n       y = \"DFBETA\")\n\n\n\n\n\n\n\n# For medium dose group\nggplot(diagnostics_pd, aes(x = id, y = dfbetas.medium)) + \n  geom_point() +\n  geom_hline(yintercept = c(cutoff_dfbetas, -cutoff_dfbetas)) + \n  geom_text(aes(label = ifelse(dfbetas.medium &lt; -cutoff_dfbetas | dfbetas.medium &gt; cutoff_dfbetas, id, \"\")), vjust = -0.5) +\n  labs(title = \"DFBETAS by ID for Medium Dose\",\n       x = \"ID\",\n       y = \"DFBETA\")\n\n\n\n\n\n\n\n# For high does group\nggplot(diagnostics_pd, aes(x = id, y = dfbetas.high)) + \n  geom_point() +\n  geom_hline(yintercept = c(cutoff_dfbetas, -cutoff_dfbetas)) + \n  geom_text(aes(label = ifelse(dfbetas.high &lt; -cutoff_dfbetas | dfbetas.high &gt; cutoff_dfbetas, id, \"\")), vjust = -0.5) +\n  labs(title = \"DFBETAS by ID for High Dose\",\n       x = \"ID\",\n       y = \"DFBETA\")\n\n\n\n\n\n\n\n\nAgain we have a lot of points that are past the cutoff here. None as far off as 168 in the attachment loss model, 239 is the only one that is concerning.\nBack to top of tabset\n\n\nThe car package in R has some handy plots that we can use to help us with diagnosing outliers.\nThe first is the influence plot, which essentially combines the jackknife residuals plot with the Cook’s D plot.\n\n# Influence plot\ninfluencePlot(model_pd, main = \"Influence Plot\",\n              sub = \"Circle size is proportional to Cook's Distance\")\n\n\n\n\n\n\n\n\n      StudRes        Hat       CookD\n5  -2.5107398 0.05000000 0.062949101\n8  -0.8158788 0.06250000 0.008905825\n12  0.4184503 0.06250000 0.002354494\n82  2.6641548 0.04761905 0.066819586\n\n\nHere we can see that none of our residuals are +- 3, but we do have some concerns with large Cook’s distance, particularly with observation 82 (ID = 239).\nAdditionally, a single line of code provides us with some useful diagnostic plots.\n\n# infIndexPlot gives us a series of plots that we need to investigate influence points\ninfIndexPlot(model_pd)\n\n\n\n\n\n\n\n\nBased on these plots, it is arguable that participant 239 is an outlier for pocket depth. Though they do not have an extreme jackknife residual, the large Cook’s distance suggests that this data point has substantial infuence on the model’s parameters.\nHowever, a closer look reveals that the influence and leverage values for the pocket depth model are all comparable to the potential outliers in the attachmnet loss model we chose to keep (Cook’s D ~0.06). Specifically, while it appears that participant 239 here has a large Cook’s D (0.068) compared to the rest of the values, it is nowhere near as high as participant 168 was in the attachment loss model (0.15)!\nBack to top of tabset\n\n\nParticipants 106 and 239 were the only potential concerns here. However, as noted, their Cook’s D is comparable to the values we kept in the attachment loss model, and are &lt; 1/2 of what the Cook’s D was for participant 168 who we plan to remove as an outlier in the attachment loss model!\n\n# Pretty print diagnostics table of potential outliers\nkable(diagnostics_pd[diagnostics_attach$id %in% c(106,239),], caption = \"Correlation Matrix\", format = \"html\") %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\"))\n\n\nCorrelation Matrix\n\n\n\nid\njackknife_residuals\nleverage\ncooks_D\ndffits\ndfbetas..Intercept.\ndfbetas.placebo\ndfbetas.low\ndfbetas.medium\ndfbetas.high\n\n\n\n\n5\n106\n-2.510740\n0.050000\n0.0629491\n-0.5760032\n0\n0\n0.0000000\n-0.4212642\n0\n\n\n82\n239\n2.664155\n0.047619\n0.0668196\n0.5957231\n0\n0\n0.4307071\n0.0000000\n0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Remove participant 168 from the dataset\ndata_missing_outlier &lt;- data_missing[data_missing$id != 168,]\n\n# Run model 1 attachment loss\nmodel_attach1_outlier &lt;- lm(attachchange ~ placebo + low + medium + high, data = data_missing_outlier)\nsummary(model_attach1_outlier)\n\n\nCall:\nlm(formula = attachchange ~ placebo + low + medium + high, data = data_missing_outlier)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.68731 -0.16279  0.04936  0.16823  0.53945 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.22169    0.05277  -4.201  5.9e-05 ***\nplacebo      0.13462    0.07463   1.804  0.07435 .  \nlow          0.20388    0.07638   2.669  0.00891 ** \nmedium       0.21514    0.07737   2.780  0.00652 ** \nhigh         0.11576    0.08399   1.378  0.17130    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2531 on 97 degrees of freedom\nMultiple R-squared:  0.09493,   Adjusted R-squared:  0.05761 \nF-statistic: 2.543 on 4 and 97 DF,  p-value: 0.04442\n\n# Run model 2 attachmnent loss\nmodel_attach2_outlier &lt;- lm(attachchange ~ control + low + medium + high, data = data_missing_outlier)\nsummary(model_attach2_outlier)\n\n\nCall:\nlm(formula = attachchange ~ control + low + medium + high, data = data_missing_outlier)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.68731 -0.16279  0.04936  0.16823  0.53945 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept) -0.08707    0.05277  -1.650   0.1022  \ncontrol     -0.13462    0.07463  -1.804   0.0743 .\nlow          0.06926    0.07638   0.907   0.3668  \nmedium       0.08052    0.07737   1.041   0.3006  \nhigh        -0.01886    0.08399  -0.225   0.8228  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2531 on 97 degrees of freedom\nMultiple R-squared:  0.09493,   Adjusted R-squared:  0.05761 \nF-statistic: 2.543 on 4 and 97 DF,  p-value: 0.04442\n\n# Run the pocket depth model\nmodel_pd_outlier &lt;- lm(pdchange ~ control + low + medium + high, data = data_missing_outlier)\nsummary(model_pd_outlier)\n\n\nCall:\nlm(formula = pdchange ~ control + low + medium + high, data = data_missing_outlier)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-0.6248 -0.1513 -0.0129  0.1616  0.6613 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.34969    0.05488  -6.372 6.26e-09 ***\ncontrol      0.01152    0.07761   0.148   0.8823    \nlow          0.14352    0.07943   1.807   0.0739 .  \nmedium       0.14714    0.08046   1.829   0.0705 .  \nhigh        -0.02437    0.08734  -0.279   0.7809    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2632 on 97 degrees of freedom\nMultiple R-squared:  0.07456,   Adjusted R-squared:  0.0364 \nF-statistic: 1.954 on 4 and 97 DF,  p-value: 0.1077\n\n\nThere is no difference in our models after removing outlier 168."
  },
  {
    "objectID": "Data_Wrangling/Treadmill_Data_Cleaning/Code/Automated_Treadmill_Data_Cleaning.html",
    "href": "Data_Wrangling/Treadmill_Data_Cleaning/Code/Automated_Treadmill_Data_Cleaning.html",
    "title": "Automated Treadmill Data Cleaning",
    "section": "",
    "text": "This was the final project for my BIOS 6644 Data Wrangling course.\nIn this project, we create a program to automate the data cleaning of vital mouse treadmill data needed to validate Genetically Encoded Voltage Indicators for visualizing neuronal activity, exponentially saving experimenter time on manually cleaning data files and greatly expediting the data analysis process.\nThe result is a program that takes a few seconds to perform what would otherwise take 1-2 hours per recording session to clean by hand!"
  },
  {
    "objectID": "Data_Wrangling/Treadmill_Data_Cleaning/Code/Automated_Treadmill_Data_Cleaning.html#the-motivator",
    "href": "Data_Wrangling/Treadmill_Data_Cleaning/Code/Automated_Treadmill_Data_Cleaning.html#the-motivator",
    "title": "Automated Treadmill Data Cleaning",
    "section": "The Motivator",
    "text": "The Motivator\nAt this point, we had successfully expressed and captured neuronal activity in a single VIP+ neuron using a GEVI, which can be seen here.\n\n\n\nImage and Plots made by Forest Speed\n\n\nHowever, we now needed to validate these signals against a control. That is, we needed to prove that we were capturing real voltage signals.\nOne approach to this problem was to perform voltage imaging with a GEVI in the motor cortex and correlate that activity to a physiological indicator of movement - mice walking on a treadmill. The idea is simple: If we were capturing true voltage signals, we would see spikes every time the mice walked.\nThus, we created a paradigm where we could image mice while they walked on a treadmill. (This experimental set-up would also allow for further experiments down the road of particular interest to my lab, such as documenting the relationships between inhibitory interneurons during locomotion).\nThis experimental set up can be seen here."
  },
  {
    "objectID": "Data_Wrangling/Treadmill_Data_Cleaning/Code/Automated_Treadmill_Data_Cleaning.html#the-problem",
    "href": "Data_Wrangling/Treadmill_Data_Cleaning/Code/Automated_Treadmill_Data_Cleaning.html#the-problem",
    "title": "Automated Treadmill Data Cleaning",
    "section": "The Problem",
    "text": "The Problem\nWe had successfully created a paradigm where we could perform voltage imaging on freely moving mice. However, the data we were receiving would have to be cleaned before we could do anything with it!\nSpecifically the recording on the treadmill was beginning before the recording on the camera. This resulted in treadmill files that were capturing data when the camera was turned off at both at the beginning and end of each treadmill file.\nThis can be visualized here.\n\n\n\nThus, the treadmill data would have to be cleaned in order for the treadmill and camera recordings to be synchronized with each other."
  },
  {
    "objectID": "Data_Wrangling/Treadmill_Data_Cleaning/Code/Automated_Treadmill_Data_Cleaning.html#outline",
    "href": "Data_Wrangling/Treadmill_Data_Cleaning/Code/Automated_Treadmill_Data_Cleaning.html#outline",
    "title": "Automated Treadmill Data Cleaning",
    "section": "Outline",
    "text": "Outline\nAs can be seen in the previous image, the recording software for the camera was detecting the timepoint that the camera was turned on as a change from 0 to ~3 Volts. This is the timestamp that we will use to sychronize the treadmill data with the camera recordings.\nTo do so, we need all of the treadmill recordings to have this timepoint where the camera switches from 0 to 3 Volts as timestamp 0.00 seconds.\nThis will also have to be done en masse, to clean as many treadmill files as needed simultaneously.\n\nPseudo Code\nThus, the pseudo code for the program is laid out as follows:\n\nIterate through all the treadmill.xlsx’s in a specified folder.\nIn each treadmill.xlsx:\n\nDelete every second row (since these are empty rows)\nCheck the second column of the treadmill.xlsx (which detects when the Camera was turned on)\nOnly consider rows where the camera was turned on (i.e. the “Camera On/Off” value &gt; 3)\nConsider the first row as time stamp 0.\nFrom that point on, all remaining time stamps will be shifted by x seconds, where x is the timestamp where the camera was turned on.\nSave the .xlsx in a new folder with the exact same file name but appended with the suffix “_cleaned.xlsx”.\n\n\nThis process can be visualized below."
  },
  {
    "objectID": "Data_Wrangling/Treadmill_Data_Cleaning/Code/Automated_Treadmill_Data_Cleaning.html#converting-treadmill-position-from-volts-to-degrees",
    "href": "Data_Wrangling/Treadmill_Data_Cleaning/Code/Automated_Treadmill_Data_Cleaning.html#converting-treadmill-position-from-volts-to-degrees",
    "title": "Automated Treadmill Data Cleaning",
    "section": "Converting Treadmill Position from Volts to Degrees",
    "text": "Converting Treadmill Position from Volts to Degrees\nIt might easier to deal with/conceptualize the treadmill position as degrees of a circle instead of volts in the future. That way we can calculate distance over time later using the radius of the circle.\nFor reference, the magnetic encoder that the treadmill is attached to captures position in volts, which can be converted directly into degrees of a circle.\n\n\n\n\n\nLet’s make that conversion now instead of later.\n\n# The highest value for Treadmill Position (in Volts) gets designated as 360 degrees, or the end of the circle\ncircle_360 = treadmill[\"Treadmill Position (V)\"].max() # Set 360 as the max value for Treadmill Position (V) \none_volt = 360 / circle_360 # One volt is therefore (360 / the max value for Treadmill Position (V)) degrees (e.g., if the max was 5 volts, then 360 / 5 = 72 degrees, or 1 volt = 72 degrees)\n\n# Create a new column that is the Volts * how many degrees one volt is equal to\ntreadmill[\"Treadmill Position (degree)\"] = treadmill[\"Treadmill Position (V)\"] * one_volt\ntreadmill\n\n\n\n\n\n\n\n\nTreadmill Position (V)\nCamera On/Off\nTime (sec)\nTreadmill Position (degree)\n\n\n\n\n145\n2.474988\n3.324904\n0.000000\n174.575180\n\n\n147\n2.461392\n3.327091\n0.012003\n173.616208\n\n\n149\n2.472487\n3.317091\n0.024006\n174.398817\n\n\n151\n2.479988\n3.322404\n0.036008\n174.927905\n\n\n153\n2.476550\n3.325373\n0.048010\n174.685407\n\n\n...\n...\n...\n...\n...\n\n\n5071\n1.836623\n3.321466\n29.997960\n129.547662\n\n\n5073\n1.834592\n3.323810\n30.009963\n129.404368\n\n\n5075\n1.848500\n3.335060\n30.021966\n130.385382\n\n\n5077\n1.839905\n3.326779\n30.033968\n129.779137\n\n\n5079\n1.823340\n3.320216\n30.045971\n128.610738\n\n\n\n\n2468 rows × 4 columns\n\n\n\nLooks good. This treadmill file is now fully cleaned."
  },
  {
    "objectID": "Data_Wrangling/Treadmill_Data_Cleaning/Code/Automated_Treadmill_Data_Cleaning.html#saving-the-cleaned-data",
    "href": "Data_Wrangling/Treadmill_Data_Cleaning/Code/Automated_Treadmill_Data_Cleaning.html#saving-the-cleaned-data",
    "title": "Automated Treadmill Data Cleaning",
    "section": "Saving the Cleaned Data",
    "text": "Saving the Cleaned Data\nWe will want to save each treadmill file after it is cleaned.\nFor thorough record keeping, let’s create a subfolder within the folder that the treadmill.xlsx files are found within and store the cleaned files there.\n\n# Name the new folder \"Cleaned Treadmill Files\", and place it within the folder that the uncleaned treadmill files will all be kept in\nnew_folder = r\"C:\\Users\\sviea\\Documents\\Portfolio\\Data_Wrangling\\Treadmill_Data_Cleaning\\Treadmill Files Uncleaned\\Cleaned Treadmill Files\"\n\n# Make the new folder\nos.makedirs(new_folder, exist_ok=True)\n\n# Save the cleaned file to this new folder as an excel file\noutput_file = os.path.join(new_folder, \"Cleaned 1.xlsx\")\ntreadmill.to_excel(output_file, index=False)"
  },
  {
    "objectID": "Data_Wrangling/Treadmill_Data_Cleaning/Code/Automated_Treadmill_Data_Cleaning.html#create-a-function",
    "href": "Data_Wrangling/Treadmill_Data_Cleaning/Code/Automated_Treadmill_Data_Cleaning.html#create-a-function",
    "title": "Automated Treadmill Data Cleaning",
    "section": "Create a Function",
    "text": "Create a Function\nWe just wrote code to import, clean, and save a single treadmill.xlsx file.\nFor the final product, we need to make a program that walks through every uncleaned treadmill file in a folder and cleans it.\nTo that end, we need to convert the file-cleaning-and-saving steps into a single function, that we can then apply to multiple files en masse.\n\n# Making a function to clean a single treadmill.xlsx file\ndef treadmill_cleaner(file_path, save_location):\n    '''\n    This function cleans a single treadmill.xlsx file\n    And saves it in the save_location folder\n    '''\n    \n    #Import the File\n    col_names = [\"Treadmill Position (V)\", \"Camera On/Off\", \"Time (sec)\"] # Define column names\n    treadmill = pd.read_excel(file_path, names = col_names) # Import treadmill data as a pd df, set column names to col_names\n\n    # Clean the File\n    treadmill = treadmill.dropna() # Drop NaN values\n    treadmill = treadmill[treadmill[\"Camera On/Off\"] &gt; 3] # Slice the treadmill df so it only includes values where the camera was turned on (i.e. value &gt; 3)\n    time_start = treadmill[\"Time (sec)\"].iloc[0] # Set the first Time (sec) value to 0\n    treadmill[\"Time (sec)\"] = treadmill[\"Time (sec)\"] - time_start # Delete all Time (sec) values by the starting time\n    circle_360 = treadmill[\"Treadmill Position (V)\"].max() # Set 360 as the max value for Treadmill Position (V) \n    one_volt = 360 / circle_360 # One volt is therefore (360 / the max value for Treadmill Position (V)) degrees (e.g., if the max was 5 volts, then 360 / 5 = 72 degrees, or 1 volt = 72 degrees)\n    treadmill[\"Treadmill Position (degree)\"] = treadmill[\"Treadmill Position (V)\"] * one_volt\n    \n    # Save the Cleaned File\n    output_file = os.path.join(save_location, filename) # Generates the name of the output file (save location + file_name)\n    base_name, extension = os.path.splitext(output_file) # Separates the file name to be \"file_name\" and \".xlsx\" (modified from ChatGPT)\n    cleaned_path = f\"{base_name}_cleaned{extension}\" # Creates a new file name that is \"file_name_cleaned.xlsx\" (modified from ChatGPT)\n    treadmill.to_excel(cleaned_path, index=False) # Saves the file and appends \"_cleaned\" to the end of it"
  },
  {
    "objectID": "Data_Wrangling/Treadmill_Data_Cleaning/Code/Automated_Treadmill_Data_Cleaning.html#clean-a-single-treadmill-file",
    "href": "Data_Wrangling/Treadmill_Data_Cleaning/Code/Automated_Treadmill_Data_Cleaning.html#clean-a-single-treadmill-file",
    "title": "Automated Treadmill Data Cleaning",
    "section": "Clean a Single Treadmill File",
    "text": "Clean a Single Treadmill File\nIf we want to run the program to clean a single file, we can run the code below.\n\n# Change file paths as needed below\nfilename = \"121523_RC1_Pace_1.xlsx\" # Change this to what you want the last part of the file name to be!\nfile_path = r\"C:\\Users\\sviea\\Documents\\Portfolio\\Data_Wrangling\\Treadmill_Data_Cleaning\\Treadmill Files Uncleaned\\121523_RC1_Pace_1.xlsx\" # Change this to the file path of the treadmill file you want to clean\nsave_location = r\"C:\\Users\\sviea\\Documents\\Portfolio\\Data_Wrangling\\Treadmill_Data_Cleaning\\Treadmill Files Uncleaned\\Cleaned Treadmill Files\" #Change this to the name of the folder you want to save in\nos.makedirs(save_location, exist_ok=True)  # Makes the save_location as a new folder, skips if it already exists\n\n# Run the program to clean a single treadmill file\ntreadmill_cleaner(file_path, save_location)"
  },
  {
    "objectID": "Data_Wrangling/Treadmill_Data_Cleaning/Code/Automated_Treadmill_Data_Cleaning.html#clean-an-entire-folder-containing-raw-treadmill-files",
    "href": "Data_Wrangling/Treadmill_Data_Cleaning/Code/Automated_Treadmill_Data_Cleaning.html#clean-an-entire-folder-containing-raw-treadmill-files",
    "title": "Automated Treadmill Data Cleaning",
    "section": "Clean an Entire Folder Containing Raw Treadmill Files",
    "text": "Clean an Entire Folder Containing Raw Treadmill Files\nIf we want to clean a folder containing multiple treadmill files, we can run the chunk below.\n\n# This program walks through every file in a designated folder and applies the above treadmill_cleaner() function to each file, saving the cleaned file in the same subfolder\n\n#__Enter the file path to the folder containing treadmill files here!!!!!__\ndirectory = r\"C:\\Users\\sviea\\Documents\\Portfolio\\Data_Wrangling\\Treadmill_Data_Cleaning\\Treadmill Files Uncleaned\" \n\n##### Create the save location as a new folder (if needed)\n# Names the save location as the folder containing treadmill files \n# + \"\\Cleaned Treadmill Files\"\nsave_location = f\"{directory}\" + \"\\Cleaned Treadmill Files\" \n# Makes the save_location as a new folder, skips if it already exists\nos.makedirs(save_location, exist_ok=True)  \nprint(\"Save location is:\", save_location)\n\n##### Walk through every file in the directory and apply the treadmill_cleaner() function\ntry: # Made this a try statement so it does not apply to subfolders\n    for filename in os.listdir(directory):\n        print(\"File name is:\", filename)\n        filepath = os.path.join(directory, filename)\n        print(\"File path is:\", filepath)\n        treadmill_cleaner(filepath, save_location)\nexcept PermissionError:  # Skips subfolders (and anything else really, will throw an error if you have the .xlsx file open!)\n    print(\"File is a folder -  skipping\")\n\n&lt;&gt;:9: SyntaxWarning:\n\ninvalid escape sequence '\\C'\n\n&lt;&gt;:9: SyntaxWarning:\n\ninvalid escape sequence '\\C'\n\nC:\\Users\\sviea\\AppData\\Local\\Temp\\ipykernel_38556\\1169796177.py:9: SyntaxWarning:\n\ninvalid escape sequence '\\C'\n\n\n\nSave location is: C:\\Users\\sviea\\Documents\\Portfolio\\Data_Wrangling\\Treadmill_Data_Cleaning\\Treadmill Files Uncleaned\\Cleaned Treadmill Files\nFile name is: 121523_RC1_Pace_1.xlsx\nFile path is: C:\\Users\\sviea\\Documents\\Portfolio\\Data_Wrangling\\Treadmill_Data_Cleaning\\Treadmill Files Uncleaned\\121523_RC1_Pace_1.xlsx\nFile name is: 121523_RC1_Pace_2.xlsx\nFile path is: C:\\Users\\sviea\\Documents\\Portfolio\\Data_Wrangling\\Treadmill_Data_Cleaning\\Treadmill Files Uncleaned\\121523_RC1_Pace_2.xlsx\nFile name is: 121523_RC1_Pace_3.xlsx\nFile path is: C:\\Users\\sviea\\Documents\\Portfolio\\Data_Wrangling\\Treadmill_Data_Cleaning\\Treadmill Files Uncleaned\\121523_RC1_Pace_3.xlsx\nFile name is: 121523_RC1_Pace_4.xlsx\nFile path is: C:\\Users\\sviea\\Documents\\Portfolio\\Data_Wrangling\\Treadmill_Data_Cleaning\\Treadmill Files Uncleaned\\121523_RC1_Pace_4.xlsx\nFile name is: 121523_RC1_Pace_5.xlsx\nFile path is: C:\\Users\\sviea\\Documents\\Portfolio\\Data_Wrangling\\Treadmill_Data_Cleaning\\Treadmill Files Uncleaned\\121523_RC1_Pace_5.xlsx\nFile name is: 121523_RC1_Pace_6.xlsx\nFile path is: C:\\Users\\sviea\\Documents\\Portfolio\\Data_Wrangling\\Treadmill_Data_Cleaning\\Treadmill Files Uncleaned\\121523_RC1_Pace_6.xlsx\nFile name is: Cleaned Treadmill Files\nFile path is: C:\\Users\\sviea\\Documents\\Portfolio\\Data_Wrangling\\Treadmill_Data_Cleaning\\Treadmill Files Uncleaned\\Cleaned Treadmill Files\nFile is a folder -  skipping\n\n\nWe just cleaned 6 files in a matter of seconds. That would have taken the better part of an hour to do by hand!"
  },
  {
    "objectID": "Data_Wrangling/Treadmill_Data_Cleaning/Code/Automated_Treadmill_Data_Cleaning.html#checking-the-output",
    "href": "Data_Wrangling/Treadmill_Data_Cleaning/Code/Automated_Treadmill_Data_Cleaning.html#checking-the-output",
    "title": "Automated Treadmill Data Cleaning",
    "section": "Checking the Output",
    "text": "Checking the Output\nFor good measure, let’s check the output of the first and last file we cleaned.\n\n# Check first file\nfile_1 = pd.read_excel(r\"C:\\Users\\sviea\\Documents\\Portfolio\\Data_Wrangling\\Treadmill_Data_Cleaning\\Treadmill Files Uncleaned\\Cleaned Treadmill Files\\121523_RC1_Pace_1_cleaned.xlsx\")\nfile_1\n\n\n\n\n\n\n\n\nTreadmill Position (V)\nCamera On/Off\nTime (sec)\nTreadmill Position (degree)\n\n\n\n\n0\n2.474988\n3.324904\n0.000000\n174.575180\n\n\n1\n2.461392\n3.327091\n0.012003\n173.616208\n\n\n2\n2.472487\n3.317091\n0.024006\n174.398817\n\n\n3\n2.479988\n3.322404\n0.036008\n174.927905\n\n\n4\n2.476550\n3.325373\n0.048010\n174.685407\n\n\n...\n...\n...\n...\n...\n\n\n2463\n1.836623\n3.321466\n29.997960\n129.547662\n\n\n2464\n1.834592\n3.323810\n30.009963\n129.404368\n\n\n2465\n1.848500\n3.335060\n30.021966\n130.385382\n\n\n2466\n1.839905\n3.326779\n30.033968\n129.779137\n\n\n2467\n1.823340\n3.320216\n30.045971\n128.610738\n\n\n\n\n2468 rows × 4 columns\n\n\n\n\n# Check last file\nfile_6 = pd.read_excel(r\"C:\\Users\\sviea\\Documents\\Portfolio\\Data_Wrangling\\Treadmill_Data_Cleaning\\Treadmill Files Uncleaned\\Cleaned Treadmill Files\\121523_RC1_Pace_6_cleaned.xlsx\")\nfile_6\n\n\n\n\n\n\n\n\nTreadmill Position (V)\nCamera On/Off\nTime (sec)\nTreadmill Position (degree)\n\n\n\n\n0\n1.552993\n3.318966\n0.000000\n109.813950\n\n\n1\n1.547993\n3.324123\n0.012003\n109.460349\n\n\n2\n1.550337\n3.318966\n0.024005\n109.626099\n\n\n3\n1.543148\n3.312560\n0.036008\n109.117798\n\n\n4\n1.544867\n3.318498\n0.048011\n109.239348\n\n\n...\n...\n...\n...\n...\n\n\n4966\n1.637379\n3.326779\n60.020822\n115.780965\n\n\n4967\n1.615189\n3.315998\n60.032824\n114.211861\n\n\n4968\n1.624409\n3.316935\n60.044827\n114.863812\n\n\n4969\n1.625190\n3.327091\n60.056829\n114.919062\n\n\n4970\n1.643161\n3.327248\n60.068832\n116.189816\n\n\n\n\n4971 rows × 4 columns\n\n\n\nLooks great!"
  },
  {
    "objectID": "Data_Management_(SAS)/Project_1/Project_1_Importing.html",
    "href": "Data_Management_(SAS)/Project_1/Project_1_Importing.html",
    "title": "Project 1 - Import Data Into SAS",
    "section": "",
    "text": "This is the first part of the main project for BIOS 6680: Data Management Using SAS, in which we plan and carry out data management activities including data cleaning/quality assurance, data documentation, analytic data set creation, and a final analysis. A key goal of this project is to demonstrate reproducible research and reporting.\nIn this part, we read in all the datasets consististing of multiple filetypes and delimiters, and create a BRFSSImpt import library"
  },
  {
    "objectID": "Data_Management_(SAS)/Project_1/Project_1_Importing.html#race-and-ethnicity",
    "href": "Data_Management_(SAS)/Project_1/Project_1_Importing.html#race-and-ethnicity",
    "title": "Project 1 - Import Data Into SAS",
    "section": "Race and Ethnicity",
    "text": "Race and Ethnicity\nRead in the race and ethnicity data set.\n\nImport Statement\nThis is a .xlsx file so we simply use PROC IMPORT with the DBMS as XLSX.\n\n* Import race and ethnicity data;\nPROC IMPORT \n        DATAFILE =  \"&CourseRoot/BRFSS/Data/1_Source/race_eth.xlsx\"\n        OUT     =   Impt.race_eth\n        DBMS    =   XLSX\n        REPLACE ;\n    RUN;\n\n\n\nPrint Statement\nLet’s examine that data set to be sure of how it looks.\n\n* Print race and ethnicity file;\nTITLE \"Race and Ethnicity\";\nPROC PRINT DATA = Impt.race_eth (OBS = 6);\n    RUN;\n\nlstlog\n\n\n&lt;!DOCTYPE html&gt;\n\n\n\n\n\nSAS Output\n\n\n\n\n\n\n\nRace and Ethnicity\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObs\n\n\nSEQNO\n\n\n_PRACE2\n\n\n_HISPANC\n\n\n\n\n\n\n1\n\n\n2022000001\n\n\n1\n\n\n2\n\n\n\n\n2\n\n\n2022000002\n\n\n1\n\n\n2\n\n\n\n\n3\n\n\n2022000003\n\n\n1\n\n\n2\n\n\n\n\n4\n\n\n2022000004\n\n\n1\n\n\n2\n\n\n\n\n5\n\n\n2022000005\n\n\n1\n\n\n2\n\n\n\n\n6\n\n\n2022000006\n\n\n1\n\n\n2\n\n\n\n\n\n\n\n\n\n\n\u001411                                                         The SAS System                      Sunday, November  3, 2024 06:39:00 AM\n\n84         ods listing close;ods html5 (id=saspy_internal) file=_tomods1 options(bitmap_mode='inline') device=svg style=HTMLBlue;\n84       ! ods graphics on / outputfmt=png;\n85         \n86         * Print race and ethnicity file;\n87         TITLE \"Race and Ethnicity\";\n88         PROC PRINT DATA = Impt.race_eth (OBS = 6);\n89          RUN;\n90         \n91         \n92         ods html5 (id=saspy_internal) close;ods listing;\n93         \n\u001412                                                         The SAS System                      Sunday, November  3, 2024 06:39:00 AM\n\n94         \n\n\n\n\nLooks good."
  },
  {
    "objectID": "Data_Management_(SAS)/Project_1/Project_1_Importing.html#general-demographics",
    "href": "Data_Management_(SAS)/Project_1/Project_1_Importing.html#general-demographics",
    "title": "Project 1 - Import Data Into SAS",
    "section": "General Demographics",
    "text": "General Demographics\nRead in the general demographics data set.\n\nImport Statement\nThis is a .csv file with the first row as the column names, thus we use MISSOVER DSD FIRSTOBS = 2 to denote a comma as the delimiter, and to start reading at the second row.\n\n* Import general demographics data;\nDATA Impt.gen_dem;\n    INFILE \"&CourseRoot/BRFSS/Data/1_Source/gen_dem.csv\" MISSOVER DSD FIRSTOBS = 2;\n    INPUT   marital         \n            trnsgndr\n            _sex\n            _age5year\n            id              :$13.       ;\n    RUN;\n\n\n\nPrint Statement\n\n* Print header to check data;\nTITLE \"General Demographics\";\nPROC PRINT DATA = Impt.gen_dem (OBS = 6);\n    RUN;\n\nlstlog\n\n\n&lt;!DOCTYPE html&gt;\n\n\n\n\n\nSAS Output\n\n\n\n\n\n\n\nGeneral Demographics\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObs\n\n\nmarital\n\n\ntrnsgndr\n\n\n_sex\n\n\n_age5year\n\n\nid\n\n\n\n\n\n\n1\n\n\n1\n\n\n.\n\n\n2\n\n\n13\n\n\n1_2022000001\n\n\n\n\n2\n\n\n3\n\n\n.\n\n\n2\n\n\n13\n\n\n1_2022000002\n\n\n\n\n3\n\n\n1\n\n\n.\n\n\n2\n\n\n8\n\n\n1_2022000003\n\n\n\n\n4\n\n\n1\n\n\n.\n\n\n2\n\n\n14\n\n\n1_2022000004\n\n\n\n\n5\n\n\n1\n\n\n.\n\n\n2\n\n\n5\n\n\n1_2022000005\n\n\n\n\n6\n\n\n1\n\n\n.\n\n\n1\n\n\n13\n\n\n1_2022000006\n\n\n\n\n\n\n\n\n\n\n\u001415                                                         The SAS System                      Sunday, November  3, 2024 06:39:00 AM\n\n115        ods listing close;ods html5 (id=saspy_internal) file=_tomods1 options(bitmap_mode='inline') device=svg style=HTMLBlue;\n115      ! ods graphics on / outputfmt=png;\n116        \n117        * Print header to check data;\n118        TITLE \"General Demographics\";\n119        PROC PRINT DATA = Impt.gen_dem (OBS = 6);\n120         RUN;\n121        \n122        \n123        ods html5 (id=saspy_internal) close;ods listing;\n124        \n\u001416                                                         The SAS System                      Sunday, November  3, 2024 06:39:00 AM\n\n125        \n\n\n\n\n\n\nData Quality Check\nI noticed that there were all missing values for trnsgndr in that output.\nLet’s sort by that variable and double check to ensure we coded it correctly.\n\n* Check that we successfully have trnsgender values imported. This was rarer in the data set so we will sort by it;\nPROC SORT DATA = Impt.gen_dem\n    OUT = sorted_data;\n    BY DESCENDING trnsgndr;\n    RUN;\n\n\n* Print sorted data to check for correct importing;\nPROC PRINT DATA = sorted_data (OBS = 6);\n     RUN;\n\nlstlog\n\n\n&lt;!DOCTYPE html&gt;\n\n\n\n\n\nSAS Output\n\n\n\n\n\n\n\nGeneral Demographics\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObs\n\n\nmarital\n\n\ntrnsgndr\n\n\n_sex\n\n\n_age5year\n\n\nid\n\n\n\n\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n12\n\n\n2_2022000020\n\n\n\n\n2\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n2_2022000035\n\n\n\n\n3\n\n\n2\n\n\n9\n\n\n2\n\n\n9\n\n\n2_2022000126\n\n\n\n\n4\n\n\n1\n\n\n9\n\n\n2\n\n\n7\n\n\n2_2022000173\n\n\n\n\n5\n\n\n9\n\n\n9\n\n\n2\n\n\n9\n\n\n2_2022000352\n\n\n\n\n6\n\n\n1\n\n\n9\n\n\n2\n\n\n10\n\n\n2_2022000503\n\n\n\n\n\n\n\n\n\n\n\u001419                                                         The SAS System                      Sunday, November  3, 2024 06:39:00 AM\n\n142        ods listing close;ods html5 (id=saspy_internal) file=_tomods1 options(bitmap_mode='inline') device=svg style=HTMLBlue;\n142      ! ods graphics on / outputfmt=png;\n143        \n144        * Print sorted data to check for correct importing;\n145        PROC PRINT DATA = sorted_data (OBS = 6);\n146          RUN;\n147        \n148        \n149        ods html5 (id=saspy_internal) close;ods listing;\n150        \n\u001420                                                         The SAS System                      Sunday, November  3, 2024 06:39:00 AM\n\n151        \n\n\n\n\nThat looks good, we can move on."
  },
  {
    "objectID": "Data_Management_(SAS)/Project_1/Project_1_Importing.html#social-and-economic-status",
    "href": "Data_Management_(SAS)/Project_1/Project_1_Importing.html#social-and-economic-status",
    "title": "Project 1 - Import Data Into SAS",
    "section": "Social and Economic Status",
    "text": "Social and Economic Status\nRead in the social and economic status data set.\n\nImport Statement\nThis is a .csv file with the first row as the column names, thus we use MISSOVER DSD FIRSTOBS = 2 to denote a comma as the delimiter, and to start reading at the second row.\n\n* Read in socioeconomic data set;\nDATA Impt.socioec;\n    INFILE \"&CourseRoot/BRFSS/Data/1_Source/socioec.csv\" MISSOVER DSD FIRSTOBS = 2;\n    INPUT   educa       \n            employ1\n            income3\n            seqno       :$13.       ;\n    RUN;\n\n\n\nPrint Statement\n\n* Print header of data set;\nTITLE \"Social and Economic Status\";\nPROC PRINT DATA = Impt.socioec (OBS = 6);\n  RUN;\n\nlstlog\n\n\n&lt;!DOCTYPE html&gt;\n\n\n\n\n\nSAS Output\n\n\n\n\n\n\n\nSocial and Economic Status\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObs\n\n\neduca\n\n\nemploy1\n\n\nincome3\n\n\nseqno\n\n\n\n\n\n\n1\n\n\n6\n\n\n7\n\n\n99\n\n\n202-200-0001\n\n\n\n\n2\n\n\n4\n\n\n2\n\n\n5\n\n\n202-200-0002\n\n\n\n\n3\n\n\n6\n\n\n7\n\n\n10\n\n\n202-200-0003\n\n\n\n\n4\n\n\n4\n\n\n7\n\n\n77\n\n\n202-200-0004\n\n\n\n\n5\n\n\n5\n\n\n5\n\n\n5\n\n\n202-200-0005\n\n\n\n\n6\n\n\n4\n\n\n7\n\n\n99\n\n\n202-200-0006\n\n\n\n\n\n\n\n\n\n\n\u001423                                                         The SAS System                      Sunday, November  3, 2024 06:39:00 AM\n\n171        ods listing close;ods html5 (id=saspy_internal) file=_tomods1 options(bitmap_mode='inline') device=svg style=HTMLBlue;\n171      ! ods graphics on / outputfmt=png;\n172        \n173        * Print header of data set;\n174        TITLE \"Social and Economic Status\";\n175        PROC PRINT DATA = Impt.socioec (OBS = 6);\n176          RUN;\n177        \n178        \n179        ods html5 (id=saspy_internal) close;ods listing;\n180        \n\u001424                                                         The SAS System                      Sunday, November  3, 2024 06:39:00 AM\n\n181"
  },
  {
    "objectID": "Data_Management_(SAS)/Project_1/Project_1_Importing.html#insurance",
    "href": "Data_Management_(SAS)/Project_1/Project_1_Importing.html#insurance",
    "title": "Project 1 - Import Data Into SAS",
    "section": "Insurance",
    "text": "Insurance\nRead in the insurance data set.\n\nImport Statement\nThis is a .txt file with spaces as the delimiter. Let’s address.\n\n* Read in insurance data set;\nDATA Impt.insurance;\n    INFILE \"&CourseRoot/BRFSS/Data/1_Source/insurance.txt\" DELIMITER = \" \" MISSOVER DSD FIRSTOBS = 2;\n    INPUT   priminisr       \n            persdoc3\n            medcost1\n            checkup1\n            seqno           ;\n    RUN;\n\n\n\nPrint Statement\n\n# Print header of data set;\nTITLE \"Insurance\";\nPROC PRINT DATA = Impt.insurance (OBS = 6);\n  RUN;\n\nlstlog\n\n\n&lt;!DOCTYPE html&gt;\n\n\n\n\n\nSAS Output\n\n\n\n\n\n\n\nInsurance\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObs\n\n\npriminisr\n\n\npersdoc3\n\n\nmedcost1\n\n\ncheckup1\n\n\nseqno\n\n\n\n\n\n\n1\n\n\n99\n\n\n1\n\n\n2\n\n\n1\n\n\n2022000001\n\n\n\n\n2\n\n\n3\n\n\n2\n\n\n2\n\n\n8\n\n\n2022000002\n\n\n\n\n3\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n2022000003\n\n\n\n\n4\n\n\n99\n\n\n1\n\n\n2\n\n\n1\n\n\n2022000004\n\n\n\n\n5\n\n\n7\n\n\n2\n\n\n2\n\n\n1\n\n\n2022000005\n\n\n\n\n6\n\n\n3\n\n\n1\n\n\n2\n\n\n1\n\n\n2022000006\n\n\n\n\n\n\n\n\n\n\n\u001427                                                         The SAS System                      Sunday, November  3, 2024 06:39:00 AM\n\n202        ods listing close;ods html5 (id=saspy_internal) file=_tomods1 options(bitmap_mode='inline') device=svg style=HTMLBlue;\n202      ! ods graphics on / outputfmt=png;\n203        \n204        # Print header of data set;\n           _\n           180\nERROR 180-322: Statement is not valid or it is used out of proper order.\n\n205        TITLE \"Insurance\";\n206        PROC PRINT DATA = Impt.insurance (OBS = 6);\n207          RUN;\n208        \n209        \n210        ods html5 (id=saspy_internal) close;ods listing;\n211        \n\u001428                                                         The SAS System                      Sunday, November  3, 2024 06:39:00 AM\n\n212"
  },
  {
    "objectID": "Data_Management_(SAS)/Project_1/Project_1_Importing.html#disabilities",
    "href": "Data_Management_(SAS)/Project_1/Project_1_Importing.html#disabilities",
    "title": "Project 1 - Import Data Into SAS",
    "section": "Disabilities",
    "text": "Disabilities\nRead in the disabilities data set.\n\nImport Statement\nThis is a .txt file with a delimiter of |. Let’s handle that.\n\n* Read in disability data set;\nDATA Impt.disability;\n    INFILE \"&CourseRoot/BRFSS/Data/1_Source/disability.txt\" DELIMITER = \"|\" MISSOVER DSD FIRSTOBS = 2;\n    INPUT   deaf\n            blind\n            decide\n            diffwalk\n            diffdres\n            diffalon\n            id          :$13.   ;\n    RUN;\n\n\n\nPrint Statement\n\n* Print header of data set;\nTITLE \"Disabilities\";\nPROC PRINT DATA = Impt.disability (OBS = 6);\n  RUN;\n\nlstlog\n\n\n&lt;!DOCTYPE html&gt;\n\n\n\n\n\nSAS Output\n\n\n\n\n\n\n\nDisabilities\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObs\n\n\ndeaf\n\n\nblind\n\n\ndecide\n\n\ndiffwalk\n\n\ndiffdres\n\n\ndiffalon\n\n\nid\n\n\n\n\n\n\n1\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n1_2022000001\n\n\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n1_2022000002\n\n\n\n\n3\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n1_2022000003\n\n\n\n\n4\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n1_2022000004\n\n\n\n\n5\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n1_2022000005\n\n\n\n\n6\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n1_2022000006\n\n\n\n\n\n\n\n\n\n\n\u001431                                                         The SAS System                      Sunday, November  3, 2024 06:39:00 AM\n\n235        ods listing close;ods html5 (id=saspy_internal) file=_tomods1 options(bitmap_mode='inline') device=svg style=HTMLBlue;\n235      ! ods graphics on / outputfmt=png;\n236        \n237        * Print header of data set;\n238        TITLE \"Disabilities\";\n239        PROC PRINT DATA = Impt.disability (OBS = 6);\n240          RUN;\n241        \n242        \n243        ods html5 (id=saspy_internal) close;ods listing;\n244        \n\u001432                                                         The SAS System                      Sunday, November  3, 2024 06:39:00 AM\n\n245"
  },
  {
    "objectID": "Data_Management_(SAS)/Project_1/Project_1_Importing.html#chronic-health-conditions",
    "href": "Data_Management_(SAS)/Project_1/Project_1_Importing.html#chronic-health-conditions",
    "title": "Project 1 - Import Data Into SAS",
    "section": "Chronic Health Conditions",
    "text": "Chronic Health Conditions\nRead in the chronic health conditions data set.\n\nImport Statement\nThis is a .dat file with a delimiter of ::. Thus we handle with a DATA statement.\n\n* Read in chronic health conditions data set;\nDATA    Impt.chronic;\n    INFILE  \"&CourseRoot/BRFSS/Data/1_Source/chronic.dat\" DLMSTR = \"::\" DSD;\n    INPUT   seqno           \n            asthma3\n            asthnow\n            chccopd3\n            diabetes4\n            cvdcrhd4\n            cvdinfr4        @@ ;\n    RUN;\n\n\n\nPrint Statement\n\n* Print header of data set;\nTITLE \"Chronic Health Conditions\";\nPROC PRINT DATA = Impt.chronic (OBS = 6);\n  RUN;\n\nlstlog\n\n\n&lt;!DOCTYPE html&gt;\n\n\n\n\n\nSAS Output\n\n\n\n\n\n\n\nChronic Health Conditions\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObs\n\n\nseqno\n\n\nasthma3\n\n\nasthnow\n\n\nchccopd3\n\n\ndiabetes4\n\n\ncvdcrhd4\n\n\ncvdinfr4\n\n\n\n\n\n\n1\n\n\n2022000001\n\n\n2\n\n\n.\n\n\n2\n\n\n1\n\n\n2\n\n\n2\n\n\n\n\n2\n\n\n2022000002\n\n\n2\n\n\n.\n\n\n2\n\n\n3\n\n\n2\n\n\n2\n\n\n\n\n3\n\n\n2022000003\n\n\n2\n\n\n.\n\n\n2\n\n\n3\n\n\n2\n\n\n2\n\n\n\n\n4\n\n\n2022000004\n\n\n1\n\n\n1\n\n\n2\n\n\n3\n\n\n2\n\n\n2\n\n\n\n\n5\n\n\n2022000005\n\n\n2\n\n\n.\n\n\n2\n\n\n3\n\n\n2\n\n\n2\n\n\n\n\n6\n\n\n2022000006\n\n\n2\n\n\n.\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n\n\n\n\n\n\n\n\n\u001435                                                         The SAS System                      Sunday, November  3, 2024 06:39:00 AM\n\n268        ods listing close;ods html5 (id=saspy_internal) file=_tomods1 options(bitmap_mode='inline') device=svg style=HTMLBlue;\n268      ! ods graphics on / outputfmt=png;\n269        \n270        * Print header of data set;\n271        TITLE \"Chronic Health Conditions\";\n272        PROC PRINT DATA = Impt.chronic (OBS = 6);\n273          RUN;\n274        \n275        \n276        ods html5 (id=saspy_internal) close;ods listing;\n277        \n\u001436                                                         The SAS System                      Sunday, November  3, 2024 06:39:00 AM\n\n278"
  },
  {
    "objectID": "Data_Management_(SAS)/Project_1/Project_1_Importing.html#mental-health",
    "href": "Data_Management_(SAS)/Project_1/Project_1_Importing.html#mental-health",
    "title": "Project 1 - Import Data Into SAS",
    "section": "Mental Health",
    "text": "Mental Health\nRead in the mental health data set.\n\nImport Statement\nThis is just another .csv we’ve already handled before.\n\n* Read in mental health data set;\nDATA    Impt.mentalhealth;\n    INFILE  \"&CourseRoot/BRFSS/Data/1_Source/mentalhealth.csv\" MISSOVER DSD FIRSTOBS=2;\n    INPUT   addepev3            \n            lsatisfy\n            emtsuprt\n            sdhisoft\n            id              :$13.   ;\n    RUN;\n\n\n\nPrint Statement\n\n* Print header of data set;\nTITLE \"Mental Health\";\nPROC PRINT DATA = Impt.mentalhealth (OBS = 6);\n  RUN;\n\nlstlog\n\n\n&lt;!DOCTYPE html&gt;\n\n\n\n\n\nSAS Output\n\n\n\n\n\n\n\nMental Health\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObs\n\n\naddepev3\n\n\nlsatisfy\n\n\nemtsuprt\n\n\nsdhisoft\n\n\nid\n\n\n\n\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n5\n\n\n1_2022000001\n\n\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n5\n\n\n1_2022000002\n\n\n\n\n3\n\n\n2\n\n\n2\n\n\n2\n\n\n3\n\n\n1_2022000003\n\n\n\n\n4\n\n\n2\n\n\n1\n\n\n1\n\n\n3\n\n\n1_2022000004\n\n\n\n\n5\n\n\n2\n\n\n1\n\n\n1\n\n\n5\n\n\n1_2022000005\n\n\n\n\n6\n\n\n2\n\n\n2\n\n\n2\n\n\n5\n\n\n1_2022000006\n\n\n\n\n\n\n\n\n\n\n\u001439                                                         The SAS System                      Sunday, November  3, 2024 06:39:00 AM\n\n299        ods listing close;ods html5 (id=saspy_internal) file=_tomods1 options(bitmap_mode='inline') device=svg style=HTMLBlue;\n299      ! ods graphics on / outputfmt=png;\n300        \n301        * Print header of data set;\n302        TITLE \"Mental Health\";\n303        PROC PRINT DATA = Impt.mentalhealth (OBS = 6);\n304          RUN;\n305        \n306        \n307        ods html5 (id=saspy_internal) close;ods listing;\n308        \n\u001440                                                         The SAS System                      Sunday, November  3, 2024 06:39:00 AM\n\n309"
  },
  {
    "objectID": "Data_Management_(SAS)/Project_1/Project_1_Importing.html#cancer",
    "href": "Data_Management_(SAS)/Project_1/Project_1_Importing.html#cancer",
    "title": "Project 1 - Import Data Into SAS",
    "section": "Cancer",
    "text": "Cancer\nRead in the cancer data set.\n\nImport Statement\nThis is a .xls file, thus we use a PROC IMPORT statement with the DBMS as XLS.\n\n* Read in cancer data set;\nPROC IMPORT\n        DATAFILE    = \"&CourseRoot/BRFSS/Data/1_Source/cancer.xls\"\n        OUT         = Impt.cancer\n        DBMS        = XLS\n        REPLACE;\n    RUN;\n\n\n\nPrint Statement\n\n* Print header of data set;\nTITLE \"Cancer\";\nPROC PRINT DATA = Impt.cancer (OBS = 6);\n  RUN;\n\nlstlog\n\n\n&lt;!DOCTYPE html&gt;\n\n\n\n\n\nSAS Output\n\n\n\n\n\n\n\nCancer\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObs\n\n\nCNCRDIFF\n\n\nCNCRAGE\n\n\nCNCRTYP2\n\n\nid\n\n\n\n\n\n\n1\n\n\n.\n\n\n.\n\n\n.\n\n\n6_2022000075\n\n\n\n\n2\n\n\n.\n\n\n.\n\n\n.\n\n\n6_2022000076\n\n\n\n\n3\n\n\n.\n\n\n.\n\n\n.\n\n\n6_2022000077\n\n\n\n\n4\n\n\n.\n\n\n.\n\n\n.\n\n\n6_2022000078\n\n\n\n\n5\n\n\n.\n\n\n.\n\n\n.\n\n\n6_2022000079\n\n\n\n\n6\n\n\n.\n\n\n.\n\n\n.\n\n\n6_2022000080\n\n\n\n\n\n\n\n\n\n\n\u001443                                                         The SAS System                      Sunday, November  3, 2024 06:39:00 AM\n\n328        ods listing close;ods html5 (id=saspy_internal) file=_tomods1 options(bitmap_mode='inline') device=svg style=HTMLBlue;\n328      ! ods graphics on / outputfmt=png;\n329        \n330        * Print header of data set;\n331        TITLE \"Cancer\";\n332        PROC PRINT DATA = Impt.cancer (OBS = 6);\n333          RUN;\n334        \n335        \n336        ods html5 (id=saspy_internal) close;ods listing;\n337        \n\u001444                                                         The SAS System                      Sunday, November  3, 2024 06:39:00 AM\n\n338        \n\n\n\n\n\n\nData Quality Check\nThat’s a lot of missing data points, but that makes sense for cancer data since instances of cancer must be rare.\nLet’s sort that data and print it again to be sure.\n\n* Sort the data;\nPROC SORT DATA = Impt.cancer\n    OUT = sorted_data;\n    BY DESCENDING CNCRDIFF;\n    RUN;\n    \n* Print the data;\nTITLE \"Sorted Data - Quality Check\";\nPROC PRINT DATA = sorted_data (OBS = 6);\n  RUN;\n\nlstlog\n\n\n&lt;!DOCTYPE html&gt;\n\n\n\n\n\nSAS Output\n\n\n\n\n\n\n\nSorted Data - Quality Check\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObs\n\n\nCNCRDIFF\n\n\nCNCRAGE\n\n\nCNCRTYP2\n\n\nid\n\n\n\n\n\n\n1\n\n\n9\n\n\n.\n\n\n.\n\n\n8_2022000168\n\n\n\n\n2\n\n\n9\n\n\n.\n\n\n.\n\n\n8_2022001164\n\n\n\n\n3\n\n\n9\n\n\n.\n\n\n.\n\n\n8_2022001249\n\n\n\n\n4\n\n\n9\n\n\n.\n\n\n.\n\n\n8_2022001934\n\n\n\n\n5\n\n\n9\n\n\n.\n\n\n.\n\n\n8_2022002358\n\n\n\n\n6\n\n\n9\n\n\n.\n\n\n.\n\n\n8_2022002377\n\n\n\n\n\n\n\n\n\n\n\u001445                                                         The SAS System                      Sunday, November  3, 2024 06:39:00 AM\n\n341        ods listing close;ods html5 (id=saspy_internal) file=_tomods1 options(bitmap_mode='inline') device=svg style=HTMLBlue;\n341      ! ods graphics on / outputfmt=png;\n342        \n343        * Sort the data;\n344        PROC SORT DATA = Impt.cancer\n345         OUT = sorted_data;\n346         BY DESCENDING CNCRDIFF;\n347         RUN;\n348         \n349        * Print the data;\n350        TITLE \"Sorted Data - Quality Check\";\n351        PROC PRINT DATA = sorted_data (OBS = 6);\n352          RUN;\n353        \n354        \n355        ods html5 (id=saspy_internal) close;ods listing;\n356        \n\u001446                                                         The SAS System                      Sunday, November  3, 2024 06:39:00 AM\n\n357        \n\n\n\n\nLooks good."
  },
  {
    "objectID": "Data_Management_(SAS)/Project_1/Project_1_Importing.html#obesity",
    "href": "Data_Management_(SAS)/Project_1/Project_1_Importing.html#obesity",
    "title": "Project 1 - Import Data Into SAS",
    "section": "Obesity",
    "text": "Obesity\nRead in the obesity data set.\n\nImport Statement\nThis is a .txt file with a delimiter of *. Let’s handle that.\n\n* Read in the data set;\nDATA    Impt.obesity;\n    INFILE  \"&CourseRoot/BRFSS/Data/1_Source/obesity.txt\" DELIMITER = \"*\" MISSOVER DSD FIRSTOBS = 2;\n    INPUT   seqno       :$13.           \n            weight      \n            height      ;\n    RUN;\n\n\n\nPrint Statement\n\n* Print header of data set;\nPROC PRINT DATA = Impt.obesity (OBS = 6);\n  RUN;\n\nlstlog\n\n\n&lt;!DOCTYPE html&gt;\n\n\n\n\n\nSAS Output\n\n\n\n\n\n\n\nSorted Data - Quality Check\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObs\n\n\nseqno\n\n\nweight\n\n\nheight\n\n\n\n\n\n\n1\n\n\n2022000001\n\n\n9999\n\n\n9999\n\n\n\n\n2\n\n\n2022000002\n\n\n150\n\n\n503\n\n\n\n\n3\n\n\n2022000003\n\n\n140\n\n\n502\n\n\n\n\n4\n\n\n2022000004\n\n\n140\n\n\n505\n\n\n\n\n5\n\n\n2022000005\n\n\n119\n\n\n502\n\n\n\n\n6\n\n\n2022000006\n\n\n187\n\n\n511\n\n\n\n\n\n\n\n\n\n\n\u001449                                                         The SAS System                      Sunday, November  3, 2024 06:39:00 AM\n\n376        ods listing close;ods html5 (id=saspy_internal) file=_tomods1 options(bitmap_mode='inline') device=svg style=HTMLBlue;\n376      ! ods graphics on / outputfmt=png;\n377        \n378        * Print header of data set;\n379        PROC PRINT DATA = Impt.obesity (OBS = 6);\n380          RUN;\n381        \n382        \n383        ods html5 (id=saspy_internal) close;ods listing;\n384        \n\u001450                                                         The SAS System                      Sunday, November  3, 2024 06:39:00 AM\n\n385"
  },
  {
    "objectID": "Data_Management_(SAS)/Project_1/Project_1_Importing.html#covid",
    "href": "Data_Management_(SAS)/Project_1/Project_1_Importing.html#covid",
    "title": "Project 1 - Import Data Into SAS",
    "section": "Covid",
    "text": "Covid\nRead in the covid data set.\n\nImport Statement\nThis is just another .xlsx file.\n\n* Read in the covid data set;\nPROC IMPORT \n        DATAFILE    = \"&CourseRoot/BRFSS/Data/1_Source/covid.xlsx\"\n        OUT         = Impt.covid\n        DBMS        = XLSX\n        REPLACE;\n    RUN;\n\n\n\nPrint Statement\n\n* Print header of data set;\nTITLE \"Covid\";\nPROC PRINT DATA = Impt.covid (OBS = 6);\n  RUN;\n\nlstlog\n\n\n&lt;!DOCTYPE html&gt;\n\n\n\n\n\nSAS Output\n\n\n\n\n\n\n\nCovid\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObs\n\n\nCOVIDPOS\n\n\nCOVIDSMP\n\n\nCOVIDPRM\n\n\nseqno\n\n\n\n\n\n\n1\n\n\n2\n\n\n.\n\n\n.\n\n\n2022000001\n\n\n\n\n2\n\n\n2\n\n\n.\n\n\n.\n\n\n2022000002\n\n\n\n\n3\n\n\n1\n\n\n1\n\n\n9\n\n\n2022000003\n\n\n\n\n4\n\n\n2\n\n\n.\n\n\n.\n\n\n2022000004\n\n\n\n\n5\n\n\n2\n\n\n.\n\n\n.\n\n\n2022000005\n\n\n\n\n6\n\n\n2\n\n\n.\n\n\n.\n\n\n2022000006\n\n\n\n\n\n\n\n\n\n\n\u001453                                                         The SAS System                      Sunday, November  3, 2024 06:39:00 AM\n\n404        ods listing close;ods html5 (id=saspy_internal) file=_tomods1 options(bitmap_mode='inline') device=svg style=HTMLBlue;\n404      ! ods graphics on / outputfmt=png;\n405        \n406        * Print header of data set;\n407        TITLE \"Covid\";\n408        PROC PRINT DATA = Impt.covid (OBS = 6);\n409          RUN;\n410        \n411        \n412        ods html5 (id=saspy_internal) close;ods listing;\n413        \n\u001454                                                         The SAS System                      Sunday, November  3, 2024 06:39:00 AM\n\n414"
  },
  {
    "objectID": "Data_Management_(SAS)/Project_1/Project_1_Importing.html#general-health",
    "href": "Data_Management_(SAS)/Project_1/Project_1_Importing.html#general-health",
    "title": "Project 1 - Import Data Into SAS",
    "section": "General Health",
    "text": "General Health\nRead in the general health data.\n\nImport Statement\nThis is a .txt file.\n\n* Read in in general health data;\nDATA Impt.gen_health;\n    INFILE  \"&CourseRoot/BRFSS/Data/1_Source/gen_health.txt\";\n    INPUT   seqno           $1-11\n            gnhlth          12-14\n            menthlth        15-17\n            physhlth        18-20\n            exerany2        21-23\n            sleptim1                ;\n    RUN;\n\n\n\nPrint Statement\n\n* Print header of data set;\nTITLE \"General Health\";\nPROC PRINT DATA = Impt.gen_health (OBS = 6);\n  RUN;\n\nlstlog\n\n\n&lt;!DOCTYPE html&gt;\n\n\n\n\n\nSAS Output\n\n\n\n\n\n\n\nGeneral Health\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObs\n\n\nseqno\n\n\ngnhlth\n\n\nmenthlth\n\n\nphyshlth\n\n\nexerany2\n\n\nsleptim1\n\n\n\n\n\n\n1\n\n\n2022000001\n\n\n2\n\n\n88\n\n\n88\n\n\n2\n\n\n8\n\n\n\n\n2\n\n\n2022000002\n\n\n1\n\n\n88\n\n\n88\n\n\n2\n\n\n6\n\n\n\n\n3\n\n\n2022000003\n\n\n2\n\n\n2\n\n\n3\n\n\n1\n\n\n5\n\n\n\n\n4\n\n\n2022000004\n\n\n1\n\n\n88\n\n\n88\n\n\n1\n\n\n7\n\n\n\n\n5\n\n\n2022000005\n\n\n4\n\n\n2\n\n\n88\n\n\n1\n\n\n9\n\n\n\n\n6\n\n\n2022000006\n\n\n5\n\n\n1\n\n\n88\n\n\n2\n\n\n7\n\n\n\n\n\n\n\n\n\n\n\u001457                                                         The SAS System                      Sunday, November  3, 2024 06:39:00 AM\n\n436        ods listing close;ods html5 (id=saspy_internal) file=_tomods1 options(bitmap_mode='inline') device=svg style=HTMLBlue;\n436      ! ods graphics on / outputfmt=png;\n437        \n438        * Print header of data set;\n439        TITLE \"General Health\";\n440        PROC PRINT DATA = Impt.gen_health (OBS = 6);\n441          RUN;\n442        \n443        \n444        ods html5 (id=saspy_internal) close;ods listing;\n445        \n\u001458                                                         The SAS System                      Sunday, November  3, 2024 06:39:00 AM\n\n446"
  },
  {
    "objectID": "Data_Management_(SAS)/Project_1/Project_1_Importing.html#marijuana-use",
    "href": "Data_Management_(SAS)/Project_1/Project_1_Importing.html#marijuana-use",
    "title": "Project 1 - Import Data Into SAS",
    "section": "Marijuana Use",
    "text": "Marijuana Use\nRead in the marijuana data set\n\nImport Statement\nThis is a simple .xlsx file.\n\n* Read in marijuana data set;\nPROC IMPORT\n    DATAFILE    =  \"&CourseRoot/BRFSS/Data/1_Source/marijuana.xlsx\"\n    OUT         =   Impt.marijuana\n    DBMS        =   XLSX\n    REPLACE;\nRUN;\n\n\n\n\nPrint Statement\n\n* Print header of data set;\nTITLE \"Marijuana Use\";\nPROC PRINT DATA = Impt.marijuana (OBS = 6);\n  RUN;\n\nlstlog\n\n\n&lt;!DOCTYPE html&gt;\n\n\n\n\n\nSAS Output\n\n\n\n\n\n\n\nMarijuana Use\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObs\n\n\nMARIJAN1\n\n\nMARJSMOK\n\n\nMARJEAT\n\n\nMARJVAPE\n\n\nMARJDAB\n\n\nMARJOTHR\n\n\nid\n\n\n\n\n\n\n1\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n1_2022000001\n\n\n\n\n2\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n1_2022000002\n\n\n\n\n3\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n1_2022000003\n\n\n\n\n4\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n1_2022000004\n\n\n\n\n5\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n1_2022000005\n\n\n\n\n6\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n1_2022000006\n\n\n\n\n\n\n\n\n\n\n\u001461                                                         The SAS System                      Sunday, November  3, 2024 06:39:00 AM\n\n466        ods listing close;ods html5 (id=saspy_internal) file=_tomods1 options(bitmap_mode='inline') device=svg style=HTMLBlue;\n466      ! ods graphics on / outputfmt=png;\n467        \n468        * Print header of data set;\n469        TITLE \"Marijuana Use\";\n470        PROC PRINT DATA = Impt.marijuana (OBS = 6);\n471          RUN;\n472        \n473        \n474        ods html5 (id=saspy_internal) close;ods listing;\n475        \n\u001462                                                         The SAS System                      Sunday, November  3, 2024 06:39:00 AM\n\n476"
  },
  {
    "objectID": "Data_Management_(SAS)/Project_1/Project_1_Importing.html#smoking-status",
    "href": "Data_Management_(SAS)/Project_1/Project_1_Importing.html#smoking-status",
    "title": "Project 1 - Import Data Into SAS",
    "section": "Smoking Status",
    "text": "Smoking Status\nRead in the smoking status data set.\n\nImport Statement\nThis is a simple .xlsx file.\n\n* Read in the smoking status data set;\nPROC IMPORT \n    DATAFILE    = \"&CourseRoot/BRFSS/Data/1_Source/smoke.xlsx\"\n    OUT         = Impt.smoke\n    DBMS        = XLSX\n    REPLACE;\nRUN;\n\n\n\nPrint Statement\n\n* Print header of data set;\nTITLE \"Smoking Status\";\nPROC PRINT DATA = Impt.smoke (OBS = 6);\n  RUN;\n\nlstlog\n\n\n&lt;!DOCTYPE html&gt;\n\n\n\n\n\nSAS Output\n\n\n\n\n\n\n\nSmoking Status\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObs\n\n\nSEQNO\n\n\nSMOKE100\n\n\nSMOKDAY2\n\n\nUSENOW3\n\n\nECIGNOW2\n\n\nLCSFIRST\n\n\nLCSLAST\n\n\nLCSNUMCG\n\n\n\n\n\n\n1\n\n\n2022000001\n\n\n2\n\n\n.\n\n\n3\n\n\n4\n\n\n.\n\n\n.\n\n\n.\n\n\n\n\n2\n\n\n2022000002\n\n\n2\n\n\n.\n\n\n3\n\n\n1\n\n\n.\n\n\n.\n\n\n.\n\n\n\n\n3\n\n\n2022000003\n\n\n2\n\n\n.\n\n\n3\n\n\n1\n\n\n.\n\n\n.\n\n\n.\n\n\n\n\n4\n\n\n2022000004\n\n\n1\n\n\n2\n\n\n3\n\n\n1\n\n\n17\n\n\n999\n\n\n2\n\n\n\n\n5\n\n\n2022000005\n\n\n2\n\n\n.\n\n\n3\n\n\n1\n\n\n.\n\n\n.\n\n\n.\n\n\n\n\n6\n\n\n2022000006\n\n\n2\n\n\n.\n\n\n3\n\n\n1\n\n\n.\n\n\n.\n\n\n.\n\n\n\n\n\n\n\n\n\n\n\u001465                                                         The SAS System                      Sunday, November  3, 2024 06:39:00 AM\n\n495        ods listing close;ods html5 (id=saspy_internal) file=_tomods1 options(bitmap_mode='inline') device=svg style=HTMLBlue;\n495      ! ods graphics on / outputfmt=png;\n496        \n497        * Print header of data set;\n498        TITLE \"Smoking Status\";\n499        PROC PRINT DATA = Impt.smoke (OBS = 6);\n500          RUN;\n501        \n502        \n503        ods html5 (id=saspy_internal) close;ods listing;\n504        \n\u001466                                                         The SAS System                      Sunday, November  3, 2024 06:39:00 AM\n\n505"
  },
  {
    "objectID": "Data_Management_(SAS)/Project_1/Project_1_Importing.html#alcohol-use",
    "href": "Data_Management_(SAS)/Project_1/Project_1_Importing.html#alcohol-use",
    "title": "Project 1 - Import Data Into SAS",
    "section": "Alcohol Use",
    "text": "Alcohol Use\nRead in the alcohol use data set.\n\nImport Statement\nThis is a simple .txt file.\n\n* Read in the alcohol use data set;\nDATA Impt.alcohol;\n    INFILE  \"&CourseRoot/BRFSS/Data/1_Source/alcohol.txt\" MISSOVER DSD FIRSTOBS = 2;\n    INPUT   alcday4         \n            avedrnk3\n            drnk3ge5\n            drnkaby6\n            maxdrnks\n            id          :$13.   ;\n    RUN;\n\n\n\nPrint Statement\n\n* Print header of data set;\nTITLE \"Alcohol Use\";\nPROC PRINT DATA = Impt.alcohol (OBS = 6);\n  RUN;\n\nlstlog\n\n\n&lt;!DOCTYPE html&gt;\n\n\n\n\n\nSAS Output\n\n\n\n\n\n\n\nAlcohol Use\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObs\n\n\nalcday4\n\n\navedrnk3\n\n\ndrnk3ge5\n\n\ndrnkaby6\n\n\nmaxdrnks\n\n\nid\n\n\n\n\n\n\n1\n\n\n888\n\n\n.\n\n\n.\n\n\n.\n\n\n2\n\n\n1_2022000001\n\n\n\n\n2\n\n\n888\n\n\n.\n\n\n.\n\n\n.\n\n\n2\n\n\n1_2022000002\n\n\n\n\n3\n\n\n888\n\n\n.\n\n\n.\n\n\n.\n\n\n2\n\n\n1_2022000003\n\n\n\n\n4\n\n\n888\n\n\n.\n\n\n.\n\n\n.\n\n\n2\n\n\n1_2022000004\n\n\n\n\n5\n\n\n203\n\n\n2\n\n\n88\n\n\n2\n\n\n1\n\n\n1_2022000005\n\n\n\n\n6\n\n\n888\n\n\n.\n\n\n.\n\n\n.\n\n\n2\n\n\n1_2022000006\n\n\n\n\n\n\n\n\n\n\n\u001469                                                         The SAS System                      Sunday, November  3, 2024 06:39:00 AM\n\n527        ods listing close;ods html5 (id=saspy_internal) file=_tomods1 options(bitmap_mode='inline') device=svg style=HTMLBlue;\n527      ! ods graphics on / outputfmt=png;\n528        \n529        * Print header of data set;\n530        TITLE \"Alcohol Use\";\n531        PROC PRINT DATA = Impt.alcohol (OBS = 6);\n532          RUN;\n533        \n534        \n535        ods html5 (id=saspy_internal) close;ods listing;\n536        \n\u001470                                                         The SAS System                      Sunday, November  3, 2024 06:39:00 AM\n\n537"
  },
  {
    "objectID": "Data_Management_(SAS)/Project_1/Project_1_Importing.html#covid-vaccination",
    "href": "Data_Management_(SAS)/Project_1/Project_1_Importing.html#covid-vaccination",
    "title": "Project 1 - Import Data Into SAS",
    "section": "Covid Vaccination",
    "text": "Covid Vaccination\nRead in the covid vaccination data set.\n\nImport Statement\n\n* Read in covid vaccination data;\nDATA Impt.covid_vax;\n    INFILE  \"&CourseRoot/BRFSS/Data/1_Source/covid_vax.csv\" MISSOVER DSD FIRSTOBS = 2;\n    INPUT   covidva1\n            covidnu1\n            covidfs1\n            covidse1\n            seqno       ;\n    RUN;\n\n\n\nPrint Statement\n\n* Print header of data set;\nTITLE \"Covid Vaccination\";\nPROC PRINT DATA = Impt.covid_vax (OBS = 6);\n  RUN;\n\nlstlog\n\n\n&lt;!DOCTYPE html&gt;\n\n\n\n\n\nSAS Output\n\n\n\n\n\n\n\nCovid Vaccination\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObs\n\n\ncovidva1\n\n\ncovidnu1\n\n\ncovidfs1\n\n\ncovidse1\n\n\nseqno\n\n\n\n\n\n\n1\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n2022000001\n\n\n\n\n2\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n2022000002\n\n\n\n\n3\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n2022000003\n\n\n\n\n4\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n2022000004\n\n\n\n\n5\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n2022000005\n\n\n\n\n6\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n2022000006\n\n\n\n\n\n\n\n\n\n\n\u001473                                                         The SAS System                      Sunday, November  3, 2024 06:39:00 AM\n\n558        ods listing close;ods html5 (id=saspy_internal) file=_tomods1 options(bitmap_mode='inline') device=svg style=HTMLBlue;\n558      ! ods graphics on / outputfmt=png;\n559        \n560        * Print header of data set;\n561        TITLE \"Covid Vaccination\";\n562        PROC PRINT DATA = Impt.covid_vax (OBS = 6);\n563          RUN;\n564        \n565        \n566        ods html5 (id=saspy_internal) close;ods listing;\n567        \n\u001474                                                         The SAS System                      Sunday, November  3, 2024 06:39:00 AM\n\n568"
  },
  {
    "objectID": "Data_Management_(SAS)/Project_3/Project3.html",
    "href": "Data_Management_(SAS)/Project_3/Project3.html",
    "title": "Project 3 - Date Set Creation",
    "section": "",
    "text": "This is part three of the main project for BIOS 6680: Data Management Using SAS, in which we plan and carry out data management activities including data cleaning/quality assurance, data documentation, analytic data set creation, and a final analysis. A key goal of this project is to demonstrate reproducible research and reporting.\nThe purpose of this project is to write a SAS program to create a combined analysis data set.\nIn this project we use:\n\nAn array\nDO loops\nIF/THEN statements\nPROC TRANSPOSE\nThe following functions:\n\nSUM()\nTRIM()\nPUT()\nINPUT()\nSUBSTR(), MDY()\nINTCK()\nCOMPRESS()"
  },
  {
    "objectID": "Data_Management_(SAS)/Project_3/Project3.html#email-from-investigator",
    "href": "Data_Management_(SAS)/Project_3/Project3.html#email-from-investigator",
    "title": "Project 3 - Date Set Creation",
    "section": "Email from Investigator",
    "text": "Email from Investigator\n\n\n\n\n\n\nMain Research Question\nThe researcher’s main question is whether there is a significant difference in BMI between California and North Carolina.\n\n\nDesired Data Set\nThey are only interested in including the following states: California, Colorado, Connecticut, Delaware, New Mexico, North Carolina, and West Virginia.\nThe investigator has asked us to create and merge the following data sets:"
  },
  {
    "objectID": "Data_Management_(SAS)/Project_3/Project3.html#consolidate-demogrpahics-data-set",
    "href": "Data_Management_(SAS)/Project_3/Project3.html#consolidate-demogrpahics-data-set",
    "title": "Project 3 - Date Set Creation",
    "section": "Consolidate Demogrpahics Data Set",
    "text": "Consolidate Demogrpahics Data Set\nHere we will collect only the variables we created which the investigator asked for, and the variables needed for the final merge;\n\nCollect Variables\n\nTITLE \"Demographics Cleaned\";\nDATA Impt.Demographics_Final;\n    SET Impt.Demographics;\n    KEEP seqno RaceCd InsuranceCd EducationCd AgeCd IncomeCd Disabilities;\n    RENAME  RaceCd = Race \n            InsuranceCd = Insurance\n            EducationCd = Education\n            AgeCd = Age\n            IncomeCd = Income;\n  RUN;\n\n\n\nView Data\n\n* Examine data set;\nPROC PRINT DATA = Impt.DEMOGRAPHICS_FINAL (OBS = 10);\n    RUN;\n\nlstlog\n\n\n&lt;!DOCTYPE html&gt;\n\n\n\n\n\nSAS Output\n\n\n\n\n\n\n\nDemographics Cleaned\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObs\n\n\nseqno\n\n\nRace\n\n\nInsurance\n\n\nEducation\n\n\nAge\n\n\nIncome\n\n\nDisabilities\n\n\n\n\n\n\n1\n\n\n202-200-0075\n\n\nNon-Hispanic White\n\n\nPrivate/commercial/Medicare/Medigap\n\n\nSome College\n\n\n65+\n\n\n50k-&lt;100k\n\n\n0\n\n\n\n\n2\n\n\n202-200-0076\n\n\nHispanic\n\n\nPrivate/commercial/Medicare/Medigap\n\n\n4+ Year College Degree\n\n\n18-44\n\n\n100k-&lt;200k\n\n\n0\n\n\n\n\n3\n\n\n202-200-0077\n\n\nNon-Hispanic White\n\n\nPrivate/commercial/Medicare/Medigap\n\n\n4+ Year College Degree\n\n\n65+\n\n\n50k-&lt;100k\n\n\n0\n\n\n\n\n4\n\n\n202-200-0078\n\n\nNon-Hispanic Black\n\n\nOther\n\n\n4+ Year College Degree\n\n\n65+\n\n\n35-&lt;50K\n\n\n1\n\n\n\n\n5\n\n\n202-200-0079\n\n\nNon-Hispanic White\n\n\nPrivate/commercial/Medicare/Medigap\n\n\n4+ Year College Degree\n\n\n65+\n\n\n.\n\n\n1\n\n\n\n\n6\n\n\n202-200-0080\n\n\nOther\n\n\nPrivate/commercial/Medicare/Medigap\n\n\n4+ Year College Degree\n\n\n.\n\n\n.\n\n\n0\n\n\n\n\n7\n\n\n202-200-0081\n\n\nNon-Hispanic White\n\n\nPrivate/commercial/Medicare/Medigap\n\n\n4+ Year College Degree\n\n\n65+\n\n\n15-&lt;35k\n\n\n1\n\n\n\n\n8\n\n\n202-200-0082\n\n\nNon-Hispanic White\n\n\nOther\n\n\nGraduated High School\n\n\n45-64\n\n\n35-&lt;50K\n\n\n0\n\n\n\n\n9\n\n\n202-200-0083\n\n\nNon-Hispanic White\n\n\nPrivate/commercial/Medicare/Medigap\n\n\n4+ Year College Degree\n\n\n65+\n\n\n.\n\n\n0\n\n\n\n\n10\n\n\n202-200-0084\n\n\nNon-Hispanic White\n\n\nPrivate/commercial/Medicare/Medigap\n\n\n4+ Year College Degree\n\n\n65+\n\n\n15-&lt;35k\n\n\n0\n\n\n\n\n\n\n\n\n\n\n\u001459                                                         The SAS System                      Sunday, November  3, 2024 06:41:00 AM\n\n569        ods listing close;ods html5 (id=saspy_internal) file=_tomods1 options(bitmap_mode='inline') device=svg style=HTMLBlue;\n569      ! ods graphics on / outputfmt=png;\n570        \n571        * Examine data set;\n572        PROC PRINT DATA = Impt.DEMOGRAPHICS_FINAL (OBS = 10);\n573         RUN;\n574        \n575        \n576        ods html5 (id=saspy_internal) close;ods listing;\n577        \n\u001460                                                         The SAS System                      Sunday, November  3, 2024 06:41:00 AM\n\n578        \n\n\n\n\n\n\nDouble Check for Correct Formats and Labels\n\n* Double check for format;\nODS SELECT VARIABLES;\nPROC CONTENTS DATA = Impt.Demographics_Final;\n    RUN;\n\nlstlog\n\n\n&lt;!DOCTYPE html&gt;\n\n\n\n\n\nSAS Output\n\n\n\n\n\n\n\nDemographics Cleaned\n\n\n\n\nThe CONTENTS Procedure\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAlphabetic List of Variables and Attributes\n\n\n\n\n#\n\n\nVariable\n\n\nType\n\n\nLen\n\n\nFormat\n\n\nInformat\n\n\nLabel\n\n\n\n\n\n\n5\n\n\nAge\n\n\nNum\n\n\n8\n\n\nAGECD.\n\n\n \n\n\nAge\n\n\n\n\n7\n\n\nDisabilities\n\n\nNum\n\n\n8\n\n\nBEST.\n\n\n \n\n\nTotal Number of Disabilities\n\n\n\n\n4\n\n\nEducation\n\n\nNum\n\n\n8\n\n\nEDUCATIONCD.\n\n\n \n\n\nEducation\n\n\n\n\n6\n\n\nIncome\n\n\nNum\n\n\n8\n\n\nINCOMECD.\n\n\n \n\n\nIncome\n\n\n\n\n3\n\n\nInsurance\n\n\nNum\n\n\n8\n\n\nINSURANCECD.\n\n\n \n\n\nInsurance\n\n\n\n\n2\n\n\nRace\n\n\nNum\n\n\n8\n\n\nRACECD.\n\n\n \n\n\nRace\n\n\n\n\n1\n\n\nseqno\n\n\nChar\n\n\n12\n\n\n$12.\n\n\n$12.\n\n\nseqno\n\n\n\n\n\n\n\n\n\n\n\n\u001461                                                         The SAS System                      Sunday, November  3, 2024 06:41:00 AM\n\n581        ods listing close;ods html5 (id=saspy_internal) file=_tomods1 options(bitmap_mode='inline') device=svg style=HTMLBlue;\n581      ! ods graphics on / outputfmt=png;\n582        \n583        * Double check for format;\n584        ODS SELECT VARIABLES;\n585        PROC CONTENTS DATA = Impt.Demographics_Final;\n586         RUN;\n587        \n588        \n589        ods html5 (id=saspy_internal) close;ods listing;\n590        \n\u001462                                                         The SAS System                      Sunday, November  3, 2024 06:41:00 AM\n\n591"
  },
  {
    "objectID": "Data_Management_(SAS)/Project_3/Project3.html#consolidate-outcomes-data-set",
    "href": "Data_Management_(SAS)/Project_3/Project3.html#consolidate-outcomes-data-set",
    "title": "Project 3 - Date Set Creation",
    "section": "Consolidate Outcomes Data Set",
    "text": "Consolidate Outcomes Data Set\nHere we will collect only the variables we created which the investigator asked for, and the variables needed for the final merge.\n\nCollect Variables\n\nTITLE \"Outcomes: Final Data Set\";\nDATA Impt.Outcomes_Final;\n    SET Impt.Outcomes;\n    KEEP SEQNO CHDattackCd AsthmaCd BMI BMIcat CovidCd CancerCd CancerAge;\n    RENAME  CHDattackCd = CHDattack\n            AsthmaCd = Asthma\n            CovidCd = Covid\n            CancerCd = Cancer;\n    RUN;\n\n\n\nView Data\n\n* Print head;\nPROC PRINT DATA = Impt.Outcomes_Final (OBS = 10);\n    RUN;\n    \n* Double check formats;\nODS SELECT VARIABLES;\nPROC CONTENTS DATA = Impt.Outcomes_Final;\n    RUN;\n\nlstlog\n\n\n&lt;!DOCTYPE html&gt;\n\n\n\n\n\nSAS Output\n\n\n\n\n\n\n\nOutcomes: Final Data Set\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObs\n\n\nSEQNO\n\n\nCHDattack\n\n\nAsthma\n\n\nBMI\n\n\nBMIcat\n\n\nCovid\n\n\nCancer\n\n\nCancerAge\n\n\n\n\n\n\n1\n\n\n2022000075\n\n\nNo\n\n\nNever\n\n\n27.2210\n\n\nOverweight\n\n\nNo\n\n\nNo\n\n\n.\n\n\n\n\n2\n\n\n2022000076\n\n\nNo\n\n\nNever\n\n\n26.7554\n\n\nOverweight\n\n\nNo\n\n\nNo\n\n\n.\n\n\n\n\n3\n\n\n2022000077\n\n\nNo\n\n\nFormer\n\n\n21.9214\n\n\nNormal Weight\n\n\nNo\n\n\nNo\n\n\n.\n\n\n\n\n4\n\n\n2022000078\n\n\nNo\n\n\nNever\n\n\n24.5327\n\n\nNormal Weight\n\n\nNo\n\n\nNo\n\n\n.\n\n\n\n\n5\n\n\n2022000079\n\n\nNo\n\n\nNever\n\n\n22.8914\n\n\nNormal Weight\n\n\nNo\n\n\nNo\n\n\n.\n\n\n\n\n6\n\n\n2022000080\n\n\nNo\n\n\nNever\n\n\n.\n\n\n.\n\n\nNo\n\n\nNo\n\n\n.\n\n\n\n\n7\n\n\n2022000081\n\n\nYes\n\n\nNever\n\n\n20.5234\n\n\nNormal Weight\n\n\nNo\n\n\nNo\n\n\n.\n\n\n\n\n8\n\n\n2022000082\n\n\nYes\n\n\nNever\n\n\n25.1377\n\n\nOverweight\n\n\nYes\n\n\nNo\n\n\n.\n\n\n\n\n9\n\n\n2022000083\n\n\nNo\n\n\nNever\n\n\n25.5953\n\n\nOverweight\n\n\nYes\n\n\nNo\n\n\n.\n\n\n\n\n10\n\n\n2022000084\n\n\nNo\n\n\nCurrent\n\n\n.\n\n\n.\n\n\nNo\n\n\nNo\n\n\n.\n\n\n\n\n\n\n\n\n\n\nOutcomes: Final Data Set\n\n\n\n\nThe CONTENTS Procedure\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAlphabetic List of Variables and Attributes\n\n\n\n\n#\n\n\nVariable\n\n\nType\n\n\nLen\n\n\nFormat\n\n\nInformat\n\n\nLabel\n\n\n\n\n\n\n3\n\n\nAsthma\n\n\nNum\n\n\n8\n\n\nASTHMACD.\n\n\n \n\n\nAsthma Status\n\n\n\n\n4\n\n\nBMI\n\n\nNum\n\n\n8\n\n\n \n\n\n \n\n\nPatient BMI\n\n\n\n\n5\n\n\nBMIcat\n\n\nNum\n\n\n8\n\n\nBMICAT.\n\n\n \n\n\nBMI Category\n\n\n\n\n2\n\n\nCHDattack\n\n\nNum\n\n\n8\n\n\nCHDATTACKCD.\n\n\n \n\n\nCHD or Heart Attack\n\n\n\n\n7\n\n\nCancer\n\n\nNum\n\n\n8\n\n\nCANCERCD.\n\n\n \n\n\nEver had Cancer\n\n\n\n\n8\n\n\nCancerAge\n\n\nNum\n\n\n8\n\n\nCANCERAGE.\n\n\n \n\n\nAge at first cancer diagnosis\n\n\n\n\n6\n\n\nCovid\n\n\nNum\n\n\n8\n\n\nCOVIDCD.\n\n\n \n\n\nEver had Covid\n\n\n\n\n1\n\n\nSEQNO\n\n\nNum\n\n\n8\n\n\nBEST12.\n\n\nBEST32.\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\u0014107                                                        The SAS System                      Sunday, November  3, 2024 06:41:00 AM\n\n976        ods listing close;ods html5 (id=saspy_internal) file=_tomods1 options(bitmap_mode='inline') device=svg style=HTMLBlue;\n976      ! ods graphics on / outputfmt=png;\n977        \n978        * Print head;\n979        PROC PRINT DATA = Impt.Outcomes_Final (OBS = 10);\n980         RUN;\n981         \n982        * Double check formats;\n983        ODS SELECT VARIABLES;\n984        PROC CONTENTS DATA = Impt.Outcomes_Final;\n985         RUN;\n986        \n987        \n988        ods html5 (id=saspy_internal) close;ods listing;\n989        \n\u0014108                                                        The SAS System                      Sunday, November  3, 2024 06:41:00 AM\n\n990"
  },
  {
    "objectID": "Data_Management_(SAS)/Project_3/Project3.html#consolidate-risk-factors-data-set",
    "href": "Data_Management_(SAS)/Project_3/Project3.html#consolidate-risk-factors-data-set",
    "title": "Project 3 - Date Set Creation",
    "section": "Consolidate Risk Factors Data Set",
    "text": "Consolidate Risk Factors Data Set\nHere we will collect only the variables we created which the investigator asked for, and the variables needed for the final merge.\n\nCollect Variables\n\nTITLE \"Risk Factors: Final Data Set\";\nDATA Impt.Risk_Final;\n    SET Impt.Risk;\n    KEEP id PhysHealth MentHealth SmokeStat SmokeNum SmokeTime Alcohol CovidVaxMonths MarijuanaPerc Marijuana;\n    RUN;\n\n\n\nView the Data Set\n\n* Print head;\nPROC PRINT DATA = Impt.Risk_Final (OBS = 10);\n    RUN;\n    \n* Double check formats;\nTITLE \"Risk Factors: Check Formats\";\nODS SELECT VARIABLES;\nPROC CONTENTS DATA = Impt.Risk_Final;\n    RUN;\n\nlstlog\n\n\n&lt;!DOCTYPE html&gt;\n\n\n\n\n\nSAS Output\n\n\n\n\n\n\n\nRisk Factors: Final Data Set\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObs\n\n\nid\n\n\nPhysHealth\n\n\nMentHealth\n\n\nSmokeStat\n\n\nSmokeTime\n\n\nAlcohol\n\n\nCovidVaxMonths\n\n\nMarijuanaPerc\n\n\nMarijuana\n\n\nSmokeNum\n\n\n\n\n\n\n1\n\n\n1_2022000001\n\n\n0.00000\n\n\n0\n\n\nNever Smoker\n\n\n0\n\n\nNo\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n\n\n2\n\n\n1_2022000002\n\n\n0.00000\n\n\n0\n\n\nNever Smoker\n\n\n0\n\n\nNo\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n\n\n3\n\n\n1_2022000003\n\n\n6.66667\n\n\n10\n\n\nNever Smoker\n\n\n0\n\n\nNo\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n\n\n4\n\n\n1_2022000004\n\n\n0.00000\n\n\n0\n\n\nCurrent Smoker\n\n\n.\n\n\nNo\n\n\n.\n\n\n.\n\n\n.\n\n\n0.10\n\n\n\n\n5\n\n\n1_2022000005\n\n\n6.66667\n\n\n0\n\n\nNever Smoker\n\n\n0\n\n\nYes\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n\n\n6\n\n\n1_2022000006\n\n\n3.33333\n\n\n0\n\n\nNever Smoker\n\n\n0\n\n\nNo\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n\n\n7\n\n\n1_2022000007\n\n\n0.00000\n\n\n0\n\n\nFormer Smoker\n\n\n42\n\n\nYes\n\n\n.\n\n\n.\n\n\n.\n\n\n1.75\n\n\n\n\n8\n\n\n1_2022000008\n\n\n0.00000\n\n\n0\n\n\nNever Smoker\n\n\n0\n\n\nNo\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n\n\n9\n\n\n1_2022000009\n\n\n0.00000\n\n\n0\n\n\nFormer Smoker\n\n\n1\n\n\nNo\n\n\n.\n\n\n.\n\n\n.\n\n\n0.25\n\n\n\n\n10\n\n\n1_2022000010\n\n\n3.33333\n\n\n0\n\n\nNever Smoker\n\n\n0\n\n\nYes\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n\n\n\n\n\n\n\n\nRisk Factors: Check Formats\n\n\n\n\nThe CONTENTS Procedure\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAlphabetic List of Variables and Attributes\n\n\n\n\n#\n\n\nVariable\n\n\nType\n\n\nLen\n\n\nFormat\n\n\nInformat\n\n\nLabel\n\n\n\n\n\n\n6\n\n\nAlcohol\n\n\nNum\n\n\n8\n\n\nALCOHOL.\n\n\n \n\n\nConsumed alcohol in past 30 days\n\n\n\n\n7\n\n\nCovidVaxMonths\n\n\nNum\n\n\n8\n\n\n \n\n\n \n\n\nNumber of months between first and second covid vaccination\n\n\n\n\n9\n\n\nMarijuana\n\n\nNum\n\n\n8\n\n\nMARIJUANA.\n\n\n \n\n\nMarijuana use in past 30 days\n\n\n\n\n8\n\n\nMarijuanaPerc\n\n\nNum\n\n\n8\n\n\n \n\n\n \n\n\nPercent of 30 days with marijuana use\n\n\n\n\n3\n\n\nMentHealth\n\n\nNum\n\n\n8\n\n\n \n\n\n \n\n\nPercent of 30 days with mental health not good\n\n\n\n\n2\n\n\nPhysHealth\n\n\nNum\n\n\n8\n\n\n \n\n\n \n\n\nPercent of 30 days with physical health not good\n\n\n\n\n10\n\n\nSmokeNum\n\n\nNum\n\n\n8\n\n\n \n\n\n \n\n\nNumer of Cigarette Packs Per Day\n\n\n\n\n4\n\n\nSmokeStat\n\n\nNum\n\n\n8\n\n\nSMOKESTAT.\n\n\n \n\n\nSmoking Status\n\n\n\n\n5\n\n\nSmokeTime\n\n\nNum\n\n\n8\n\n\n \n\n\n \n\n\nHow many years smoked\n\n\n\n\n1\n\n\nid\n\n\nChar\n\n\n13\n\n\n$13.\n\n\n$13.\n\n\nid\n\n\n\n\n\n\n\n\n\n\n\n\u0014153                                                        The SAS System                      Sunday, November  3, 2024 06:41:00 AM\n\n1377       ods listing close;ods html5 (id=saspy_internal) file=_tomods1 options(bitmap_mode='inline') device=svg style=HTMLBlue;\n1377     ! ods graphics on / outputfmt=png;\n1378       \n1379       * Print head;\n1380       PROC PRINT DATA = Impt.Risk_Final (OBS = 10);\n1381        RUN;\n1382        \n1383       * Double check formats;\n1384       TITLE \"Risk Factors: Check Formats\";\n1385       ODS SELECT VARIABLES;\n1386       PROC CONTENTS DATA = Impt.Risk_Final;\n1387        RUN;\n1388       \n1389       \n1390       ods html5 (id=saspy_internal) close;ods listing;\n1391       \n\u0014154                                                        The SAS System                      Sunday, November  3, 2024 06:41:00 AM\n\n1392"
  },
  {
    "objectID": "Data_Management_(SAS)/Project_3/Project3.html#create-date-variable-in-reference-table",
    "href": "Data_Management_(SAS)/Project_3/Project3.html#create-date-variable-in-reference-table",
    "title": "Project 3 - Date Set Creation",
    "section": "Create Date Variable in Reference Table",
    "text": "Create Date Variable in Reference Table\n\nCreate Variable\n\n* Create and Format date variable;\nDATA Impt.Ref_Final;\n    SET Impt.ref_table_long;\n    IntvDate = MDY(IMONTH, IDAY, IYEAR);\n    ATTRIB IntvDate LABEL = \"Interview Date\" FORMAT = MMDDYY10.;\n    KEEP id SEQNO IntvDate;\n    RUN;\n\n\n\nDouble Check for Correct Conversion\n\n* Double check by printing;\nPROC PRINT DATA = Impt.Ref_Final (OBS = 10);\n    RUN;\n\nlstlog\n\n\n&lt;!DOCTYPE html&gt;\n\n\n\n\n\nSAS Output\n\n\n\n\n\n\n\nRisk Factors: Check Formats\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObs\n\n\nid\n\n\nSEQNO\n\n\nIntvDate\n\n\n\n\n\n\n1\n\n\n6_2022000075\n\n\n2022000075\n\n\n07/05/2022\n\n\n\n\n2\n\n\n6_2022000076\n\n\n2022000076\n\n\n06/28/2022\n\n\n\n\n3\n\n\n6_2022000077\n\n\n2022000077\n\n\n06/27/2022\n\n\n\n\n4\n\n\n6_2022000078\n\n\n2022000078\n\n\n06/29/2022\n\n\n\n\n5\n\n\n6_2022000079\n\n\n2022000079\n\n\n07/05/2022\n\n\n\n\n6\n\n\n6_2022000080\n\n\n2022000080\n\n\n07/05/2022\n\n\n\n\n7\n\n\n6_2022000081\n\n\n2022000081\n\n\n07/09/2022\n\n\n\n\n8\n\n\n6_2022000082\n\n\n2022000082\n\n\n06/28/2022\n\n\n\n\n9\n\n\n6_2022000083\n\n\n2022000083\n\n\n07/04/2022\n\n\n\n\n10\n\n\n6_2022000084\n\n\n2022000084\n\n\n07/09/2022\n\n\n\n\n\n\n\n\n\n\n\u0014157                                                        The SAS System                      Sunday, November  3, 2024 06:41:00 AM\n\n1411       ods listing close;ods html5 (id=saspy_internal) file=_tomods1 options(bitmap_mode='inline') device=svg style=HTMLBlue;\n1411     ! ods graphics on / outputfmt=png;\n1412       \n1413       * Double check by printing;\n1414       PROC PRINT DATA = Impt.Ref_Final (OBS = 10);\n1415        RUN;\n1416       \n1417       \n1418       ods html5 (id=saspy_internal) close;ods listing;\n1419       \n\u0014158                                                        The SAS System                      Sunday, November  3, 2024 06:41:00 AM\n\n1420"
  },
  {
    "objectID": "Data_Management_(SAS)/Project_3/Project3.html#merge-reference-table-demographics-and-outcomes-on-seqno",
    "href": "Data_Management_(SAS)/Project_3/Project3.html#merge-reference-table-demographics-and-outcomes-on-seqno",
    "title": "Project 3 - Date Set Creation",
    "section": "Merge Reference Table, Demographics, and Outcomes on SEQNO",
    "text": "Merge Reference Table, Demographics, and Outcomes on SEQNO\n\nEnsure Same Format and Type for SEQNO\n\n* Convert SEQNO in the reference table to numeric and SORT;\nTITLE \"Final Data Set\";\nDATA Impt.Ref_Final;\n    SET Impt.Ref_Final;\n    SEQNO_num = INPUT(SEQNO, BEST12.);\n    FORMAT SEQNO_num BEST12.;\n    DROP SEQNO;\n    RENAME SEQNO_NUM = SEQNO;\n    RUN;\n\n\n* Correct the SEQNO format in the demographics data set;\nDATA Impt.Demographics_Final;\n    SET Impt.Demographics_Final;\n    SEQNO = COMPRESS(SEQNO, \"-\");\n    SEQNO_num = INPUT(SEQNO, BEST12.);\n    FORMAT SEQNO_num BEST12.;\n    DROP SEQNO;\n    RENAME SEQNO_num = SEQNO;\n    RUN;\n\n\n\nSort Data Sets\n\n* Sort Demographics_Final by SEQNO;\nPROC SORT DATA = Impt.Demographics_Final;\n    BY SEQNO;\n    RUN;\n\n* Sort Outcomes_Final by SEQNO;\nPROC SORT DATA = Impt.Outcomes_Final;\n    BY SEQNO;\n    RUN;    \n\n* Sort Ref_Final by SEQNO;\nPROC SORT DATA = Impt.Ref_Final;\n    BY SEQNO;\n    RUN;\n\n\n\nPerform Merge on SEQNO\n\n* First merge by SEQNO;\nDATA Impt.BRFSS_Final;\n    MERGE Impt.Demographics_Final (IN = a)\n          Impt.Ref_Final (IN = b)\n          Impt.Outcomes_Final (IN = c);\n    BY SEQNO;\n    IF a AND b AND c;\n    RUN;"
  },
  {
    "objectID": "Data_Management_(SAS)/Project_3/Project3.html#merge-risk-factors-on-id",
    "href": "Data_Management_(SAS)/Project_3/Project3.html#merge-risk-factors-on-id",
    "title": "Project 3 - Date Set Creation",
    "section": "Merge Risk Factors on ID",
    "text": "Merge Risk Factors on ID\n\nSort by ID\n\n* Sort by ID;\nPROC SORT DATA = Impt.BRFSS_Final;\n    BY id;\n    RUN;\n    \n* Sort by ID;\nPROC SORT DATA = impt.Risk_Final;\n    BY id;\n    RUN;\n\n\n\nPerform Merge on ID\n\n* Then merge by Id;\nDATA Impt.BRFSS_Final;\n    MERGE Impt.BRFSS_Final (IN = a)\n          Impt.Risk_Final (IN = b); \n    BY id;\n    IF a AND b;\n    RUN;"
  },
  {
    "objectID": "Data_Management_(SAS)/Project_3/Project3.html#clean-final-data-set",
    "href": "Data_Management_(SAS)/Project_3/Project3.html#clean-final-data-set",
    "title": "Project 3 - Date Set Creation",
    "section": "Clean Final Data Set",
    "text": "Clean Final Data Set\n\n* Rearrange and drop extraneous variables for a cleaner data set;\nDATA Impt.BRFSS_Final;\n    RETAIN id SEQNO IntvDate;\n    SET Impt.BRFSS_Final;\n    DROP SEQNO_OUTCOMES SEQNO_REF SEQNO_DEMO id_RISK id SEQNO;\n    ATTRIB SEQNO LABEL = \"Sequence Number\";\n    ATTRIB id LABEL = \"ID\";\n    RUN;"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Sean Vieau",
    "section": "",
    "text": "Graduate student in the Masters in Applied Biostatistics program at the Colorado School of Public Health. I have compiled this portfolio to showcase my most significant projects and skills.\nIn my spare time I am an avid dancer, jiu jitsu practitioner, and drummer!"
  },
  {
    "objectID": "Project_1_Regression/Project_1_Regression_SAS/Project_1_Regression_SAS.html",
    "href": "Project_1_Regression/Project_1_Regression_SAS/Project_1_Regression_SAS.html",
    "title": "Project 1 - Regression (SAS)",
    "section": "",
    "text": "The aim of the current study is to assess whether a new gel treatment for gum disease results in lower whole-mouth average pocket depth and attachment loss after one year. Average pocket depth and attachment loss were taken at baseline, and participants were assigned into one of five groups (1 = placebo, 2 = no treatment, 3 = low concentration, 4 = medium concentration, 5 = high concentration) and instructed to apply the gel to their gums twice a day. After 1-year, average pocket depth and attachment loss were recorded again. Data was received as a .csv file containing treatment level, average pocket depth and attachment loss at baseline and at one-year, demographic information, gender, age, number of sites measured, and smoking status. The clinical hypothesis is that average pocket depth and attachment loss in participants who applied the gel will be lower compared to participants who did not. Gender, age, ethnicity, and smoking status will be investigated as potential covariates.\nThe project description provided by the PI is available below:\n\n\nThis document was created in RStudio with Quarto.\nTo code in SAS we must first connect RStudio to the SAS server.\n\n# This code connects to SAS On Demand\nlibrary(configSAS)\nconfigSAS::set_sas_engine()\n\nUsing SAS Config named: oda\nSAS Connection established. Subprocess id is 26008\n\n#  This code allows you to run ```{sas}``` chunks\nsas = knitr::opts_chunk$get(\"sas\")\n\n\n\n\n\n\nWe begin by making our library for the project.\n\n* Create library;\n%LET CourseRoot = /home/u63376223/sasuser.v94/Advanced Data Analysis;\nLIBNAME Proj1 \"&CourseRoot/Project 1\";\n\n\n\n\nThen we will import the dataset\n\n* Import the dataset;\nPROC IMPORT\n    DATAFILE = \"&CourseRoot/Project 1/Project1_data.csv\"\n    OUT = Proj1.raw_data\n    REPLACE;\n    RUN;\n\n\n\n\nWe have missing values in this data set as ‘NA’. SAS will not be able to handle those. We need to convert them to missing values in SAS format (“” for characters and . for numeric);\n\n* Fix missing value format for attachment loss;\nDATA Proj1.data;\n    SET Proj1.raw_data;\n    if attach1year = \"NA\" then attach1year = \"\";\n    attach1year = INPUT(attach1year, best32.);\n    RUN;\n\n* Fix missing value format for pocket depth change;\nDATA Proj1.data;\n    SET Proj1.data;\n    if pd1year = \"NA\" then pd1year = \"\";\n    pd1year = INPUT(pd1year, best32.);\n    RUN;\n\n\n/* Convert 'NA' to missing values and ensure numeric format for multiple variables */\n/* Code from ChatGPT */\nDATA Proj1.data;\n    SET Proj1.data;\n    \n    /* Define arrays for character and numeric variables */\n    array char_vars[2] attach1year pd1year ; /* Add more variables as needed */\n    array num_vars[2] attach1year_num pd1year_num; /* Corresponding numeric variables */\n    \n    /* Loop through the arrays to handle 'NA' and convert to numeric */\n    do i = 1 to dim(char_vars);\n        if char_vars[i] = 'NA' then char_vars[i] = '';\n        num_vars[i] = input(char_vars[i], best32.);\n    end;\n    \n    /* Drop the original character variables and rename the numeric ones */\n    drop attach1year pd1year i; /* Add more variables as needed */\n    rename attach1year_num = attach1year pd1year_num = pd1year; /* Corresponding renaming */\nRUN;\n\n\n\n\nWe will then create labels for the categorical variables.\n\n* Create format;\nPROC FORMAT;\n    VALUE trtgroup_fmt\n        1 = \"Placebo\"\n        2 = \"Control\"\n        3 = \"Low\"\n        4 = \"Medium\"\n        5 = \"High\";\n    VALUE gender_fmt\n        1 = \"Male\"\n        2 = \"Female\";\n    VALUE race_fmt\n        1 = \"Native American\"\n        2 = \"African American\"\n        4 = \"White\"\n        5 = \"Asian\";\n    VALUE smoker_fmt\n        0 = \"Non-Smoker\"\n        1 = \"Smoker\";\n    RUN;\n\n* Apply format;\nDATA Proj1.data;\n    SET Proj1.data;\n    FORMAT  trtgroup trtgroup_fmt.\n            gender gender_fmt.\n            race race_fmt.\n            smoker smoker_fmt.;\n    RUN;\n\nLet’s also change the labels of all variable names so they are capitalized and make our output look more professional.\n\n* Change labels to capitalize variable names;\nDATA Proj1.data;\n    SET Proj1.data;\n    LABEL \n        id = \"ID\"\n        trtgroup = \"Treatment Group\"\n        gender = \"Gender\"\n        race= \"Race\"\n        age = \"Age\"\n        smoker = \"Smoking Status\"\n        sites = \"Sites\"\n        attachbase = \"Attachment Loss Baseline\"\n        attach1year = \"Attachment Loss One Year\"\n        pdbase = \"Pocket Depth Baseline\"\n        pd1year = \"Pocket Depth One Year\"\n        ;\n    RUN;\n\n\n\n\nFinally let’s visualize the data set.\n\n* View Dataset;\nPROC PRINT DATA = Proj1.data (OBS = 10);\n    RUN;\n\nlstlog\n\n\n&lt;!DOCTYPE html&gt;\n\n\n\n\n\nSAS Output\n\n\n\n\n\n\n\nThe SAS System\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObs\n\n\nid\n\n\ntrtgroup\n\n\ngender\n\n\nrace\n\n\nage\n\n\nsmoker\n\n\nsites\n\n\nattachbase\n\n\npdbase\n\n\nattach1year\n\n\npd1year\n\n\n\n\n\n\n1\n\n\n101\n\n\nMedium\n\n\nFemale\n\n\nAsian\n\n\n44.57221082\n\n\nSmoker\n\n\n162\n\n\n2.432098765\n\n\n3.24691358\n\n\n2.57764\n\n\n3.40741\n\n\n\n\n2\n\n\n102\n\n\nHigh\n\n\nFemale\n\n\nAsian\n\n\n35.57289528\n\n\nSmoker\n\n\n162\n\n\n2.543209877\n\n\n3.00617284\n\n\n.\n\n\n.\n\n\n\n\n3\n\n\n103\n\n\nControl\n\n\nFemale\n\n\nAsian\n\n\n47.94524298\n\n\nSmoker\n\n\n144\n\n\n2.881944444\n\n\n3.118055556\n\n\n3.07639\n\n\n3.12500\n\n\n\n\n4\n\n\n104\n\n\nLow\n\n\nFemale\n\n\nAsian\n\n\n55.17864476\n\n\nSmoker\n\n\n138\n\n\n4.956521739\n\n\n5.217391304\n\n\n5.30435\n\n\n4.89130\n\n\n\n\n5\n\n\n105\n\n\nPlacebo\n\n\nFemale\n\n\nAfrican American\n\n\n43.79739904\n\n\nSmoker\n\n\n168\n\n\n1.773809524\n\n\n3.363095238\n\n\n1.45238\n\n\n2.89881\n\n\n\n\n6\n\n\n106\n\n\nMedium\n\n\nFemale\n\n\nAsian\n\n\n42.14921287\n\n\nSmoker\n\n\n168\n\n\n2.369047619\n\n\n3.910714286\n\n\n1.92262\n\n\n3.08333\n\n\n\n\n7\n\n\n107\n\n\nLow\n\n\nFemale\n\n\nAsian\n\n\n37.15811088\n\n\nSmoker\n\n\n156\n\n\n3.544871795\n\n\n3.897435897\n\n\n2.83974\n\n\n3.23718\n\n\n\n\n8\n\n\n108\n\n\nControl\n\n\nFemale\n\n\nAsian\n\n\n67.41957563\n\n\nSmoker\n\n\n162\n\n\n2.493827161\n\n\n3.481481482\n\n\n2.19136\n\n\n3.08025\n\n\n\n\n9\n\n\n109\n\n\nHigh\n\n\nFemale\n\n\nAfrican American\n\n\n40.59137577\n\n\nSmoker\n\n\n156\n\n\n1.92948718\n\n\n3.301282051\n\n\n1.62821\n\n\n2.71154\n\n\n\n\n10\n\n\n110\n\n\nPlacebo\n\n\nFemale\n\n\nAsian\n\n\n49.2128679\n\n\nSmoker\n\n\n162\n\n\n1.790123457\n\n\n3.277777778\n\n\n1.46296\n\n\n2.41975\n\n\n\n\n\n\n\n\n\n\n\u001417                                                         The SAS System                      Sunday, November  3, 2024 06:45:00 AM\n\n213        ods listing close;ods html5 (id=saspy_internal) file=_tomods1 options(bitmap_mode='inline') device=svg style=HTMLBlue;\n213      ! ods graphics on / outputfmt=png;\n214        \n215        * View Dataset;\n216        PROC PRINT DATA = Proj1.data (OBS = 10);\n217         RUN;\n218        \n219        \n220        ods html5 (id=saspy_internal) close;ods listing;\n221        \n\u001418                                                         The SAS System                      Sunday, November  3, 2024 06:45:00 AM\n\n222        \n\n\n\n\nOur data set is comprised of 130 observations, with 11 variables.\n\n\n\nThe variables in the provided data set are as follows:\n\nid: Patient ID\ntrtgroup: Treatment group (1 = placebo, 2 = no treatment control, 3 = low concentration, 4 = medium concentration, 5 = high concentration gel)\ngender: Gender (1 = male, 2 = female)\nrace: Race, (1 = Native American, 2 = African American, 3 = Not used, 4 = Asian, 5 = White)\nage: Age in years\nsmoker: Smoking status (0 = No, 1 = Yes)\nsites: Number of sites gum measurements were averaged from\nattachbase: Whole-mouth average attachment loss taken at base timepoint\nattach1year: Whole-mouth average attachment loss after 1 year\npdbase: Whole-mouth average pocket loss at base timepoint\npd1year: Whole-mouth average pocket loss after 1 year\n\n\n\n\n\n\n\nNote\n\n\n\nPocket depth and attachment loss are both measurements of how far the gums have pulled away from the teeth, hence smaller values are better"
  },
  {
    "objectID": "Project_1_Regression/Project_1_Regression_SAS/Project_1_Regression_SAS.html#load-sas",
    "href": "Project_1_Regression/Project_1_Regression_SAS/Project_1_Regression_SAS.html#load-sas",
    "title": "Project 1 - Regression (SAS)",
    "section": "",
    "text": "This document was created in RStudio with Quarto.\nTo code in SAS we must first connect RStudio to the SAS server.\n\n# This code connects to SAS On Demand\nlibrary(configSAS)\nconfigSAS::set_sas_engine()\n\nUsing SAS Config named: oda\nSAS Connection established. Subprocess id is 26008\n\n#  This code allows you to run ```{sas}``` chunks\nsas = knitr::opts_chunk$get(\"sas\")"
  },
  {
    "objectID": "Project_1_Regression/Project_1_Regression_SAS/Project_1_Regression_SAS.html#data-preparation",
    "href": "Project_1_Regression/Project_1_Regression_SAS/Project_1_Regression_SAS.html#data-preparation",
    "title": "Project 1 - Regression (SAS)",
    "section": "",
    "text": "We begin by making our library for the project.\n\n* Create library;\n%LET CourseRoot = /home/u63376223/sasuser.v94/Advanced Data Analysis;\nLIBNAME Proj1 \"&CourseRoot/Project 1\";\n\n\n\n\nThen we will import the dataset\n\n* Import the dataset;\nPROC IMPORT\n    DATAFILE = \"&CourseRoot/Project 1/Project1_data.csv\"\n    OUT = Proj1.raw_data\n    REPLACE;\n    RUN;\n\n\n\n\nWe have missing values in this data set as ‘NA’. SAS will not be able to handle those. We need to convert them to missing values in SAS format (“” for characters and . for numeric);\n\n* Fix missing value format for attachment loss;\nDATA Proj1.data;\n    SET Proj1.raw_data;\n    if attach1year = \"NA\" then attach1year = \"\";\n    attach1year = INPUT(attach1year, best32.);\n    RUN;\n\n* Fix missing value format for pocket depth change;\nDATA Proj1.data;\n    SET Proj1.data;\n    if pd1year = \"NA\" then pd1year = \"\";\n    pd1year = INPUT(pd1year, best32.);\n    RUN;\n\n\n/* Convert 'NA' to missing values and ensure numeric format for multiple variables */\n/* Code from ChatGPT */\nDATA Proj1.data;\n    SET Proj1.data;\n    \n    /* Define arrays for character and numeric variables */\n    array char_vars[2] attach1year pd1year ; /* Add more variables as needed */\n    array num_vars[2] attach1year_num pd1year_num; /* Corresponding numeric variables */\n    \n    /* Loop through the arrays to handle 'NA' and convert to numeric */\n    do i = 1 to dim(char_vars);\n        if char_vars[i] = 'NA' then char_vars[i] = '';\n        num_vars[i] = input(char_vars[i], best32.);\n    end;\n    \n    /* Drop the original character variables and rename the numeric ones */\n    drop attach1year pd1year i; /* Add more variables as needed */\n    rename attach1year_num = attach1year pd1year_num = pd1year; /* Corresponding renaming */\nRUN;\n\n\n\n\nWe will then create labels for the categorical variables.\n\n* Create format;\nPROC FORMAT;\n    VALUE trtgroup_fmt\n        1 = \"Placebo\"\n        2 = \"Control\"\n        3 = \"Low\"\n        4 = \"Medium\"\n        5 = \"High\";\n    VALUE gender_fmt\n        1 = \"Male\"\n        2 = \"Female\";\n    VALUE race_fmt\n        1 = \"Native American\"\n        2 = \"African American\"\n        4 = \"White\"\n        5 = \"Asian\";\n    VALUE smoker_fmt\n        0 = \"Non-Smoker\"\n        1 = \"Smoker\";\n    RUN;\n\n* Apply format;\nDATA Proj1.data;\n    SET Proj1.data;\n    FORMAT  trtgroup trtgroup_fmt.\n            gender gender_fmt.\n            race race_fmt.\n            smoker smoker_fmt.;\n    RUN;\n\nLet’s also change the labels of all variable names so they are capitalized and make our output look more professional.\n\n* Change labels to capitalize variable names;\nDATA Proj1.data;\n    SET Proj1.data;\n    LABEL \n        id = \"ID\"\n        trtgroup = \"Treatment Group\"\n        gender = \"Gender\"\n        race= \"Race\"\n        age = \"Age\"\n        smoker = \"Smoking Status\"\n        sites = \"Sites\"\n        attachbase = \"Attachment Loss Baseline\"\n        attach1year = \"Attachment Loss One Year\"\n        pdbase = \"Pocket Depth Baseline\"\n        pd1year = \"Pocket Depth One Year\"\n        ;\n    RUN;\n\n\n\n\nFinally let’s visualize the data set.\n\n* View Dataset;\nPROC PRINT DATA = Proj1.data (OBS = 10);\n    RUN;\n\nlstlog\n\n\n&lt;!DOCTYPE html&gt;\n\n\n\n\n\nSAS Output\n\n\n\n\n\n\n\nThe SAS System\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObs\n\n\nid\n\n\ntrtgroup\n\n\ngender\n\n\nrace\n\n\nage\n\n\nsmoker\n\n\nsites\n\n\nattachbase\n\n\npdbase\n\n\nattach1year\n\n\npd1year\n\n\n\n\n\n\n1\n\n\n101\n\n\nMedium\n\n\nFemale\n\n\nAsian\n\n\n44.57221082\n\n\nSmoker\n\n\n162\n\n\n2.432098765\n\n\n3.24691358\n\n\n2.57764\n\n\n3.40741\n\n\n\n\n2\n\n\n102\n\n\nHigh\n\n\nFemale\n\n\nAsian\n\n\n35.57289528\n\n\nSmoker\n\n\n162\n\n\n2.543209877\n\n\n3.00617284\n\n\n.\n\n\n.\n\n\n\n\n3\n\n\n103\n\n\nControl\n\n\nFemale\n\n\nAsian\n\n\n47.94524298\n\n\nSmoker\n\n\n144\n\n\n2.881944444\n\n\n3.118055556\n\n\n3.07639\n\n\n3.12500\n\n\n\n\n4\n\n\n104\n\n\nLow\n\n\nFemale\n\n\nAsian\n\n\n55.17864476\n\n\nSmoker\n\n\n138\n\n\n4.956521739\n\n\n5.217391304\n\n\n5.30435\n\n\n4.89130\n\n\n\n\n5\n\n\n105\n\n\nPlacebo\n\n\nFemale\n\n\nAfrican American\n\n\n43.79739904\n\n\nSmoker\n\n\n168\n\n\n1.773809524\n\n\n3.363095238\n\n\n1.45238\n\n\n2.89881\n\n\n\n\n6\n\n\n106\n\n\nMedium\n\n\nFemale\n\n\nAsian\n\n\n42.14921287\n\n\nSmoker\n\n\n168\n\n\n2.369047619\n\n\n3.910714286\n\n\n1.92262\n\n\n3.08333\n\n\n\n\n7\n\n\n107\n\n\nLow\n\n\nFemale\n\n\nAsian\n\n\n37.15811088\n\n\nSmoker\n\n\n156\n\n\n3.544871795\n\n\n3.897435897\n\n\n2.83974\n\n\n3.23718\n\n\n\n\n8\n\n\n108\n\n\nControl\n\n\nFemale\n\n\nAsian\n\n\n67.41957563\n\n\nSmoker\n\n\n162\n\n\n2.493827161\n\n\n3.481481482\n\n\n2.19136\n\n\n3.08025\n\n\n\n\n9\n\n\n109\n\n\nHigh\n\n\nFemale\n\n\nAfrican American\n\n\n40.59137577\n\n\nSmoker\n\n\n156\n\n\n1.92948718\n\n\n3.301282051\n\n\n1.62821\n\n\n2.71154\n\n\n\n\n10\n\n\n110\n\n\nPlacebo\n\n\nFemale\n\n\nAsian\n\n\n49.2128679\n\n\nSmoker\n\n\n162\n\n\n1.790123457\n\n\n3.277777778\n\n\n1.46296\n\n\n2.41975\n\n\n\n\n\n\n\n\n\n\n\u001417                                                         The SAS System                      Sunday, November  3, 2024 06:45:00 AM\n\n213        ods listing close;ods html5 (id=saspy_internal) file=_tomods1 options(bitmap_mode='inline') device=svg style=HTMLBlue;\n213      ! ods graphics on / outputfmt=png;\n214        \n215        * View Dataset;\n216        PROC PRINT DATA = Proj1.data (OBS = 10);\n217         RUN;\n218        \n219        \n220        ods html5 (id=saspy_internal) close;ods listing;\n221        \n\u001418                                                         The SAS System                      Sunday, November  3, 2024 06:45:00 AM\n\n222        \n\n\n\n\nOur data set is comprised of 130 observations, with 11 variables.\n\n\n\nThe variables in the provided data set are as follows:\n\nid: Patient ID\ntrtgroup: Treatment group (1 = placebo, 2 = no treatment control, 3 = low concentration, 4 = medium concentration, 5 = high concentration gel)\ngender: Gender (1 = male, 2 = female)\nrace: Race, (1 = Native American, 2 = African American, 3 = Not used, 4 = Asian, 5 = White)\nage: Age in years\nsmoker: Smoking status (0 = No, 1 = Yes)\nsites: Number of sites gum measurements were averaged from\nattachbase: Whole-mouth average attachment loss taken at base timepoint\nattach1year: Whole-mouth average attachment loss after 1 year\npdbase: Whole-mouth average pocket loss at base timepoint\npd1year: Whole-mouth average pocket loss after 1 year\n\n\n\n\n\n\n\nNote\n\n\n\nPocket depth and attachment loss are both measurements of how far the gums have pulled away from the teeth, hence smaller values are better"
  },
  {
    "objectID": "Project_1_Regression/Project_1_Regression_SAS/Project_1_Regression_SAS.html#Descriptives",
    "href": "Project_1_Regression/Project_1_Regression_SAS/Project_1_Regression_SAS.html#Descriptives",
    "title": "Project 1 - Regression (SAS)",
    "section": "Descriptive Statistics",
    "text": "Descriptive Statistics\nHere we will acquire the descriptive statistics of our data set and create Table 1.\n\n* Generate Table 1;\nTITLE \"Table 1\";\nPROC TABULATE DATA=Proj1.data;\n    CLASS trtgroup gender race smoker;\n    VAR age sites attachbase attach1year pdbase pd1year attachchange pdchange;\n    TABLE  \n        (gender race smoker)*(n) (age sites attachbase attach1year pdbase pd1year attachchange pdchange)*(n mean std min max), \n        trtgroup ALL;\nRUN;\n\nlstlog\n\n\n&lt;!DOCTYPE html&gt;\n\n\n\n\n\nSAS Output\n\n\n\n\n\n\n\nTable 1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\nTreatment Group\n\n\nAll\n\n\n\n\nPlacebo\n\n\nControl\n\n\nLow\n\n\nMedium\n\n\nHigh\n\n\n\n\n\n\nGender\n\n\n \n\n\n11\n\n\n10\n\n\n11\n\n\n10\n\n\n11\n\n\n53\n\n\n\n\nMale\n\n\nN\n\n\n\n\nFemale\n\n\nN\n\n\n15\n\n\n16\n\n\n15\n\n\n15\n\n\n15\n\n\n76\n\n\n\n\nRace\n\n\n \n\n\n.\n\n\n1\n\n\n1\n\n\n.\n\n\n2\n\n\n4\n\n\n\n\nNative American\n\n\nN\n\n\n\n\nAfrican American\n\n\nN\n\n\n2\n\n\n1\n\n\n5\n\n\n.\n\n\n1\n\n\n9\n\n\n\n\nWhite\n\n\nN\n\n\n1\n\n\n1\n\n\n.\n\n\n1\n\n\n.\n\n\n3\n\n\n\n\nAsian\n\n\nN\n\n\n23\n\n\n23\n\n\n20\n\n\n24\n\n\n23\n\n\n113\n\n\n\n\nSmoking Status\n\n\n \n\n\n15\n\n\n17\n\n\n18\n\n\n14\n\n\n17\n\n\n81\n\n\n\n\nNon-Smoker\n\n\nN\n\n\n\n\nSmoker\n\n\nN\n\n\n11\n\n\n9\n\n\n8\n\n\n11\n\n\n9\n\n\n48\n\n\n\n\nAge\n\n\nN\n\n\n25\n\n\n26\n\n\n26\n\n\n25\n\n\n26\n\n\n128\n\n\n\n\nMean\n\n\n47.11\n\n\n50.75\n\n\n51.92\n\n\n49.05\n\n\n50.82\n\n\n49.96\n\n\n\n\nStd\n\n\n8.61\n\n\n9.90\n\n\n10.78\n\n\n9.69\n\n\n11.20\n\n\n10.07\n\n\n\n\nMin\n\n\n30.40\n\n\n36.13\n\n\n36.95\n\n\n28.57\n\n\n34.12\n\n\n28.57\n\n\n\n\nMax\n\n\n67.14\n\n\n73.25\n\n\n71.90\n\n\n70.89\n\n\n74.53\n\n\n74.53\n\n\n\n\nSites\n\n\nN\n\n\n26\n\n\n26\n\n\n26\n\n\n25\n\n\n26\n\n\n129\n\n\n\n\nMean\n\n\n159.69\n\n\n154.38\n\n\n160.62\n\n\n157.12\n\n\n157.38\n\n\n157.84\n\n\n\n\nStd\n\n\n10.05\n\n\n10.94\n\n\n8.54\n\n\n13.54\n\n\n9.65\n\n\n10.71\n\n\n\n\nMin\n\n\n138.00\n\n\n126.00\n\n\n138.00\n\n\n114.00\n\n\n138.00\n\n\n114.00\n\n\n\n\nMax\n\n\n168.00\n\n\n168.00\n\n\n168.00\n\n\n168.00\n\n\n168.00\n\n\n168.00\n\n\n\n\nAttachment Loss Baseline\n\n\nN\n\n\n26\n\n\n26\n\n\n26\n\n\n25\n\n\n26\n\n\n129\n\n\n\n\nMean\n\n\n1.79\n\n\n2.46\n\n\n2.07\n\n\n2.19\n\n\n2.24\n\n\n2.15\n\n\n\n\nStd\n\n\n0.65\n\n\n0.69\n\n\n0.99\n\n\n0.66\n\n\n0.86\n\n\n0.80\n\n\n\n\nMin\n\n\n0.90\n\n\n1.22\n\n\n0.90\n\n\n1.02\n\n\n1.26\n\n\n0.90\n\n\n\n\nMax\n\n\n3.64\n\n\n4.39\n\n\n4.96\n\n\n4.01\n\n\n5.09\n\n\n5.09\n\n\n\n\nAttachment Loss One Year\n\n\nN\n\n\n23\n\n\n23\n\n\n21\n\n\n19\n\n\n16\n\n\n102\n\n\n\n\nMean\n\n\n1.74\n\n\n2.33\n\n\n2.08\n\n\n2.27\n\n\n2.15\n\n\n2.11\n\n\n\n\nStd\n\n\n0.54\n\n\n0.55\n\n\n1.06\n\n\n0.66\n\n\n0.92\n\n\n0.77\n\n\n\n\nMin\n\n\n0.96\n\n\n1.46\n\n\n0.87\n\n\n1.35\n\n\n1.22\n\n\n0.87\n\n\n\n\nMax\n\n\n3.10\n\n\n3.49\n\n\n5.30\n\n\n3.83\n\n\n4.04\n\n\n5.30\n\n\n\n\nPocket Depth Baseline\n\n\nN\n\n\n26\n\n\n26\n\n\n26\n\n\n25\n\n\n26\n\n\n129\n\n\n\n\nMean\n\n\n3.09\n\n\n3.28\n\n\n3.17\n\n\n3.04\n\n\n3.11\n\n\n3.14\n\n\n\n\nStd\n\n\n0.37\n\n\n0.47\n\n\n0.59\n\n\n0.41\n\n\n0.27\n\n\n0.44\n\n\n\n\nMin\n\n\n2.47\n\n\n2.65\n\n\n2.26\n\n\n2.42\n\n\n2.62\n\n\n2.26\n\n\n\n\nMax\n\n\n4.08\n\n\n4.77\n\n\n5.22\n\n\n3.91\n\n\n3.60\n\n\n5.22\n\n\n\n\nPocket Depth One Year\n\n\nN\n\n\n23\n\n\n23\n\n\n21\n\n\n19\n\n\n16\n\n\n102\n\n\n\n\nMean\n\n\n2.75\n\n\n2.95\n\n\n3.02\n\n\n2.83\n\n\n2.80\n\n\n2.87\n\n\n\n\nStd\n\n\n0.48\n\n\n0.46\n\n\n0.58\n\n\n0.48\n\n\n0.42\n\n\n0.49\n\n\n\n\nMin\n\n\n1.96\n\n\n2.24\n\n\n2.16\n\n\n2.05\n\n\n2.04\n\n\n1.96\n\n\n\n\nMax\n\n\n3.75\n\n\n4.07\n\n\n4.89\n\n\n3.78\n\n\n3.40\n\n\n4.89\n\n\n\n\nAttachment Loss Change\n\n\nN\n\n\n23\n\n\n23\n\n\n21\n\n\n19\n\n\n16\n\n\n102\n\n\n\n\nMean\n\n\n-0.09\n\n\n-0.22\n\n\n-0.02\n\n\n-0.00\n\n\n-0.16\n\n\n-0.10\n\n\n\n\nStd\n\n\n0.24\n\n\n0.28\n\n\n0.27\n\n\n0.24\n\n\n0.33\n\n\n0.28\n\n\n\n\nMin\n\n\n-0.60\n\n\n-0.90\n\n\n-0.71\n\n\n-0.45\n\n\n-1.05\n\n\n-1.05\n\n\n\n\nMax\n\n\n0.45\n\n\n0.19\n\n\n0.35\n\n\n0.34\n\n\n0.20\n\n\n0.45\n\n\n\n\nPocket Depth Change\n\n\nN\n\n\n23\n\n\n23\n\n\n21\n\n\n19\n\n\n16\n\n\n102\n\n\n\n\nMean\n\n\n-0.35\n\n\n-0.34\n\n\n-0.21\n\n\n-0.21\n\n\n-0.38\n\n\n-0.30\n\n\n\n\nStd\n\n\n0.28\n\n\n0.23\n\n\n0.28\n\n\n0.28\n\n\n0.24\n\n\n0.27\n\n\n\n\nMin\n\n\n-0.86\n\n\n-0.76\n\n\n-0.66\n\n\n-0.83\n\n\n-0.85\n\n\n-0.86\n\n\n\n\nMax\n\n\n0.16\n\n\n0.01\n\n\n0.46\n\n\n0.17\n\n\n0.05\n\n\n0.46\n\n\n\n\n\n\n\n\n\n\n\n\u001431                                                         The SAS System                      Sunday, November  3, 2024 06:45:00 AM\n\n328        ods listing close;ods html5 (id=saspy_internal) file=_tomods1 options(bitmap_mode='inline') device=svg style=HTMLBlue;\n328      ! ods graphics on / outputfmt=png;\n329        \n330        * Generate Table 1;\n331        TITLE \"Table 1\";\n332        PROC TABULATE DATA=Proj1.data;\n333            CLASS trtgroup gender race smoker;\n334            VAR age sites attachbase attach1year pdbase pd1year attachchange pdchange;\n335            TABLE\n336                (gender race smoker)*(n) (age sites attachbase attach1year pdbase pd1year attachchange pdchange)*(n mean std min\n336      ! max),\n337                trtgroup ALL;\n338        RUN;\n339        \n340        \n341        ods html5 (id=saspy_internal) close;ods listing;\n342        \n\u001432                                                         The SAS System                      Sunday, November  3, 2024 06:45:00 AM\n\n343"
  },
  {
    "objectID": "Project_1_Regression/Project_1_Regression_SAS/Project_1_Regression_SAS.html#Preliminary",
    "href": "Project_1_Regression/Project_1_Regression_SAS/Project_1_Regression_SAS.html#Preliminary",
    "title": "Project 1 - Regression (SAS)",
    "section": "Preliminary Evaluation of Assumptions",
    "text": "Preliminary Evaluation of Assumptions\nBefore we begin digging into the data, let’s take a closer look at the relationships between our variables.\nWe will first run a correlation matrix to assess the correlations between our IVs. Then we will explore any high correlations that which could affect our analysis.\nFinally we will make histograms of attachment loss and pocket depth change to gauge whether they will be normally distributed or if we need to run some transformations.\n\nCorrelation MatrixNormality of Dependent Variables\n\n\nFirst we will begin by making a correlation matrix to assess whether any of our IVs are related to each other (multicollinearity). This will inform which variables to incorporate into the final model.\n\n* Create and visualize the correlation matrix;\nTITLE \"Correlation Matrix\";\nODS GRAPHICS ON;\nPROC CORR DATA = Proj1.data PLOTS = MATRIX(HISTOGRAM);\n    VAR age sites attachbase attach1year pdbase pd1year attachchange pdchange;\n    RUN;\nODS GRAPHICS OFF;\n\nlstlog\n\n\n&lt;!DOCTYPE html&gt;\n\n\n\n\n\nSAS Output\n\n\n\n\n\n\n\nCorrelation Matrix\n\n\n\n\nThe CORR Procedure\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n8 Variables:\n\n\nage sites attachbase attach1year pdbase pd1year attachchange pdchange\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSimple Statistics\n\n\n\n\nVariable\n\n\nN\n\n\nMean\n\n\nStd Dev\n\n\nSum\n\n\nMinimum\n\n\nMaximum\n\n\nLabel\n\n\n\n\n\n\nage\n\n\n129\n\n\n49.94347\n\n\n10.03233\n\n\n6443\n\n\n28.57221\n\n\n74.53251\n\n\nAge\n\n\n\n\nsites\n\n\n130\n\n\n157.50769\n\n\n11.34125\n\n\n20476\n\n\n114.00000\n\n\n168.00000\n\n\nSites\n\n\n\n\nattachbase\n\n\n130\n\n\n2.14608\n\n\n0.79705\n\n\n278.98979\n\n\n0.89506\n\n\n5.08929\n\n\nAttachment Loss Baseline\n\n\n\n\nattach1year\n\n\n103\n\n\n2.10139\n\n\n0.77188\n\n\n216.44302\n\n\n0.86538\n\n\n5.30435\n\n\nAttachment Loss One Year\n\n\n\n\npdbase\n\n\n130\n\n\n3.13837\n\n\n0.43672\n\n\n407.98821\n\n\n2.26282\n\n\n5.21739\n\n\nPocket Depth Baseline\n\n\n\n\npd1year\n\n\n103\n\n\n2.87516\n\n\n0.48755\n\n\n296.14175\n\n\n1.96429\n\n\n4.89130\n\n\nPocket Depth One Year\n\n\n\n\nattachchange\n\n\n103\n\n\n-0.09945\n\n\n0.27603\n\n\n-10.24346\n\n\n-1.04762\n\n\n0.45238\n\n\nAttachment Loss Change\n\n\n\n\npdchange\n\n\n103\n\n\n-0.29435\n\n\n0.26761\n\n\n-30.31849\n\n\n-0.85802\n\n\n0.45513\n\n\nPocket Depth Change\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPearson Correlation CoefficientsProb &gt; |r| under H0: Rho=0Number of Observations\n\n\n\n\n \n\n\nage\n\n\nsites\n\n\nattachbase\n\n\nattach1year\n\n\npdbase\n\n\npd1year\n\n\nattachchange\n\n\npdchange\n\n\n\n\n\n\n\n\nage\n\n\nAge\n\n\n\n\n\n\n1.00000\n\n\n \n\n\n129\n\n\n\n\n\n\n-0.11266\n\n\n0.2037\n\n\n129\n\n\n\n\n\n\n0.12162\n\n\n0.1698\n\n\n129\n\n\n\n\n\n\n0.08605\n\n\n0.3898\n\n\n102\n\n\n\n\n\n\n-0.11097\n\n\n0.2106\n\n\n129\n\n\n\n\n\n\n-0.12526\n\n\n0.2097\n\n\n102\n\n\n\n\n\n\n-0.17458\n\n\n0.0793\n\n\n102\n\n\n\n\n\n\n-0.07422\n\n\n0.4585\n\n\n102\n\n\n\n\n\n\n\n\nsites\n\n\nSites\n\n\n\n\n\n\n-0.11266\n\n\n0.2037\n\n\n129\n\n\n\n\n\n\n1.00000\n\n\n \n\n\n130\n\n\n\n\n\n\n-0.39708\n\n\n&lt;.0001\n\n\n130\n\n\n\n\n\n\n-0.36885\n\n\n0.0001\n\n\n103\n\n\n\n\n\n\n-0.16634\n\n\n0.0586\n\n\n130\n\n\n\n\n\n\n-0.18852\n\n\n0.0565\n\n\n103\n\n\n\n\n\n\n0.16209\n\n\n0.1019\n\n\n103\n\n\n\n\n\n\n-0.04812\n\n\n0.6293\n\n\n103\n\n\n\n\n\n\n\n\nattachbase\n\n\nAttachment Loss Baseline\n\n\n\n\n\n\n0.12162\n\n\n0.1698\n\n\n129\n\n\n\n\n\n\n-0.39708\n\n\n&lt;.0001\n\n\n130\n\n\n\n\n\n\n1.00000\n\n\n \n\n\n130\n\n\n\n\n\n\n0.94556\n\n\n&lt;.0001\n\n\n103\n\n\n\n\n\n\n0.58869\n\n\n&lt;.0001\n\n\n130\n\n\n\n\n\n\n0.54983\n\n\n&lt;.0001\n\n\n103\n\n\n\n\n\n\n-0.41437\n\n\n&lt;.0001\n\n\n103\n\n\n\n\n\n\n-0.03676\n\n\n0.7124\n\n\n103\n\n\n\n\n\n\n\n\nattach1year\n\n\nAttachment Loss One Year\n\n\n\n\n\n\n0.08605\n\n\n0.3898\n\n\n102\n\n\n\n\n\n\n-0.36885\n\n\n0.0001\n\n\n103\n\n\n\n\n\n\n0.94556\n\n\n&lt;.0001\n\n\n103\n\n\n\n\n\n\n1.00000\n\n\n \n\n\n103\n\n\n\n\n\n\n0.60498\n\n\n&lt;.0001\n\n\n103\n\n\n\n\n\n\n0.66049\n\n\n&lt;.0001\n\n\n103\n\n\n\n\n\n\n-0.09561\n\n\n0.3367\n\n\n103\n\n\n\n\n\n\n0.15128\n\n\n0.1272\n\n\n103\n\n\n\n\n\n\n\n\npdbase\n\n\nPocket Depth Baseline\n\n\n\n\n\n\n-0.11097\n\n\n0.2106\n\n\n129\n\n\n\n\n\n\n-0.16634\n\n\n0.0586\n\n\n130\n\n\n\n\n\n\n0.58869\n\n\n&lt;.0001\n\n\n130\n\n\n\n\n\n\n0.60498\n\n\n&lt;.0001\n\n\n103\n\n\n\n\n\n\n1.00000\n\n\n \n\n\n130\n\n\n\n\n\n\n0.84327\n\n\n&lt;.0001\n\n\n103\n\n\n\n\n\n\n-0.13472\n\n\n0.1749\n\n\n103\n\n\n\n\n\n\n-0.20267\n\n\n0.0401\n\n\n103\n\n\n\n\n\n\n\n\npd1year\n\n\nPocket Depth One Year\n\n\n\n\n\n\n-0.12526\n\n\n0.2097\n\n\n102\n\n\n\n\n\n\n-0.18852\n\n\n0.0565\n\n\n103\n\n\n\n\n\n\n0.54983\n\n\n&lt;.0001\n\n\n103\n\n\n\n\n\n\n0.66049\n\n\n&lt;.0001\n\n\n103\n\n\n\n\n\n\n0.84327\n\n\n&lt;.0001\n\n\n103\n\n\n\n\n\n\n1.00000\n\n\n \n\n\n103\n\n\n\n\n\n\n0.16531\n\n\n0.0952\n\n\n103\n\n\n\n\n\n\n0.35543\n\n\n0.0002\n\n\n103\n\n\n\n\n\n\n\n\nattachchange\n\n\nAttachment Loss Change\n\n\n\n\n\n\n-0.17458\n\n\n0.0793\n\n\n102\n\n\n\n\n\n\n0.16209\n\n\n0.1019\n\n\n103\n\n\n\n\n\n\n-0.41437\n\n\n&lt;.0001\n\n\n103\n\n\n\n\n\n\n-0.09561\n\n\n0.3367\n\n\n103\n\n\n\n\n\n\n-0.13472\n\n\n0.1749\n\n\n103\n\n\n\n\n\n\n0.16531\n\n\n0.0952\n\n\n103\n\n\n\n\n\n\n1.00000\n\n\n \n\n\n103\n\n\n\n\n\n\n0.53546\n\n\n&lt;.0001\n\n\n103\n\n\n\n\n\n\n\n\npdchange\n\n\nPocket Depth Change\n\n\n\n\n\n\n-0.07422\n\n\n0.4585\n\n\n102\n\n\n\n\n\n\n-0.04812\n\n\n0.6293\n\n\n103\n\n\n\n\n\n\n-0.03676\n\n\n0.7124\n\n\n103\n\n\n\n\n\n\n0.15128\n\n\n0.1272\n\n\n103\n\n\n\n\n\n\n-0.20267\n\n\n0.0401\n\n\n103\n\n\n\n\n\n\n0.35543\n\n\n0.0002\n\n\n103\n\n\n\n\n\n\n0.53546\n\n\n&lt;.0001\n\n\n103\n\n\n\n\n\n\n1.00000\n\n\n \n\n\n103\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001433                                                         The SAS System                      Sunday, November  3, 2024 06:45:00 AM\n\n346        ods listing close;ods html5 (id=saspy_internal) file=_tomods1 options(bitmap_mode='inline') device=svg style=HTMLBlue;\n346      ! ods graphics on / outputfmt=png;\n347        \n348        * Create and visualize the correlation matrix;\n349        TITLE \"Correlation Matrix\";\n350        ODS GRAPHICS ON;\n351        PROC CORR DATA = Proj1.data PLOTS = MATRIX(HISTOGRAM);\n352         VAR age sites attachbase attach1year pdbase pd1year attachchange pdchange;\n353         RUN;\n354        ODS GRAPHICS OFF;\n355        \n356        \n357        ods html5 (id=saspy_internal) close;ods listing;\n358        \n\u001434                                                         The SAS System                      Sunday, November  3, 2024 06:45:00 AM\n\n359        \n\n\n\n\nVisually, we can see a cluster of high positive correlations between all of attachment loss and pocket depth at baseline and 1 year. This makes sense, as all measurements were taken from the same sites in the gums. This will be explored later (See Exploratory Data Analysis - Dependent Variables)\nBack to top of tabset\n\n\nHere we will simply plot the histograms of attachment loss and pocket depth changes scores, to assess if they appear normally distributed or if we will have to perform a transformation of some kind.\n\n* Assess Normality of Dependent Variables (attachchange, pdchange);\n* Note: Here we would use the Kolmogorov-Smirnov test, since our sample size is &gt; 50\n    if the sample size was &lt; 50 you would use Shapiro Wilks test;\nTITLE \"Normality Assesment\";\nPROC UNIVARIATE DATA = Proj1.data;\n    VAR attachchange pdchange;\n    HISTOGRAM attachchange pdchange / NORMAL;\n    QQPLOT attachchange pdchange / NORMAL (MU=EST SIGMA = EST);\n    RUN;\n\nlstlog\n\n\n&lt;!DOCTYPE html&gt;\n\n\n\n\n\nSAS Output\n\n\n\n\n\n\n\nNormality Assesment\n\n\n\n\nThe UNIVARIATE Procedure\n\n\nVariable: attachchange (Attachment Loss Change)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMoments\n\n\n\n\n\n\nN\n\n\n103\n\n\nSum Weights\n\n\n103\n\n\n\n\nMean\n\n\n-0.099451\n\n\nSum Observations\n\n\n-10.243457\n\n\n\n\nStd Deviation\n\n\n0.27603039\n\n\nVariance\n\n\n0.07619277\n\n\n\n\nSkewness\n\n\n-0.7927997\n\n\nKurtosis\n\n\n1.03785174\n\n\n\n\nUncorrected SS\n\n\n8.79038543\n\n\nCorrected SS\n\n\n7.77166295\n\n\n\n\nCoeff Variation\n\n\n-277.55404\n\n\nStd Error Mean\n\n\n0.02719808\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBasic Statistical Measures\n\n\n\n\nLocation\n\n\nVariability\n\n\n\n\n\n\nMean\n\n\n-0.09945\n\n\nStd Deviation\n\n\n0.27603\n\n\n\n\nMedian\n\n\n-0.06790\n\n\nVariance\n\n\n0.07619\n\n\n\n\nMode\n\n\n-0.18519\n\n\nRange\n\n\n1.50000\n\n\n\n\n \n\n\n \n\n\nInterquartile Range\n\n\n0.36684\n\n\n\n\n\n\nNote: The mode displayed is the smallest of 3 modes with a count of 2.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTests for Location: Mu0=0\n\n\n\n\nTest\n\n\nStatistic\n\n\np Value\n\n\n\n\n\n\nStudent's t\n\n\nt\n\n\n-3.65655\n\n\nPr &gt; |t|\n\n\n0.0004\n\n\n\n\nSign\n\n\nM\n\n\n-12.5\n\n\nPr &gt;= |M|\n\n\n0.0176\n\n\n\n\nSigned Rank\n\n\nS\n\n\n-926.5\n\n\nPr &gt;= |S|\n\n\n0.0020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuantiles (Definition 5)\n\n\n\n\nLevel\n\n\nQuantile\n\n\n\n\n\n\n100% Max\n\n\n0.4523810\n\n\n\n\n99%\n\n\n0.3478261\n\n\n\n\n95%\n\n\n0.3148148\n\n\n\n\n90%\n\n\n0.2222222\n\n\n\n\n75% Q3\n\n\n0.0952381\n\n\n\n\n50% Median\n\n\n-0.0679012\n\n\n\n\n25% Q1\n\n\n-0.2716049\n\n\n\n\n10%\n\n\n-0.4487179\n\n\n\n\n5%\n\n\n-0.5434783\n\n\n\n\n1%\n\n\n-0.9008889\n\n\n\n\n0% Min\n\n\n-1.0476190\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExtreme Observations\n\n\n\n\nLowest\n\n\nHighest\n\n\n\n\nValue\n\n\nObs\n\n\nValue\n\n\nObs\n\n\n\n\n\n\n-1.047619\n\n\n67\n\n\n0.320513\n\n\n16\n\n\n\n\n-0.900889\n\n\n44\n\n\n0.327160\n\n\n64\n\n\n\n\n-0.810000\n\n\n46\n\n\n0.339286\n\n\n34\n\n\n\n\n-0.705128\n\n\n7\n\n\n0.347826\n\n\n4\n\n\n\n\n-0.598765\n\n\n43\n\n\n0.452381\n\n\n13\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMissing Values\n\n\n\n\nMissingValue\n\n\nCount\n\n\nPercent Of\n\n\n\n\nAll Obs\n\n\nMissing Obs\n\n\n\n\n\n\n.\n\n\n27\n\n\n20.77\n\n\n100.00\n\n\n\n\n\n\n\n\nNormality Assesment\n\n\n\n\nThe UNIVARIATE Procedure\n\n\n\n\n\n\n\n\n\n\n\nNormality Assesment\n\n\n\n\nThe UNIVARIATE Procedure\n\n\nFitted Normal Distribution for attachchange (Attachment Loss Change)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nParameters for Normal Distribution\n\n\n\n\nParameter\n\n\nSymbol\n\n\nEstimate\n\n\n\n\n\n\nMean\n\n\nMu\n\n\n-0.09945\n\n\n\n\nStd Dev\n\n\nSigma\n\n\n0.27603\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGoodness-of-Fit Tests for Normal Distribution\n\n\n\n\nTest\n\n\nStatistic\n\n\np Value\n\n\n\n\n\n\nKolmogorov-Smirnov\n\n\nD\n\n\n0.08091601\n\n\nPr &gt; D\n\n\n0.095\n\n\n\n\nCramer-von Mises\n\n\nW-Sq\n\n\n0.12605262\n\n\nPr &gt; W-Sq\n\n\n0.049\n\n\n\n\nAnderson-Darling\n\n\nA-Sq\n\n\n0.79367755\n\n\nPr &gt; A-Sq\n\n\n0.040\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuantiles for Normal Distribution\n\n\n\n\nPercent\n\n\nQuantile\n\n\n\n\nObserved\n\n\nEstimated\n\n\n\n\n\n\n1.0\n\n\n-0.90089\n\n\n-0.74159\n\n\n\n\n5.0\n\n\n-0.54348\n\n\n-0.55348\n\n\n\n\n10.0\n\n\n-0.44872\n\n\n-0.45320\n\n\n\n\n25.0\n\n\n-0.27160\n\n\n-0.28563\n\n\n\n\n50.0\n\n\n-0.06790\n\n\n-0.09945\n\n\n\n\n75.0\n\n\n0.09524\n\n\n0.08673\n\n\n\n\n90.0\n\n\n0.22222\n\n\n0.25430\n\n\n\n\n95.0\n\n\n0.31481\n\n\n0.35458\n\n\n\n\n99.0\n\n\n0.34783\n\n\n0.54269\n\n\n\n\n\n\n\n\n\n\nNormality Assesment\n\n\n\n\nThe UNIVARIATE Procedure\n\n\n\n\n\n\n\n\n\n\n\n\n\nNormality Assesment\n\n\n\n\nThe UNIVARIATE Procedure\n\n\nVariable: pdchange (Pocket Depth Change)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMoments\n\n\n\n\n\n\nN\n\n\n103\n\n\nSum Weights\n\n\n103\n\n\n\n\nMean\n\n\n-0.2943543\n\n\nSum Observations\n\n\n-30.318488\n\n\n\n\nStd Deviation\n\n\n0.2676105\n\n\nVariance\n\n\n0.07161538\n\n\n\n\nSkewness\n\n\n0.07023632\n\n\nKurtosis\n\n\n-0.2191411\n\n\n\n\nUncorrected SS\n\n\n16.2291445\n\n\nCorrected SS\n\n\n7.30476874\n\n\n\n\nCoeff Variation\n\n\n-90.914434\n\n\nStd Error Mean\n\n\n0.02636845\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBasic Statistical Measures\n\n\n\n\nLocation\n\n\nVariability\n\n\n\n\n\n\nMean\n\n\n-0.29435\n\n\nStd Deviation\n\n\n0.26761\n\n\n\n\nMedian\n\n\n-0.28395\n\n\nVariance\n\n\n0.07162\n\n\n\n\nMode\n\n\n-0.50000\n\n\nRange\n\n\n1.31315\n\n\n\n\n \n\n\n \n\n\nInterquartile Range\n\n\n0.38448\n\n\n\n\n\n\nNote: The mode displayed is the smallest of 3 modes with a count of 2.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTests for Location: Mu0=0\n\n\n\n\nTest\n\n\nStatistic\n\n\np Value\n\n\n\n\n\n\nStudent's t\n\n\nt\n\n\n-11.1631\n\n\nPr &gt; |t|\n\n\n&lt;.0001\n\n\n\n\nSign\n\n\nM\n\n\n-35.5\n\n\nPr &gt;= |M|\n\n\n&lt;.0001\n\n\n\n\nSigned Rank\n\n\nS\n\n\n-2352\n\n\nPr &gt;= |S|\n\n\n&lt;.0001\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuantiles (Definition 5)\n\n\n\n\nLevel\n\n\nQuantile\n\n\n\n\n\n\n100% Max\n\n\n0.4551282\n\n\n\n\n99%\n\n\n0.2976190\n\n\n\n\n95%\n\n\n0.1604938\n\n\n\n\n90%\n\n\n0.0476190\n\n\n\n\n75% Q3\n\n\n-0.0952381\n\n\n\n\n50% Median\n\n\n-0.2839506\n\n\n\n\n25% Q1\n\n\n-0.4797178\n\n\n\n\n10%\n\n\n-0.6845238\n\n\n\n\n5%\n\n\n-0.7283951\n\n\n\n\n1%\n\n\n-0.8452381\n\n\n\n\n0% Min\n\n\n-0.8580247\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExtreme Observations\n\n\n\n\nLowest\n\n\nHighest\n\n\n\n\nValue\n\n\nObs\n\n\nValue\n\n\nObs\n\n\n\n\n\n\n-0.858025\n\n\n10\n\n\n0.160494\n\n\n89\n\n\n\n\n-0.845238\n\n\n18\n\n\n0.160714\n\n\n130\n\n\n\n\n-0.827381\n\n\n6\n\n\n0.174638\n\n\n21\n\n\n\n\n-0.759259\n\n\n66\n\n\n0.297619\n\n\n37\n\n\n\n\n-0.749094\n\n\n54\n\n\n0.455128\n\n\n99\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMissing Values\n\n\n\n\nMissingValue\n\n\nCount\n\n\nPercent Of\n\n\n\n\nAll Obs\n\n\nMissing Obs\n\n\n\n\n\n\n.\n\n\n27\n\n\n20.77\n\n\n100.00\n\n\n\n\n\n\n\n\nNormality Assesment\n\n\n\n\nThe UNIVARIATE Procedure\n\n\n\n\n\n\n\n\n\n\n\nNormality Assesment\n\n\n\n\nThe UNIVARIATE Procedure\n\n\nFitted Normal Distribution for pdchange (Pocket Depth Change)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nParameters for Normal Distribution\n\n\n\n\nParameter\n\n\nSymbol\n\n\nEstimate\n\n\n\n\n\n\nMean\n\n\nMu\n\n\n-0.29435\n\n\n\n\nStd Dev\n\n\nSigma\n\n\n0.267611\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGoodness-of-Fit Tests for Normal Distribution\n\n\n\n\nTest\n\n\nStatistic\n\n\np Value\n\n\n\n\n\n\nKolmogorov-Smirnov\n\n\nD\n\n\n0.04137085\n\n\nPr &gt; D\n\n\n&gt;0.150\n\n\n\n\nCramer-von Mises\n\n\nW-Sq\n\n\n0.02560441\n\n\nPr &gt; W-Sq\n\n\n&gt;0.250\n\n\n\n\nAnderson-Darling\n\n\nA-Sq\n\n\n0.19521495\n\n\nPr &gt; A-Sq\n\n\n&gt;0.250\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuantiles for Normal Distribution\n\n\n\n\nPercent\n\n\nQuantile\n\n\n\n\nObserved\n\n\nEstimated\n\n\n\n\n\n\n1.0\n\n\n-0.84524\n\n\n-0.91691\n\n\n\n\n5.0\n\n\n-0.72840\n\n\n-0.73453\n\n\n\n\n10.0\n\n\n-0.68452\n\n\n-0.63731\n\n\n\n\n25.0\n\n\n-0.47972\n\n\n-0.47485\n\n\n\n\n50.0\n\n\n-0.28395\n\n\n-0.29435\n\n\n\n\n75.0\n\n\n-0.09524\n\n\n-0.11385\n\n\n\n\n90.0\n\n\n0.04762\n\n\n0.04860\n\n\n\n\n95.0\n\n\n0.16049\n\n\n0.14583\n\n\n\n\n99.0\n\n\n0.29762\n\n\n0.32820\n\n\n\n\n\n\n\n\n\n\nNormality Assesment\n\n\n\n\nThe UNIVARIATE Procedure\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001435                                                         The SAS System                      Sunday, November  3, 2024 06:45:00 AM\n\n362        ods listing close;ods html5 (id=saspy_internal) file=_tomods1 options(bitmap_mode='inline') device=svg style=HTMLBlue;\n362      ! ods graphics on / outputfmt=png;\n363        \n364        * Assess Normality of Dependent Variables (attachchange, pdchange);\n365        * Note: Here we would use the Kolmogorov-Smirnov test, since our sample size is &gt; 50\n366         if the sample size was &lt; 50 you would use Shapiro Wilks test;\n367        TITLE \"Normality Assesment\";\n368        PROC UNIVARIATE DATA = Proj1.data;\n369         VAR attachchange pdchange;\n370         HISTOGRAM attachchange pdchange / NORMAL;\n371         QQPLOT attachchange pdchange / NORMAL (MU=EST SIGMA = EST);\n372         RUN;\n373        \n374        \n375        ods html5 (id=saspy_internal) close;ods listing;\n376        \n\u001436                                                         The SAS System                      Sunday, November  3, 2024 06:45:00 AM\n\n377        \n\n\n\n\n\nConclusion\nBoth attachment loss and pocket depth change appear to be normally distributed and will not need to be transformed. Attachment loss change is slightly left-tailed but this could be due to outliers, or may just not impact the analysis.\nBack to top of tabset"
  },
  {
    "objectID": "Project_1_Regression/Project_1_Regression_SAS/Project_1_Regression_SAS.html#Exploratory",
    "href": "Project_1_Regression/Project_1_Regression_SAS/Project_1_Regression_SAS.html#Exploratory",
    "title": "Project 1 - Regression (SAS)",
    "section": "Exploratory Data Analysis",
    "text": "Exploratory Data Analysis\nHere I will plot the data and perform a number of simple linear regressions to examine relationships between variables in order to determine which covariates to include in the model.\n\nPrimary Explanatory VariableCovariatesSummary\n\n\n\n5 Treatment Groups\n\n\nLet’s examine if there appears to be a difference in the average attachment loss and pocket depth change according to treatment level\n\n* Boxplot of attachchange vs treatment condition;\nTITLE \"Primary Attachment Loss Change by Treatment Condition\";\nPROC SGPLOT DATA = Proj1.data;\n    VBOX attachchange / CATEGORY = trtgroup;\n    RUN;\n\n* Boxplot of pdchange vs treatment condition;\nTITLE \"Primary Pocket Depth Change by Treatment Condition\";\nPROC SGPLOT DATA = Proj1.data;\n    VBOX pdchange / CATEGORY = trtgroup;\n    RUN;\n\nlstlog\n\n\n&lt;!DOCTYPE html&gt;\n\n\n\n\n\nSAS Output\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001437                                                         The SAS System                      Sunday, November  3, 2024 06:45:00 AM\n\n380        ods listing close;ods html5 (id=saspy_internal) file=_tomods1 options(bitmap_mode='inline') device=svg style=HTMLBlue;\n380      ! ods graphics on / outputfmt=png;\n381        \n382        * Boxplot of attachchange vs treatment condition;\n383        TITLE \"Primary Attachment Loss Change by Treatment Condition\";\n384        PROC SGPLOT DATA = Proj1.data;\n385         VBOX attachchange / CATEGORY = trtgroup;\n386         RUN;\n387        \n388        * Boxplot of pdchange vs treatment condition;\n389        TITLE \"Primary Pocket Depth Change by Treatment Condition\";\n390        PROC SGPLOT DATA = Proj1.data;\n391         VBOX pdchange / CATEGORY = trtgroup;\n392         RUN;\n393        \n394        \n395        ods html5 (id=saspy_internal) close;ods listing;\n396        \n\u001438                                                         The SAS System                      Sunday, November  3, 2024 06:45:00 AM\n\n397        \n\n\n\n\n\nAttachment Loss Change\nVisually, there does not appear to be a difference between the treatment group levels (low, medium, high concentration gel) compared to each other. Additionally, there does not seem to be a difference between the treatment groups and the placebo for attachment loss change.\n\n\nPocket Depth Change\nInterestingly, the high concentration condition appears to have had a negative effect. Additionally, there seems to be a difference in the treatment groups compared to the no treatment control, but NOT when compared to the placebo. The exception is the low concentration condition compared to the placebo when looking at pocket depth change. Further analysis will reveal whether these differences are statistically significant or not.\nBack to top of tabset\n\n\n\n\n\n\nNow we will exlore the relationships between our potential covariates and attachment loss and pocket depth change\n\nGenderAgeSitesRaceSmoking Status\n\n\nIt is conceivable that there are sex differences in regards to gum health. Let’s examine if there is a difference in attachment loss or pocket depth change based on gender\n\n* Create boxplot of gender vs attachmnent loss change;\nTITLE \"Boxplot of Gender vs Attachment Loss Change\";\nPROC SGPLOT DATA = Proj1.data;\n    VBOX attachchange / CATEGORY = gender;\n    RUN;\n    \n* Create boxplot of gender vs pocket depth change;\nTITLE \"Boxplot of Gender vs Pocket Depth Change\";\nPROC SGPLOT DATA = Proj1.data;\n    VBOX pdchange / CATEGORY = gender;\n    RUN;\n    \n* Note: Males may have more pocket depth change than females\n    Run a t-test to assess;\nPROC TTEST DATA = Proj1.data;\n    CLASS gender;\n    VAR pdchange;\n    RUN;\n\nlstlog\n\n\n&lt;!DOCTYPE html&gt;\n\n\n\n\n\nSAS Output\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBoxplot of Gender vs Pocket Depth Change\n\n\n\n\nThe TTEST Procedure\n\n\n \n\n\nVariable: pdchange (Pocket Depth Change)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ngender\n\n\nMethod\n\n\nN\n\n\nMean\n\n\nStd Dev\n\n\nStd Err\n\n\nMinimum\n\n\nMaximum\n\n\n\n\n\n\nMale\n\n\n \n\n\n36\n\n\n-0.2151\n\n\n0.2362\n\n\n0.0394\n\n\n-0.6964\n\n\n0.4551\n\n\n\n\nFemale\n\n\n \n\n\n67\n\n\n-0.3369\n\n\n0.2754\n\n\n0.0336\n\n\n-0.8580\n\n\n0.2976\n\n\n\n\nDiff (1-2)\n\n\nPooled\n\n\n \n\n\n0.1219\n\n\n0.2625\n\n\n0.0542\n\n\n \n\n\n \n\n\n\n\nDiff (1-2)\n\n\nSatterthwaite\n\n\n \n\n\n0.1219\n\n\n \n\n\n0.0518\n\n\n \n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ngender\n\n\nMethod\n\n\nMean\n\n\n95% CL Mean\n\n\nStd Dev\n\n\n95% CL Std Dev\n\n\n\n\n\n\nMale\n\n\n \n\n\n-0.2151\n\n\n-0.2950\n\n\n-0.1352\n\n\n0.2362\n\n\n0.1916\n\n\n0.3081\n\n\n\n\nFemale\n\n\n \n\n\n-0.3369\n\n\n-0.4041\n\n\n-0.2698\n\n\n0.2754\n\n\n0.2353\n\n\n0.3319\n\n\n\n\nDiff (1-2)\n\n\nPooled\n\n\n0.1219\n\n\n0.0143\n\n\n0.2295\n\n\n0.2625\n\n\n0.2307\n\n\n0.3044\n\n\n\n\nDiff (1-2)\n\n\nSatterthwaite\n\n\n0.1219\n\n\n0.0188\n\n\n0.2249\n\n\n \n\n\n \n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMethod\n\n\nVariances\n\n\nDF\n\n\nt Value\n\n\nPr &gt; |t|\n\n\n\n\n\n\nPooled\n\n\nEqual\n\n\n101\n\n\n2.25\n\n\n0.0268\n\n\n\n\nSatterthwaite\n\n\nUnequal\n\n\n81.683\n\n\n2.35\n\n\n0.0210\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEquality of Variances\n\n\n\n\nMethod\n\n\nNum DF\n\n\nDen DF\n\n\nF Value\n\n\nPr &gt; F\n\n\n\n\n\n\nFolded F\n\n\n66\n\n\n35\n\n\n1.36\n\n\n0.3250\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001439                                                         The SAS System                      Sunday, November  3, 2024 06:45:00 AM\n\n400        ods listing close;ods html5 (id=saspy_internal) file=_tomods1 options(bitmap_mode='inline') device=svg style=HTMLBlue;\n400      ! ods graphics on / outputfmt=png;\n401        \n402        * Create boxplot of gender vs attachmnent loss change;\n403        TITLE \"Boxplot of Gender vs Attachment Loss Change\";\n404        PROC SGPLOT DATA = Proj1.data;\n405         VBOX attachchange / CATEGORY = gender;\n406         RUN;\n407         \n408        * Create boxplot of gender vs pocket depth change;\n409        TITLE \"Boxplot of Gender vs Pocket Depth Change\";\n410        PROC SGPLOT DATA = Proj1.data;\n411         VBOX pdchange / CATEGORY = gender;\n412         RUN;\n413         \n414        * Note: Males may have more pocket depth change than females\n415         Run a t-test to assess;\n416        PROC TTEST DATA = Proj1.data;\n417         CLASS gender;\n418         VAR pdchange;\n419         RUN;\n420        \n421        \n422        ods html5 (id=saspy_internal) close;ods listing;\n423        \n\u001440                                                         The SAS System                      Sunday, November  3, 2024 06:45:00 AM\n\n424        \n\n\n\n\nThere is a significant difference in attachment loss change score based on gender (t = 2.0502, p = 0.04299)\nThere is also a statistically significant difference in pocket depth change based on gender (t = 2.2626, p = 0.02641).\nTherefore I will include gender as a covariate in the analysis!\nBack to top of tabset\n\n\n\n* Create scatterplot of age vs attachment loss change;\nTITLE \"Age vs Attachment Loss Change\";\nPROC SGPLOT DATA = Proj1.data;\n    SCATTER X = age y = attachchange / GROUP = trtgroup;\n    RUN;\n    \n* Create scatterplot of sites vs pocket depth change;\nTITLE \"Age vs Pocket Depth Change\";\nPROC SGPLOT DATA = Proj1.data;\n    SCATTER X = age y = pdchange / GROUP = trtgroup;\n    RUN;\n\nlstlog\n\n\n&lt;!DOCTYPE html&gt;\n\n\n\n\n\nSAS Output\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001441                                                         The SAS System                      Sunday, November  3, 2024 06:45:00 AM\n\n427        ods listing close;ods html5 (id=saspy_internal) file=_tomods1 options(bitmap_mode='inline') device=svg style=HTMLBlue;\n427      ! ods graphics on / outputfmt=png;\n428        \n429        * Create scatterplot of age vs attachment loss change;\n430        TITLE \"Age vs Attachment Loss Change\";\n431        PROC SGPLOT DATA = Proj1.data;\n432         SCATTER X = age y = attachchange / GROUP = trtgroup;\n433         RUN;\n434         \n435        * Create scatterplot of sites vs pocket depth change;\n436        TITLE \"Age vs Pocket Depth Change\";\n437        PROC SGPLOT DATA = Proj1.data;\n438         SCATTER X = age y = pdchange / GROUP = trtgroup;\n439         RUN;\n440        \n441        \n442        ods html5 (id=saspy_internal) close;ods listing;\n443        \n\u001442                                                         The SAS System                      Sunday, November  3, 2024 06:45:00 AM\n\n444        \n\n\n\n\nThere does not appear to be any kind of linear relationship between age and attachment loss change or pocket depth change. I will run a model where I include age, but it will likely be removed in the final model.\nBack to top of tabset\n\n\nIs there any relationship between the number of sites measured from and attachment loss change or pocket depth change (e.g. a lower number of sites could lead to less accurate readings)? If so this could be something to include in our data analysis\n\n* Create scatterplot of sites vs attachment loss change;\nTITLE \"Sites vs Attachment Loss Change\";\nPROC SGPLOT DATA = Proj1.data;\n    SCATTER X = sites y = attachchange / GROUP = trtgroup;\n    RUN;\n    \n* Create scatterplot of sites vs pocket depth change;\nTITLE \"Sites vs Pocket Depth Change\";\nPROC SGPLOT DATA = Proj1.data;\n    SCATTER X = sites y = pdchange / GROUP = trtgroup;\n    RUN;\n\nlstlog\n\n\n&lt;!DOCTYPE html&gt;\n\n\n\n\n\nSAS Output\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001443                                                         The SAS System                      Sunday, November  3, 2024 06:45:00 AM\n\n447        ods listing close;ods html5 (id=saspy_internal) file=_tomods1 options(bitmap_mode='inline') device=svg style=HTMLBlue;\n447      ! ods graphics on / outputfmt=png;\n448        \n449        * Create scatterplot of sites vs attachment loss change;\n450        TITLE \"Sites vs Attachment Loss Change\";\n451        PROC SGPLOT DATA = Proj1.data;\n452         SCATTER X = sites y = attachchange / GROUP = trtgroup;\n453         RUN;\n454         \n455        * Create scatterplot of sites vs pocket depth change;\n456        TITLE \"Sites vs Pocket Depth Change\";\n457        PROC SGPLOT DATA = Proj1.data;\n458         SCATTER X = sites y = pdchange / GROUP = trtgroup;\n459         RUN;\n460        \n461        \n462        ods html5 (id=saspy_internal) close;ods listing;\n463        \n\u001444                                                         The SAS System                      Sunday, November  3, 2024 06:45:00 AM\n\n464        \n\n\n\n\nThere does not seem to be a dramatic difference in attachment loss or pocket depth change based on the number of sites measured from. It’s a bit hard to tell with those two low site subjects, but the points are similar enough to the rest of the dataset that I do not think we have to worry about site number in our analysis.\nBack to top of tabset\n\n\n\n* Create boxplot of race vs attachment loss change;\nTITLE \"Race vs Attachment Loss Change\";\nPROC SGPLOT DATA = Proj1.data;\n    VBOX attachchange / CATEGORY = race;\n    RUN;\n    \n* Create boxplot of race vs pocket depth change;\nTITLE \"Race vs Pocket Depth Change\";\nPROC SGPLOT DATA = Proj1.data;\n    VBOX pdchange / CATEGORY = race;\n    RUN;\n\nlstlog\n\n\n&lt;!DOCTYPE html&gt;\n\n\n\n\n\nSAS Output\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001445                                                         The SAS System                      Sunday, November  3, 2024 06:45:00 AM\n\n467        ods listing close;ods html5 (id=saspy_internal) file=_tomods1 options(bitmap_mode='inline') device=svg style=HTMLBlue;\n467      ! ods graphics on / outputfmt=png;\n468        \n469        * Create boxplot of race vs attachment loss change;\n470        TITLE \"Race vs Attachment Loss Change\";\n471        PROC SGPLOT DATA = Proj1.data;\n472         VBOX attachchange / CATEGORY = race;\n473         RUN;\n474         \n475        * Create boxplot of race vs pocket depth change;\n476        TITLE \"Race vs Pocket Depth Change\";\n477        PROC SGPLOT DATA = Proj1.data;\n478         VBOX pdchange / CATEGORY = race;\n479         RUN;\n480        \n481        \n482        ods html5 (id=saspy_internal) close;ods listing;\n483        \n\u001446                                                         The SAS System                      Sunday, November  3, 2024 06:45:00 AM\n\n484        \n\n\n\n\nThere does not seem to be much of a difference in attachment loss or pocket depth based on race. MAYBE African Americans (2) have more pocket depth loss compared to Asians(4), but the sample size was very small for both races (88.1% of the sample identified as White)\nBack to top of tabset\n\n\nWhether the participant is a smoker or not likely has a dramatic effect on gum health. Let’s assess\n\nTITLE \"Smoking Status vs Attachment Loss Change\";\nPROC SGPLOT DATA = Proj1.data;\n    VBOX attachchange / CATEGORY = smoker;\n    RUN;\n\n* Create boxplot of smoking status vs pocket depth change;\nTITLE \"Smoking Status vs Pocket Depth Change\";\nPROC SGPLOT DATA = Proj1.data;\n    VBOX pdchange / CATEGORY = smoker;\n    RUN;\n\nlstlog\n\n\n&lt;!DOCTYPE html&gt;\n\n\n\n\n\nSAS Output\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001447                                                         The SAS System                      Sunday, November  3, 2024 06:45:00 AM\n\n487        ods listing close;ods html5 (id=saspy_internal) file=_tomods1 options(bitmap_mode='inline') device=svg style=HTMLBlue;\n487      ! ods graphics on / outputfmt=png;\n488        \n489        TITLE \"Smoking Status vs Attachment Loss Change\";\n490        PROC SGPLOT DATA = Proj1.data;\n491         VBOX attachchange / CATEGORY = smoker;\n492         RUN;\n493        \n494        * Create boxplot of smoking status vs pocket depth change;\n495        TITLE \"Smoking Status vs Pocket Depth Change\";\n496        PROC SGPLOT DATA = Proj1.data;\n497         VBOX pdchange / CATEGORY = smoker;\n498         RUN;\n499        \n500        \n501        ods html5 (id=saspy_internal) close;ods listing;\n502        \n\u001448                                                         The SAS System                      Sunday, November  3, 2024 06:45:00 AM\n\n503        \n\n\n\n\nThere does not appear to be any difference in attachment loss or pocket depth change based on smoking status. I will test a model with smoking status included, but it will likely be dropped in the final model.\nBack to top of tabset\n\n\n\n\n\n\nGender\nGender is related to attachment loss and pocket depth change, and as such will be included in the final model.\n\n\nSupressor / Counfound Variables\nA model will be tested with all covariates to assess for possible suppressor / confound variables. But the other potential covariates of race, age, sites, and smoking status were not related to attachment loss or pocket depth change by themselves.\nBack to top of tabset"
  },
  {
    "objectID": "Project_1_Regression/Project_1_Regression_SAS/Project_1_Regression_SAS.html#Model_1",
    "href": "Project_1_Regression/Project_1_Regression_SAS/Project_1_Regression_SAS.html#Model_1",
    "title": "Project 1 - Regression (SAS)",
    "section": "Running the Model",
    "text": "Running the Model\nFor ease of interpretation I will be performing this analysis as two separate linear regressions, one for each outcome variable of either attachment loss or pocket depth change. I will begin with a simple linear regression only including the PEV and either attachment loss change or pocket depth change.\n\nAttachment Loss ChangePocket Depth ChangePost-Hoc AnalysisEvaluating Assumptions\n\n\nWe start by constructing a model with attachment loss change as the dependent variable, and treatment group as the independent variable.\nFirst we need to create the dummy variables needed for these analyses.\n\n* Create Dummy Variables;\nTITLE \"Create Dummy Variables\";\nDATA Proj1.data;\n    SET Proj1.data;\n    IF trtgroup ne . THEN DO;\n        placebo = (trtgroup = 1);\n        control = (trtgroup = 2);\n        low     = (trtgroup = 3);\n        medium  = (trtgroup = 4);\n        high    = (trtgroup = 5);\n    END;\nRUN;\n\n* Check Dummy Variables Created Correctly;\nPROC PRINT DATA = Proj1.data (OBS = 6);\n    RUN;\n\nlstlog\n\n\n&lt;!DOCTYPE html&gt;\n\n\n\n\n\nSAS Output\n\n\n\n\n\n\n\nCreate Dummy Variables\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObs\n\n\nid\n\n\ntrtgroup\n\n\ngender\n\n\nrace\n\n\nage\n\n\nsmoker\n\n\nsites\n\n\nattachbase\n\n\npdbase\n\n\nattach1year\n\n\npd1year\n\n\nattachchange\n\n\npdchange\n\n\nplacebo\n\n\ncontrol\n\n\nlow\n\n\nmedium\n\n\nhigh\n\n\n\n\n\n\n1\n\n\n101\n\n\nMedium\n\n\nFemale\n\n\nAsian\n\n\n44.57221082\n\n\nSmoker\n\n\n162\n\n\n2.432098765\n\n\n3.24691358\n\n\n2.57764\n\n\n3.40741\n\n\n0.14554\n\n\n0.16049\n\n\n0\n\n\n0\n\n\n0\n\n\n1\n\n\n0\n\n\n\n\n2\n\n\n102\n\n\nHigh\n\n\nFemale\n\n\nAsian\n\n\n35.57289528\n\n\nSmoker\n\n\n162\n\n\n2.543209877\n\n\n3.00617284\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n1\n\n\n\n\n3\n\n\n103\n\n\nControl\n\n\nFemale\n\n\nAsian\n\n\n47.94524298\n\n\nSmoker\n\n\n144\n\n\n2.881944444\n\n\n3.118055556\n\n\n3.07639\n\n\n3.12500\n\n\n0.19444\n\n\n0.00694\n\n\n0\n\n\n1\n\n\n0\n\n\n0\n\n\n0\n\n\n\n\n4\n\n\n104\n\n\nLow\n\n\nFemale\n\n\nAsian\n\n\n55.17864476\n\n\nSmoker\n\n\n138\n\n\n4.956521739\n\n\n5.217391304\n\n\n5.30435\n\n\n4.89130\n\n\n0.34783\n\n\n-0.32609\n\n\n0\n\n\n0\n\n\n1\n\n\n0\n\n\n0\n\n\n\n\n5\n\n\n105\n\n\nPlacebo\n\n\nFemale\n\n\nAfrican American\n\n\n43.79739904\n\n\nSmoker\n\n\n168\n\n\n1.773809524\n\n\n3.363095238\n\n\n1.45238\n\n\n2.89881\n\n\n-0.32143\n\n\n-0.46429\n\n\n1\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n\n\n6\n\n\n106\n\n\nMedium\n\n\nFemale\n\n\nAsian\n\n\n42.14921287\n\n\nSmoker\n\n\n168\n\n\n2.369047619\n\n\n3.910714286\n\n\n1.92262\n\n\n3.08333\n\n\n-0.44643\n\n\n-0.82738\n\n\n0\n\n\n0\n\n\n0\n\n\n1\n\n\n0\n\n\n\n\n\n\n\n\n\n\n\u001449                                                         The SAS System                      Sunday, November  3, 2024 06:45:00 AM\n\n506        ods listing close;ods html5 (id=saspy_internal) file=_tomods1 options(bitmap_mode='inline') device=svg style=HTMLBlue;\n506      ! ods graphics on / outputfmt=png;\n507        \n508        * Create Dummy Variables;\n509        TITLE \"Create Dummy Variables\";\n510        DATA Proj1.data;\n511            SET Proj1.data;\n512            IF trtgroup ne . THEN DO;\n513                placebo = (trtgroup = 1);\n514                control = (trtgroup = 2);\n515                low     = (trtgroup = 3);\n516                medium  = (trtgroup = 4);\n517                high    = (trtgroup = 5);\n518         END;\n519        RUN;\n520        \n521        * Check Dummy Variables Created Correctly;\n522        PROC PRINT DATA = Proj1.data (OBS = 6);\n523         RUN;\n524        \n525        \n526        ods html5 (id=saspy_internal) close;ods listing;\n527        \n\u001450                                                         The SAS System                      Sunday, November  3, 2024 06:45:00 AM\n\n528        \n\n\n\n\n\n* Run regression of attachment loss by treatment condition with control as reference;\nTITLE \"SLR of Attachment Loss Change by Treatment Condition - Control as Reference\";\nPROC REG DATA = Proj1.data;\n    MODEL attachchange = placebo low medium high;\n    RUN;\n\nlstlog\n\n\n&lt;!DOCTYPE html&gt;\n\n\n\n\n\nSAS Output\n\n\n\n\n\n\n\nSLR of Attachment Loss Change by Treatment Condition - Control as Reference\n\n\n\n\nThe REG Procedure\n\n\nModel: MODEL1\n\n\nDependent Variable: attachchange Attachment Loss Change\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNumber of Observations Read\n\n\n130\n\n\n\n\nNumber of Observations Used\n\n\n103\n\n\n\n\nNumber of Observations with Missing Values\n\n\n27\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnalysis of Variance\n\n\n\n\nSource\n\n\nDF\n\n\nSum ofSquares\n\n\nMeanSquare\n\n\nF Value\n\n\nPr &gt; F\n\n\n\n\n\n\nModel\n\n\n4\n\n\n0.72806\n\n\n0.18202\n\n\n2.53\n\n\n0.0451\n\n\n\n\nError\n\n\n98\n\n\n7.04360\n\n\n0.07187\n\n\n \n\n\n \n\n\n\n\nCorrected Total\n\n\n102\n\n\n7.77166\n\n\n \n\n\n \n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRoot MSE\n\n\n0.26809\n\n\nR-Square\n\n\n0.0937\n\n\n\n\nDependent Mean\n\n\n-0.09945\n\n\nAdj R-Sq\n\n\n0.0567\n\n\n\n\nCoeff Var\n\n\n-269.57211\n\n\n \n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nParameter Estimates\n\n\n\n\nVariable\n\n\nLabel\n\n\nDF\n\n\nParameterEstimate\n\n\nStandardError\n\n\nt Value\n\n\nPr &gt; |t|\n\n\n\n\n\n\nIntercept\n\n\nIntercept\n\n\n1\n\n\n-0.22169\n\n\n0.05590\n\n\n-3.97\n\n\n0.0001\n\n\n\n\nplacebo\n\n\n \n\n\n1\n\n\n0.13462\n\n\n0.07906\n\n\n1.70\n\n\n0.0918\n\n\n\n\nlow\n\n\n \n\n\n1\n\n\n0.20388\n\n\n0.08092\n\n\n2.52\n\n\n0.0134\n\n\n\n\nmedium\n\n\n \n\n\n1\n\n\n0.21514\n\n\n0.08197\n\n\n2.62\n\n\n0.0101\n\n\n\n\nhigh\n\n\n \n\n\n1\n\n\n0.05690\n\n\n0.08728\n\n\n0.65\n\n\n0.5159\n\n\n\n\n\n\n\n\n\n\nSLR of Attachment Loss Change by Treatment Condition - Control as Reference\n\n\n\n\nThe REG Procedure\n\n\nModel: MODEL1\n\n\nDependent Variable: attachchange Attachment Loss Change\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001451                                                         The SAS System                      Sunday, November  3, 2024 06:45:00 AM\n\n531        ods listing close;ods html5 (id=saspy_internal) file=_tomods1 options(bitmap_mode='inline') device=svg style=HTMLBlue;\n531      ! ods graphics on / outputfmt=png;\n532        \n533        * Run regression of attachment loss by treatment condition with control as reference;\n534        TITLE \"SLR of Attachment Loss Change by Treatment Condition - Control as Reference\";\n535        PROC REG DATA = Proj1.data;\n536         MODEL attachchange = placebo low medium high;\n537         RUN;\n538        \n539        \n540        ods html5 (id=saspy_internal) close;ods listing;\n541        \n\u001452                                                         The SAS System                      Sunday, November  3, 2024 06:45:00 AM\n\n542        \n\n\n\n\n\nInterpretation\nThe overall model is significant (F(4,98) = 2.53, p = 0.0451). Looking at the individual t-statistics, we see that - following adjustment for multiple pairwise comparisons - the placebo group (p = 0.549), low concentration group (p = 0.0668), and high concentration group (p = 1.000) are not statistically different from the control group. The medium concentration group is approaching significance (p = 0.0503).\n\n\nChange Reference to Placebo Group\nThat was with the no treatment control group as the reference. Let’s see if any of our treatment conditions were different from the placebo group.\n\nTITLE \"SLR of Attachment Loss Change by Treatment Condition - Placebo as Reference\";\nPROC REG DATA = Proj1.data;\n    MODEL attachchange = control low medium high;\n    RUN;\n\nlstlog\n\n\n&lt;!DOCTYPE html&gt;\n\n\n\n\n\nSAS Output\n\n\n\n\n\n\n\n\n\n\nSLR of Attachment Loss Change by Treatment Condition - Placebo as Reference\n\n\n\n\nThe REG Procedure\n\n\nModel: MODEL1\n\n\nDependent Variable: attachchange Attachment Loss Change\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNumber of Observations Read\n\n\n130\n\n\n\n\nNumber of Observations Used\n\n\n103\n\n\n\n\nNumber of Observations with Missing Values\n\n\n27\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnalysis of Variance\n\n\n\n\nSource\n\n\nDF\n\n\nSum ofSquares\n\n\nMeanSquare\n\n\nF Value\n\n\nPr &gt; F\n\n\n\n\n\n\nModel\n\n\n4\n\n\n0.72806\n\n\n0.18202\n\n\n2.53\n\n\n0.0451\n\n\n\n\nError\n\n\n98\n\n\n7.04360\n\n\n0.07187\n\n\n \n\n\n \n\n\n\n\nCorrected Total\n\n\n102\n\n\n7.77166\n\n\n \n\n\n \n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRoot MSE\n\n\n0.26809\n\n\nR-Square\n\n\n0.0937\n\n\n\n\nDependent Mean\n\n\n-0.09945\n\n\nAdj R-Sq\n\n\n0.0567\n\n\n\n\nCoeff Var\n\n\n-269.57211\n\n\n \n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nParameter Estimates\n\n\n\n\nVariable\n\n\nLabel\n\n\nDF\n\n\nParameterEstimate\n\n\nStandardError\n\n\nt Value\n\n\nPr &gt; |t|\n\n\n\n\n\n\nIntercept\n\n\nIntercept\n\n\n1\n\n\n-0.08707\n\n\n0.05590\n\n\n-1.56\n\n\n0.1225\n\n\n\n\ncontrol\n\n\n \n\n\n1\n\n\n-0.13462\n\n\n0.07906\n\n\n-1.70\n\n\n0.0918\n\n\n\n\nlow\n\n\n \n\n\n1\n\n\n0.06926\n\n\n0.08092\n\n\n0.86\n\n\n0.3941\n\n\n\n\nmedium\n\n\n \n\n\n1\n\n\n0.08052\n\n\n0.08197\n\n\n0.98\n\n\n0.3284\n\n\n\n\nhigh\n\n\n \n\n\n1\n\n\n-0.07772\n\n\n0.08728\n\n\n-0.89\n\n\n0.3754\n\n\n\n\n\n\n\n\n\n\nSLR of Attachment Loss Change by Treatment Condition - Placebo as Reference\n\n\n\n\nThe REG Procedure\n\n\nModel: MODEL1\n\n\nDependent Variable: attachchange Attachment Loss Change\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001453                                                         The SAS System                      Sunday, November  3, 2024 06:45:00 AM\n\n545        ods listing close;ods html5 (id=saspy_internal) file=_tomods1 options(bitmap_mode='inline') device=svg style=HTMLBlue;\n545      ! ods graphics on / outputfmt=png;\n546        \n547        TITLE \"SLR of Attachment Loss Change by Treatment Condition - Placebo as Reference\";\n548        PROC REG DATA = Proj1.data;\n549         MODEL attachchange = control low medium high;\n550         RUN;\n551        \n552        \n553        ods html5 (id=saspy_internal) close;ods listing;\n554        \n\u001454                                                         The SAS System                      Sunday, November  3, 2024 06:45:00 AM\n\n555        \n\n\n\n\nAfter applying a bonferroni correction, none of the groups are significantly different from the placebo group (p &gt; .05).\n\n\nConclusion\nWhile the overall model was significant, the only groups that were statistically different from the no treatment control were the low and medium concentration gel groups (p &lt; 0.05). However, since this was a placebo-controlled RCT, and none of the treatment groups were significantly different from the placebo group, we can conclude that we fail to reject the null hypothesis that the average attachment loss over 1 year is the same between all groups.\nBack to top of tabset\n\n\n\nNow we will create our model with pocket depth change as the dependent variable and treatment group as the independent variable.\n\n* Simple Linear Regression Predicting Pocket Depth Change by Treatment Condition;\nTITLE \"SLR of Pocket Depth Change by Treatment Condition - Control as Reference\";\nPROC REG DATA = Proj1.data;\n    MODEL pdchange = placebo low medium high;\n    RUN;\n\nlstlog\n\n\n&lt;!DOCTYPE html&gt;\n\n\n\n\n\nSAS Output\n\n\n\n\n\n\n\n\n\n\nSLR of Pocket Depth Change by Treatment Condition - Control as Reference\n\n\n\n\nThe REG Procedure\n\n\nModel: MODEL1\n\n\nDependent Variable: pdchange Pocket Depth Change\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNumber of Observations Read\n\n\n130\n\n\n\n\nNumber of Observations Used\n\n\n103\n\n\n\n\nNumber of Observations with Missing Values\n\n\n27\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnalysis of Variance\n\n\n\n\nSource\n\n\nDF\n\n\nSum ofSquares\n\n\nMeanSquare\n\n\nF Value\n\n\nPr &gt; F\n\n\n\n\n\n\nModel\n\n\n4\n\n\n0.57020\n\n\n0.14255\n\n\n2.07\n\n\n0.0899\n\n\n\n\nError\n\n\n98\n\n\n6.73457\n\n\n0.06872\n\n\n \n\n\n \n\n\n\n\nCorrected Total\n\n\n102\n\n\n7.30477\n\n\n \n\n\n \n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRoot MSE\n\n\n0.26215\n\n\nR-Square\n\n\n0.0781\n\n\n\n\nDependent Mean\n\n\n-0.29435\n\n\nAdj R-Sq\n\n\n0.0404\n\n\n\n\nCoeff Var\n\n\n-89.05770\n\n\n \n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nParameter Estimates\n\n\n\n\nVariable\n\n\nLabel\n\n\nDF\n\n\nParameterEstimate\n\n\nStandardError\n\n\nt Value\n\n\nPr &gt; |t|\n\n\n\n\n\n\nIntercept\n\n\nIntercept\n\n\n1\n\n\n-0.33817\n\n\n0.05466\n\n\n-6.19\n\n\n&lt;.0001\n\n\n\n\nplacebo\n\n\n \n\n\n1\n\n\n-0.01152\n\n\n0.07730\n\n\n-0.15\n\n\n0.8818\n\n\n\n\nlow\n\n\n \n\n\n1\n\n\n0.13200\n\n\n0.07912\n\n\n1.67\n\n\n0.0984\n\n\n\n\nmedium\n\n\n \n\n\n1\n\n\n0.13562\n\n\n0.08015\n\n\n1.69\n\n\n0.0938\n\n\n\n\nhigh\n\n\n \n\n\n1\n\n\n-0.04413\n\n\n0.08534\n\n\n-0.52\n\n\n0.6063\n\n\n\n\n\n\n\n\n\n\nSLR of Pocket Depth Change by Treatment Condition - Control as Reference\n\n\n\n\nThe REG Procedure\n\n\nModel: MODEL1\n\n\nDependent Variable: pdchange Pocket Depth Change\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001455                                                         The SAS System                      Sunday, November  3, 2024 06:45:00 AM\n\n558        ods listing close;ods html5 (id=saspy_internal) file=_tomods1 options(bitmap_mode='inline') device=svg style=HTMLBlue;\n558      ! ods graphics on / outputfmt=png;\n559        \n560        * Simple Linear Regression Predicting Pocket Depth Change by Treatment Condition;\n561        TITLE \"SLR of Pocket Depth Change by Treatment Condition - Control as Reference\";\n562        PROC REG DATA = Proj1.data;\n563         MODEL pdchange = placebo low medium high;\n564         RUN;\n565        \n566        \n567        ods html5 (id=saspy_internal) close;ods listing;\n568        \n\u001456                                                         The SAS System                      Sunday, November  3, 2024 06:45:00 AM\n\n569        \n\n\n\n\nThe overall model is not statistically significant (F(4,98)= 2.074, p = 0.0899). Based on this model, it appears that the gel treatment has no effect on pocket depth change after 1 year.\nBack to top of tabset\n\n\nWe did not find a main effect based on treatment group. However I am still interested if including any of the covariates changes the results.\n\nModel Including Gender\n\n* Try a model of attachment loss change by treatment condition, controlling for gender;\nTITLE \"MLR of Attachment Loss Change by Treatment Condition and Gender\";\nPROC REG DATA = Proj1.data;\n    MODEL attachchange = placebo low medium high gender;\n    RUN;\n\nlstlog\n\n\n&lt;!DOCTYPE html&gt;\n\n\n\n\n\nSAS Output\n\n\n\n\n\n\n\n\n\n\nMLR of Attachment Loss Change by Treatment Condition and Gender\n\n\n\n\nThe REG Procedure\n\n\nModel: MODEL1\n\n\nDependent Variable: attachchange Attachment Loss Change\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNumber of Observations Read\n\n\n130\n\n\n\n\nNumber of Observations Used\n\n\n103\n\n\n\n\nNumber of Observations with Missing Values\n\n\n27\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnalysis of Variance\n\n\n\n\nSource\n\n\nDF\n\n\nSum ofSquares\n\n\nMeanSquare\n\n\nF Value\n\n\nPr &gt; F\n\n\n\n\n\n\nModel\n\n\n5\n\n\n0.89852\n\n\n0.17970\n\n\n2.54\n\n\n0.0335\n\n\n\n\nError\n\n\n97\n\n\n6.87314\n\n\n0.07086\n\n\n \n\n\n \n\n\n\n\nCorrected Total\n\n\n102\n\n\n7.77166\n\n\n \n\n\n \n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRoot MSE\n\n\n0.26619\n\n\nR-Square\n\n\n0.1156\n\n\n\n\nDependent Mean\n\n\n-0.09945\n\n\nAdj R-Sq\n\n\n0.0700\n\n\n\n\nCoeff Var\n\n\n-267.65934\n\n\n \n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nParameter Estimates\n\n\n\n\nVariable\n\n\nLabel\n\n\nDF\n\n\nParameterEstimate\n\n\nStandardError\n\n\nt Value\n\n\nPr &gt; |t|\n\n\n\n\n\n\nIntercept\n\n\nIntercept\n\n\n1\n\n\n-0.07460\n\n\n0.10988\n\n\n-0.68\n\n\n0.4988\n\n\n\n\nplacebo\n\n\n \n\n\n1\n\n\n0.12330\n\n\n0.07883\n\n\n1.56\n\n\n0.1210\n\n\n\n\nlow\n\n\n \n\n\n1\n\n\n0.19310\n\n\n0.08064\n\n\n2.39\n\n\n0.0186\n\n\n\n\nmedium\n\n\n \n\n\n1\n\n\n0.21118\n\n\n0.08143\n\n\n2.59\n\n\n0.0110\n\n\n\n\nhigh\n\n\n \n\n\n1\n\n\n0.06704\n\n\n0.08690\n\n\n0.77\n\n\n0.4423\n\n\n\n\ngender\n\n\nGender\n\n\n1\n\n\n-0.08675\n\n\n0.05593\n\n\n-1.55\n\n\n0.1242\n\n\n\n\n\n\n\n\n\n\nMLR of Attachment Loss Change by Treatment Condition and Gender\n\n\n\n\nThe REG Procedure\n\n\nModel: MODEL1\n\n\nDependent Variable: attachchange Attachment Loss Change\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001457                                                         The SAS System                      Sunday, November  3, 2024 06:45:00 AM\n\n572        ods listing close;ods html5 (id=saspy_internal) file=_tomods1 options(bitmap_mode='inline') device=svg style=HTMLBlue;\n572      ! ods graphics on / outputfmt=png;\n573        \n574        * Try a model of attachment loss change by treatment condition, controlling for gender;\n575        TITLE \"MLR of Attachment Loss Change by Treatment Condition and Gender\";\n576        PROC REG DATA = Proj1.data;\n577         MODEL attachchange = placebo low medium high gender;\n578         RUN;\n579        \n580        \n581        ods html5 (id=saspy_internal) close;ods listing;\n582        \n\u001458                                                         The SAS System                      Sunday, November  3, 2024 06:45:00 AM\n\n583        \n\n\n\n\nA model including gender does not seem to help anything. What about with all covariates?\n\n\nSaturated Model\n\nTITLE \"MLR of Pocket Depth Change by Treatment Condition and Gender\";\nPROC REG DATA = Proj1.data;\n    MODEL pdchange = placebo low medium high gender;\n    RUN;\n\nlstlog\n\n\n&lt;!DOCTYPE html&gt;\n\n\n\n\n\nSAS Output\n\n\n\n\n\n\n\n\n\n\nMLR of Pocket Depth Change by Treatment Condition and Gender\n\n\n\n\nThe REG Procedure\n\n\nModel: MODEL1\n\n\nDependent Variable: pdchange Pocket Depth Change\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNumber of Observations Read\n\n\n130\n\n\n\n\nNumber of Observations Used\n\n\n103\n\n\n\n\nNumber of Observations with Missing Values\n\n\n27\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnalysis of Variance\n\n\n\n\nSource\n\n\nDF\n\n\nSum ofSquares\n\n\nMeanSquare\n\n\nF Value\n\n\nPr &gt; F\n\n\n\n\n\n\nModel\n\n\n5\n\n\n0.85534\n\n\n0.17107\n\n\n2.57\n\n\n0.0313\n\n\n\n\nError\n\n\n97\n\n\n6.44943\n\n\n0.06649\n\n\n \n\n\n \n\n\n\n\nCorrected Total\n\n\n102\n\n\n7.30477\n\n\n \n\n\n \n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRoot MSE\n\n\n0.25785\n\n\nR-Square\n\n\n0.1171\n\n\n\n\nDependent Mean\n\n\n-0.29435\n\n\nAdj R-Sq\n\n\n0.0716\n\n\n\n\nCoeff Var\n\n\n-87.60007\n\n\n \n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nParameter Estimates\n\n\n\n\nVariable\n\n\nLabel\n\n\nDF\n\n\nParameterEstimate\n\n\nStandardError\n\n\nt Value\n\n\nPr &gt; |t|\n\n\n\n\n\n\nIntercept\n\n\nIntercept\n\n\n1\n\n\n-0.14793\n\n\n0.10644\n\n\n-1.39\n\n\n0.1678\n\n\n\n\nplacebo\n\n\n \n\n\n1\n\n\n-0.02615\n\n\n0.07636\n\n\n-0.34\n\n\n0.7327\n\n\n\n\nlow\n\n\n \n\n\n1\n\n\n0.11806\n\n\n0.07812\n\n\n1.51\n\n\n0.1339\n\n\n\n\nmedium\n\n\n \n\n\n1\n\n\n0.13050\n\n\n0.07888\n\n\n1.65\n\n\n0.1013\n\n\n\n\nhigh\n\n\n \n\n\n1\n\n\n-0.03102\n\n\n0.08418\n\n\n-0.37\n\n\n0.7133\n\n\n\n\ngender\n\n\nGender\n\n\n1\n\n\n-0.11219\n\n\n0.05418\n\n\n-2.07\n\n\n0.0410\n\n\n\n\n\n\n\n\n\n\nMLR of Pocket Depth Change by Treatment Condition and Gender\n\n\n\n\nThe REG Procedure\n\n\nModel: MODEL1\n\n\nDependent Variable: pdchange Pocket Depth Change\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001459                                                         The SAS System                      Sunday, November  3, 2024 06:45:00 AM\n\n586        ods listing close;ods html5 (id=saspy_internal) file=_tomods1 options(bitmap_mode='inline') device=svg style=HTMLBlue;\n586      ! ods graphics on / outputfmt=png;\n587        \n588        TITLE \"MLR of Pocket Depth Change by Treatment Condition and Gender\";\n589        PROC REG DATA = Proj1.data;\n590         MODEL pdchange = placebo low medium high gender;\n591         RUN;\n592        \n593        \n594        ods html5 (id=saspy_internal) close;ods listing;\n595        \n\u001460                                                         The SAS System                      Sunday, November  3, 2024 06:45:00 AM\n\n596        \n\n\n\n\nNope. Interestingly the best predictor of attachment loss at 1 year is attachment loss at baseline. The results for pocket depth are likely the same.\n\n\n\nThe beauty of SAS is that the output to check all of the assumptions is included in the output of each model!\nThus we have seen that in each model so far, all of our assumptions have been met!"
  },
  {
    "objectID": "Project_2/Project_2_R/Code/Project2LMM.html",
    "href": "Project_2/Project_2_R/Code/Project2LMM.html",
    "title": "Linear Mixed Model",
    "section": "",
    "text": "The objective of this project is to gain experience in the application of Linear Mixed Models for statistical analysis.\nThis is the exact same data set as Project 2. Please see Project 2 - MLR with Confounding and Interaction for a detailed description of the data cleaning process.\n\n\nThe aim of the current study is to assess how treatment response differs for HIV+ patients 2 years after initiating Highly Active Antiretroviral Therapy (HAART) based on hard drug usage (such as heroin or cocaine). This study is of particular scientific interest because it is unclear whether the use of hard drugs inhibits the immune system in humans; treatment strategies may differ based on these results. The researchers are interested in comparing subjects who never used hard drugs to current hard drug users (those that use hard drugs at year 2) or previous hard drug users (those who used drugs at year 0 or 1). Outcomes of interest are: viral load (HIV copies in a mL of blood), CD4+ T cell count (a measure of immunologic health), and aggregate physical and quality of life scores from the SF-36.\nThe clinical hypothesis is that, if hard drugs inhibit the immune system in humans, subjects who currently or previously used hard drugs will have higher viral load and lower CD4+ T cell counts than those who never used hard drugs. Additionally, the researchers are interested in knowing if potential differences between the drug use groups can be explained by differences in adherence to the treatment regimen. The researchers are agnostic on how quality of life changes after treatment, since side effects of the treatment are significant.\nThe project description provided by the PI is available below:"
  },
  {
    "objectID": "Project_2/Project_2_R/Code/Project2LMM.html#project-description",
    "href": "Project_2/Project_2_R/Code/Project2LMM.html#project-description",
    "title": "Linear Mixed Model",
    "section": "",
    "text": "The aim of the current study is to assess how treatment response differs for HIV+ patients 2 years after initiating Highly Active Antiretroviral Therapy (HAART) based on hard drug usage (such as heroin or cocaine). This study is of particular scientific interest because it is unclear whether the use of hard drugs inhibits the immune system in humans; treatment strategies may differ based on these results. The researchers are interested in comparing subjects who never used hard drugs to current hard drug users (those that use hard drugs at year 2) or previous hard drug users (those who used drugs at year 0 or 1). Outcomes of interest are: viral load (HIV copies in a mL of blood), CD4+ T cell count (a measure of immunologic health), and aggregate physical and quality of life scores from the SF-36.\nThe clinical hypothesis is that, if hard drugs inhibit the immune system in humans, subjects who currently or previously used hard drugs will have higher viral load and lower CD4+ T cell counts than those who never used hard drugs. Additionally, the researchers are interested in knowing if potential differences between the drug use groups can be explained by differences in adherence to the treatment regimen. The researchers are agnostic on how quality of life changes after treatment, since side effects of the treatment are significant.\nThe project description provided by the PI is available below:"
  },
  {
    "objectID": "Project_2/Project_2_R/Code/Project2LMM.html#Spaghetti",
    "href": "Project_2/Project_2_R/Code/Project2LMM.html#Spaghetti",
    "title": "Linear Mixed Model",
    "section": "Spaghetti Plots",
    "text": "Spaghetti Plots\nIn order to get an initial feel for the data, we will create some spaghetti plots to visualize each outcome variable for all patients over 2 years.\n\nViral LoadCD4+ T Cell CountMental QOLPhysical QOLSummary\n\n\nLet’s examine log viral load since that is our main PEV.\n\n# Create log vload for 8 year dataset\ndata &lt;- data |&gt; \n  mutate(VLOAD_log = log(VLOAD))\n\n# Turn newid into a factor\ndata &lt;- data |&gt; \n  mutate(newid = factor(newid),\n         ADH = factor(ADH))\n  \n# Create spaghetti plot for log viral load\nggplot(data, aes(y = VLOAD_log, x = years, group = newid)) +\n  geom_line(alpha = 0.1) +\n  theme_minimal() +\n  labs(title = \"Spaghetti Plot of Log Viral Load\",\n       x = \"Year\",\n       y = \"Log Viral Load (copies/mL)\")\n\n\n\n\n\n\n\n\nThat’s interesting, we can see that for most patients, they had a decrease in log viral load over the course of the study.\nI wonder if those patients with higher viral loads tend to be those who are hard drug users. Let’s explore.\n\n# Create spaghetti plot for log viral load\nggplot(data, aes(y = VLOAD_log, x = years, group = newid, color = as.factor(hard_drugs))) +\n  geom_line(alpha = 0.2) +\n  theme_minimal() +\n  labs(title = \"Spaghetti Plot of Log Viral Load\",\n       x = \"Year\",\n       y = \"Log Viral Load (copies/mL)\")\n\nWarning: Removed 86 rows containing missing values or values outside the scale range\n(`geom_line()`).\n\n\n\n\n\n\n\n\n\nHuh, that’s interesting. There seem to really only be a few hard drug users in the sample. Additionally, if patients used hard drugs, they seem to have consistenly used them throughout the study and never quit.\n\ntable(data$hard_drugs)\n\n\n   0    1 \n3215  406 \n\n\nAbout 12.6% of visits were by patients who used hard drugs since the previous visit.\nThese plots are interesting, but there are too many data points to extrapolate any more meaningful information from them.\nLet’s move on to plotting the averages of each outcome variable based on hard drug use and protocol adherence.\nTop of Tabset\n\n\nLet’s examine what happened to leukocyte count over the course of the study.\n\n# Create log vload for 8 year dataset\ndata &lt;- data |&gt; \n  mutate(VLOAD_log = log(VLOAD))\n\n# Turn newid into a factor\ndata &lt;- data |&gt; \n  mutate(newid = factor(newid),\n         ADH = factor(ADH))\n  \n# Create spaghetti plot for log viral load\nggplot(data, aes(y = LEU3N, x = years, group = newid)) +\n  geom_line(alpha = 0.1) +\n  theme_minimal() +\n  labs(title = \"Spaghetti Plot of CD4+ T Cell Count\",\n       x = \"Year\",\n       y = \"CD4+ T Cell Count\")\n\n\n\n\n\n\n\n\nFor the most part it increasd, with a lot of variation for some patients.\nTop of Tabset\n\n\nNow let’s examine how mental QOL changed over time.\n\n# Create log vload for 8 year dataset\ndata &lt;- data |&gt; \n  mutate(VLOAD_log = log(VLOAD))\n\n# Turn newid into a factor\ndata &lt;- data |&gt; \n  mutate(newid = factor(newid),\n         ADH = factor(ADH))\n  \n# Create spaghetti plot for log viral load\nggplot(data, aes(y = AGG_MENT, x = years, group = newid)) +\n  geom_line(alpha = 0.1) +\n  theme_minimal() +\n  labs(title = \"Spaghetti Plot of Mental QOL\",\n       x = \"Year\",\n       y = \"Mental QOL\")\n\n\n\n\n\n\n\n\nIt stayed high for the most part but with a LOT of variation for some patients.\nThis looks like good evidnece to include random slopes into our model for mental QOL.\nTop of Tabset\n\n\nFinally let’s assess how physical QOL changed over time.\n\n# Create log vload for 8 year data set\ndata &lt;- data |&gt; \n  mutate(VLOAD_log = log(VLOAD))\n\n# Turn newid into a factor\ndata &lt;- data |&gt; \n  mutate(newid = factor(newid),\n         ADH = factor(ADH))\n  \n# Create spaghetti plot for log viral load\nggplot(data, aes(y = AGG_PHYS, x = years, group = newid)) +\n  geom_line(alpha = 0.1) +\n  theme_minimal() +\n  labs(title = \"Spaghetti Plot of Physical QOL\",\n       x = \"Year\",\n       y = \"Physical QOL\")\n\n\n\n\n\n\n\n\nJust like mental QOL, it was high for the most part, but with a lot of variation within patients.\nThis is good evidence for including random slopes into our model for physical QOL.\nTop of Tabset\n\n\nWe saw over the 8 year course of the study an overall decrease in viral load and increase in CD4+ T Cell count.\nThe mental and physical QOL scores appeared remain about the same overall, but with a lot of variation within patients. We will include random slopes into these models to account for this variation."
  },
  {
    "objectID": "Project_2/Project_2_R/Code/Project2LMM.html#Bivariate",
    "href": "Project_2/Project_2_R/Code/Project2LMM.html#Bivariate",
    "title": "Linear Mixed Model",
    "section": "Bivariate Relationships",
    "text": "Bivariate Relationships\nSpaghetti plots were informative to see general trends in each outcome variable over the course of the study, but they don’t give us a fine tuned look into the important bivariate relationships between the outcome variables and our two primary explanatory variables: hard_drugs_grp and ADH_HIGHVSLOW.\nTo do that, we will plot the average of each outcome group by our PEVs.\n\nMain Outcome Variables\n\nViral LoadCD4+ T Cell CountMental QOLPhysical QOLSummary\n\n\n\nViral Load by Hard Drugs Group\n\n# calculate the average vload log at each year for each hard drugs use group\nsummary_data &lt;- data_2 |&gt; \n  group_by(hard_drugs_grp, years) |&gt; \n  summarize(avg_VLOAD_log = mean(VLOAD_log, na.rm = TRUE))\n\n# Plot\nggplot(summary_data, aes(x=years, y= avg_VLOAD_log, color = hard_drugs_grp)) +\n  geom_line(size = 1) + \n  scale_color_brewer(palette = \"Pastel2\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nInterestingly, it appears that previous users have the lowest average log viral load over 2 years. This difference may not be statistically significant however.\n\n\nViral Load by Adherence\n\n# calculate the average vload log at each year for each hard drugs use group\nsummary_data &lt;- data_2 |&gt; \n  group_by(ADH_HIGHVSLOW, years) |&gt; \n  summarize(avg_VLOAD_log = mean(VLOAD_log, na.rm = TRUE))\n\n# Plot\nggplot(summary_data, aes(x=years, y= avg_VLOAD_log, color = ADH_HIGHVSLOW)) +\n  geom_line(size = 1) +\n  theme_minimal() +\n  scale_color_brewer(palette = \"Pastel2\")\n\n\n\n\n\n\n\n\nThis shows the expected result, that those with high adherence have lower viral load. Additionally, they have different slopes, so there’s an interaction with time here.\n\n\nSummary\nThose with low adherence have a higher average log viral load. Interestingly, previous users have a lower log viral load, but this may not be statistically significant in the full model.\nTop of Tabset\n\n\n\n\nCD4+ T Cell Count by Hard Drug Use\n\n# Calculate the average CD4+ T Cell Count at each year for each hard drugs use group\nsummary_data &lt;- data_2 |&gt; \n  group_by(hard_drugs_grp, years) |&gt; \n  summarize(avg_LEU3N = mean(LEU3N, na.rm = TRUE))\n\n# Plot\nggplot(summary_data, aes(x=years, y= avg_LEU3N, color = hard_drugs_grp)) +\n  geom_line(size = 1) +\n  theme_minimal() +\n  scale_color_brewer(palette = \"Pastel2\")\n\n\n\n\n\n\n\n\nHere it appears that current drug users have the highest leukocyte count, and previous drug users have the lowest. This is not really as expected. Never users should have the highest white blood cell count since they are the healthiest.\n\n\nCD4+ T Cell Count by Adherence\n\n# Calculate the average CD4+ T Cell Count at each year by adherence\nsummary_data &lt;- data_2 |&gt; \n  group_by(ADH_HIGHVSLOW, years) |&gt; \n  summarize(avg_LEU3N = mean(LEU3N, na.rm = TRUE))\n\n# Plot\nggplot(summary_data, aes(x=years, y= avg_LEU3N, color = ADH_HIGHVSLOW)) +\n  geom_line(size = 1) +\n  theme_minimal() +\n  scale_color_brewer(palette = \"Pastel2\")\n\n\n\n\n\n\n\n\nThose with low adherence have lower leukocyte count. That’s expected. The slope does not change by adherence group.\n\n\nSummary\nPrevious hard drug users and those with low adherence have lower white blood cell count.\nTop of Tabset\n\n\n\n\nMental QOL by Hard Drug Use\n\n# Calculate the average mental QOL at each year for each hard drugs use group\nsummary_data &lt;- data_2 |&gt; \n  group_by(hard_drugs_grp, years) |&gt; \n  summarize(avg_AGG_MENT = mean(AGG_MENT, na.rm = TRUE))\n\n# Plot\nggplot(summary_data, aes(x=years, y= avg_AGG_MENT, color = hard_drugs_grp)) +\n  geom_line(size = 1) +\n  theme_minimal() +\n  scale_color_brewer(palette = \"Pastel2\")\n\n\n\n\n\n\n\n\nThis shows that previous users have the lowest average mental QOL, and never users the highest. This is as expected, especially if we consider that previous users are likely dealing with the side effects of recently quitting hard drugs.\n\n\nMental QOL by Adherence\n\n# Calculate the average mental QOL at each year by adherence\nsummary_data &lt;- data_2 |&gt; \n  group_by(ADH_HIGHVSLOW, years) |&gt; \n  summarize(avg_AGG_MENT = mean(AGG_MENT, na.rm = TRUE))\n\n# Plot\nggplot(summary_data, aes(x=years, y= avg_AGG_MENT, color = ADH_HIGHVSLOW)) +\n  geom_line(size = 1) +\n  theme_minimal() +\n  scale_color_brewer(palette = \"Pastel2\")\n\n\n\n\n\n\n\n\nThose with high adherence have a drastically higher mental QOL score. That’s expected.\n\n\nSummary\nThose with low adherence have worse mental QOL, and previous users have worse mental QOL.\nTop of Tabset\n\n\n\n\nPhysical QOL by Hard Drug Use\n\n# Calculate the average physical QOL at each year for each hard drugs use group\nsummary_data &lt;- data_2 |&gt; \n  group_by(hard_drugs_grp, years) |&gt; \n  summarize(avg_AGG_PHYS = mean(AGG_PHYS, na.rm = TRUE))\n\n# Plot\nggplot(summary_data, aes(x=years, y= avg_AGG_PHYS, color = hard_drugs_grp)) +\n  geom_line(size = 1) +\n  theme_minimal() +\n  scale_color_brewer(palette = \"Pastel2\")\n\n\n\n\n\n\n\n\nOverall it appears that never users have higher physical QOL across the study. While initally high, the physical QOL for current users plummets after year 1. Additionally, previous users have the lowest physical QOL, which also makes sense with the understanding we’ve mentioned that previous users are dealing with the side effects of quitting and are lacking their coping mechanism.\n\n\nPhysical QOL by Adherence\n\n# Calculate the average physical QOL at each year for each hard drugs use group\nsummary_data &lt;- data_2 |&gt; \n  group_by(ADH_HIGHVSLOW, years) |&gt; \n  summarize(avg_AGG_PHYS = mean(AGG_PHYS, na.rm = TRUE))\n\n# Plot\nggplot(summary_data, aes(x=years, y= avg_AGG_PHYS, color = ADH_HIGHVSLOW)) +\n  geom_line(size = 1) +\n  theme_minimal() +\n  scale_color_brewer(palette = \"Pastel2\")\n\n\n\n\n\n\n\n\n\n\nSummary\nThose with low adherence have worse physical QOL, and previous users have worse physical QOL.\nTop of Tabset\n\n\n\nFor hard drug use group, previous users have:\n\nThe lowest average log viral load\nThe lowest leukocyte count, and highest mental and physical QOL.\n\nFor adherence, those with low adherence have:\n\nThe highest average log viral load\nThe lowest leukocyte count, and lowest mental and physical QOL.\n\nTop of Tabset\n\n\n\n\n\nCovariates\n\nDepressionFrailty Related PhenotypeCollege Education\n\n\nDepression was a confounder in the model for mental QOL and must be accounted for.\n\n# Calculate the average depression at each year for each hard drugs use group\nsummary_data &lt;- data_2 |&gt; \n  group_by(hard_drugs_grp, years) |&gt; \n  summarize(avg_CESD = mean(CESD, na.rm = TRUE))\n\n# Plot\nggplot(summary_data, aes(x=years, y= avg_CESD, color = hard_drugs_grp)) +\n  geom_line(size = 1) +\n  theme_minimal() +\n  scale_color_brewer(palette = \"Pastel2\")\n\n\n\n\n\n\n\n\nWe see the expected result, previous drug users are more depressed.\n\n# Calculate the average mental QOL at each year by adherence\nsummary_data &lt;- data_2 |&gt; \n  group_by(ADH_HIGHVSLOW, years) |&gt; \n  summarize(avg_CESD = mean(CESD, na.rm = TRUE))\n\n# Plot\nggplot(summary_data, aes(x=years, y= avg_CESD, color = ADH_HIGHVSLOW)) +\n  geom_line(size = 1) +\n  theme_minimal() +\n  scale_color_brewer(palette = \"Pastel2\")\n\n\n\n\n\n\n\n\nWe see the expected result, those with low adherence were more depressed.\nTop of Tabset\n\n\nFrailty Related Phenotype is a precision variable for physical QOL, and was previously significant for CD4+ T Cell Count.\n\n# Calculate the average CD4+ T Cell Count for FRP\nsummary_data &lt;- data_2 |&gt; \n  group_by(FRP, years) |&gt; \n  summarize(avg_LEU3N = mean(LEU3N, na.rm = TRUE))\n\n# Plot\nggplot(summary_data, aes(x=years, y= avg_LEU3N, color = FRP)) +\n  geom_line(size = 1) +\n  theme_minimal() +\n  scale_color_brewer(palette = \"Pastel2\")\n\n\n\n\n\n\n\n\nThose with a frailty related phenotype have lower average CD4+ T Cell Count.\n\n# Calculate the average Physical QOL for FRP\nsummary_data &lt;- data_2 |&gt; \n  group_by(FRP, years) |&gt; \n  summarize(avg_AGG_PHYS = mean(AGG_PHYS, na.rm = TRUE))\n\n# Plot\nggplot(summary_data, aes(x=years, y= avg_AGG_PHYS, color = FRP)) +\n  geom_line(size = 1) +\n  theme_minimal() +\n  scale_color_brewer(palette = \"Pastel2\")\n\n\n\n\n\n\n\n\nThey also have drastically lower physical QOL.\nTop of Tabset\n\n\nThis was significant for the model with VLOAD_log. Let’s assess.\n\n# Calculate the average VLOAD log by college education\nsummary_data &lt;- data_2 |&gt; \n  group_by(EDUC_COLLEGE, years) |&gt; \n  summarize(avg_VLOAD_log = mean(VLOAD_log, na.rm = TRUE))\n\n# Plot\nggplot(summary_data, aes(x=years, y= avg_VLOAD_log, color = EDUC_COLLEGE)) +\n  geom_line(size = 1) +\n  theme_minimal() +\n  scale_color_brewer(palette = \"Pastel2\")\n\n\n\n\n\n\n\n\nThose with no college have slightly higher average log vload. This does not appear to be significantly different.\n\n# Calculate the average LEU3N by college education\nsummary_data &lt;- data_2 |&gt; \n  group_by(EDUC_COLLEGE, years) |&gt; \n  summarize(avg_LEU3N = mean(LEU3N, na.rm = TRUE))\n\n# Plot\nggplot(summary_data, aes(x=years, y= avg_LEU3N, color = EDUC_COLLEGE)) +\n  geom_line(size = 1) +\n  theme_minimal() +\n  scale_color_brewer(palette = \"Pastel2\")\n\n\n\n\n\n\n\n\nThose with no college had lower white blood cell count.\n\n# Calculate the average VLOAD log by college education\nsummary_data &lt;- data_2 |&gt; \n  group_by(EDUC_COLLEGE, years) |&gt; \n  summarize(avg_AGG_PHYS = mean(AGG_PHYS, na.rm = TRUE))\n\n# Plot\nggplot(summary_data, aes(x=years, y= avg_AGG_PHYS, color = EDUC_COLLEGE)) +\n  geom_line(size = 1) +\n  theme_minimal() +\n  scale_color_brewer(palette = \"Pastel2\")\n\n\n\n\n\n\n\n\nThose with no college have lower physical QOL.\n\n# Calculate the mental QOL by college education\nsummary_data &lt;- data_2 |&gt; \n  group_by(EDUC_COLLEGE, years) |&gt; \n  summarize(avg_AGG_MENT = mean(AGG_MENT, na.rm = TRUE))\n\n# Plot\nggplot(summary_data, aes(x=years, y= avg_AGG_MENT, color = EDUC_COLLEGE)) +\n  geom_line(size = 1) +\n  theme_minimal() +\n  scale_color_brewer(palette = \"Pastel2\")\n\n\n\n\n\n\n\n\nThose with no college also appear to have lower mental QOL.\nReally it appears that now, since we are plotting and modelling data longitudinally instead of with change score, EDUC_COLLEGE should be included in every model as a precision variable.\nTop of Tabset"
  },
  {
    "objectID": "Project_2/Project_2_R/Code/Project2LMM.html#Viral",
    "href": "Project_2/Project_2_R/Code/Project2LMM.html#Viral",
    "title": "Linear Mixed Model",
    "section": "Log Viral Load",
    "text": "Log Viral Load\nBased on the previous interactive model selection in Project 2, the final covariates for this model were hard_drugs_grp, and ADH, and EDUC_COLLEGE\n\nModel SelectionAnalysisInterpretationVisualization\n\n\nTo answer the researcher’s main question, we will run a model of hard_drugs_grp, ADH_HIGHVSLOW, years, and an interaction between hard_drugs_grp*years to see if the slopes differ based on hard drug usage.\nNote: An unstructured covariance matrix is not needed here since we are only using random intercepts.\n\n# Ensure 'hard_drugs_grp' is a factor\ndata_2$hard_drugs_grp &lt;- as.factor(data_2$hard_drugs_grp)\n\n# Relevel the factor, setting \"Never User\" as the reference level\ndata_2$hard_drugs_grp &lt;- relevel(data_2$hard_drugs_grp, ref = \"Never User\")\n\n# Create LMM\nmodel_VLOAD1 &lt;- lmer(VLOAD_log ~ hard_drugs_grp + ADH_HIGHVSLOW + years + hard_drugs_grp*years + (1|newid), data = data_2)\n\n# Get model summary\nsummary(model_VLOAD1)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: VLOAD_log ~ hard_drugs_grp + ADH_HIGHVSLOW + years + hard_drugs_grp *  \n    years + (1 | newid)\n   Data: data_2\n\nREML criterion at convergence: 7864.3\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-2.86075 -0.55033  0.02914  0.59685  2.85626 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n newid    (Intercept) 2.856    1.690   \n Residual             5.883    2.425   \nNumber of obs: 1601, groups:  newid, 541\n\nFixed effects:\n                                    Estimate Std. Error         df t value\n(Intercept)                          9.81995    0.13770 1112.94700  71.315\nhard_drugs_grpCurrent User          -0.42074    0.38385 1154.65848  -1.096\nhard_drugs_grpPrevious User         -0.69690    0.43200 1154.95449  -1.613\nADH_HIGHVSLOWLow Adherence           0.43019    0.31315  525.18166   1.374\nyears                               -3.13791    0.08277 1054.68010 -37.910\nhard_drugs_grpCurrent User:years     0.39401    0.23638 1050.22268   1.667\nhard_drugs_grpPrevious User:years    0.48772    0.26608 1050.09154   1.833\n                                             Pr(&gt;|t|)    \n(Intercept)                       &lt;0.0000000000000002 ***\nhard_drugs_grpCurrent User                     0.2733    \nhard_drugs_grpPrevious User                    0.1070    \nADH_HIGHVSLOWLow Adherence                     0.1701    \nyears                             &lt;0.0000000000000002 ***\nhard_drugs_grpCurrent User:years               0.0958 .  \nhard_drugs_grpPrevious User:years              0.0671 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) hr__CU hr__PU ADH_HA years  h__CU:\nhrd_drgs_CU -0.346                                   \nhrd_drgs_PU -0.296  0.107                            \nADH_HIGHVSA -0.236  0.030 -0.019                     \nyears       -0.596  0.214  0.190  0.000              \nhrd_drg_CU:  0.209 -0.615 -0.066  0.000 -0.350       \nhrd_drg_PU:  0.185 -0.066 -0.615  0.000 -0.311  0.109\n\n\nChange reference level\n\n# Ensure 'hard_drugs_grp' is a factor\ndata_2$hard_drugs_grp &lt;- as.factor(data_2$hard_drugs_grp)\n\n# Relevel the factor, setting \"Never User\" as the reference level\ndata_2$hard_drugs_grp &lt;- relevel(data_2$hard_drugs_grp, ref = \"Previous User\")\n\n# Create LMM\nmodel_VLOAD1 &lt;- lmer(VLOAD_log ~ hard_drugs_grp + ADH_HIGHVSLOW + years + hard_drugs_grp*years + (1|newid), data = data_2)\n\n# Get model summary\nsummary(model_VLOAD1)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: VLOAD_log ~ hard_drugs_grp + ADH_HIGHVSLOW + years + hard_drugs_grp *  \n    years + (1 | newid)\n   Data: data_2\n\nREML criterion at convergence: 7864.3\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-2.86075 -0.55033  0.02914  0.59685  2.85626 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n newid    (Intercept) 2.856    1.690   \n Residual             5.883    2.425   \nNumber of obs: 1601, groups:  newid, 541\n\nFixed effects:\n                                  Estimate Std. Error        df t value\n(Intercept)                         9.1230     0.4127 1146.7510  22.105\nhard_drugs_grpNever User            0.6969     0.4320 1154.9545   1.613\nhard_drugs_grpCurrent User          0.2762     0.5462 1153.8155   0.506\nADH_HIGHVSLOWLow Adherence          0.4302     0.3131  525.1817   1.374\nyears                              -2.6502     0.2529 1049.6005 -10.480\nhard_drugs_grpNever User:years     -0.4877     0.2661 1050.0915  -1.833\nhard_drugs_grpCurrent User:years   -0.0937     0.3361 1049.6005  -0.279\n                                            Pr(&gt;|t|)    \n(Intercept)                      &lt;0.0000000000000002 ***\nhard_drugs_grpNever User                      0.1070    \nhard_drugs_grpCurrent User                    0.6132    \nADH_HIGHVSLOWLow Adherence                    0.1701    \nyears                            &lt;0.0000000000000002 ***\nhard_drugs_grpNever User:years                0.0671 .  \nhard_drugs_grpCurrent User:years              0.7805    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) hr__NU hr__CU ADH_HA years  h__NU:\nhrd_drgs_NU -0.948                                   \nhrd_drgs_CU -0.752  0.715                            \nADH_HIGHVSA -0.099  0.019  0.037                     \nyears       -0.613  0.585  0.463  0.000              \nhrd_drg_NU:  0.582 -0.615 -0.440  0.000 -0.950       \nhrd_drg_CU:  0.461 -0.440 -0.615  0.000 -0.752  0.715\n\n\nThe interaction is not significant. Let’s run the model with out it.\n\n# Relevel the factor, setting \"Never User\" as the reference level\ndata_2$hard_drugs_grp &lt;- relevel(data_2$hard_drugs_grp, ref = \"Never User\")\n\n# Create LMM\nmodel_VLOAD2 &lt;- lmer(VLOAD_log ~ hard_drugs_grp + ADH_HIGHVSLOW + years + (1|newid), data = data_2)\n\n# Get model summary\nsummary(model_VLOAD2)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: VLOAD_log ~ hard_drugs_grp + ADH_HIGHVSLOW + years + (1 | newid)\n   Data: data_2\n\nREML criterion at convergence: 7867.9\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-2.82180 -0.55906  0.01816  0.58112  2.81516 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n newid    (Intercept) 2.850    1.688   \n Residual             5.902    2.429   \nNumber of obs: 1601, groups:  newid, 541\n\nFixed effects:\n                              Estimate Std. Error         df t value\n(Intercept)                    9.73462    0.13285 1010.37558  73.277\nhard_drugs_grpPrevious User   -0.20992    0.34055  522.69614  -0.616\nhard_drugs_grpCurrent User    -0.02746    0.30267  522.94637  -0.091\nADH_HIGHVSLOWLow Adherence     0.43023    0.31318  525.10679   1.374\nyears                         -3.05185    0.07425 1055.62327 -41.105\n                                       Pr(&gt;|t|)    \n(Intercept)                 &lt;0.0000000000000002 ***\nhard_drugs_grpPrevious User               0.538    \nhard_drugs_grpCurrent User                0.928    \nADH_HIGHVSLOWLow Adherence                0.170    \nyears                       &lt;0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) hr__PU hr__CU ADH_HA\nhrd_drgs_PU -0.240                     \nhrd_drgs_CU -0.286  0.107              \nADH_HIGHVSA -0.245 -0.024  0.039       \nyears       -0.554 -0.002 -0.002  0.000\n\n\nThat looks pretty good, however, we did see that high adherence vs low adherence had differing slopes when we plotted them. Let’s include an interaction term for adherence.\n\n# Relevel the factor, setting \"Never User\" as the reference level\ndata_2$hard_drugs_grp &lt;- relevel(data_2$hard_drugs_grp, ref = \"Never User\")\n\n# Create LMM\nmodel_VLOAD3 &lt;- lmer(VLOAD_log ~ hard_drugs_grp + ADH_HIGHVSLOW + years + ADH_HIGHVSLOW*years + (1|newid), data = data_2)\n\n# Get model summary\nsummary(model_VLOAD3)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: VLOAD_log ~ hard_drugs_grp + ADH_HIGHVSLOW + years + ADH_HIGHVSLOW *  \n    years + (1 | newid)\n   Data: data_2\n\nREML criterion at convergence: 7852.6\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-2.77922 -0.55214  0.02611  0.58116  2.47095 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n newid    (Intercept) 2.882    1.698   \n Residual             5.816    2.412   \nNumber of obs: 1601, groups:  newid, 541\n\nFixed effects:\n                                   Estimate Std. Error         df t value\n(Intercept)                         9.83455    0.13483 1052.37684  72.938\nhard_drugs_grpPrevious User        -0.21036    0.34066  522.78164  -0.618\nhard_drugs_grpCurrent User         -0.02743    0.30276  523.02950  -0.091\nADH_HIGHVSLOWLow Adherence         -0.55205    0.39593 1148.27523  -1.394\nyears                              -3.15231    0.07776 1054.58618 -40.541\nADH_HIGHVSLOWLow Adherence:years    0.98968    0.24398 1054.61543   4.056\n                                             Pr(&gt;|t|)    \n(Intercept)                      &lt; 0.0000000000000002 ***\nhard_drugs_grpPrevious User                     0.537    \nhard_drugs_grpCurrent User                      0.928    \nADH_HIGHVSLOWLow Adherence                      0.163    \nyears                            &lt; 0.0000000000000002 ***\nADH_HIGHVSLOWLow Adherence:years            0.0000535 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n                 (Intr) hr__PU hr__CU ADH_HIGHVSLOWLwA years \nhrd_drgs_PU      -0.236                                      \nhrd_drgs_CU      -0.282  0.107                               \nADH_HIGHVSLOWLwA -0.303 -0.019  0.030                        \nyears            -0.572 -0.002 -0.002  0.195                 \nADH_HIGHVSLOWLA:  0.182  0.000  0.000 -0.612           -0.319\n\n\nWe have a significant interaciton. Let’s perform model selection based on AIC and BIC.\n\n# Model selection with AIC and BIC\nAIC(model_VLOAD1, model_VLOAD2, model_VLOAD3)\n\n             df      AIC\nmodel_VLOAD1  9 7882.253\nmodel_VLOAD2  7 7881.917\nmodel_VLOAD3  8 7868.561\n\nBIC(model_VLOAD1, model_VLOAD2, model_VLOAD3)\n\n             df      BIC\nmodel_VLOAD1  9 7930.659\nmodel_VLOAD2  7 7919.566\nmodel_VLOAD3  8 7911.588\n\n\nAIC and BIC both prefer model 3, with the ADH_HIGHVSLOW*years interaction. Thus that is our final model.\nTop of Tabset\n\n\nBased on model selection, the final model for log viral load includes hard_drugs_grp, ADH_HIGHVSLOW, years, and an interaction between ADH_HIGHVSLOW*years.\n\n# Relevel the factor, setting \"Never User\" as the reference level\ndata_2$hard_drugs_grp &lt;- relevel(data_2$hard_drugs_grp, ref = \"Never User\")\n\n# Create LMM\nmodel_VLOAD1 &lt;- lmer(VLOAD_log ~ hard_drugs_grp + ADH_HIGHVSLOW + years + ADH_HIGHVSLOW*years + (1|newid), data = data_2)\n\n# Get model summary\nsummary(model_VLOAD1)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: VLOAD_log ~ hard_drugs_grp + ADH_HIGHVSLOW + years + ADH_HIGHVSLOW *  \n    years + (1 | newid)\n   Data: data_2\n\nREML criterion at convergence: 7852.6\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-2.77922 -0.55214  0.02611  0.58116  2.47095 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n newid    (Intercept) 2.882    1.698   \n Residual             5.816    2.412   \nNumber of obs: 1601, groups:  newid, 541\n\nFixed effects:\n                                   Estimate Std. Error         df t value\n(Intercept)                         9.83455    0.13483 1052.37684  72.938\nhard_drugs_grpPrevious User        -0.21036    0.34066  522.78164  -0.618\nhard_drugs_grpCurrent User         -0.02743    0.30276  523.02950  -0.091\nADH_HIGHVSLOWLow Adherence         -0.55205    0.39593 1148.27523  -1.394\nyears                              -3.15231    0.07776 1054.58618 -40.541\nADH_HIGHVSLOWLow Adherence:years    0.98968    0.24398 1054.61543   4.056\n                                             Pr(&gt;|t|)    \n(Intercept)                      &lt; 0.0000000000000002 ***\nhard_drugs_grpPrevious User                     0.537    \nhard_drugs_grpCurrent User                      0.928    \nADH_HIGHVSLOWLow Adherence                      0.163    \nyears                            &lt; 0.0000000000000002 ***\nADH_HIGHVSLOWLow Adherence:years            0.0000535 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n                 (Intr) hr__PU hr__CU ADH_HIGHVSLOWLwA years \nhrd_drgs_PU      -0.236                                      \nhrd_drgs_CU      -0.282  0.107                               \nADH_HIGHVSLOWLwA -0.303 -0.019  0.030                        \nyears            -0.572 -0.002 -0.002  0.195                 \nADH_HIGHVSLOWLA:  0.182  0.000  0.000 -0.612           -0.319\n\npretty_print(confint(model_VLOAD1))\n\nComputing profile confidence intervals ...\n\n\n\n\n\n\n2.5 %\n97.5 %\n\n\n\n\n.sig01\n1.5112544\n1.8703721\n\n\n.sigma\n2.3099745\n2.5160618\n\n\n(Intercept)\n9.5707668\n10.0982986\n\n\nhard_drugs_grpPrevious User\n-0.8767521\n0.4559816\n\n\nhard_drugs_grpCurrent User\n-0.6196802\n0.5647824\n\n\nADH_HIGHVSLOWLow Adherence\n-1.3265854\n0.2225392\n\n\nyears\n-3.3047431\n-2.9999461\n\n\nADH_HIGHVSLOWLow Adherence:years\n0.5114453\n1.4678017\n\n\n\n\n\n\n\nAfter controlling for adherence and between subject means (intercepts), there was no difference in average log viral load for current (p = 0.93, 95% CI: -0.62, 0.56) and previous (p = 0.54, 95% CI: -0.88, 0.46) hard drug users compared to never users.\nAfter controlling for hard drug use and between subject means, those with low adherence did not differ significantly in average log viral load compared to those with high adherence (p = 0.16, 95% CI: -1.32, 0.22).\nThere was a significant decrease in log viral load over time while controlling for subject-specific means, hard drug use, and adherence, with an average decrease of 3.15 log copies/mL per year (p &lt; 0.0001, 95% CI: -3.30, -3.00).\nSubject ID contributes greatly to log viral load, as it accounts for 2.882 / (2.882 + 5.816) = 0.331 or 33.1% of the variance in log viral load.\nThe slope for viral load over time differed significantly between those with low and high adherence. Those with low adherence had on average a 0.99 increase in log viral load each year compared to those with high adherence (p &lt; 0.0001, 95% CI: 0.51, 1.47).\n\nChange Reference Group\n\n# Relevel the factor, setting \"Never User\" as the reference level\ndata_2$hard_drugs_grp &lt;- relevel(data_2$hard_drugs_grp, ref = \"Previous User\")\n\n# Create LMM\nmodel_VLOAD1 &lt;- lmer(VLOAD_log ~ hard_drugs_grp + ADH_HIGHVSLOW + years + ADH_HIGHVSLOW*years + (1|newid), data = data_2)\n\n# Get model summary\nsummary(model_VLOAD1)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: VLOAD_log ~ hard_drugs_grp + ADH_HIGHVSLOW + years + ADH_HIGHVSLOW *  \n    years + (1 | newid)\n   Data: data_2\n\nREML criterion at convergence: 7852.6\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-2.77922 -0.55214  0.02611  0.58116  2.47095 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n newid    (Intercept) 2.882    1.698   \n Residual             5.816    2.412   \nNumber of obs: 1601, groups:  newid, 541\n\nFixed effects:\n                                   Estimate Std. Error         df t value\n(Intercept)                         9.62419    0.33541  582.65751  28.694\nhard_drugs_grpNever User            0.21036    0.34066  522.78172   0.618\nhard_drugs_grpCurrent User          0.18294    0.43075  522.08749   0.425\nADH_HIGHVSLOWLow Adherence         -0.55205    0.39593 1148.27547  -1.394\nyears                              -3.15231    0.07776 1054.58612 -40.541\nADH_HIGHVSLOWLow Adherence:years    0.98968    0.24398 1054.61537   4.056\n                                             Pr(&gt;|t|)    \n(Intercept)                      &lt; 0.0000000000000002 ***\nhard_drugs_grpNever User                        0.537    \nhard_drugs_grpCurrent User                      0.671    \nADH_HIGHVSLOWLow Adherence                      0.163    \nyears                            &lt; 0.0000000000000002 ***\nADH_HIGHVSLOWLow Adherence:years            0.0000535 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n                 (Intr) hr__NU hr__CU ADH_HIGHVSLOWLwA years \nhrd_drgs_NU      -0.921                                      \nhrd_drgs_CU      -0.731  0.715                               \nADH_HIGHVSLOWLwA -0.141  0.019  0.037                        \nyears            -0.232  0.002  0.000  0.195                 \nADH_HIGHVSLOWLA:  0.073  0.000  0.000 -0.612           -0.319\n\npretty_print(confint(model_VLOAD1))\n\nComputing profile confidence intervals ...\n\n\n\n\n\n\n2.5 %\n97.5 %\n\n\n\n\n.sig01\n1.5112544\n1.8703721\n\n\n.sigma\n2.3099745\n2.5160618\n\n\n(Intercept)\n8.9680979\n10.2802687\n\n\nhard_drugs_grpNever User\n-0.4559816\n0.8767521\n\n\nhard_drugs_grpCurrent User\n-0.6596545\n1.0255272\n\n\nADH_HIGHVSLOWLow Adherence\n-1.3265854\n0.2225392\n\n\nyears\n-3.3047431\n-2.9999461\n\n\nADH_HIGHVSLOWLow Adherence:years\n0.5114453\n1.4678017\n\n\n\n\n\n\n\nPrevious hard drug users did not differ significantly in average log viral load compared to current users (p = 0.67, 95% CI: -0.66, 1.03).\nTop of Tabset\n\n\n\n\nHard Drug Use Group\nAfter controlling for adherence and between subject means (intercepts), there was no difference in average log viral load for current (p = 0.93, 95% CI: -0.62, 0.56) and previous (p = 0.54, 95% CI: -0.88, 0.46) hard drug users compared to never users. Previous hard drug users did not differ significantly in average log viral load compared to current users (p = 0.67, 95% CI: -0.66, 1.03).\n\n\nAdherence\nAfter controlling for hard drug use and between subject means, those with low adherence did not differ significantly in average log viral load compared to those with high adherence (p = 0.16, 95% CI: -1.32, 0.22).\n\n\nTime\nThere was a significant decrease in log viral load over time while controlling for subject-specific means, hard drug use, and adherence, with an average decrease of 3.15 log copies/mL per year (p &lt; 0.0001, 95% CI: -3.30, -3.00).\n\n\nViral Load over Time by Adherence\nThe slope for viral load over time differed significantly between those with low and high adherence. Those with low adherence had on average a 0.99 increase in log viral load each year compared to those with high adherence (p &lt; 0.0001, 95% CI: 0.51, 1.47). The other in group comparisons were not significant (p &gt; 0.05).\n\n\nWithin Subject Means\nSubject ID contributes greatly to log viral load, as it accounts for 2.882 / (2.882 + 5.816) = 0.331 or 33.1% of the variance in log viral load.\nTop of Tabset\n\n\n\nWe can plot the predicted values of our model in order to visualize the interaction and understand it better.\n\n# Ungroup data_2\ndata_2 &lt;- data_2 |&gt; \n  ungroup()\n\n# Prepare data for plotting\ndata_2_VLOAD &lt;- data_2 |&gt;\n  select(newid, VLOAD_log, ADH_HIGHVSLOW, hard_drugs_grp, years) %&gt;%\n  filter(complete.cases(.)) |&gt; \n  mutate(fitted = predict(model_VLOAD3)) |&gt; \n  group_by(newid)\n\n# Plot the interaction\nggplot(data_2_VLOAD, aes(x = years, y = fitted, color = ADH_HIGHVSLOW)) +\n  geom_smooth(method = \"lm\", se = FALSE) + \n  labs(title = \"Predicted Log Viral Load over Years with Interaction by Adherence\",\n       x = \"Year\",\n       y = \"Predicted Log Viral Load\") +\n  theme_minimal() +\n  scale_color_brewer(palette = \"Pastel2\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nHere we can see that those with high adherence had a greater decrease in log viral load over time compared to those who had high adherence.\nWe can also see that the slopes over time were not different based on hard drug use.\n\n# Plot the interaction\nggplot(data_2_VLOAD, aes(x = years, y = fitted, color = hard_drugs_grp)) +\n  geom_smooth(method = \"lm\", se = FALSE) + \n  labs(title = \"Predicted Log Viral Load over Years by Hard Drug Use\",\n       x = \"Year\",\n       y = \"Predicted Log Viral Load\") +\n  theme_minimal() +\n  scale_color_brewer(palette = \"Pastel2\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nTop of Tabset"
  },
  {
    "objectID": "Project_2/Project_2_R/Code/Project2LMM.html#CD4",
    "href": "Project_2/Project_2_R/Code/Project2LMM.html#CD4",
    "title": "Linear Mixed Model",
    "section": "CD4+ T Cell Count",
    "text": "CD4+ T Cell Count\n\nModel SelectionAnalysisInterpretationVisualization\n\n\nFirst let us fit the full model of interest using a structured covariance matrix, and hard_drugs_grp, ADH_HIGHVSLOW, years, and the hard_drugs_grp*years interaction term.\n\n# Filter data set\ndata_LEU3N &lt;- data_2 |&gt; \n  select(newid, years, hard_drugs_grp, ADH_HIGHVSLOW, LEU3N) %&gt;% \n  filter(complete.cases(.))\n\n# Create LMM\nmodel_LEU3N1 &lt;- lme(LEU3N ~ hard_drugs_grp + ADH_HIGHVSLOW + years + hard_drugs_grp*years, \n                   random = list(newid = pdSymm(~ 1)), \n                   data = data_LEU3N)\n\n# Get model summary\nsummary(model_LEU3N1)\n\nLinear mixed-effects model fit by REML\n  Data: data_LEU3N \n       AIC      BIC    logLik\n  21085.29 21133.68 -10533.64\n\nRandom effects:\n Formula: ~1 | newid\n        (Intercept) Residual\nStdDev:    214.9289 115.5901\n\nFixed effects:  LEU3N ~ hard_drugs_grp + ADH_HIGHVSLOW + years + hard_drugs_grp *      years \n                                    Value Std.Error   DF   t-value p-value\n(Intercept)                      333.6108  35.54933 1061  9.384449  0.0000\nhard_drugs_grpNever User          47.3576  37.13030  538  1.275443  0.2027\nhard_drugs_grpCurrent User       163.7339  46.96733  538  3.486124  0.0005\nADH_HIGHVSLOWLow Adherence       -33.6629  32.05338  538 -1.050215  0.2941\nyears                             57.1508  12.05110 1061  4.742373  0.0000\nhard_drugs_grpNever User:years    32.3086  12.68127 1061  2.547745  0.0110\nhard_drugs_grpCurrent User:years -27.0024  16.01784 1061 -1.685768  0.0921\n Correlation: \n                                 (Intr) hr__NU hr__CU ADH_HA years  h__NU:\nhard_drugs_grpNever User         -0.947                                   \nhard_drugs_grpCurrent User       -0.752  0.716                            \nADH_HIGHVSLOWLow Adherence       -0.118  0.023  0.044                     \nyears                            -0.339  0.325  0.257  0.000              \nhard_drugs_grpNever User:years    0.322 -0.341 -0.244  0.000 -0.950       \nhard_drugs_grpCurrent User:years  0.255 -0.244 -0.341  0.000 -0.752  0.715\n\nStandardized Within-Group Residuals:\n       Min         Q1        Med         Q3        Max \n-3.9360359 -0.4855393 -0.0484215  0.4241892  5.4905529 \n\nNumber of Observations: 1606\nNumber of Groups: 542 \n\n\nThen let us run the same model but with an unstructured covariance matrix.\n\n# Create LMM\nmodel_LEU3N2 &lt;- lme(LEU3N ~ hard_drugs_grp + ADH_HIGHVSLOW + years + hard_drugs_grp*years, \n                   random = list(newid = pdSymm(~ years)), \n                   data = data_LEU3N)\n\n# Get model summary\nsummary(model_LEU3N2)\n\nLinear mixed-effects model fit by REML\n  Data: data_LEU3N \n       AIC      BIC    logLik\n  20935.06 20994.21 -10456.53\n\nRandom effects:\n Formula: ~years | newid\n Structure: General positive-definite\n            StdDev    Corr  \n(Intercept) 182.03482 (Intr)\nyears        60.68956 0.49  \nResidual     98.31914       \n\nFixed effects:  LEU3N ~ hard_drugs_grp + ADH_HIGHVSLOW + years + hard_drugs_grp *      years \n                                    Value Std.Error   DF   t-value p-value\n(Intercept)                      331.7197  30.16098 1061 10.998307  0.0000\nhard_drugs_grpNever User          47.7472  31.47512  538  1.516984  0.1299\nhard_drugs_grpCurrent User       164.6585  39.81726  538  4.135354  0.0000\nADH_HIGHVSLOWLow Adherence       -19.1640  28.89178  538 -0.663304  0.5074\nyears                             57.1508  13.60670 1061  4.200195  0.0000\nhard_drugs_grpNever User:years    32.1522  14.31691 1061  2.245750  0.0249\nhard_drugs_grpCurrent User:years -27.0024  18.08549 1061 -1.493040  0.1357\n Correlation: \n                                 (Intr) hr__NU hr__CU ADH_HA years  h__NU:\nhard_drugs_grpNever User         -0.946                                   \nhard_drugs_grpCurrent User       -0.751  0.716                            \nADH_HIGHVSLOWLow Adherence       -0.125  0.025  0.046                     \nyears                             0.031 -0.029 -0.023  0.000              \nhard_drugs_grpNever User:years   -0.029  0.031  0.022  0.000 -0.950       \nhard_drugs_grpCurrent User:years -0.023  0.022  0.031  0.000 -0.752  0.715\n\nStandardized Within-Group Residuals:\n        Min          Q1         Med          Q3         Max \n-3.01629771 -0.44050109 -0.04862548  0.37263572  4.08134359 \n\nNumber of Observations: 1606\nNumber of Groups: 542 \n\n\nAIC, BIC, and -log likelihood all prefer the unstructured covariance matrix is better, and thus we select it as our final model.\nTop of Tabset\n\n\nHere we perform the analysis with the final selected model, using an unstructured covariance matrix.\n\n# Create LMM\nmodel_LEU3N2 &lt;- lme(LEU3N ~ hard_drugs_grp + ADH_HIGHVSLOW + years + hard_drugs_grp*years, \n                   random = list(newid = pdSymm(~ years)), \n                   data = data_LEU3N)\n\n# Get model summary\nsummary(model_LEU3N2)\n\nLinear mixed-effects model fit by REML\n  Data: data_LEU3N \n       AIC      BIC    logLik\n  20935.06 20994.21 -10456.53\n\nRandom effects:\n Formula: ~years | newid\n Structure: General positive-definite\n            StdDev    Corr  \n(Intercept) 182.03482 (Intr)\nyears        60.68956 0.49  \nResidual     98.31914       \n\nFixed effects:  LEU3N ~ hard_drugs_grp + ADH_HIGHVSLOW + years + hard_drugs_grp *      years \n                                    Value Std.Error   DF   t-value p-value\n(Intercept)                      331.7197  30.16098 1061 10.998307  0.0000\nhard_drugs_grpNever User          47.7472  31.47512  538  1.516984  0.1299\nhard_drugs_grpCurrent User       164.6585  39.81726  538  4.135354  0.0000\nADH_HIGHVSLOWLow Adherence       -19.1640  28.89178  538 -0.663304  0.5074\nyears                             57.1508  13.60670 1061  4.200195  0.0000\nhard_drugs_grpNever User:years    32.1522  14.31691 1061  2.245750  0.0249\nhard_drugs_grpCurrent User:years -27.0024  18.08549 1061 -1.493040  0.1357\n Correlation: \n                                 (Intr) hr__NU hr__CU ADH_HA years  h__NU:\nhard_drugs_grpNever User         -0.946                                   \nhard_drugs_grpCurrent User       -0.751  0.716                            \nADH_HIGHVSLOWLow Adherence       -0.125  0.025  0.046                     \nyears                             0.031 -0.029 -0.023  0.000              \nhard_drugs_grpNever User:years   -0.029  0.031  0.022  0.000 -0.950       \nhard_drugs_grpCurrent User:years -0.023  0.022  0.031  0.000 -0.752  0.715\n\nStandardized Within-Group Residuals:\n        Min          Q1         Med          Q3         Max \n-3.01629771 -0.44050109 -0.04862548  0.37263572  4.08134359 \n\nNumber of Observations: 1606\nNumber of Groups: 542 \n\n# Get 95% CIs\nintervals(model_LEU3N2, level = 0.95)\n\nApproximate 95% confidence intervals\n\n Fixed effects:\n                                      lower      est.      upper\n(Intercept)                      272.537742 331.71968 390.901628\nhard_drugs_grpNever User         -14.081955  47.74724 109.576442\nhard_drugs_grpCurrent User        86.442127 164.65849 242.874849\nADH_HIGHVSLOWLow Adherence       -75.918561 -19.16404  37.590486\nyears                             30.451702  57.15081  83.849913\nhard_drugs_grpNever User:years     4.059531  32.15220  60.244878\nhard_drugs_grpCurrent User:years -62.489748 -27.00236   8.485025\n\n Random Effects:\n  Level: newid \n                             lower        est.       upper\nsd((Intercept))        168.7261224 182.0348208 196.3932763\nsd(years)               51.7123748  60.6895579  71.2251652\ncor((Intercept),years)   0.2792255   0.4900844   0.6558527\n\n Within-group standard error:\n    lower      est.     upper \n 92.55578  98.31914 104.44138 \n\n\nAfter controlling for adherence and between subject means (intercepts), there was no difference in average CD4+ T Cell count for previous users compared to never hard drug users (p = 0.13, 95% CI: -109.54, 14.08). However, current users had on average a 116.91 cell higher CD4+ T Cell count compared to never users (p &lt; 0.0001, 95% CI: 61.97, 171.85).\nAfter controlling for hard drug use and between subject means, those with low adherence did not differ significantly in average CD4+ T Cell count compared to those with high adherence (p = 0.51, 95% CI: -75.92, 37.59).\nThere was a significant increase in CD4+ T Cell count over time while controlling for subject-specific means, hard drug use, and adherence, with an average increase of 89.30 log copies/mL per year (p &lt; 0.0001, 95% CI: -3.30, -3.00).\nThe slope for CD4+ T Cell count over time differed significantly based on hard drug use. On average, previous hard drug users had a 32.15 decrease in CD4+ T Cell count per year compared to never drug users (p = 0.025, 95% CI: -60.24, -4.06), and current users had a 59.15 decrease in CD4+ T Cell count each year compared to never hard drug users (p &lt; 0.0001, 95% CI: -84.11, -34.20).\n\nChange the Reference Group\n\n# Change reference level\ndata_LEU3N$hard_drugs_grp &lt;- relevel(data_LEU3N$hard_drugs_grp, ref = \"Previous User\")\n\n# Create LMM\nmodel_LEU3N2 &lt;- lme(LEU3N ~ hard_drugs_grp + ADH_HIGHVSLOW + years + hard_drugs_grp*years, \n                   random = list(newid = pdSymm(~ years)), \n                   data = data_LEU3N)\n\n# Get model summary\nsummary(model_LEU3N2)\n\nLinear mixed-effects model fit by REML\n  Data: data_LEU3N \n       AIC      BIC    logLik\n  20935.06 20994.21 -10456.53\n\nRandom effects:\n Formula: ~years | newid\n Structure: General positive-definite\n            StdDev    Corr  \n(Intercept) 182.03482 (Intr)\nyears        60.68956 0.49  \nResidual     98.31914       \n\nFixed effects:  LEU3N ~ hard_drugs_grp + ADH_HIGHVSLOW + years + hard_drugs_grp *      years \n                                    Value Std.Error   DF   t-value p-value\n(Intercept)                      331.7197  30.16098 1061 10.998307  0.0000\nhard_drugs_grpNever User          47.7472  31.47512  538  1.516984  0.1299\nhard_drugs_grpCurrent User       164.6585  39.81726  538  4.135354  0.0000\nADH_HIGHVSLOWLow Adherence       -19.1640  28.89178  538 -0.663304  0.5074\nyears                             57.1508  13.60670 1061  4.200195  0.0000\nhard_drugs_grpNever User:years    32.1522  14.31691 1061  2.245750  0.0249\nhard_drugs_grpCurrent User:years -27.0024  18.08549 1061 -1.493040  0.1357\n Correlation: \n                                 (Intr) hr__NU hr__CU ADH_HA years  h__NU:\nhard_drugs_grpNever User         -0.946                                   \nhard_drugs_grpCurrent User       -0.751  0.716                            \nADH_HIGHVSLOWLow Adherence       -0.125  0.025  0.046                     \nyears                             0.031 -0.029 -0.023  0.000              \nhard_drugs_grpNever User:years   -0.029  0.031  0.022  0.000 -0.950       \nhard_drugs_grpCurrent User:years -0.023  0.022  0.031  0.000 -0.752  0.715\n\nStandardized Within-Group Residuals:\n        Min          Q1         Med          Q3         Max \n-3.01629771 -0.44050109 -0.04862548  0.37263572  4.08134359 \n\nNumber of Observations: 1606\nNumber of Groups: 542 \n\n# Get 95% CIs\nintervals(model_LEU3N2, level = 0.95)\n\nApproximate 95% confidence intervals\n\n Fixed effects:\n                                      lower      est.      upper\n(Intercept)                      272.537742 331.71968 390.901628\nhard_drugs_grpNever User         -14.081955  47.74724 109.576442\nhard_drugs_grpCurrent User        86.442127 164.65849 242.874849\nADH_HIGHVSLOWLow Adherence       -75.918561 -19.16404  37.590486\nyears                             30.451702  57.15081  83.849913\nhard_drugs_grpNever User:years     4.059531  32.15220  60.244878\nhard_drugs_grpCurrent User:years -62.489748 -27.00236   8.485025\n\n Random Effects:\n  Level: newid \n                             lower        est.       upper\nsd((Intercept))        168.7261224 182.0348208 196.3932763\nsd(years)               51.7123748  60.6895579  71.2251652\ncor((Intercept),years)   0.2792255   0.4900844   0.6558527\n\n Within-group standard error:\n    lower      est.     upper \n 92.55578  98.31914 104.44138 \n\n\nPrevious hard drug users had on average a 164.66 lower CD4+ T Cell count compared to current users (p &lt; 0.0001, 95% CI: 86.44, 242.87).\nThe slope for CD4+ T Cell count over time did not differ between previous and current hard drug users (p = 0.14, 95% CI: -62.49, 8.49).\nTop of Tabset\n\n\n\n\nHard Drug Use\nAfter controlling for adherence and between subject means (intercepts), there was no difference in average CD4+ T Cell count for previous users compared to never hard drug users (p = 0.13, 95% CI: -109.54, 14.08). However, current users had on average a 116.91 cell higher CD4+ T Cell count compared to never users (p &lt; 0.0001, 95% CI: 61.97, 171.85). Previous hard drug users had on average a 164.66 higher CD4+ T Cell count compared to current users (p &lt; 0.0001, 95% CI: 86.44, 242.87).\n\n\nAdherence\nAfter controlling for hard drug use and between subject means, those with low adherence did not differ significantly in average CD4+ T Cell count compared to those with high adherence (p = 0.51, 95% CI: -75.92, 37.59).\n\n\nCD4+ T Cell Count by Hard Drug Use\nThe slope for CD4+ T Cell count over time differed significantly based on hard drug use. On average, previous hard drug users had a 32.15 decrease in CD4+ T Cell count per year compared to never drug users (p = 0.025, 95% CI: -60.24, -4.06), and current users had a 59.15 decrease in CD4+ T Cell count each year compared to never hard drug users (p &lt; 0.0001, 95% CI: -84.11, -34.20). The slope for CD4+ T Cell count over time did not differ between previous and current hard drug users (p = 0.14, 95% CI: -62.49, 8.49).\n\n\nTime\nThere was a significant increase in CD4+ T Cell count over time while controlling for subject-specific means, hard drug use, and adherence, with an average increase of 89.30 log copies/mL per year (p &lt; 0.0001, 95% CI: -3.30, -3.00).\nTop of Tabset\n\n\n\n\n# Prepare data for plotting\ndata_LEU3N &lt;- data_LEU3N |&gt;\n  mutate(fitted = predict(model_LEU3N1))\n\n# Plot the interaction\nggplot(data_LEU3N, aes(x = years, y = fitted, color = hard_drugs_grp)) +\n  geom_smooth(method = \"lm\", se = FALSE) + \n  labs(title = \"Predicted CD4+ T Cell Coount over Years by Hard Drug Use\",\n       x = \"Year\",\n       y = \"Predicted CD4+ T Cell Count\") +\n  theme_minimal() +\n  scale_color_brewer(palette = \"Pastel2\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nHere we see the interaction base on hard drug use. Never drug users had a higher increase in CD4+ T Cell count each year compared to previous and current hard drug users.\n\n# Prepare data for plotting\ndata_LEU3N &lt;- data_LEU3N |&gt;\n  mutate(fitted = predict(model_LEU3N1))\n\n# Plot the interaction\nggplot(data_LEU3N, aes(x = years, y = fitted, color = ADH_HIGHVSLOW)) +\n  geom_smooth(method = \"lm\", se = FALSE) + \n  labs(title = \"Predicted CD4+ T Cell Coount over Years by Hard Drug Use\",\n       x = \"Year\",\n       y = \"Predicted CD4+ T Cell Count\") +\n  theme_minimal() +\n  scale_color_brewer(palette = \"Pastel2\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nLikewise, here we see that thosewith low adherence had lower CD4+ T Cell count compared to those with high adherence, but the differences over time did not differ.\nTop of Tabset"
  },
  {
    "objectID": "Project_2/Project_2_R/Code/Project2LMM.html#Ment",
    "href": "Project_2/Project_2_R/Code/Project2LMM.html#Ment",
    "title": "Linear Mixed Model",
    "section": "Mental QOL",
    "text": "Mental QOL\nWe will be fitting the full model with hard_drugs_grp, ADH_HIGHVSLOW, years, CESD, and the hard_drugs_grp*years interaction.\nWe will also use an unstructured covariance matrix to account for unequal variances.\nThis can be visualized here.\n\nggplot(data_2, aes(x = AGG_MENT, y = ADH_HIGHVSLOW, fill = ADH_HIGHVSLOW)) +\n  geom_density_ridges(alpha = 0.7, scale = 1) +\n  theme_ridges() +\n  scale_color_brewer(palette = \"Pastel2\") +\n  labs(title = \"Ridge Plot of Mental QOL by Hard Drug Use\",\n       x = \"Mental QOL\",\n       y = \"Hard Drug Use\") +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\nModel SelectionAnalysisInterpretationVisualization\n\n\nFit the model.\n\n# Create the data set for mental QOL\ndata_MENT &lt;- data_2 |&gt; \n  select(newid, AGG_MENT, hard_drugs_grp, ADH_HIGHVSLOW, years, CESD) %&gt;%\n  filter(complete.cases(.))\n\n# Change reference level\ndata_MENT$hard_drugs_grp &lt;- relevel(data_MENT$hard_drugs_grp, ref = \"Never User\")\n\n# Create LMM\nmodel_MENT1 &lt;- lme(AGG_MENT ~ hard_drugs_grp + years + ADH_HIGHVSLOW + hard_drugs_grp*years + CESD,\n                   random = list(newid = pdSymm(~ years)), \n                   data = data_MENT)\n\n# Get model summary\nsummary(model_MENT1)\n\nLinear mixed-effects model fit by REML\n  Data: data_MENT \n       AIC      BIC    logLik\n  10848.59 10913.16 -5412.294\n\nRandom effects:\n Formula: ~years | newid\n Structure: General positive-definite\n            StdDev    Corr  \n(Intercept) 4.3455421 (Intr)\nyears       0.3717221 -1    \nResidual    6.0377346       \n\nFixed effects:  AGG_MENT ~ hard_drugs_grp + years + ADH_HIGHVSLOW + hard_drugs_grp *      years + CESD \n                                     Value Std.Error   DF   t-value p-value\n(Intercept)                       57.87606 0.4190797 1059 138.10276  0.0000\nhard_drugs_grpPrevious User        0.78568 1.0942518  546   0.71801  0.4731\nhard_drugs_grpCurrent User        -2.27986 0.9972527  546  -2.28614  0.0226\nyears                              0.30811 0.2073133 1059   1.48621  0.1375\nADH_HIGHVSLOWLow Adherence         0.85071 0.7612683  546   1.11749  0.2643\nCESD                              -0.91187 0.0180253 1059 -50.58831  0.0000\nhard_drugs_grpPrevious User:years -0.36332 0.6647696 1059  -0.54654  0.5848\nhard_drugs_grpCurrent User:years   1.69469 0.6106482 1059   2.77524  0.0056\n Correlation: \n                                  (Intr) hr__PU hr__CU years  ADH_HA CESD  \nhard_drugs_grpPrevious User       -0.184                                   \nhard_drugs_grpCurrent User        -0.291  0.100                            \nyears                             -0.581  0.193  0.225                     \nADH_HIGHVSLOWLow Adherence        -0.142 -0.014  0.021 -0.006              \nCESD                              -0.569 -0.104  0.029  0.091 -0.068       \nhard_drugs_grpPrevious User:years  0.159 -0.657 -0.069 -0.308 -0.001  0.011\nhard_drugs_grpCurrent User:years   0.230 -0.060 -0.679 -0.345  0.008 -0.089\n                                  h__PU:\nhard_drugs_grpPrevious User             \nhard_drugs_grpCurrent User              \nyears                                   \nADH_HIGHVSLOWLow Adherence              \nCESD                                    \nhard_drugs_grpPrevious User:years       \nhard_drugs_grpCurrent User:years   0.104\n\nStandardized Within-Group Residuals:\n        Min          Q1         Med          Q3         Max \n-3.44300701 -0.46895838  0.05349899  0.54923036  4.31899276 \n\nNumber of Observations: 1613\nNumber of Groups: 550 \n\n\nWe have a significant interaction betwee hard drug use and time.\nLet’s see use BIC and AIC to determine if we need that interaction term.\n\n# Create the data set for mental QOL\ndata_MENT &lt;- data_2 |&gt; \n  select(newid, AGG_MENT, hard_drugs_grp, ADH_HIGHVSLOW, years, CESD) %&gt;%\n  filter(complete.cases(.))\n\n# Change reference level\ndata_MENT$hard_drugs_grp &lt;- relevel(data_MENT$hard_drugs_grp, ref = \"Never User\")\n\n# Create LMM\nmodel_MENT1 &lt;- lme(AGG_MENT ~ hard_drugs_grp + years + ADH_HIGHVSLOW,\n                   random = list(newid = pdSymm(~ years)), \n                   data = data_MENT)\n\n# Get model summary\nsummary(model_MENT1)\n\nLinear mixed-effects model fit by REML\n  Data: data_MENT \n       AIC      BIC    logLik\n  12203.19 12251.63 -6092.593\n\nRandom effects:\n Formula: ~years | newid\n Structure: General positive-definite\n            StdDev    Corr  \n(Intercept) 11.438345 (Intr)\nyears        3.518495 -0.513\nResidual     7.212184       \n\nFixed effects:  AGG_MENT ~ hard_drugs_grp + years + ADH_HIGHVSLOW \n                               Value Std.Error   DF  t-value p-value\n(Intercept)                 46.02166 0.6294490 1062 73.11419  0.0000\nhard_drugs_grpPrevious User -4.91599 1.6803885  546 -2.92551  0.0036\nhard_drugs_grpCurrent User  -2.01880 1.4955259  546 -1.34989  0.1776\nyears                        1.10189 0.2683723 1062  4.10581  0.0000\nADH_HIGHVSLOWLow Adherence  -1.91815 1.5516904  546 -1.23617  0.2169\n Correlation: \n                            (Intr) hr__PU hr__CU years \nhard_drugs_grpPrevious User -0.245                     \nhard_drugs_grpCurrent User  -0.289  0.105              \nyears                       -0.516 -0.001 -0.006       \nADH_HIGHVSLOWLow Adherence  -0.249 -0.028  0.035  0.002\n\nStandardized Within-Group Residuals:\n        Min          Q1         Med          Q3         Max \n-3.39660950 -0.43259762  0.09274777  0.46245152  3.02485528 \n\nNumber of Observations: 1613\nNumber of Groups: 550 \n\n\nAIC, BIC, and logLikelihood all prefer the most simplified model without the interaction term. Under this model hard drug use and adherence are not significant predictors of mental QOL. This is because depression is a confounder.\nI will therefore include the model with depression and the interaction term, so we can still investigate the main research question.\nTherefore model_MENT1 is the final model.\nTop of Tabset\n\n\n\n# Create the data set for mental QOL\ndata_MENT &lt;- data_2 |&gt; \n  select(newid, AGG_MENT, hard_drugs_grp, ADH_HIGHVSLOW, years, CESD) %&gt;%\n  filter(complete.cases(.))\n\n# Change reference level\ndata_MENT$hard_drugs_grp &lt;- relevel(data_MENT$hard_drugs_grp, ref = \"Never User\")\n\n# Create LMM\nmodel_MENT1 &lt;- lme(AGG_MENT ~ hard_drugs_grp + years + ADH_HIGHVSLOW + hard_drugs_grp*years + CESD,\n                   random = list(newid = pdSymm(~ years)), \n                   data = data_MENT)\n\n# Get model summary\nsummary(model_MENT1)\n\nLinear mixed-effects model fit by REML\n  Data: data_MENT \n       AIC      BIC    logLik\n  10848.59 10913.16 -5412.294\n\nRandom effects:\n Formula: ~years | newid\n Structure: General positive-definite\n            StdDev    Corr  \n(Intercept) 4.3455421 (Intr)\nyears       0.3717221 -1    \nResidual    6.0377346       \n\nFixed effects:  AGG_MENT ~ hard_drugs_grp + years + ADH_HIGHVSLOW + hard_drugs_grp *      years + CESD \n                                     Value Std.Error   DF   t-value p-value\n(Intercept)                       57.87606 0.4190797 1059 138.10276  0.0000\nhard_drugs_grpPrevious User        0.78568 1.0942518  546   0.71801  0.4731\nhard_drugs_grpCurrent User        -2.27986 0.9972527  546  -2.28614  0.0226\nyears                              0.30811 0.2073133 1059   1.48621  0.1375\nADH_HIGHVSLOWLow Adherence         0.85071 0.7612683  546   1.11749  0.2643\nCESD                              -0.91187 0.0180253 1059 -50.58831  0.0000\nhard_drugs_grpPrevious User:years -0.36332 0.6647696 1059  -0.54654  0.5848\nhard_drugs_grpCurrent User:years   1.69469 0.6106482 1059   2.77524  0.0056\n Correlation: \n                                  (Intr) hr__PU hr__CU years  ADH_HA CESD  \nhard_drugs_grpPrevious User       -0.184                                   \nhard_drugs_grpCurrent User        -0.291  0.100                            \nyears                             -0.581  0.193  0.225                     \nADH_HIGHVSLOWLow Adherence        -0.142 -0.014  0.021 -0.006              \nCESD                              -0.569 -0.104  0.029  0.091 -0.068       \nhard_drugs_grpPrevious User:years  0.159 -0.657 -0.069 -0.308 -0.001  0.011\nhard_drugs_grpCurrent User:years   0.230 -0.060 -0.679 -0.345  0.008 -0.089\n                                  h__PU:\nhard_drugs_grpPrevious User             \nhard_drugs_grpCurrent User              \nyears                                   \nADH_HIGHVSLOWLow Adherence              \nCESD                                    \nhard_drugs_grpPrevious User:years       \nhard_drugs_grpCurrent User:years   0.104\n\nStandardized Within-Group Residuals:\n        Min          Q1         Med          Q3         Max \n-3.44300701 -0.46895838  0.05349899  0.54923036  4.31899276 \n\nNumber of Observations: 1613\nNumber of Groups: 550 \n\n# Get 95% CI's\nintervals(model_MENT1, which = \"fixed\")\n\nApproximate 95% confidence intervals\n\n Fixed effects:\n                                        lower       est.      upper\n(Intercept)                       57.05373769 57.8760586 58.6983795\nhard_drugs_grpPrevious User       -1.36377666  0.7856821  2.9351409\nhard_drugs_grpCurrent User        -4.23878095 -2.2798591 -0.3209373\nyears                             -0.09867992  0.3081115  0.7149030\nADH_HIGHVSLOWLow Adherence        -0.64466145  0.8507117  2.3460849\nCESD                              -0.94723782 -0.9118685 -0.8764992\nhard_drugs_grpPrevious User:years -1.66773753 -0.3633223  0.9410929\nhard_drugs_grpCurrent User:years   0.49647472  1.6946926  2.8929104\n\n\nAfter controlling for adherence and between subject means (intercepts), there was no difference in average mental QOL between previous and current hard drug users (p = 0.47, 95% CI: -1.36, 2.94). On average, current hard drug users had a mental QOL score that was 2.28 points lower than never drug users (p = 0.023, 95% CI: -4.24, -0.32).\nAfter controlling for hard drug use and between subject means, those with low adherence did not differ significantly in average mental QOL compared to those with high adherence (p = 0.26, 95% CI: -0.64, 2.35).\nThere was a significant no change in mental QOL over time while controlling for subject-specific means, hard drug use, and adherence, with an average increase of 0.31 points per year (p 0.14, 95% CI: -0.099, 0.71).\nThe slope for mental QOL over time differed significantly based on hard drug use. On average, current hard drug users had a 1.69 point increase in physical QOL per year compared to never drug users (p = 0.056, 95% CI: 0.50, 2.89). The slope for physical QOL over time did not differ between never and previous users (p = 0.58, 95% CI: -1.67, 0.94).\nControlling for adherence, hard drug use, and time, depression was a significant predictor of mental QOL. On average, mental QOL score decreased by 0.91 points for every 1 point increase in depression score (p &lt; 0.0001, 95% CI: -0.95, -0.88).\n\nChange the Reference Level\n\n# Create the data set for mental QOL\ndata_MENT &lt;- data_2 |&gt; \n  select(newid, AGG_MENT, hard_drugs_grp, ADH_HIGHVSLOW, years, CESD) %&gt;%\n  filter(complete.cases(.))\n\n# Change reference level\ndata_MENT$hard_drugs_grp &lt;- relevel(data_MENT$hard_drugs_grp, ref = \"Previous User\")\n\n# Create LMM\nmodel_MENT1 &lt;- lme(AGG_MENT ~ hard_drugs_grp + years + ADH_HIGHVSLOW + hard_drugs_grp*years + CESD,\n                   random = list(newid = pdSymm(~ years)), \n                   data = data_MENT)\n\n# Get model summary\nsummary(model_MENT1)\n\nLinear mixed-effects model fit by REML\n  Data: data_MENT \n       AIC      BIC    logLik\n  10848.59 10913.16 -5412.294\n\nRandom effects:\n Formula: ~years | newid\n Structure: General positive-definite\n            StdDev    Corr  \n(Intercept) 4.3455460 (Intr)\nyears       0.3717269 -1    \nResidual    6.0377348       \n\nFixed effects:  AGG_MENT ~ hard_drugs_grp + years + ADH_HIGHVSLOW + hard_drugs_grp *      years + CESD \n                                    Value Std.Error   DF   t-value p-value\n(Intercept)                      58.66174 1.0974260 1059  53.45394  0.0000\nhard_drugs_grpNever User         -0.78568 1.0942522  546  -0.71801  0.4731\nhard_drugs_grpCurrent User       -3.06554 1.4047201  546  -2.18231  0.0295\nyears                            -0.05521 0.6324041 1059  -0.08730  0.9304\nADH_HIGHVSLOWLow Adherence        0.85071 0.7612681  546   1.11749  0.2643\nCESD                             -0.91187 0.0180253 1059 -50.58831  0.0000\nhard_drugs_grpNever User:years    0.36332 0.6647696 1059   0.54654  0.5848\nhard_drugs_grpCurrent User:years  2.05801 0.8546116 1059   2.40813  0.0162\n Correlation: \n                                 (Intr) hr__NU hr__CU years  ADH_HA CESD  \nhard_drugs_grpNever User         -0.927                                   \nhard_drugs_grpCurrent User       -0.730  0.708                            \nyears                            -0.635  0.628  0.490                     \nADH_HIGHVSLOWLow Adherence       -0.068  0.014  0.026 -0.003              \nCESD                             -0.320  0.104  0.101  0.042 -0.068       \nhard_drugs_grpNever User:years    0.595 -0.657 -0.463 -0.950  0.001 -0.011\nhard_drugs_grpCurrent User:years  0.483 -0.469 -0.672 -0.742  0.006 -0.072\n                                 h__NU:\nhard_drugs_grpNever User               \nhard_drugs_grpCurrent User             \nyears                                  \nADH_HIGHVSLOWLow Adherence             \nCESD                                   \nhard_drugs_grpNever User:years         \nhard_drugs_grpCurrent User:years  0.704\n\nStandardized Within-Group Residuals:\n        Min          Q1         Med          Q3         Max \n-3.44300908 -0.46895846  0.05349847  0.54923016  4.31899084 \n\nNumber of Observations: 1613\nNumber of Groups: 550 \n\n# Get 95% CI's\nintervals(model_MENT1, which = \"fixed\")\n\nApproximate 95% confidence intervals\n\n Fixed effects:\n                                      lower        est.      upper\n(Intercept)                      56.5083643 58.66174078 60.8151173\nhard_drugs_grpNever User         -2.9351417 -0.78568215  1.3637774\nhard_drugs_grpCurrent User       -5.8248585 -3.06554112 -0.3062238\nyears                            -1.2961183 -0.05521077  1.1856968\nADH_HIGHVSLOWLow Adherence       -0.6446619  0.85071103  2.3460840\nCESD                             -0.9472378 -0.91186850 -0.8764992\nhard_drugs_grpNever User:years   -0.9410930  0.36332233  1.6677377\nhard_drugs_grpCurrent User:years  0.3810903  2.05801476  3.7349392\n\n\nCurrent users had on average a 3.07 point lower mental QOL score compared to previous users (p = 0.03, 95% CI: 5.82, -0.31).\nThe slope for mental QOL over time also differed by hard drug use group. On average, current hard drug users had a 2.06 point increase in mental QOL compared to previous users (p = 0.016, 95% CI: 0.38, 3.73).\nTop of Tabset\n\n\n\n\nHard Drug Use\nAfter controlling for adherence and between subject means (intercepts), there was no difference in average mental QOL between never and previous hard drug users (p = 0.47, 95% CI: -1.36, 2.94). On average, current hard drug users had a mental QOL score that was 2.28 points lower than never drug users (p = 0.023, 95% CI: -4.24, -0.32) and 3.07 point lower mental QOL score compared to previous users (p = 0.03, 95% CI: 5.82, -0.31).\n\n\nAdherence\nAfter controlling for hard drug use and between subject means, those with low adherence did not differ significantly in average mental QOL compared to those with high adherence (p = 0.26, 95% CI: -0.64, 2.35)\n\n\nPhysical Quality of life by Hard Drug Use over Time\nThe slope for mental QOL over time differed significantly based on hard drug use. On average, current hard drug users had a 1.69 point increase in physical QOL per year compared to never drug users (p = 0.056, 95% CI: 0.50, 2.89). The slope for physical QOL over time did not differ between never and previous users (p = 0.58, 95% CI: -1.67, 0.94). On average, current hard drug users had a 2.06 point increase in mental QOL compared to previous users (p = 0.016, 95% CI: 0.38, 3.73).\n\n\nTime\nThere was a significant no change in mental QOL over time while controlling for subject-specific means, hard drug use, and adherence, with an average increase of 0.31 points per year (p 0.14, 95% CI: -0.099, 0.71).\n\n\nDepression\nControlling for adherence, hard drug use, and time, depression was a significant predictor of mental QOL. On average, mental QOL score decreased by 0.91 points for every 1 point increase in depression score (p &lt; 0.0001, 95% CI: -0.95, -0.88).\nTop of Tabset\n\n\n\nLet’s examine the main interaction for physical QOL by hard drug use group over time.\n\n# Get fitted values for plotting\ndata_MENT &lt;- data_MENT %&gt;%\n  mutate(fitted = predict(model_MENT1))\n\n# Plot the interaction\nggplot(data_MENT, aes(x = years, y = fitted, color = hard_drugs_grp)) +\n  geom_smooth(method = \"lm\", se = FALSE) + \n  labs(title = \"Predicted Mental QOL over Years by Hard Drug Use\",\n       x = \"Year\",\n       y = \"Predicted Mental QOL\") +\n  theme_minimal() +\n  scale_color_brewer(palette = \"Pastel2\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nHere we see the main effect that current users had lower mental QOL compared to previous and never users. The slopes were significantly different between current and previous users, but this was likely not significant following correction for pairwise comparisons."
  },
  {
    "objectID": "Project_2/Project_2_R/Code/Project2LMM.html#Phys",
    "href": "Project_2/Project_2_R/Code/Project2LMM.html#Phys",
    "title": "Linear Mixed Model",
    "section": "Physical QOL",
    "text": "Physical QOL\n\nModel SelectionAnalysisInterpretationVisualization\n\n\nWe will begin by fitting the full model predicting physical QOL, including as covariates: hard_drugs_grp, ADH_HIGHVSLOW, FRP, years, and the hard_drugs_grp*years interaction term.\nWe know that we do not meet the assumption of equality of variances (as pictured below), so we will employ an unstructured covariance matrix to account for this.\n\nggplot(data_2, aes(x = AGG_PHYS, y = hard_drugs_grp, fill = hard_drugs_grp)) +\n  geom_density_ridges(alpha = 0.7, scale = 1) +\n  theme_ridges() +\n  scale_color_brewer(palette = \"Pastel2\") +\n  labs(title = \"Ridge Plot of AGG_PHYS by hard_drugs_grp\",\n       x = \"AGG_PHYS\",\n       y = \"hard_drugs_grp\") +\n  theme(legend.position = \"none\")\n\nPicking joint bandwidth of 2.83\n\n\nWarning: Removed 10 rows containing non-finite outside the scale range\n(`stat_density_ridges()`).\n\n\n\n\n\n\n\n\n\nLet’s fit the model.\n\n# Filter data set\ndata_PHYS &lt;- data_2 |&gt; \n  select(newid, years, hard_drugs_grp, ADH_HIGHVSLOW, AGG_PHYS, FRP) %&gt;%\n  filter(complete.cases(.))\n\n# Fit LMM\nmodel_PHYS1 &lt;- lme(AGG_PHYS ~ hard_drugs_grp + ADH_HIGHVSLOW + years + hard_drugs_grp*years + FRP,\n                   random = list(newid = pdSymm(~ years)), \n                   data = data_PHYS)\n\n# Examine summary\nsummary(model_PHYS1)\n\nLinear mixed-effects model fit by REML\n  Data: data_PHYS \n       AIC      BIC    logLik\n  11148.49 11213.26 -5562.246\n\nRandom effects:\n Formula: ~years | newid\n Structure: General positive-definite\n            StdDev    Corr  \n(Intercept) 6.2152343 (Intr)\nyears       0.4360586 1     \nResidual    5.4292458       \n\nFixed effects:  AGG_PHYS ~ hard_drugs_grp + ADH_HIGHVSLOW + years + hard_drugs_grp *      years + FRP \n                                     Value Std.Error   DF   t-value p-value\n(Intercept)                       49.65356 1.1832279 1086  41.96449  0.0000\nhard_drugs_grpNever User           2.01176 1.2326815  546   1.63202  0.1033\nhard_drugs_grpCurrent User         3.08028 1.5625444  546   1.97132  0.0492\nADH_HIGHVSLOWLow Adherence        -1.71602 1.0419602  546  -1.64691  0.1002\nyears                             -1.38631 0.5706312 1086  -2.42943  0.0153\nFRPYes                           -14.03646 0.7584225 1086 -18.50745  0.0000\nhard_drugs_grpNever User:years     0.80634 0.5998861 1086   1.34416  0.1792\nhard_drugs_grpCurrent User:years  -1.01094 0.7605046 1086  -1.32931  0.1840\n Correlation: \n                                 (Intr) hr__NU hr__CU ADH_HA years  FRPYes\nhard_drugs_grpNever User         -0.948                                   \nhard_drugs_grpCurrent User       -0.753  0.717                            \nADH_HIGHVSLOWLow Adherence       -0.115  0.025  0.043                     \nyears                            -0.392  0.374  0.297  0.000              \nFRPYes                           -0.075  0.039  0.064  0.003  0.058       \nhard_drugs_grpNever User:years    0.373 -0.393 -0.283  0.000 -0.952 -0.061\nhard_drugs_grpCurrent User:years  0.298 -0.283 -0.395  0.000 -0.753 -0.093\n                                 h__NU:\nhard_drugs_grpNever User               \nhard_drugs_grpCurrent User             \nADH_HIGHVSLOWLow Adherence             \nyears                                  \nFRPYes                                 \nhard_drugs_grpNever User:years         \nhard_drugs_grpCurrent User:years  0.717\n\nStandardized Within-Group Residuals:\n        Min          Q1         Med          Q3         Max \n-3.95690610 -0.45956434  0.05295024  0.53606641  2.83142953 \n\nNumber of Observations: 1640\nNumber of Groups: 550 \n\n\nLet’s also check if that adherence by years interaction is significant.\n\n# Fit LMM\nmodel_PHYS2 &lt;- lme(AGG_PHYS ~ hard_drugs_grp + ADH_HIGHVSLOW + years + hard_drugs_grp*years + FRP + ADH_HIGHVSLOW*years,\n                   random = list(newid = pdSymm(~ years)), \n                   data = data_PHYS)\n\n# Examine summary\nsummary(model_PHYS2)\n\nLinear mixed-effects model fit by REML\n  Data: data_PHYS \n       AIC      BIC    logLik\n  11136.41 11206.57 -5555.206\n\nRandom effects:\n Formula: ~years | newid\n Structure: General positive-definite\n            StdDev    Corr  \n(Intercept) 6.2348253 (Intr)\nyears       0.4295268 1     \nResidual    5.3969297       \n\nFixed effects:  AGG_PHYS ~ hard_drugs_grp + ADH_HIGHVSLOW + years + hard_drugs_grp *      years + FRP + ADH_HIGHVSLOW * years \n                                     Value Std.Error   DF   t-value p-value\n(Intercept)                       49.43982 1.1841316 1085  41.75196  0.0000\nhard_drugs_grpNever User           2.06036 1.2322716  546   1.67200  0.0951\nhard_drugs_grpCurrent User         3.18660 1.5621690  546   2.03986  0.0418\nADH_HIGHVSLOWLow Adherence        -0.09878 1.1316120  546  -0.08730  0.9305\nyears                             -1.12156 0.5716636 1085  -1.96192  0.0500\nFRPYes                           -14.01236 0.7546578 1085 -18.56783  0.0000\nhard_drugs_grpNever User:years     0.74548 0.5964647 1085   1.24983  0.2116\nhard_drugs_grpCurrent User:years  -1.14212 0.7566907 1085  -1.50936  0.1315\nADH_HIGHVSLOWLow Adherence:years  -2.02171 0.5499329 1085  -3.67628  0.0002\n Correlation: \n                                 (Intr) hr__NU hr__CU ADH_HIGHVSLOWLwA years \nhard_drugs_grpNever User         -0.947                                      \nhard_drugs_grpCurrent User       -0.753  0.717                               \nADH_HIGHVSLOWLow Adherence       -0.125  0.027  0.046                        \nyears                            -0.392  0.370  0.295  0.049                 \nFRPYes                           -0.074  0.038  0.064  0.000            0.057\nhard_drugs_grpNever User:years    0.372 -0.391 -0.282 -0.010           -0.947\nhard_drugs_grpCurrent User:years  0.297 -0.281 -0.393 -0.018           -0.752\nADH_HIGHVSLOWLow Adherence:years  0.048 -0.010 -0.018 -0.389           -0.125\n                                 FRPYes h__NU: h__CU:\nhard_drugs_grpNever User                             \nhard_drugs_grpCurrent User                           \nADH_HIGHVSLOWLow Adherence                           \nyears                                                \nFRPYes                                               \nhard_drugs_grpNever User:years   -0.060              \nhard_drugs_grpCurrent User:years -0.093  0.717       \nADH_HIGHVSLOWLow Adherence:years  0.006  0.027  0.046\n\nStandardized Within-Group Residuals:\n        Min          Q1         Med          Q3         Max \n-4.01686500 -0.48052970  0.06048571  0.54335717  2.85109147 \n\nNumber of Observations: 1640\nNumber of Groups: 550 \n\n\nAIC, BIC, and logLik all prefer the model with both interactions. This will be our final model.\nTop of Tabset\n\n\n\n# Change reference level\ndata_PHYS$hard_drugs_grp &lt;- relevel(data_PHYS$hard_drugs_grp, ref = \"Never User\")\n\n# Fit LMM\nmodel_PHYS2 &lt;- lme(AGG_PHYS ~ hard_drugs_grp + ADH_HIGHVSLOW + years + hard_drugs_grp*years + FRP + ADH_HIGHVSLOW*years,\n                   random = list(newid = pdSymm(~ years)), \n                   data = data_PHYS)\n\n# Examine summary\nsummary(model_PHYS2)\n\nLinear mixed-effects model fit by REML\n  Data: data_PHYS \n       AIC      BIC    logLik\n  11136.41 11206.57 -5555.206\n\nRandom effects:\n Formula: ~years | newid\n Structure: General positive-definite\n            StdDev   Corr  \n(Intercept) 6.234822 (Intr)\nyears       0.429531 1     \nResidual    5.396929       \n\nFixed effects:  AGG_PHYS ~ hard_drugs_grp + ADH_HIGHVSLOW + years + hard_drugs_grp *      years + FRP + ADH_HIGHVSLOW * years \n                                      Value Std.Error   DF   t-value p-value\n(Intercept)                        51.50018 0.3965111 1085 129.88333  0.0000\nhard_drugs_grpPrevious User        -2.06036 1.2322712  546  -1.67200  0.0951\nhard_drugs_grpCurrent User          1.12624 1.0950838  546   1.02846  0.3042\nADH_HIGHVSLOWLow Adherence         -0.09878 1.1316116  546  -0.08730  0.9305\nyears                              -0.37608 0.1915536 1085  -1.96331  0.0499\nFRPYes                            -14.01235 0.7546578 1085 -18.56782  0.0000\nhard_drugs_grpPrevious User:years  -0.74548 0.5964647 1085  -1.24983  0.2116\nhard_drugs_grpCurrent User:years   -1.88760 0.5300278 1085  -3.56132  0.0004\nADH_HIGHVSLOWLow Adherence:years   -2.02171 0.5499329 1085  -3.67628  0.0002\n Correlation: \n                                  (Intr) hr__PU hr__CU ADH_HIGHVSLOWLwA years \nhard_drugs_grpPrevious User       -0.280                                      \nhard_drugs_grpCurrent User        -0.343  0.103                               \nADH_HIGHVSLOWLow Adherence        -0.289 -0.027  0.036                        \nyears                             -0.386  0.112  0.132  0.113                 \nFRPYes                            -0.102 -0.038  0.048  0.000           -0.019\nhard_drugs_grpPrevious User:years  0.105 -0.391 -0.038  0.010           -0.287\nhard_drugs_grpCurrent User:years   0.139 -0.038 -0.391 -0.014           -0.340\nADH_HIGHVSLOWLow Adherence:years   0.112  0.010 -0.014 -0.389           -0.290\n                                  FRPYes h__PU: h__CU:\nhard_drugs_grpPrevious User                           \nhard_drugs_grpCurrent User                            \nADH_HIGHVSLOWLow Adherence                            \nyears                                                 \nFRPYes                                                \nhard_drugs_grpPrevious User:years  0.060              \nhard_drugs_grpCurrent User:years  -0.065  0.101       \nADH_HIGHVSLOWLow Adherence:years   0.006 -0.027  0.035\n\nStandardized Within-Group Residuals:\n        Min          Q1         Med          Q3         Max \n-4.01686316 -0.48052935  0.06048594  0.54335666  2.85109182 \n\nNumber of Observations: 1640\nNumber of Groups: 550 \n\n# Get 95% CI's\nintervals(model_PHYS2, which = \"fixed\")\n\nApproximate 95% confidence intervals\n\n Fixed effects:\n                                        lower         est.          upper\n(Intercept)                        50.7221620  51.50017727  52.2781925500\nhard_drugs_grpPrevious User        -4.4809294  -2.06035669   0.3602160769\nhard_drugs_grpCurrent User         -1.0248484   1.12624474   3.2773378408\nADH_HIGHVSLOWLow Adherence         -2.3216295  -0.09878414   2.1240612074\nyears                              -0.7519374  -0.37607985  -0.0002223435\nFRPYes                            -15.4931073 -14.01235342 -12.5315995323\nhard_drugs_grpPrevious User:years  -1.9158339  -0.74547894   0.4248760243\nhard_drugs_grpCurrent User:years   -2.9275931  -1.88759751  -0.8476019473\nADH_HIGHVSLOWLow Adherence:years   -3.1007595  -2.02170706  -0.9426546025\n\n\nAfter controlling for adherence and between subject means (intercepts), there was no difference in average physical QOL between never hard drug users and previous users (p = 0.95, 95% CI: -4.48, 0.36) and current users (p = 0.30, 95% CI: -1.02, 3.27).\nAfter controlling for hard drug use and between subject means, those with low adherence did not differ significantly in average physical QOL compared to those with high adherence (p = 0.93, 95% CI: -2.32, 2.12).\nThere was a significant decrease in physical QOL over time while controlling for subject-specific means, hard drug use, and adherence, with an average decrease of 0.38 points per year (p &lt; 0.05, 95% CI: -0.75, -0.002). However this value was borderline significant and may not be a true relationshi, especially following correction for multiple pairwise comparisons.\nThe slope for physical QOL over time differed significantly based on hard drug use. On average, current hard drug users had a 1.89 point decrease in physical QOL per year compared to never drug users (p = 0.0004, 95% CI: -2.93, -0.85). The slope for physical QOL over time did not differ between never and previous users (p = 0.2116, 95% CI: -1.916, 0.42).\nThe slope for physical QOL over time also differed significantly based on adherence. On average, those with low adherence had a 2.02 point decrease in physical QOL per year compared to those with high adherence (p = 0.0002, 95% CI: -3.10, -0.94).\nAfter controlling for hard drug user, adherence, and their interactions over time, and between subject means, those with a frailty related phenotype had on average a physical QOL score that was 14.01 points lower than those without a frailty related phenotype (p &lt; 0.0001, 95% CI: -15.49, -12.53).\n\nChange Reference Level\n\n# Change reference level\ndata_PHYS$hard_drugs_grp &lt;- relevel(data_PHYS$hard_drugs_grp, ref = \"Previous User\")\n\n# Fit LMM\nmodel_PHYS2 &lt;- lme(AGG_PHYS ~ hard_drugs_grp + ADH_HIGHVSLOW + years + hard_drugs_grp*years + FRP + ADH_HIGHVSLOW*years,\n                   random = list(newid = pdSymm(~ years)), \n                   data = data_PHYS)\n\n# Examine summary\nsummary(model_PHYS2)\n\nLinear mixed-effects model fit by REML\n  Data: data_PHYS \n       AIC      BIC    logLik\n  11136.41 11206.57 -5555.206\n\nRandom effects:\n Formula: ~years | newid\n Structure: General positive-definite\n            StdDev    Corr  \n(Intercept) 6.2348253 (Intr)\nyears       0.4295268 1     \nResidual    5.3969297       \n\nFixed effects:  AGG_PHYS ~ hard_drugs_grp + ADH_HIGHVSLOW + years + hard_drugs_grp *      years + FRP + ADH_HIGHVSLOW * years \n                                     Value Std.Error   DF   t-value p-value\n(Intercept)                       49.43982 1.1841316 1085  41.75196  0.0000\nhard_drugs_grpNever User           2.06036 1.2322716  546   1.67200  0.0951\nhard_drugs_grpCurrent User         3.18660 1.5621690  546   2.03986  0.0418\nADH_HIGHVSLOWLow Adherence        -0.09878 1.1316120  546  -0.08730  0.9305\nyears                             -1.12156 0.5716636 1085  -1.96192  0.0500\nFRPYes                           -14.01236 0.7546578 1085 -18.56783  0.0000\nhard_drugs_grpNever User:years     0.74548 0.5964647 1085   1.24983  0.2116\nhard_drugs_grpCurrent User:years  -1.14212 0.7566907 1085  -1.50936  0.1315\nADH_HIGHVSLOWLow Adherence:years  -2.02171 0.5499329 1085  -3.67628  0.0002\n Correlation: \n                                 (Intr) hr__NU hr__CU ADH_HIGHVSLOWLwA years \nhard_drugs_grpNever User         -0.947                                      \nhard_drugs_grpCurrent User       -0.753  0.717                               \nADH_HIGHVSLOWLow Adherence       -0.125  0.027  0.046                        \nyears                            -0.392  0.370  0.295  0.049                 \nFRPYes                           -0.074  0.038  0.064  0.000            0.057\nhard_drugs_grpNever User:years    0.372 -0.391 -0.282 -0.010           -0.947\nhard_drugs_grpCurrent User:years  0.297 -0.281 -0.393 -0.018           -0.752\nADH_HIGHVSLOWLow Adherence:years  0.048 -0.010 -0.018 -0.389           -0.125\n                                 FRPYes h__NU: h__CU:\nhard_drugs_grpNever User                             \nhard_drugs_grpCurrent User                           \nADH_HIGHVSLOWLow Adherence                           \nyears                                                \nFRPYes                                               \nhard_drugs_grpNever User:years   -0.060              \nhard_drugs_grpCurrent User:years -0.093  0.717       \nADH_HIGHVSLOWLow Adherence:years  0.006  0.027  0.046\n\nStandardized Within-Group Residuals:\n        Min          Q1         Med          Q3         Max \n-4.01686500 -0.48052970  0.06048571  0.54335717  2.85109147 \n\nNumber of Observations: 1640\nNumber of Groups: 550 \n\n# Get 95% CI's\nintervals(model_PHYS2, which = \"fixed\")\n\nApproximate 95% confidence intervals\n\n Fixed effects:\n                                       lower         est.          upper\n(Intercept)                       47.1163737  49.43982096  51.7632681681\nhard_drugs_grpNever User          -0.3602171   2.06035649   4.4809301203\nhard_drugs_grpCurrent User         0.1180039   3.18660100   6.2551980783\nADH_HIGHVSLOWLow Adherence        -2.3216303  -0.09878418   2.1240619640\nyears                             -2.2432503  -1.12155894   0.0001324472\nFRPYes                           -15.4931105 -14.01235656 -12.5316026397\nhard_drugs_grpNever User:years    -0.4248758   0.74547909   1.9158339765\nhard_drugs_grpCurrent User:years  -2.6268611  -1.14211827   0.3426245694\nADH_HIGHVSLOWLow Adherence:years  -3.1007594  -2.02170698  -0.9426546059\n\n\nCurrent hard drug users had on average a physical QOL score that was 3.19 points higher than previous users (p = 0.042, 95% CI: 0.12, 6.26).\nThe slope for physical QOL over time did not differ between current and previous hard drug users (p = 0.13, 95% CI: -2.63, 0.34).\nTop of Tabest\n\n\n\n\nHard Drug Use\nAfter controlling for adherence and between subject means (intercepts), there was no difference in average physical QOL between never hard drug users and previous users (p = 0.95, 95% CI: -4.48, 0.36) and current users (p = 0.30, 95% CI: -1.02, 3.27). Current hard drug users had on average a physical QOL score that was 3.19 points higher than previous users (p = 0.042, 95% CI: 0.12, 6.26).\n\n\nAdherence\nAfter controlling for hard drug use and between subject means, those with low adherence did not differ significantly in average physical QOL compared to those with high adherence (p = 0.93, 95% CI: -2.32, 2.12).\n\n\nPhysical QOL by Hard Drug Use over Time\nThe slope for physical QOL over time differed significantly based on hard drug use. On average, current hard drug users had a 1.89 point decrease in physical QOL per year compared to never drug users (p = 0.0004, 95% CI: -2.93, -0.85). The slope for physical QOL over time did not differ between never and previous users (p = 0.2116, 95% CI: -1.916, 0.42). The slope for physical QOL over time did not differ between current and previous hard drug users (p = 0.13, 95% CI: -2.63, 0.34).\n\n\nPhysical QOL by Adherence over Time\nThe slope for physical QOL over time also differed significantly based on adherence. On average, those with low adherence had a 2.02 point decrease in physical QOL per year compared to those with high adherence (p = 0.0002, 95% CI: -3.10, -0.94).\n\n\nTime\nThere was a significant decrease in physical QOL over time while controlling for subject-specific means, hard drug use, and adherence, with an average decrease of 0.38 points per year (p &lt; 0.05, 95% CI: -0.75, -0.002). However this value was borderline significant and may not be a true relationshi, especially following correction for multiple pairwise comparisons.\n\n\nFrailty Related Phenotype\nAfter controlling for hard drug user, adherence, and their interactions over time, and between subject means, those with a frailty related phenotype had on average a physical QOL score that was 14.01 points lower than those without a frailty related phenotype (p &lt; 0.0001, 95% CI: -15.49, -12.53).\nTop of Tabset\n\n\n\nTop of Tabset\nFirst we examine the interaction between hard drug use and time for physical QOL.\n\n# Get fitted values for plotting\ndata_PHYS &lt;- data_PHYS |&gt;\n  mutate(fitted = predict(model_PHYS2))\n\n# Plot the interaction\nggplot(data_PHYS, aes(x = years, y = fitted, color = hard_drugs_grp)) +\n  geom_smooth(method = \"lm\", se = FALSE) + \n  labs(title = \"Predicted Physical QOL over Years by Hard Drug Use\",\n       x = \"Year\",\n       y = \"Predicted Physical QOL\") +\n  theme_minimal() +\n  scale_color_brewer(palette = \"Pastel2\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nFrom this plot, we can see the main interaction that current hard drug users had a steeper negative slope compared to never users. This difference was not statistically significant between current and previous users, although it looks like it should be.\nThat is, current drug users had more of a decline in physical QOL over time compared to previous and current drug users.\nLet’s then examine the interaction for adherence over time.\n\n# Plot the interaction\nggplot(data_PHYS, aes(x = years, y = fitted, color = ADH_HIGHVSLOW)) +\n  geom_smooth(method = \"lm\", se = FALSE) + \n  labs(title = \"Predicted Physical QOL over Years by Adherence\",\n       x = \"Year\",\n       y = \"Predicted Physical QOL\") +\n  theme_minimal() +\n  scale_color_brewer(palette = \"Pastel2\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nLikewise, we can see the main interaction here: those with low adherence had a steeper slope compared to those with high adherence. That is, if a patient had low adherence, they had more of a decline in physical QOL compared to those with high adherence."
  },
  {
    "objectID": "Project_2/Project_2_R/Code/Project2LMM.html#linear-mixed-model-vs-multiple-linear-regression",
    "href": "Project_2/Project_2_R/Code/Project2LMM.html#linear-mixed-model-vs-multiple-linear-regression",
    "title": "Linear Mixed Model",
    "section": "Linear Mixed Model vs Multiple Linear Regression",
    "text": "Linear Mixed Model vs Multiple Linear Regression\nFinally, we learned how much better suited a linear mixed model is to handling repeated measures than a multiple linear regression with change scores!"
  },
  {
    "objectID": "Project_3_Predictive_Modelling/Project_3_R/Code/Project_3_Predictive_Modelling.html",
    "href": "Project_3_Predictive_Modelling/Project_3_R/Code/Project_3_Predictive_Modelling.html",
    "title": "Predictive Modelling",
    "section": "",
    "text": "The aim of the current study is to investigate whether MRI image features extracted from baseline kidney images can enhance the prediction of disease progression in young Autosomal Dominant Polycystic Kidney Disease (ADPKD) patients. This study is important because recent literature has demonstrated the inclusion of MRI image features improves the prognostic accuracy of models in adults, but this has not yet been demonstrated in children. Improving prediction models could greatly assist in diagnosing and predicting kidney disease outcomes in children, which is more challenging than in adults and could lead to improved treatment.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe two clinical hypotheses for this study are that including MRI image features will improve model performance compared to using baseline kidney volume alone in models predicting\n\nPercentage change of total kidney volume growth\nClassification of a patient as having fast or slow progression of the disease."
  },
  {
    "objectID": "Project_3_Predictive_Modelling/Project_3_R/Code/Project_3_Predictive_Modelling.html#gabor-transform",
    "href": "Project_3_Predictive_Modelling/Project_3_R/Code/Project_3_Predictive_Modelling.html#gabor-transform",
    "title": "Predictive Modelling",
    "section": "Gabor Transform",
    "text": "Gabor Transform\n\nWhat is it\nThe Gabor Transform, named after Dennis Gabor, is a powerful technique for analyzing the texture of MRI images. By convolving the image with a Gabor filter, it becomes possible to discriminate between features based on intensity differences. This allows the target region and background to be differentiated if they possess distinct features, as they will exhibit different intensity levels. Moreover, the Gabor Transform has tunable parameters, such as the frequency of the sinusoidal wave, which can be adjusted to extract specific textures from the images. Higher frequencies are ideal for capturing fine textures, while lower frequencies are better suited for coarse textures.\n\n\nHow it Works\nThe Gabor Filter first applies a Gaussian Envelope to focus on a small region of the image. It then applies a sinusoidal wave that oscillates at a specific frequency and captures the variation in intensities at that frequency and orientation within that region.\n\n\nExample of Gabor Transform\n\n\n\nExample of applying a Gaussian, then Gabor, and Laplacian Filter (GGL) for differentiation between two different textures (+’s vs L’s) (2).\n\n\n\n\nImage Features Provided by the Gabor Transform\nIn general, Gabor functions can easily extract features of:\n\nSpatial Frequency (e.g. how often pixel intensity changes in a given area)\nDensity (e.g. concentration of features within a certain area)\nOrientation (e.g. Textures or edges at 0°, 45°, 90°, 135°)\nPhase (e.g. alignment/distance of features)\nEnergy (e.g. overall intensity)\n\nSources: 1, 2"
  },
  {
    "objectID": "Project_3_Predictive_Modelling/Project_3_R/Code/Project_3_Predictive_Modelling.html#gray-level-co-occurrence-matrix",
    "href": "Project_3_Predictive_Modelling/Project_3_R/Code/Project_3_Predictive_Modelling.html#gray-level-co-occurrence-matrix",
    "title": "Predictive Modelling",
    "section": "Gray Level Co-Occurrence Matrix",
    "text": "Gray Level Co-Occurrence Matrix\n\nWhat is it\nThe Gray Level Co-Occurrence Matrix (GLCM) is another powerful tool for extracting texture features from an MRI image. GLCM works under the assumption that the textural information in an image is contained in the “average” spatial relationship that the gray tones in the image have to each other. For example, when a small patch of the picture has little variation of features, the dominant property is tone; When a small patch of a picture has high variation, the dominant property is texture (3).\n\n\nHow it Works\nGLCM examines the spatial relationship between pairs of pixels in an image and calculates how often pairs of pixel values occur at a specified distance and orientation. It does this by computing a set of gray-tone spacial-dependence matrices for various angular relationships and distances between neighboring resolution cell pairs on the image (3).\n\n\nExample of GLCM\n\n\n\nExample of textural features extracted from two different land-use category images (3).\n\n\n\n\nImage Features Provided by GLCM\nIn general, GLCM provides information on the following features:\n\nHomogeneity\nLinear Structure\nContrast\nNumber and Nature of Boundaries\nComplexity of the Image"
  },
  {
    "objectID": "Project_3_Predictive_Modelling/Project_3_R/Code/Project_3_Predictive_Modelling.html#local-binary-pattern",
    "href": "Project_3_Predictive_Modelling/Project_3_R/Code/Project_3_Predictive_Modelling.html#local-binary-pattern",
    "title": "Predictive Modelling",
    "section": "Local Binary Pattern",
    "text": "Local Binary Pattern\n\nWhat is it\nThe Local Binary Pattern (LBP) is a third powerful method for extracting texture features from an image. An important and fundamental property of texture is how uniform the patterns are, and LBP captures this by detecting these uniform patterns in circular neighborhoods at any rotation and spatial resolution. LBP is rotation invariant, meaning it does not matter what rotation the image is at; it will always extract very similar features) (4).\n\n\nHow it Works\nLBP works by comparing the intensity of a central pixel with its neighboring pixels and encoding this relationship into a binary pattern. For each neighbor, if it’s intensity is greater or equal to the central pixel, it gets assigned a 1 (otherwise a 0). The binary values of all neighbors are then concatenated to form a binary number, and this number is converted into a decimal that represent the LBP for the central pixel (4).\n\n\nExample of LBP\n\n\n\nExample of the 36 unique comparisons that can be made between neighboring pixels (4).\n\n\n\n\nCode Information\n\n\n\n\n\n\n\nImage Features Provided by the LBP\nIn general, the LBP provides image features on:\n\nUniformity\nLocal Contrast\nTexture Description\nSpatial Patterns\nGray Level Distribution"
  },
  {
    "objectID": "Project_3_Predictive_Modelling/Project_3_R/Code/Project_3_Predictive_Modelling.html#study-design",
    "href": "Project_3_Predictive_Modelling/Project_3_R/Code/Project_3_Predictive_Modelling.html#study-design",
    "title": "Predictive Modelling",
    "section": "Study Design",
    "text": "Study Design\nThe investigators recruited 71 young patients with ADPKD and collected MRI data at baseline and after 3 years. Additionally, the height corrected kidney volume for each patient was collected at baseline and 3 years by a physician, and the percentage change calculated. Patients were classified as having slow or fast progression of the disease based on this percentage change. Image features were extracted from the baseline MRI images including 2 image features on kidney geometric information, 5 features based on Gabor transform, 2 features based on gray level co-occurrence matrix, 5 features based on image textures, and 5 features based on local binary pattern.\n\nStatistical Hypotheses\n\nA linear regression model predicting percentage change of total kidney volume growth including MRI image features and baseline kidney volume will have better performance than a model with baseline kidney volume alone, as determined by specificity, sensitivity, PPV,  NPV,  accuracy, and AUC.\nA logistic regression model predicting classification of disease progression as slow or fast including MRI image features and baseline kidney volume will have better performance than a model with baseline kidney volume alone, as determined by specificity, sensitivity, PPV,  NPV,  accuracy, and AUC."
  },
  {
    "objectID": "Project_3_Predictive_Modelling/Project_3_R/Code/Project_3_Predictive_Modelling.html#univariate-distributions",
    "href": "Project_3_Predictive_Modelling/Project_3_R/Code/Project_3_Predictive_Modelling.html#univariate-distributions",
    "title": "Predictive Modelling",
    "section": "Univariate Distributions",
    "text": "Univariate Distributions\n\nOutcome Variables\n\nKidney VolumeDisease ProgressionSummary\n\n\n\nBaseline Kidney Volume\n\n# Visualize baseline kidney volume\nggplot(data, aes(x = kidvol_base)) +\n  geom_histogram() + \n  theme_minimal() + \n  labs(title = \"Histogram for Baseline Kidney Volume\",\n       x = \"Baseline Kidney Volume\",\n       y = \"Count\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nKidney volume at baseline appears to not be normally distributed.\nHowever, I learned in the last project that taking the change score (or percent) will make the outcome normally distributed, as it is a way of accounting for between subject variance. Let’s check that that’s the case this time.\n\n\nPercent Change in Kidney Volume\n\n# Create histogram\nhist_plot &lt;- ggplot(data, aes(x = kidvol_change)) +\n  geom_histogram(binwidth = 4.5) +\n  theme_minimal() +\n  labs(title = \"Histogram of Kidney Volume Change\",\n       y = \"Count\",\n       x = \"Kidney Volume Change (%)\")\n\n# Create qqplot\nqq_plot &lt;- ggplot(data, aes(sample = kidvol_change)) +\n  geom_qq() +\n  geom_qq_line() +\n  theme_minimal() +\n  labs(title = \"QQ Plot of Kidney Volume Chage\")\n\n# Plot side by side\ngrid.arrange(hist_plot, qq_plot, ncol = 2)\n\n\n\n\n\n\n\n\nkidvol_change is approximately normally distributed.\nIt also appears that we have two outlier values. Let’s assess using boxplots.\n\n# Calculate IQR and identify outliers\nIQR_val &lt;- IQR(data$kidvol_change)\nQ1 &lt;- quantile(data$kidvol_change, 0.25)\nQ3 &lt;- quantile(data$kidvol_change, 0.75)\nlower_bound &lt;- Q1 - (1.5*IQR_val)\nupper_bound &lt;- Q3 + (1.5*IQR_val)\n\n# Create boxplot and flag outliers\nggplot(data, aes(x = \"\", y = kidvol_change)) +\n  geom_boxplot(alpha = 0.8) +\n  geom_text(aes(label = ifelse(kidvol_change &lt; lower_bound | kidvol_change &gt; upper_bound, Subject_ID, \"\")), hjust = -0.5, color = \"red\") +\n  theme_minimal() +\n  labs(title = \"Boxplot of Kidney Volume Change\",\n       x = \"\",\n       y = \"Kidney Volume Change\")\n\n\n\n\n\n\n\n\nPatients 37 and 51 are flagging as potential outliers. Let’s examine them.\n\n# Examine outlier patients\ndata |&gt; \n  arrange(desc(kidvol_change)) |&gt; \n  head() |&gt; \n  pretty_print()\n\n\n\n\nSubject_ID\ngeom1\ngeom2\ngabor1\ngabor2\ngabor3\ngabor4\ngabor5\nglcm1\nglcm2\ntxti1\ntxti2\ntxti3\ntxti4\ntxti5\nlbp1\nlbp2\nlbp3\nlbp4\nlbp5\nkidvol_base\nkidvol_visit2\nprogression\nkidvol_change\n\n\n\n\n37\n-3.860771\n14.90555\n0.2621932\n-0.1983384\n-0.0520030\n0.0687453\n0.0393381\n871.6238\n759728.00\n-14.72259\n-1.9694239\n28.995016\n216.7546\n3.8786306\n-0.0557547\n0.0722141\n-0.0040263\n0.0031086\n0.0052149\n173\n372\nFast\n38.19\n\n\n51\n-43.393038\n1882.95573\n0.0047377\n0.0544573\n0.0002580\n0.0000224\n0.0029656\n2454.8065\n6026075.10\n-14.67151\n-9.3555748\n137.260393\n215.2532\n87.5267799\n0.1577583\n0.1676594\n0.0264497\n0.0248877\n0.0281097\n185\n387\nFast\n36.28\n\n\n52\n77.054904\n5937.45816\n0.4598489\n0.1556082\n0.0715563\n0.2114611\n0.0242139\n-638.4451\n407612.16\n20.26785\n9.6868879\n196.332342\n410.7855\n93.8357967\n9.7525713\n1.1625327\n11.3376835\n95.1126464\n1.3514824\n358\n640\nFast\n26.18\n\n\n16\n-53.668735\n2880.33311\n0.0179566\n0.0169461\n0.0003043\n0.0003224\n0.0002872\n-1045.7503\n1093593.77\n22.59997\n6.9449272\n156.955173\n510.7588\n48.2320141\n1.2350248\n-0.0196489\n-0.0242669\n1.5252862\n0.0003861\n124\n218\nFast\n25.19\n\n\n50\n33.361504\n1112.98996\n-0.1270457\n-0.0812793\n0.0103262\n0.0161406\n0.0066063\n1063.3743\n1130764.98\n-26.04777\n0.2502787\n-6.519202\n678.4864\n0.0626394\n0.5581614\n-1.1616502\n-0.6483883\n0.3115442\n1.3494312\n334\n583\nFast\n24.77\n\n\n60\n-37.454576\n1402.84529\n0.0584093\n0.0359878\n0.0021020\n0.0034116\n0.0012951\n-243.9828\n59527.61\n12.34253\n-2.8642032\n-35.351520\n152.3381\n8.2036599\n0.6218992\n-0.0617567\n-0.0384065\n0.3867586\n0.0038139\n188\n322\nFast\n23.84\n\n\n\n\n\n\n\nInteresting, it looks like these patients also have high values for certain MRI image features, such as GLCM 1 and 2. Additionally, they do not have the highest visit 2 kidney volumes, they simply increases in size the most.\n\n\nSummary\nWhile patients 37 and 51 are potential outliers on kidvol_change, these are NOT erroneous values. On the contrary, they are backed up by similar high values on other measurements such as glcm2, but are not universally high on all MRI image features. In other words, these appear to be very realistic values.\nIn summary, all values are derived from authentic biological data. They will be examined using the jackknife residuals to assess for leverage and influence, but we are motivated to retain them in the analysis.\nTop of Tabset\n\n\n\n\n\n\n# Create bar plot\nggplot(data, aes(x = progression, fill = progression)) +\n  geom_bar() +\n  theme_minimal() +\n  scale_fill_brewer(palette = \"Pastel2\") +\n  ylim(0, 45) +\n  labs(title = \"Count of Slow vs Fast Disease Progression\",\n       x = \"Disease Progression\",\n       y = \"Count\",\n       fill = \"Progression\")\n\n\n\n\n\n\n\n# Create Table\ntable(data$progression) \n\n\nSlow Fast \n  34   37 \n\n\nWe have an almost equal amount of patients that had slow vs fast disease progression.\nTop of Tabset\n\n\n\n\nKidney Volume Change\nkidvol_change is approximately normally distributed, with 2 potential outliers we will keep an eye on.\n\n\nSlow vs Fast Disease Progression\nWe have an approximately even count between patients who had slow vs fast disease progression.\nTop of Tabset"
  },
  {
    "objectID": "SetUpSAS.html",
    "href": "SetUpSAS.html",
    "title": "saspy test",
    "section": "",
    "text": "This is a test to get SAS up and running in Quarto\nI need to create two files:\n\nsascfg_personal.py contains details on how to connect to a SAS instance\n_authinfo contains authentication details\n\nprint(\"Hello World\")\nimport saspy\nsas_session = saspy.SASsession(cfgfile=\"C:\\\\Users\\\\sviea\\\\anaconda3\\\\Lib\\\\site-packages\\\\saspy\\\\sascfg_personal.py\")\nsas_session\ninstall.packages(\"configSAS\")\n\n\nCode\n#| capture: log\nproc candisc data=sashelp.iris out=outcan distance anova;\n   class Species;\n   var SepalLength SepalWidth PetalLength PetalWidth;\nrun;\n\n\n\u00145                                                          The SAS System                      Sunday, November  3, 2024 06:54:00 AM\n\n24         ods listing close;ods html5 (id=saspy_internal) file=_tomods1 options(bitmap_mode='inline') device=svg style=HTMLBlue;\n24       ! ods graphics on / outputfmt=png;\n25         \n26         #| capture: log\n           _\n           180\nERROR 180-322: Statement is not valid or it is used out of proper order.\n\n27         proc candisc data=sashelp.iris out=outcan distance anova;\n28            class Species;\n              _____\n              180\nERROR 180-322: Statement is not valid or it is used out of proper order.\n\n29            var SepalLength SepalWidth PetalLength PetalWidth;\n              ___\n              180\nERROR 180-322: Statement is not valid or it is used out of proper order.\n\n30         run;\n31         \n32         \n33         ods html5 (id=saspy_internal) close;ods listing;\n34         \n\u00146                                                          The SAS System                      Sunday, November  3, 2024 06:54:00 AM\n\n35         \n\n\nERROR 180-322: Statement is not valid or it is used out of proper order. None\n\n\nresults = sas_session.submit(\"\"\"\ndata BPressure;\n    length PatientID $2;\n    input PatientID $ Systolic Diastolic @@;\n    datalines;\nCK 120 50  SS 96  60 FR 100 70\nCP 120 75  BL 140 90 ES 120 70\nCP 165 110 JI 110 40 MC 119 66\nFC 125 76  RW 133 60 KD 108 54\nDS 110 50  JW 130 80 BH 120 65\nJW 134 80  SB 118 76 NS 122 78\nGS 122 70  AB 122 78 EC 112 62\nHH 122 82\n;\n\ntitle 'Systolic and Diastolic Blood Pressure';\nods select BasicMeasures Quantiles;\nproc univariate data=BPressure;\n   var Systolic Diastolic;\nrun;\n\"\"\")\nfrom IPython.display import display, HTML\ndef show(x):\n    display(HTML(\"&lt;h6&gt;Log&lt;/h6&gt;\"))\n    print(x['LOG'])\n    display(HTML(\"&lt;h6&gt;Output&lt;/h6&gt;\"))\n    display(HTML(x.get('LST', '')))\nshow(results)\nimport sas_kernel\nll = sas_session.submit(\"\"\"\nlibname work list;\n\nproc sql;\n   select type, count(*) as 'number of models'n, avg(MPG_city) as 'Avg(MPG_City)'n\n   from sashelp.cars\n   group by type\n   order by 3 desc;\nquit; \n\"\"\")\nll.keys()\nsas_session.HTML(ll['LST'])\nll = sas_session.submitLST(\"\"\"\nlibname work list;\n\nproc sql;\n   select type, count(*) as 'number of models'n, avg(MPG_city) as 'Avg(MPG_City)'n\n   from sashelp.cars\n   group by type\n   order by 3 desc;\nquit; \n\"\"\")\nresult = sas_session.submit(\"proc print data=sashelp.class; run;\")\nprint(result['LOG'])\ndata_sas = sas_session.sasdata('cars', 'sashelp')\ndata_sas.means()\ndata_sas.bar('EngineSize')\nfrom IPython.display import HTML, display\nimport saspy\n\n# Start SAS session\nsas = saspy.SASsession(cfgname='oda')\n\n# Submit SAS code\nresult = sas.submit(\"proc print data=sashelp.class; run;\")\n\n# Display the output in the Quarto document\ndisplay(HTML(result['LST']))\n\n\nCode\nproc candisc data=sashelp.iris out=outcan distance anova;\n   class Species;\n   var SepalLength SepalWidth PetalLength PetalWidth;\nrun;\n\n\n\n\n\n\n\nSAS Output\n\n\n\n\n\nThe SAS System \n\n\nThe CANDISC Procedure\n\n\n\n\n\n\nTotal Sample Size\n150\nDF Total\n149\n\n\nVariables\n4\nDF Within Classes\n147\n\n\nClasses\n3\nDF Between Classes\n2\n\n\n\n\n\n\n\n\n\n\nNumber of Observations Read\n150\n\n\nNumber of Observations Used\n150\n\n\n\n\n\n\n\n\n\n\nClass Level Information\n\n\nSpecies\nVariable\nName\nFrequency\nWeight\nProportion\n\n\n\n\nSetosa\nSetosa\n50\n50.0000\n0.333333\n\n\nVersicolor\nVersicolor\n50\n50.0000\n0.333333\n\n\nVirginica\nVirginica\n50\n50.0000\n0.333333\n\n\n\n\n\n\n\nThe SAS System \n\n\nThe CANDISC Procedure\n\n\n\n\n\n\n\nSquared Distance to Species\n\n\nFrom Species\nSetosa\nVersicolor\nVirginica\n\n\n\n\nSetosa\n0\n89.86419\n179.38471\n\n\nVersicolor\n89.86419\n0\n17.20107\n\n\nVirginica\n179.38471\n17.20107\n0\n\n\n\n\n\n\n\n\n\n\nF Statistics, NDF=4, DDF=144 for Squared Distance to Species\n\n\nFrom Species\nSetosa\nVersicolor\nVirginica\n\n\n\n\nSetosa\n0\n550.18889\n1098\n\n\nVersicolor\n550.18889\n0\n105.31265\n\n\nVirginica\n1098\n105.31265\n0\n\n\n\n\n\n\n\n\n\n\nProb &gt; Mahalanobis Distance for Squared Distance to Species\n\n\nFrom Species\nSetosa\nVersicolor\nVirginica\n\n\n\n\nSetosa\n1.0000\n&lt;.0001\n&lt;.0001\n\n\nVersicolor\n&lt;.0001\n1.0000\n&lt;.0001\n\n\nVirginica\n&lt;.0001\n&lt;.0001\n1.0000\n\n\n\n\n\n\n\n\nThe SAS System \n\n\nThe CANDISC Procedure\n\n\n\n\n\n\nUnivariate Test Statistics\n\n\nF Statistics, Num DF=2, Den DF=147\n\n\nVariable\nLabel\nTotal\nStandard\nDeviation\nPooled\nStandard\nDeviation\nBetween\nStandard\nDeviation\nR-Square\nR-Square\n/ (1-RSq)\nF Value\nPr &gt; F\n\n\n\n\nSepalLength\nSepal Length (mm)\n8.2807\n5.1479\n7.9506\n0.6187\n1.6226\n119.26\n&lt;.0001\n\n\nSepalWidth\nSepal Width (mm)\n4.3587\n3.3969\n3.3682\n0.4008\n0.6688\n49.16\n&lt;.0001\n\n\nPetalLength\nPetal Length (mm)\n17.6530\n4.3033\n20.9070\n0.9414\n16.0566\n1180.16\n&lt;.0001\n\n\nPetalWidth\nPetal Width (mm)\n7.6224\n2.0465\n8.9673\n0.9289\n13.0613\n960.01\n&lt;.0001\n\n\n\n\n\n\n\n\n\n\nAverage R-Square\n\n\n\n\nUnweighted\n0.7224358\n\n\nWeighted by Variance\n0.8689444\n\n\n\n\n\n\n\n\n\n\nMultivariate Statistics and F Approximations\n\n\nS=2 M=0.5 N=71\n\n\nStatistic\nValue\nF Value\nNum DF\nDen DF\nPr &gt; F\n\n\n\n\nWilks' Lambda\n0.02343863\n199.15\n8\n288\n&lt;.0001\n\n\nPillai's Trace\n1.19189883\n53.47\n8\n290\n&lt;.0001\n\n\nHotelling-Lawley Trace\n32.47732024\n582.20\n8\n203.4\n&lt;.0001\n\n\nRoy's Greatest Root\n32.19192920\n1166.96\n4\n145\n&lt;.0001\n\n\n\nNOTE: F Statistic for Roy's Greatest Root is an upper bound.\n\n\nNOTE: F Statistic for Wilks' Lambda is exact.\n\n\n\n\n\n\n\n\nThe SAS System \n\n\nThe CANDISC Procedure\n\n\n\n\n\n\n\n \nCanonical\nCorrelation\nAdjusted\nCanonical\nCorrelation\nApproximate\nStandard\nError\nSquared\nCanonical\nCorrelation\nEigenvalues of Inv(E)*H\n= CanRsq/(1-CanRsq)\nTest of H0: The canonical correlations in the current row and all that follow are zero\n\n\n \nEigenvalue\nDifference\nProportion\nCumulative\nLikelihood\nRatio\nApproximate\nF Value\nNum DF\nDen DF\nPr &gt; F\n\n\n\n\n1\n0.984821\n0.984508\n0.002468\n0.969872\n32.1919\n31.9065\n0.9912\n0.9912\n0.02343863\n199.15\n8\n288\n&lt;.0001\n\n\n2\n0.471197\n0.461445\n0.063734\n0.222027\n0.2854\n \n0.0088\n1.0000\n0.77797337\n13.79\n3\n145\n&lt;.0001\n\n\n\n\n\n\n\nThe SAS System \n\n\nThe CANDISC Procedure\n\n\n\n\n\n\n\nTotal Canonical Structure\n\n\nVariable\nLabel\nCan1\nCan2\n\n\n\n\nSepalLength\nSepal Length (mm)\n0.791888\n0.217593\n\n\nSepalWidth\nSepal Width (mm)\n-0.530759\n0.757989\n\n\nPetalLength\nPetal Length (mm)\n0.984951\n0.046037\n\n\nPetalWidth\nPetal Width (mm)\n0.972812\n0.222902\n\n\n\n\n\n\n\n\n\n\nBetween Canonical Structure\n\n\nVariable\nLabel\nCan1\nCan2\n\n\n\n\nSepalLength\nSepal Length (mm)\n0.991468\n0.130348\n\n\nSepalWidth\nSepal Width (mm)\n-0.825658\n0.564171\n\n\nPetalLength\nPetal Length (mm)\n0.999750\n0.022358\n\n\nPetalWidth\nPetal Width (mm)\n0.994044\n0.108977\n\n\n\n\n\n\n\n\n\n\nPooled Within Canonical Structure\n\n\nVariable\nLabel\nCan1\nCan2\n\n\n\n\nSepalLength\nSepal Length (mm)\n0.222596\n0.310812\n\n\nSepalWidth\nSepal Width (mm)\n-0.119012\n0.863681\n\n\nPetalLength\nPetal Length (mm)\n0.706065\n0.167701\n\n\nPetalWidth\nPetal Width (mm)\n0.633178\n0.737242\n\n\n\n\n\n\n\n\nThe SAS System \n\n\nThe CANDISC Procedure\n\n\n\n\n\n\n\nTotal-Sample Standardized Canonical Coefficients\n\n\nVariable\nLabel\nCan1\nCan2\n\n\n\n\nSepalLength\nSepal Length (mm)\n-0.686779533\n0.019958173\n\n\nSepalWidth\nSepal Width (mm)\n-0.668825075\n0.943441829\n\n\nPetalLength\nPetal Length (mm)\n3.885795047\n-1.645118866\n\n\nPetalWidth\nPetal Width (mm)\n2.142238715\n2.164135931\n\n\n\n\n\n\n\n\n\n\nPooled Within-Class Standardized Canonical Coefficients\n\n\nVariable\nLabel\nCan1\nCan2\n\n\n\n\nSepalLength\nSepal Length (mm)\n-.4269548486\n0.0124075316\n\n\nSepalWidth\nSepal Width (mm)\n-.5212416758\n0.7352613085\n\n\nPetalLength\nPetal Length (mm)\n0.9472572487\n-.4010378190\n\n\nPetalWidth\nPetal Width (mm)\n0.5751607719\n0.5810398645\n\n\n\n\n\n\n\n\n\n\nRaw Canonical Coefficients\n\n\nVariable\nLabel\nCan1\nCan2\n\n\n\n\nSepalLength\nSepal Length (mm)\n-.0829377642\n0.0024102149\n\n\nSepalWidth\nSepal Width (mm)\n-.1534473068\n0.2164521235\n\n\nPetalLength\nPetal Length (mm)\n0.2201211656\n-.0931921210\n\n\nPetalWidth\nPetal Width (mm)\n0.2810460309\n0.2839187853\n\n\n\n\n\n\n\n\n\n\n\nClass Means on Canonical Variables\n\n\nSpecies\nCan1\nCan2\n\n\n\n\nSetosa\n-7.607599927\n0.215133017\n\n\nVersicolor\n1.825049490\n-0.727899622\n\n\nVirginica\n5.782550437\n0.512766605"
  },
  {
    "objectID": "Project_3_Predictive_Modelling/Project_3_R/Code/Project_3_Predictive_Modelling.html#glcm1",
    "href": "Project_3_Predictive_Modelling/Project_3_R/Code/Project_3_Predictive_Modelling.html#glcm1",
    "title": "Predictive Modelling",
    "section": "GLCM1",
    "text": "GLCM1\n\n# Make plots\ndistr_plots(data, \"glcm1\", 10)\n\n\n\n\n\n\n\n\nglcm1 is approximately normally distributed.\nTop of Tabset"
  },
  {
    "objectID": "Project_3_Predictive_Modelling/Project_3_R/Code/Project_3_Predictive_Modelling.html#glcm2",
    "href": "Project_3_Predictive_Modelling/Project_3_R/Code/Project_3_Predictive_Modelling.html#glcm2",
    "title": "Predictive Modelling",
    "section": "GLCM2",
    "text": "GLCM2\n\n# Make plots\ndistr_plots(data, \"glcm2\", 10)\n\n\n\n\n\n\n\n\nglcm2 may be logarithmic.\n\n# Create log variable\ndata &lt;- data |&gt; \n  mutate(glcm2_log = log(glcm2))\n\n#| fig-height: 3.5\n#| fig-width: 8\n# Make plots\ndistr_plots(data, \"glcm2_log\", 10)\n\n\n\n\n\n\n\n\nThat’s better, with perhaps a negative outlier.\nTop of Tabset"
  }
]