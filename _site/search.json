[
  {
    "objectID": "test.html",
    "href": "test.html",
    "title": "Sean Vieau",
    "section": "",
    "text": "My Quarto Website"
  },
  {
    "objectID": "Statistical_Consulting/Opioid_Epidemic/GeoSpatial_Opioid_Epidemic.html",
    "href": "Statistical_Consulting/Opioid_Epidemic/GeoSpatial_Opioid_Epidemic.html",
    "title": "The Opioid Epidemic in Colorado: A Geospatial Analysis",
    "section": "",
    "text": "This is the final project for BIOS 6640: R for Data Science. In this project we analyze spatial variation patterns in the presence of Behavioral Health Treatment Service Providers throughout Colorado to investigate the opioid epidemic.\nBackground\nThe opioid epidemic is a significant public health crisis in Colorado, with severe impacts on communities, families, and individuals themselves. Prescription pain killers such as oxycodone and hydrocodone are significant causes of opioid addiction, with approximately 70% of heroin users in Colorado reporting that their drug use began with these prescription medications (1). Additionally, fentanyl overdoses have increased by 10x since 2016, with the number of deaths from opioid overdose increasing by 54% from 2019 to 2020 alone (2).\nThe data set used for this analysis is from the Substance Abuse and Mental Health Services Administration (SAMHSA) in collaboration from the Colorado Public Health and Environment Department (CDPHE), and tracks the different substance use treatment facilities in Colorado. This data set includes the geospatial location (latitude and longitude) of each facility, as well as the type (e.g. mental health facility). The two questions that will be addressed are:\n\nAre the spatial distributions of Behavioral Health Treatment Service Providers through-out Colorado different by Facility Type?\nAre the spatial distributions of Behavioral Health Treatment Service Providers through-out Colorado different based on whether Naltrexone or Buprenorphine is used in treatment?"
  },
  {
    "objectID": "Statistical_Consulting/Opioid_Epidemic/GeoSpatial_Opioid_Epidemic.html#variable-creation",
    "href": "Statistical_Consulting/Opioid_Epidemic/GeoSpatial_Opioid_Epidemic.html#variable-creation",
    "title": "The Opioid Epidemic in Colorado: A Geospatial Analysis",
    "section": "Variable Creation",
    "text": "Variable Creation\nWe will first create a variable for whether the facility is a mental health treatment or substance abuse treatment facility.\n\nFacility Type\n\n# Examine Facility Types\npretty_print(table(data$Type_))\n\n\n\n\nVar1\nFreq\n\n\n\n\nDetoxification\n3\n\n\nDetoxification, Substance abuse treatment\n26\n\n\nDetoxification, Substance abuse treatment, Transitional housing or halfway house\n4\n\n\nMental health treatment\n149\n\n\nSubstance abuse treatment\n307\n\n\nSubstance abuse treatment, Transitional housing or halfway house\n16\n\n\nTransitional housing or halfway house\n1\n\n\n\n\n\n\n\nWe will condense these descriptions into two facility types:\n\nSubstance Abuse Treatment\nMental Abuse Treatment\n\n\n# Aggregate facility types\ndata &lt;- data |&gt; \n  mutate(facility = ifelse(Type_ %in% c(\"Mental health treatment\", \"Transitional housing or halfway house\"), \"Mental Health Treatment\", \"Substance Abuse Treatment\"))\n\n# Create table of facility type\npretty_print(table(data$facility))\n\n\n\n\nVar1\nFreq\n\n\n\n\nMental Health Treatment\n150\n\n\nSubstance Abuse Treatment\n356\n\n\n\n\n\n\n\n\n\nNaltrexone or Buprenorphrine Used in Treatment\nFor question 2 we need to dummy code whether the facility uses naltrexone or buprenorphine in treatment\n\n# Create dummy variables\ndata &lt;- data |&gt; \n  mutate(nal = ifelse(is.na(Naltrexone_used_in_Treatment), 0, 1),\n         bup = ifelse(is.na(Buprenorphine_used_in_Treatment), 0, 1),\n         nal_bup = ifelse(!is.na(Buprenorphine_used_in_Treatment) | !is.na(Naltrexone_used_in_Treatment), \"Naltrexone or Buprenorphine Used\", \"Neither Used\"))\n\n# Examine\npretty_print(table(data$nal_bup))\n\n\n\n\nVar1\nFreq\n\n\n\n\nNaltrexone or Buprenorphine Used\n98\n\n\nNeither Used\n408\n\n\n\n\n\n\n\nThere are 408 facilities that do not use naltrexone or buprenorphine, and 98 that use at least one.\n\n\nSummary\nWe successfully created variables for facility type (for question 1), and use of naltrexone or buprenorphine (for question 2)"
  },
  {
    "objectID": "Statistical_Consulting/Opioid_Epidemic/GeoSpatial_Opioid_Epidemic.html#mental-health-vs-substance-abuse-treatment-facility",
    "href": "Statistical_Consulting/Opioid_Epidemic/GeoSpatial_Opioid_Epidemic.html#mental-health-vs-substance-abuse-treatment-facility",
    "title": "The Opioid Epidemic in Colorado: A Geospatial Analysis",
    "section": "Mental Health vs Substance Abuse Treatment Facility",
    "text": "Mental Health vs Substance Abuse Treatment Facility\nTo answer question one “Are the spatial distributions of Behavioral Health Treatment Service Providers through-out Colorado different by Facility Type” we will first visualize the difference in distributions over a map of Colorado. We will then test whether these distributions are significantly different using the estimated K-Functions.\n\nVisualize Distribution of Facilities Across Colorado\nFirst, let us begin by visualizing the distribution of behavioral care facilities across Colorado, separated by mental health vs substance abuse treatment.\n\n# API Key from https://client.stadiamaps.com/signup\n\n# Register Stadia API so we can use it to retrieve maps\nregister_stadiamaps(key = \"4a4ea0df-9bd4-4ce8-a9ff-f1a7d86c0da9\")\n\n# Define the bounding box for Colorado\ncolo &lt;- c(left = -109, bottom = 37, right = -102, top = 41)\n\n# Retrieve and plot the stamen toner map of colorado\ncolo_map &lt;- get_stadiamap(colo, zoom = 8, maptype = \"stamen_terrain\") |&gt; ggmap()\n\n# Plot the distribution of behavioral care facilities across Colorado, separated by facility type\ncolo_map +\n  geom_point(aes(LONGITUDE, LATITUDE), data = data, color = \"red\") +\n  stat_density_2d(aes(LONGITUDE, LATITUDE, fill = ..level..), data = data, geom = \"polygon\", alpha = 0.3) +\n  scale_fill_gradient2(\"Facility\\nPropensity\", low = \"white\", mid = \"yellow\", high = \"red\") +\n  facet_wrap(~facility, ncol=1) +\n  labs(title = \"Distribution of Behavioral Health Treatment Service Providers throughout Colorado\",\n       x = \"Longitude\",\n       y = \"Latitude\")\n\n\n\n\n\n\n\n\nIt appears that there is a higher concentration of substance abuse treatment facilities around Denver compared to mental health treatment facilities. This could be due to the smaller sample size of mental health treatment facilities however.\n\n\nDescriptive Statistics\nWe will perform a simple density plot to compare the densities of both treatment facilities.\n\nCreate Geospatial Objects\n\n# Code adjusted from SpatialPart2.Rmd from class\n# Create owin and ppp objects\nbbx &lt;- owin(xrange=range(data$LONGITUDE),yrange=range(data$LATITUDE))\nX &lt;- ppp(data$LONGITUDE,data$LATITUDE,window=bbx)\n\n# Get row length of dataset\nn &lt;- dim(data)[1]\n\n# Create a blank vector with 0's to that specification\ndata_vec &lt;- rep(0,n)\n\n# Search facility column for mental health units\ntmp &lt;- grep(\"Mental\", data$facility)\n\n# Mark them as 1\ndata_vec[tmp] &lt;- 1\n\n# Ensure values are a factor and not numeric\ndata_vec_fact &lt;- factor(data_vec, levels = c(0,1), labels = c(\"Substance Abuse Facility\", \"Mental Health Facility\"))\n\n# Use the ppp function from the spatstat package to create a point pattern object X.m1\nX.m1 &lt;- ppp(data$LONGITUDE,data$LATITUDE,window=bbx,marks=data_vec_fact)\n\n# Create Plot\nplot(X.m1)  \n\n\n\n\n\n\n\n\n\n# Plot the densities of each treatment facility type\npretty_print(intensity(X.m1))\n\n\n\n\n\nx\n\n\n\n\nSubstance Abuse Facility\n13.803652\n\n\nMental Health Facility\n5.816146\n\n\n\n\n\n\nplot(density(split(X.m1)))\n\n\n\n\n\n\n\n\n\n\nInterpretation\nSubstance Abuse Facility: The intensity is approximately 13.80 facilities per unit area.\nMental Health Facility: The intensity is approximately 5.82 facilities per unit area.\nThis means that Substance Abuse Facilities are more densely distributed compared to Mental Health Facilities in Colorado.\n\n\n\nStatistical Inference\nNow that we have visualized the distribution of facility types across Colorado, let’s test whether there is a significant difference in the distribution between mental health and substance abuse treatment facilities.\nThis spatial data analysis will use the K-Function. This is the empirical distribution of the pairwise distance between points. It includes the variance based on bootstrapping, and describes the clustering/dispersion of points.\n\nPerform Statistical Inference\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1, 2, 3, .5....10....15....20....25....30....35....40....45....50..\n..55....60....65....70....75....80....85....90....95....100....105....110\n....115....120....125....130....135....140....145....150....155....160....165...\n.170....175....180....185....190....195....200....205....210....215....220....225.\n...230....235....240....245....250....255....260....265....270....275....280....\n285....290....295....300....305....310....315....320....325....330....335....340..\n..345....350....355\n356.\n1, 2, 3, 4.6.8.10.12.14.16.18.20.22.24.26.28.30.32.34\n.36.38.40.42.44.46.48.50.52.54.56.58.60.62.64.66.68.70.72.74\n.76.78.80.82.84.86.88.90.92.94.96.98.100.102.104.106.108.110.112.114\n.116.118.120.122.124.126.128.130.132.134.136.138.140.142.144.146.148.\n150.\n\n\n\n\n\n\n\n\n\n\n\nInterpretation\nThe K-function measures the number of points within a given distance from a typical point, compared to what would be expected under complete spatial randomness (CSR). The null hypothesis here is that of complete spatial randomness (i.e. the red dotted line).\nThe estimated K-functions for both mental health treatment and substance abuse treatment facilities lie above the theoretical K-Function, indicating that the distributions for each facility type do not exhibit complete spatial randomness. Additionally, since the 95% CIs do not overlap, we can conclude that this difference is statistically significant.\nThe estimated K-Function for substance abuse treatment facilities is significantly greater than that of mental health treatment clinics indicating that the substance abuse treatment facilities are significantly more clustered than the mental health treatment clinics.\n\n\n\nSummary\nWe have discovered that the distributions for both facility types are not random, and that substance abuse clinics are more clustered together compared to mental health clinics, with a approximately 13.80 Substance Abuse facilities per unit area, and approximately 5.82 Mental Health facilities per unit area."
  },
  {
    "objectID": "Statistical_Consulting/Opioid_Epidemic/GeoSpatial_Opioid_Epidemic.html#naltrexone-and-buprenorphine-use",
    "href": "Statistical_Consulting/Opioid_Epidemic/GeoSpatial_Opioid_Epidemic.html#naltrexone-and-buprenorphine-use",
    "title": "The Opioid Epidemic in Colorado: A Geospatial Analysis",
    "section": "Naltrexone and Buprenorphine Use",
    "text": "Naltrexone and Buprenorphine Use\nTo answer question two, “Are the spatial distributions of Behavioral Health Treatment Service Providers through-out Colorado different based on whether Naltrexone or Buprenorphine is used in treatment?” we will examine the quadrat plots and compare distributions using the estimated K-Functions.\n\nVisualize Distribution of Facilities Across Colorado\nFirst, let us begin by visualizing the distribution of facilities that use buprenorphrine or naltrexone across Colorado.\n\n# Plot the distribution of behavioral care facilities across Colorado, separated by facility type\ncolo_map +\n  geom_point(aes(LONGITUDE, LATITUDE), data = data, color = \"red\") +\n  stat_density_2d(aes(LONGITUDE, LATITUDE, fill = ..level..), data = data, geom = \"polygon\", alpha = 0.3) +\n  scale_fill_gradient2(\"Facility\\nPropensity\", low = \"white\", mid = \"yellow\", high = \"red\") +\n  facet_wrap(~nal_bup, ncol=1) +\n  labs(title = \"Distribution of Behavioral Health Treatment Service Providers throughout Colorado\",\n       x = \"Longitude\",\n       y = \"Latitude\")\n\n\n\n\n\n\n\n\nIt appears that facilities that use naltrexone or buprenorphine may be more localized around Denver compared to facilities that do not.\n\n\nDescriptive Statistics\nWe will be examining the quadrat plots to examine the distributions of facilities.\n\nPrepare and Split the Data\n\n# Split the data into two groups\nbup_or_nal &lt;- subset(data, nal_bup == \"Naltrexone or Buprenorphine Used\")\nneither &lt;- subset(data, nal_bup == \"Neither Used\")\n\n# Create ppp for bup or nal data set\nbup_or_nal_ppp &lt;- ppp(bup_or_nal$LONGITUDE, bup_or_nal$LATITUDE, window=bbx)\n\n# Create ppp for neither data set\nneither_ppp &lt;- ppp(neither$LONGITUDE, neither$LATITUDE, window=bbx)\n\n\n\nPerform Quadrat Count\n\n# Define the number of quadrats\nnx &lt;- 5\nny &lt;- 5\n\n# Quadrat counts for Bup or Nal and Neither\nbup_or_nal_quadrat &lt;- quadratcount(bup_or_nal_ppp, nx = nx, ny = ny)\nneither_quadrat &lt;- quadratcount(neither_ppp, nx = nx, ny = ny)\n\n\n\nPlot Quadrat Counts\n\n# Plot quadrat counts for Bup or Nal\nplot(bup_or_nal_quadrat, col=\"blue\", main = \"Naltrexone or Buprenorphine Used\")\n\n\n\n\n\n\n\n# Plot quadrat counts for Neither\nplot(neither_quadrat, col=\"red\", main = \"Neither Used\")\n\n\n\n\n\n\n\n\n\n\nCompare Quadrat Counts\n\n# Summary of quadrat counts\nquadrat.test(bup_or_nal_quadrat)\n\nWarning: Some expected counts are small; chi^2 approximation may be inaccurate\n\n\n\n    Chi-squared test of CSR using quadrat counts\n\ndata:  \nX2 = 525.98, df = 24, p-value &lt; 0.00000000000000022\nalternative hypothesis: two.sided\n\nQuadrats: 5 by 5 grid of tiles\n\nquadrat.test(neither_quadrat)\n\n\n    Chi-squared test of CSR using quadrat counts\n\ndata:  \nX2 = 1415.8, df = 24, p-value &lt; 0.00000000000000022\nalternative hypothesis: two.sided\n\nQuadrats: 5 by 5 grid of tiles\n\n# Chi-squared test for comparison\nobserved_counts &lt;- rbind(as.vector(bup_or_nal_quadrat), as.vector(neither_quadrat))\nchisq.test(observed_counts)\n\nWarning in chisq.test(observed_counts): Chi-squared approximation may be\nincorrect\n\n\n\n    Pearson's Chi-squared test\n\ndata:  observed_counts\nX-squared = 24.761, df = 24, p-value = 0.4188\n\n\n\n\nInterpretation\nThe results for the Chi-squared test of complete spatial randomness for facilities that use nalextrone or buprenorphrine (2 = 525.98, df = 24, p-value &lt; 0.0001) and those that use neither (X2 = 1415.8, df = 24, p-value &lt; 0.0001) are highly significant, indicating that there is spatial preference for both facility types (i.e. they are not CSR).\nHowever, the Pearson Chi-Squared test comparing the distributions of both was not significant (X-squared = 24.761, df = 24, p-value = 0.42), indicating that there is not a significant difference in the distribution of facilities based on nalextrone or buprenorprhine use.\n\n\n\nSensitivity Analysis\nWe can also perform this analysis as many different bin sizes.\n\n# Define the number of quadrats\nnx &lt;- 3\nny &lt;- 3\n\n# Quadrat counts for Bup or Nal and Neither\nbup_or_nal_quadrat &lt;- quadratcount(bup_or_nal_ppp, nx = nx, ny = ny)\nneither_quadrat &lt;- quadratcount(neither_ppp, nx = nx, ny = ny)\n\n# Summary of quadrat counts\nquadrat.test(bup_or_nal_quadrat)\n\n\n    Chi-squared test of CSR using quadrat counts\n\ndata:  \nX2 = 144.45, df = 8, p-value &lt; 0.00000000000000022\nalternative hypothesis: two.sided\n\nQuadrats: 3 by 3 grid of tiles\n\nquadrat.test(neither_quadrat)\n\n\n    Chi-squared test of CSR using quadrat counts\n\ndata:  \nX2 = 623.56, df = 8, p-value &lt; 0.00000000000000022\nalternative hypothesis: two.sided\n\nQuadrats: 3 by 3 grid of tiles\n\n# Chi-squared test for comparison\nobserved_counts &lt;- rbind(as.vector(bup_or_nal_quadrat), as.vector(neither_quadrat))\nchisq.test(observed_counts)\n\nWarning in chisq.test(observed_counts): Chi-squared approximation may be\nincorrect\n\n\n\n    Pearson's Chi-squared test\n\ndata:  observed_counts\nX-squared = 6.4499, df = 8, p-value = 0.597\n\n\nEven at 9 bins total, we get the same results, giving us more confidence in our findings.\n\n\nStatistical Inference\nWe will again perform K-Function analyses to compare the distributions between facilities that use nalextrone/buprenorphrine and those that do not.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1, 2, 3, .5....10....15....20....25....30....35....40....45....50..\n..55....60....65....70....75....80....85....90....95....100....105....110\n....115....120....125....130....135....140....145....150....155....160....165...\n.170....175....180....185....190....195....200....205....210....215....220....225.\n...230....235....240....245....250....255....260....265....270....275....280....\n285....290....295....300....305....310....315....320....325....330....335....340..\n..345....350....355....360....365....370....375....380....385....390....395....400\n....405..\n408.\n1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20,\n21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40,\n41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60,\n61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80,\n81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, \n98.\n\n\n\n\n\n\n\n\n\n\nInterpretation\nHere we can see that the CI’s for facilities that use Naltrexone/Buprenorphrine and those that do not overlap, confirming the results from the quadrat plots that these distributions across Colorado are not significantly different.\n\n\n\nSummary\nWe discovered that facilities that use naltrexone/buprenorphrine and those that do not both show spatial preference. Additionally, we learned that their distributions did not significantly differ from each other."
  },
  {
    "objectID": "Resume/Publications_and_Presentations.html",
    "href": "Resume/Publications_and_Presentations.html",
    "title": "Publications and Presentations",
    "section": "",
    "text": "Publications\nSpeed, F., Saladrigas, C., Teel, A., Vieau, S., Bright, V., Gopinath, J., Welle, C., Restrepo, D., Gibson, E. (2024). High-Speed in Vivo Calcium Recording Using Structured Illumination with Self-Supervised Denoising.” Optics Continuum3 (11): 2044. https://doi.org/10.1364/optcon.532996.\n\n\nProfessional Presentations\nSpeed, F., Kumar, V., Saladrigas, C., Vieau, S., Bright, V., Restrepo, D., Welle, C., Gopinath, J., Kymissis, I., Gibson, E. (2023). Illumination methods for voltage imaging with a microLED array. Poster presented at the Society for Neuroscience\nPavlova, M. , Castillo-Flores, J., Vieau, S., Velumurgan, B., McGrath, P. S., Bush, K., Hopkin, A., Bruckner, A., Kogut, I., Roop, D., Bilousova, G. (2022). A novel preclinical model for translating an induced pluripotent stem cell therapy for the treatment of skin diseases. A poster presented at the International Society for Stem Cell Research 2022.\nSpeed, F., Vieau, S., Parra, A., Kumar, V., Suckow, R., Supekar, D., Peng, X., Sias, A., Hansen, S., Martinez, G., Peet, G., Bright, V., Hughes, E., Gopinath, J., Kymissis, J., Restrepo, D., Welle, C., Gibson, E. (2022). Development of the 3DFAST-GEVI Optical Neural Interface. Poster presented at the Neuroscience Society Retreat.\nVieau, S., Badanes, L. (2019). Acceptance, Reappraisal, and Inhibition: Potential Buffers Against Heightened Cortisol Reactivity Associated with Early Life Stress. A poster presented at the 31st Association for Psychological Science Annual Convention, Washington, D.C.\nVieau, S., Mahalingam, R. (2018). Using CRISPR-Cas9 to deactivate latent varicella zoster virus. A poster presented at the 2018 Impact and Innovation Showcase, Denver, Colorado.\n\n\nUndergraduate Presentations\n(*faculty advisor) (**regional award winner of outstanding empirical research, RMPA)\nAlvarez, L., Maxwell, I. A., Slagle, D. R., Howell, K. S., Hickman, H. A., & Vieau, S. (2016). Neuroticism and mindfulness as predictors of depression and mindfulness as a moderator for depression and neuroticism. A paper symposium presented at the annual conference of the Rocky Mountain Psychological Association, Denver, Colorado.  \nHowell, K., Snyders, J., Vieau, S., Davis, K., Martinez, T., Malek, J., & Gale, B. (2016). Musical preferences as predictors of creativity. A paper symposium presented at the annual Undergraduate Research Conference: A Symposium of Scholarly Work and Creative Projects, Denver, Colorado.\nLee, A., Vieau, S., & Stem, D. (2018). Meditation: Effects on working memory in trauma and non-trauma individuals. A paper symposium presented at the annual conference of the Rocky Mountain Psychological Association, Denver, Colorado.\nMaxwell, I. A., Vieau, S., Haider, A., Malek, J., & Wiggins, A. (2016). The role of gender, stress, and cognitive function. A paper symposium presented at the annual conference of the Rocky Mountain Psychological Association, Denver, Colorado.\nMortensen, C. R*., Cialdini, R. B., Jaeger, C. M., Jacobson, R. P., Maxwell, I. A., Vieau, S., & Neel, R. (2016). Preventing backfire effects with trend information when using social norms for water conservation. A poster presented at the annual conference of the Rocky Mountain Psychological Association, Denver, Colorado.\nRichmond, A. S*., Gale, B., Murphy, P., & Vieau, S. (2016). The validity and reliability of the learner-centered syllabus scale. A poster presented at the annual conference of the Rocky Mountain Psychological Association, Denver, Colorado.\nSchneider, D. G., Stem, D., Vieau, S., & Garofalo, A. M. (2018). Recommended physical activity and mental health. A paper symposium presented at the annual conference of the Rocky Mountain Psychological Association, Denver, Colorado.\nSchneider, D. G., Vieau, S., Kusick, M., & Hanson, T. (2017). 5-minute mindfulness meditation provides relief from state anxiety for experienced and novice meditators. a paper symposium presented at the annual conference of the Rocky Mountain Psychological Association, Salt Lake City, Utah.\nSnyders, J., Vieau, S., Hanson, T., & Rowland, A. (2017). Mindfulness meditation: Threshold of an adverse effect. A paper symposium presented at the annual conference of the Rocky Mountain Psychological Association, Salt Lake City, Utah.\nStem, D., Lee, A., Eccles, R. D., McDonald, E., Vieau, S., & Schneider, D. (2018). The effects of anxiety prime, state anxiety, and trauma. A paper symposium presented at the annual conference of the Rocky Mountain Psychological Association, Denver, Colorado.\nStem, D., Ricciardelli, L., & Vieau, S. (2018). Life satisfaction, meaning in life, and quality of life as predictors of depression in college students. A poster presented at the annual conference of the Rocky Mountain Psychological Association, Denver, Colorado.\n**Vieau, S., Maxwell, I. A., Moore, M., Stem, D., & Davis, K. N. (2017). The effect of synchrony and mindfulness on prosocial attitudes. a paper symposium presented at the annual conference of the Rocky Mountain Psychological Association, Salt Lake City, Utah.\nVieau, S., Noel, S. M., & Kusick, M. (2017). The experience of synchrony leads to harmonization of velocity. A paper symposium presented at the annual conference of the Rocky Mountain Psychological Association, Salt Lake City, Utah.\n**White, M., Pletcher, J., Vieau, S., Garofalo, A. M., Eccles, R. D., & Stem, D. (2018). Does mindfulness moderate sexual assault and PTSD symptoms?. A paper symposium presented at the annual conference of the Rocky Mountain Psychological Association, Denver, Colorado."
  },
  {
    "objectID": "Project_3_Predictive_Modelling/Project_3_SAS/Code/images/Megan_Watson_Project.html",
    "href": "Project_3_Predictive_Modelling/Project_3_SAS/Code/images/Megan_Watson_Project.html",
    "title": "Data Checking - Statistical Consulting",
    "section": "",
    "text": "This is the Quarto markdown for the live consultation in BIOS 6621: Statistical Consulting.\nThis is a research project encompassing the entire data analysis pipeline, from initial consultation to scope of work writing, execution of statistical analysis, and completion of project deliverables."
  },
  {
    "objectID": "Project_3_Predictive_Modelling/Project_3_SAS/Code/images/Megan_Watson_Project.html#general-information",
    "href": "Project_3_Predictive_Modelling/Project_3_SAS/Code/images/Megan_Watson_Project.html#general-information",
    "title": "Data Checking - Statistical Consulting",
    "section": "General Information",
    "text": "General Information\n\n\n\n\n\n\n\n\n\nInvestigator\nMegan Watson\nDate\nSeptember 30, 2024\n\n\nProject Title\nQuality Improvement into Standard Practice: Persistent Practice Changes and Decreased Length of Stay for 3 years following a Medical ICU Physical Therapy Quality Improvement Project"
  },
  {
    "objectID": "Project_3_Predictive_Modelling/Project_3_SAS/Code/images/Megan_Watson_Project.html#project-description",
    "href": "Project_3_Predictive_Modelling/Project_3_SAS/Code/images/Megan_Watson_Project.html#project-description",
    "title": "Data Checking - Statistical Consulting",
    "section": "Project Description",
    "text": "Project Description\n\nBackground\n             The objective of this study is to assess the impact of a MICU physical therapy (PT) quality improvement (QI) project on physical therapist practice, and quantify the changes in length of stay (LOS) and mechanical ventilation (MV) time. The goals of the QI Initiative were to increase percentage of MICU admissions receiving PT, decrease time from MICU admission to first PT, and increase the frequency of PT visits. The main question the researcher came to us with was: how to correctly model the longitudinal data in the study?\n\n\nStudy Design\n              This was a retrospective cohort study, comparing 3 main time periods: Pre-QI Initiative (before April 2015), During QI Initiative (April 2015-Dec 2015), and After-QI Initiative (Jan 2016 and later). The QI initiative was deployed during the 9-month period between April 2015 and December 2015, during which time there were education and staffing changes. Patients were stratified as either having mechanical ventilation or not having mechanical ventilation. The primary outcomes of interest were MICU LOS and time on MV. Secondary outcomes of interest were the non-ICU LOS, total hospital LOS, and discharge location.\n\n\nDescription of the Data\n              Data was received as a de-identified .csv. Data points consist of: Admit and Discharge Time, PT Consult Time, Admit to Consult Time, First Therapy Time, Consult to Therapy Time, Admit to Therapy Time, Total Therapy Visits, Days with Therapy Visits, Average, min, and Max Therapy Length, Admit and Discharge Date/Time, Total MV Days, Hospital LOS, Patient Age, Sex, “Problem List”, and Codes (presumably ICD9/10).\n\n\nAnticipated Sample Size / Study Population\n              The sample size is N = ~3000-4000, consisting of patients admitted to the MICU with a LOS between 2 and 30 days (I.e. exclude &lt; 2 days or &gt; 30 days). This was a single site study performed here at Colorado University.\n\n\nHypothesis\n              The hypothesis for the study was that LOS and time on MV would decrease when comparing the pre- and intra-initiative time periods, and when comparing the pre- and post-initiative time periods. Clinical significance for this outcome is considered as a 1-2 day change in LOS. The intended end product of this project is a publication in a PT journal.\n\n\nAnalysis Plan (Sketch)\n              Data should first be examined for missingness, as this has not been performed. Similarly, repeat MICU admissions should be explored and controlled for to ensure independence of observations. Death during MICU stay should also be examined and adjusted for if needed. The analysis plan discussed was to utilize splines to capture the trajectory of LOS or time on MV over time by fitting smooth curves to the data. Linear contrasts should be performed to assess if there is a significant difference in LOS or time on MV when comparing two timepoints (i.e. pre- and intra-, pre- and post- , and post- and intra-QI Initiative. Supplementary analyses should be performed to assess secondary outcomes of interest of non-ICU LOS and total hospital LOS."
  },
  {
    "objectID": "Project_3_Predictive_Modelling/Project_3_SAS/Code/images/Megan_Watson_Project.html#project-deliverables",
    "href": "Project_3_Predictive_Modelling/Project_3_SAS/Code/images/Megan_Watson_Project.html#project-deliverables",
    "title": "Data Checking - Statistical Consulting",
    "section": "Project Deliverables",
    "text": "Project Deliverables\n             At this stage, Megan has only asked for insight into what method to use to account for variance in time for her analysis. There are currently no other deliverables.\nTimeline / Deadlines\nAt this time, Megan has not asked for data analysis efforts from us.\nShould this change, the anticipated project start date will be 1 week after our next meeting with her, where revisions to the Comprehensive Analysis Report are also expected to be completed.  The assigned analysts will reach out to the investigator to confirm the project timeframe and the project start date may be altered if necessary. 2 weeks will be allowed for initial data analysis by the assigned analyst.\nA follow-up meeting is to occur no later than 1 month after the next meeting with Megan. During the follow-up meeting, the project team will discuss and preliminary results of the analysis, mockup the desired tables and figures (1-2 descriptive tables, 1-2 analysis results tables and up to 4 graphs), and discuss next steps. During the follow-up meeting, the team will establish a meeting schedule for the remainder of the project. \nEstimated Cost\nThis represents our best estimate of the effort needed to complete the project. Should the scope of work change substantially, the analyst(s) will discuss changes with the investigator and issue a new or amended project agreement.\n\n\n\n\nEffort\n\n\n\n\nMonths\nPhD Faculty\nMaster’s Faculty\nStudent\n\n\n\n\n\n\n\n\n1\n20%\n0%\n80%\n\n\n\n\nTotal Costs\n’Bout Tree Fiddy"
  },
  {
    "objectID": "Project_3_Predictive_Modelling/Project_3_SAS/Code/images/Megan_Watson_Project.html#Outcome_Variables",
    "href": "Project_3_Predictive_Modelling/Project_3_SAS/Code/images/Megan_Watson_Project.html#Outcome_Variables",
    "title": "Data Checking - Statistical Consulting",
    "section": "Outcome Variables",
    "text": "Outcome Variables\n\nMechanical VentilationMICU Length of StayHospital Length of StayNon-ICU Length of StayDischarge Location\n\n\nFirst we will begin with mechanical ventilation time (MV)\n\n# Create histogram and qqplot\ndistr_plots(data_ex, \"MV_Total\")\n\nWarning: `aes_string()` was deprecated in ggplot2 3.0.0.\nℹ Please use tidy evaluation idioms with `aes()`.\nℹ See also `vignette(\"ggplot2-in-packages\")` for more information.\n\n\nWarning: Removed 3166 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\nWarning: Removed 3166 rows containing non-finite outside the scale range\n(`stat_qq()`).\n\n\nWarning: Removed 3166 rows containing non-finite outside the scale range\n(`stat_qq_line()`).\n\n\n\n\n\n\n\n\n\nThat’s some severe right skewness! Is that logarithmic? Let’s try and transform and see what happens.\n\n# Perform a log transform of mechanical ventilation time and plot\ndata_ex$MV_Total_log &lt;- log(data_ex$MV_Total)\n\n# Create histogram and qqplot\ndistr_plots(data_ex, \"MV_Total_log\", 10)\n\nWarning: Removed 3166 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\nWarning: Removed 3166 rows containing non-finite outside the scale range\n(`stat_qq()`).\n\n\nWarning: Removed 3166 rows containing non-finite outside the scale range\n(`stat_qq_line()`).\n\n\n\n\n\n\n\n\n\nThat’s better. Maybe try a square root transformation?\n\n# Perform a sqrt transformation and plot\ndata_ex$MV_Total_sqrt &lt;- sqrt(data_ex$MV_Total)\n\n# Create histogram and qqplot\ndistr_plots(data_ex, \"MV_Total_sqrt\", 10)\n\nWarning: Removed 3166 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\nWarning: Removed 3166 rows containing non-finite outside the scale range\n(`stat_qq()`).\n\n\nWarning: Removed 3166 rows containing non-finite outside the scale range\n(`stat_qq_line()`).\n\n\n\n\n\n\n\n\n\nThat looks worse. Out of curiosity I’m going to try using the bestNormalize package I’ve been learning to see if it can recommend the best tranformation.\n\nBNobject &lt;- bestNormalize(data_ex$MV_Total)\n\nWarning: `progress_estimated()` was deprecated in dplyr 1.0.0.\nℹ The deprecated feature was likely used in the bestNormalize package.\n  Please report the issue to the authors.\n\nBNobject\n\nBest Normalizing transformation with 3264 Observations\n Estimated Normality Statistics (Pearson P / df, lower =&gt; more normal):\n - arcsinh(x): 24.3658\n - Box-Cox: 24.6526\n - Center+scale: 23.8755\n - Double Reversed Log_b(x+a): 25.0356\n - Exp(x): 343.5991\n - Log_b(x+a): 24.3607\n - orderNorm (ORQ): 24.2459\n - sqrt(x + a): 24.1957\n - Yeo-Johnson: 24.6575\nEstimation method: Out-of-sample via CV with 10 folds and 5 repeats\n \nBased off these, bestNormalize chose:\ncenter_scale(x) Transformation with 3264 nonmissing obs.\n Estimated statistics:\n - mean (before standardization) = 6.102022 \n - sd (before standardization) = 5.111536 \n\n\nThe short explanation of how these values work is that the value closest to 1 is the best transformation, but these are all VERY far from one. So that didn’t help.\n\nBoxplot\n\n#Create boxplot to assess for outliers\nggplot(data_ex, aes(y = MV_Total_log)) +\n  geom_boxplot() +\n  labs(title = \"Boxplot of MV Total Log\",\n       y = \"MV Total Log\")\n\nWarning: Removed 3166 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\n\n\n\n\nThere are also no outliers for MV_Total_Log!\n\n\nSummary\nIt looks like the best option we have is a log transformation of MV TOTAL.\nAlternatively, we will likely end up running a Poisson or Negative Binomial regression to handle this data.\nTop of Tabset\n\n\n\n\n# Create histogram and qqplot\ndistr_plots(data_ex, \"Unit_LOS\", 10)\n\n\n\n\n\n\n\n\nThat’s super skewed as well! Let’s try a log transform.\n\n# Perform log transform of MICU LOS\ndata_ex$Unit_LOS_log &lt;- log(data_ex$Unit_LOS)\n\n# Create histogram and qqplot\ndistr_plots(data_ex, \"Unit_LOS_log\", 10)\n\n\n\n\n\n\n\n\nWe’ve got more of an S shape going there. I wonder if a BoxCox transformation is more appropriate?\n\nlibrary(bestNormalize)\n# Use bestNormalize to try to find best transformation\nBNobject &lt;- bestNormalize(data_ex$Unit_LOS)\nBNobject\n\nBest Normalizing transformation with 6430 Observations\n Estimated Normality Statistics (Pearson P / df, lower =&gt; more normal):\n - arcsinh(x): 9.3062\n - Box-Cox: 5.7002\n - Center+scale: 34.3931\n - Double Reversed Log_b(x+a): 56.7025\n - Log_b(x+a): 9.313\n - orderNorm (ORQ): 1.2666\n - sqrt(x + a): 19.5835\n - Yeo-Johnson: 5.7212\nEstimation method: Out-of-sample via CV with 10 folds and 5 repeats\n \nBased off these, bestNormalize chose:\norderNorm Transformation with 6430 nonmissing obs and ties\n - 515 unique values \n - Original quantiles:\n    0%    25%    50%    75%   100% \n 48.00  66.00  95.00 166.75 720.00 \n\n# Perform boxcox transformation\ndata_ex$Unit_LOS_boxcox &lt;- boxcox(data_ex$Unit_LOS)$x.t\n\n# Create histogram and qqplot\ndistr_plots(data_ex, \"Unit_LOS_boxcox\", 25)\n\n\n\n\n\n\n\n\n\n# Create boxplot to assess for outliers\nggplot(data_ex, aes(y = Unit_LOS_boxcox)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\nThere are no outliers for boxcox Unit LOS.\nThat histogram looks better at least, and we don’t have any outliers. The bestNormalize package offers ways to back transform after you run your analysis, but I haven’t gotten that far in learning it yet. It looks like a boxcox transformation is a candidate however.\n\nSummary\nWill come back to this. I think there is a specific link function we use in this situation when we have an s-shaped qq plot like that.\nTop of Tabset\n\n\n\n\n# Create histogram and qqplot\ndistr_plots(data_ex, \"`HOSPITAL LOS`\", 25)\n\n\n\n\n\n\n\n\nHospital LOS also looks logarithmic.\nLet’s transform and assess.\n\n# Perform log transform \ndata_ex$HOSPITAL_LOS_log &lt;- log(data_ex$`HOSPITAL LOS`)\n\n# Create histogram and qqplot\ndistr_plots(data_ex, \"HOSPITAL_LOS_log\", 25)\n\n\n\n\n\n\n\nggplot(data_ex, aes(y = HOSPITAL_LOS_log)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\nLooks good! There are a handful of outliers in there, but the data looks normal and those values will likely be included.\n\nSummary\nHOSPITAL LOS is normal after a log transform. We will either perform that or a Poisson or Negative Binomial Top of Tabset\n\n\n\nWe must compute NON_ICU_LOS by subtracting Unit_LOS from HOSPITAL LOS.\n\n# Compute variable for non icu LOS.\ndata_ex &lt;- data_ex |&gt; \n  mutate(NON_ICU_LOS = `HOSPITAL LOS` - Unit_LOS)\n\n# Create histogram and qqplot\ndistr_plots(data_ex, \"NON_ICU_LOS\", 25)\n\n\n\n\n\n\n\n\nAs expected, that looks exponential.\n\n# Perform log transform\ndata_ex$NON_ICU_LOS_log &lt;- log(data_ex$NON_ICU_LOS)\n\n# Create histogram and qqplot\ndistr_plots(data_ex, \"NON_ICU_LOS_log\", 25)\n\nWarning: Removed 106 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\nWarning: Removed 106 rows containing non-finite outside the scale range\n(`stat_qq()`).\n\n\nWarning: Removed 106 rows containing non-finite outside the scale range\n(`stat_qq_line()`).\n\n\n\n\n\n\n\n\n\nThat’s bimodal. Odd.\nLet’s try a square root transform\n\n# Perform log transform\ndata_ex$NON_ICU_LOS_sqrt &lt;- sqrt(data_ex$NON_ICU_LOS)\n\n# Create histogram and qqplot\ndistr_plots(data_ex, \"NON_ICU_LOS_sqrt\", 25)\n\n\n\n\n\n\n\n\nLooks crummy.\nLet’s try bestNormalize\n\nBNobject &lt;- bestNormalize(data_ex$NON_ICU_LOS)\nBNobject\n\nBest Normalizing transformation with 6430 Observations\n Estimated Normality Statistics (Pearson P / df, lower =&gt; more normal):\n - arcsinh(x): 7.2477\n - Center+scale: 51.5028\n - Double Reversed Log_b(x+a): 57.401\n - Log_b(x+a): 8.7998\n - orderNorm (ORQ): 1.7176\n - sqrt(x + a): 10.5751\n - Yeo-Johnson: 5.3716\nEstimation method: Out-of-sample via CV with 10 folds and 5 repeats\n \nBased off these, bestNormalize chose:\norderNorm Transformation with 6430 nonmissing obs and ties\n - 1045 unique values \n - Original quantiles:\n    0%    25%    50%    75%   100% \n   0.0   32.0  110.5  284.0 9206.0 \n\n# Perform boxcox transformation\ndata_ex$NON_ICU_LOS_yeo &lt;- yeojohnson(data_ex$NON_ICU_LOS)$x.t\n\n# Create histogram and qqplot\ndistr_plots(data_ex, \"NON_ICU_LOS_yeo\", 25)\n\n\n\n\n\n\n\n\nLooks better! Still bimodal but much more normal.\n\nSummary\nLooks like Yeo-Johnson transformation may be the way to go.\nTop of Tabset\n\n\n\n\n# Examine discharge locations\npretty_print(table(data_ex$DISCH_DISP))\n\n\n\n\nVar1\nFreq\n\n\n\n\nAcute IP to RPCU\n5\n\n\nAdministrative Discharge\n1\n\n\nAdmitted as an Inpatient\n3\n\n\nAgainst Clinical Advice\n22\n\n\nAnother Health Care Institution Not Defined w/Planned Readmission\n1\n\n\nCourt/Law Enforcement\n22\n\n\nDischarge Lounge\n6\n\n\nDischarged to Other Facility\n23\n\n\nDischarged/transferred to a Designated Cancer Center or Children’s Hospital\n41\n\n\nDrug/Alch Detox D/C\n4\n\n\nExpired\n1137\n\n\nExpired in Medical Facility\n8\n\n\nExpired Readmit as Organ Donor\n7\n\n\nFederal Hospital\n43\n\n\nHome-Health Care Svc\n759\n\n\nHome or Self Care\n2417\n\n\nHome or Self Care w/Planned Readmission\n1\n\n\nHospice/Home\n213\n\n\nHospice/Medical Facility\n211\n\n\nIntermediate Care Facility\n14\n\n\nIV Therapy Provider\n1\n\n\nLeft Against Medical Advice\n49\n\n\nLong Term Care\n451\n\n\nMental Health Facility\n2\n\n\nNursing Facility\n37\n\n\nPsychiatric Hospital\n34\n\n\nRehab Facility\n249\n\n\nShort Term Hospital\n71\n\n\nSkilled Nursing Facility\n596\n\n\nSwing Bed\n1\n\n\nTRANSFER TO CANCER OR CHILDRENS\n1\n\n\n\n\n\n\n\nThere are a few options we have for to analyze discharge location.\n\nWe can spit on expired vs not expired\nWe can split on hom vs other\nWe can just look at all the groups with the largest N, and collapse the rest into a single variable of ‘other’. This would be\n\nExpired (1137), Home (3177), Hospice (424), long term care (451), rehab (249), skilled Nursing facility (596)\n\n\nWe will have to ask the researcher how she wants to group up these discharge locations! Keeping in mind that since we are looking at three different time periods, each group will essentially by a third of what is shown here."
  },
  {
    "objectID": "Project_3_Predictive_Modelling/Project_3_SAS/Code/images/Megan_Watson_Project.html#average-micu-los",
    "href": "Project_3_Predictive_Modelling/Project_3_SAS/Code/images/Megan_Watson_Project.html#average-micu-los",
    "title": "Data Checking - Statistical Consulting",
    "section": "Average MICU LOS",
    "text": "Average MICU LOS\n\n# Summarize and plot the data\ndata_ex |&gt; \n  group_by(Admit_MonthYear) |&gt; \n  summarise(avg_Unit_LOS = mean(Unit_LOS, na.rm = T)) |&gt; \n  ggplot(aes(x = Admit_MonthYear, y = avg_Unit_LOS)) +\n  geom_line() +\n  theme_minimal() +\n  geom_rect(data = highlight_periods, aes(xmin = start, xmax = end, ymin = -Inf, ymax = Inf), \n            inherit.aes = FALSE, fill = \"lightblue\", alpha = 0.2) +\n  labs(title = \"Average MICU LOS over Time\",\n       x = \"Admit Date\",\n       y = \"Avg MICU LOS\")\n\n\n\n\n\n\n\n\nIt does not appear that MICU_LOS has decreased across any of the time periods.\nThis is including patients that did not receive PT, so of course there’s no difference.\n\nMICU LOS over Time by Mechanical Ventilation\n\n# Dummy code MV status \ndata_ex$MV_YN &lt;- ifelse(is.na(data_ex$MV_Total), 0, 1)\n\n# Make it a factor for better plotting\ndata_ex &lt;- data_ex |&gt; \n  mutate(MV_YN = as.factor(MV_YN))\n\n# Summarize and plot the data\ndata_ex |&gt; \n  group_by(Admit_MonthYear, MV_YN) |&gt; \n  summarise(avg_Unit_LOS = mean(Unit_LOS, na.rm = T)) |&gt; \n  ggplot(aes(x = Admit_MonthYear, y = avg_Unit_LOS, color = MV_YN, group = MV_YN)) +\n  geom_line() +\n  theme_minimal() +\n  geom_rect(data = highlight_periods, aes(xmin = start, xmax = end, ymin = -Inf, ymax = Inf), \n            inherit.aes = FALSE, fill = \"lightblue\", alpha = 0.2) +\n  labs(title = \"Average MICU LOS over Time by Mechanical Ventilation\",\n       x = \"Admit Date\",\n       y = \"Avg MICU LOS\")\n\n`summarise()` has grouped output by 'Admit_MonthYear'. You can override using\nthe `.groups` argument.\n\n\n\n\n\n\n\n\n\nThis shows that those who were mechanically ventilated had a higher MICU length of stay. This does not appear to have differed in relation to the QI iniative.\nI am also including those that did not receive PT here, so that would explain the null finding.\n\ndata_ex &lt;- data_ex |&gt; \n  mutate(PT_YESNO = as.factor(PT_YESNO))\n\n# Summarize and plot the data\ndata_ex |&gt; \n  group_by(Admit_MonthYear, MV_YN, PT_YESNO) |&gt; \n  summarise(avg_Unit_LOS = mean(Unit_LOS, na.rm = T)) |&gt; \n  ggplot(aes(x = Admit_MonthYear, y = avg_Unit_LOS, color = PT_YESNO)) +\n  geom_point() +\n  geom_smooth(method = \"loess\", se = F) +\n  theme_minimal() +\n  geom_rect(data = highlight_periods, aes(xmin = start, xmax = end, ymin = -Inf, ymax = Inf), \n            inherit.aes = FALSE, fill = \"lightblue\", alpha = 0.2) +\n  labs(title = \"Average MICU LOS over Time by Mechanical Ventilation\",\n       x = \"Admit Date\",\n       y = \"Avg MICU LOS\") +\n  facet_wrap(~MV_YN)\n\n`summarise()` has grouped output by 'Admit_MonthYear', 'MV_YN'. You can\noverride using the `.groups` argument.\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nI think this might show an interaction. For those on mechanical ventilation, they seemed to decrease in MICU LOS more if they received PT.\nFor those not on MV, the PT did not seem to change their average LOS.\n\n\nAverage MICU LOS over Time by Physical Therapy\n\n# Make it a factor for better plotting\ndata_ex &lt;- data_ex |&gt; \n  mutate(PT_YESNO = as.factor(PT_YESNO))\n\n# Summarize and plot the data\ndata_ex |&gt; \n  group_by(Admit_MonthYear, PT_YESNO) |&gt; \n  summarise(avg_Unit_LOS = mean(Unit_LOS, na.rm = T)) |&gt; \n  filter(Admit_MonthYear &gt; as.Date(\"2012-01-01\")) |&gt; \n  ggplot(aes(x = Admit_MonthYear, y = avg_Unit_LOS, color = PT_YESNO, group = PT_YESNO)) +\n  geom_line() +\n  theme_minimal() +\n  geom_rect(data = highlight_periods, aes(xmin = start, xmax = end, ymin = -Inf, ymax = Inf), \n            inherit.aes = FALSE, fill = \"lightblue\", alpha = 0.2) +\n  labs(title = \"Average MICU LOS over Time by Physical Therapy\",\n       x = \"Admit Date\",\n       y = \"Avg MICU LOS\")\n\n`summarise()` has grouped output by 'Admit_MonthYear'. You can override using\nthe `.groups` argument.\n\n\n\n\n\n\n\n\n\n\n# Summarize and plot the data\ndata_ex |&gt; \n  group_by(Admit_MonthYear, PT_YESNO) |&gt; \n  summarise(avg_Unit_LOS = mean(Unit_LOS, na.rm = T)) |&gt; \n  filter(Admit_MonthYear &gt; as.Date(\"2012-01-01\")) |&gt; \n  ggplot(aes(x = Admit_MonthYear, y = avg_Unit_LOS, color = PT_YESNO, group = PT_YESNO)) +\n  geom_point() +\n  geom_smooth(method = \"loess\", se = F) +\n  theme_minimal() +\n  geom_rect(data = highlight_periods, aes(xmin = start, xmax = end, ymin = -Inf, ymax = Inf), \n            inherit.aes = FALSE, fill = \"lightblue\", alpha = 0.2) +\n  labs(title = \"Average MICU LOS over Time by Physical Therapy\",\n       x = \"Admit Date\",\n       y = \"Avg MICU LOS\")\n\n`summarise()` has grouped output by 'Admit_MonthYear'. You can override using\nthe `.groups` argument.\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nInteresting! It does look like average MICU length of stay decreased slighly immediately after the QI initiative. It also looks like it may be increasing slightly from then on.\nThat does appear that those received PT had a decrease before and after the iniative\nBut overall it looks like the QI may have successfuly decreased MICU LOS.\n\n\nFitting with Splines\n\nlibrary(splines)\n# Summarize and plot the data\ndata_ex |&gt; \n  group_by(Admit_MonthYear, PT_YESNO) |&gt; \n  summarise(avg_Unit_LOS = mean(Unit_LOS, na.rm = T)) |&gt; \n  filter(Admit_MonthYear &gt; as.Date(\"2012-01-01\")) |&gt; \n  ggplot(aes(x = Admit_MonthYear, y = avg_Unit_LOS, color = PT_YESNO, group = PT_YESNO)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", formula = y ~ ns(x, df = 4), se = F) +\n  theme_minimal() +\n  geom_rect(data = highlight_periods, aes(xmin = start, xmax = end, ymin = -Inf, ymax = Inf), \n            inherit.aes = FALSE, fill = \"lightblue\", alpha = 0.2) +\n  labs(title = \"Average MICU LOS over Time by Physical Therapy\",\n       x = \"Admit Date\",\n       y = \"Avg MICU LOS\")\n\n`summarise()` has grouped output by 'Admit_MonthYear'. You can override using\nthe `.groups` argument.\n\n\n\n\n\n\n\n\n\nLooking just at those who received PT\n\n# Make it a factor for better plotting\ndata_ex &lt;- data_ex |&gt; \n  mutate(PT_YESNO = as.factor(PT_YESNO))\n\n# Summarize and plot the data\ndata_ex |&gt; \n  group_by(Admit_MonthYear, PT_YESNO) |&gt; \n  summarise(avg_Unit_LOS = mean(Unit_LOS, na.rm = T)) |&gt; \n  filter(Admit_MonthYear &gt; as.Date(\"2012-01-01\")) |&gt; \n  filter(PT_YESNO == 1) |&gt; \n  ggplot(aes(x = Admit_MonthYear, y = avg_Unit_LOS, group = PT_YESNO)) +\n  geom_point() +\n  geom_smooth(method = \"loess\", se = F) +\n  theme_minimal() +\n  geom_rect(data = highlight_periods, aes(xmin = start, xmax = end, ymin = -Inf, ymax = Inf), \n            inherit.aes = FALSE, fill = \"lightblue\", alpha = 0.2) +\n  labs(title = \"Average MICU LOS over Time by Physical Therapy\",\n       x = \"Admit Date\",\n       y = \"Avg MICU LOS\")\n\n`summarise()` has grouped output by 'Admit_MonthYear'. You can override using\nthe `.groups` argument.\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nFitting a loess curve, it does appear that for those that receive PT, they had a decrease in average MICU LOS during the intervention compared to before. It also appears that they are increasing in average MICU following the interention, but this may still be lower compared to the pre-intervention period.\n:::"
  },
  {
    "objectID": "Project_3_Predictive_Modelling/Project_3_SAS/Code/images/Megan_Watson_Project.html#increase-frequency-of-pt-visits",
    "href": "Project_3_Predictive_Modelling/Project_3_SAS/Code/images/Megan_Watson_Project.html#increase-frequency-of-pt-visits",
    "title": "Data Checking - Statistical Consulting",
    "section": "Increase Frequency of PT Visits",
    "text": "Increase Frequency of PT Visits\n\n# Summarize and plot the data\ndata_ex |&gt; \n  group_by(Admit_MonthYear) |&gt; \n  summarise(avg_pct_therapy = mean(`PERCENT OF DAYS WITH THERAPY VISITS`, na.rm = T)) |&gt; \n  ggplot(aes(x = Admit_MonthYear, y = avg_pct_therapy)) +\n  geom_line() +\n  theme_minimal() +\n  geom_rect(data = highlight_periods, aes(xmin = start, xmax = end, ymin = -Inf, ymax = Inf), \n            inherit.aes = FALSE, fill = \"lightblue\", alpha = 0.2) +\n  labs(title = \"Average Percentage of Days with Physical Therapy Over Time\",\n       x = \"Admit Date\",\n       y = \"Average Days with Physical Therapy (%)\")\n\n\n\n\n\n\n\n\nInteresting! They successfully and drastically increased the percentage of days that patients had physical therapy (roughly from 10-20% pre-iniative to 50-60% post iniative). This percentage did seem to drop off over time during the post- initiative period however, though it is still higher than it was pre-initiative (now roughly 30-40%).\n\nFit with Loess\n\n# Summarize and plot the data\ndata_ex |&gt; \n  group_by(Admit_MonthYear) |&gt; \n  summarise(avg_pct_therapy = mean(`PERCENT OF DAYS WITH THERAPY VISITS`, na.rm = T)) |&gt; \n  ggplot(aes(x = Admit_MonthYear, y = avg_pct_therapy)) +\n  geom_point() +\n  geom_smooth(method = \"loess\", se = F) +\n  theme_minimal() +\n  geom_rect(data = highlight_periods, aes(xmin = start, xmax = end, ymin = -Inf, ymax = Inf), \n            inherit.aes = FALSE, fill = \"lightblue\", alpha = 0.2) +\n  labs(title = \"Average Percentage of Days with Physical Therapy Over Time\",\n       x = \"Admit Date\",\n       y = \"Average Days with Physical Therapy (%)\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\nFit with Splines\n\n# Summarize and plot the data\ndata_ex |&gt; \n  group_by(Admit_MonthYear) |&gt; \n  summarise(avg_pct_therapy = mean(`PERCENT OF DAYS WITH THERAPY VISITS`, na.rm = T)) |&gt; \n  ggplot(aes(x = Admit_MonthYear, y = avg_pct_therapy)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", formula = y ~ ns(x, df = 5), se = F) +\n  theme_minimal() +\n  geom_rect(data = highlight_periods, aes(xmin = start, xmax = end, ymin = -Inf, ymax = Inf), \n            inherit.aes = FALSE, fill = \"lightblue\", alpha = 0.2) +\n  labs(title = \"Average Percentage of Days with Physical Therapy Over Time\",\n       x = \"Admit Date\",\n       y = \"Average Days with Physical Therapy (%)\")"
  },
  {
    "objectID": "Project_3_Predictive_Modelling/Project_3_SAS/Code/images/Megan_Watson_Project.html#decrease-time-from-admit-to-therapy",
    "href": "Project_3_Predictive_Modelling/Project_3_SAS/Code/images/Megan_Watson_Project.html#decrease-time-from-admit-to-therapy",
    "title": "Data Checking - Statistical Consulting",
    "section": "Decrease Time from Admit to Therapy",
    "text": "Decrease Time from Admit to Therapy\n\n# Summarize and plot the data\ndata_ex |&gt; \n  group_by(Admit_MonthYear) |&gt; \n  summarise(avg_ADMIT_TO_TREAT = mean(`ADMIT TO TREAT`, na.rm = T)) |&gt; \n  ggplot(aes(x = Admit_MonthYear, y = avg_ADMIT_TO_TREAT)) +\n  geom_line() +\n  theme_minimal() +\n  geom_rect(data = highlight_periods, aes(xmin = start, xmax = end, ymin = -Inf, ymax = Inf), \n            inherit.aes = FALSE, fill = \"lightblue\", alpha = 0.2) +\n  annotate(\"text\", x = as.Date(\"2015-09-01\"), y = Inf, \n           label = \"QI-Initiative\", color = \"skyblue2\", size = 4, vjust = 1.5,) +\n  labs(title = \"Time from Admit to Therapy over Time\",\n       x = \"Admit Date\",\n       y = \"Time from Admit to Therapy\")\n\n\n\n\n\n\n\n\nThey successfully decreased the time from a patient’s first admit to their first physical therapy session.\nThis means that patients were receiving PT sooner.\n\nPlot with Loess\n\n# Summarize and plot the data\ndata_ex |&gt; \n  group_by(Admit_MonthYear) |&gt; \n  summarise(avg_ADMIT_TO_TREAT = mean(`ADMIT TO TREAT`, na.rm = T)) |&gt; \n  ggplot(aes(x = Admit_MonthYear, y = avg_ADMIT_TO_TREAT)) +\n  geom_point() +\n  geom_smooth(method = \"loess\", se = F) +\n  theme_minimal() +\n  geom_rect(data = highlight_periods, aes(xmin = start, xmax = end, ymin = -Inf, ymax = Inf), \n            inherit.aes = FALSE, fill = \"lightblue\", alpha = 0.2) +\n  annotate(\"text\", x = as.Date(\"2015-09-01\"), y = Inf, \n           label = \"QI-Initiative\", color = \"skyblue2\", size = 4, vjust = 1.5,) +\n  labs(title = \"Time from Admit to Therapy over Time\",\n       x = \"Admit Date\",\n       y = \"Time from Admit to Therapy\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nYeah looks crummier with loess, will likely delete."
  },
  {
    "objectID": "Project_3_Predictive_Modelling/Project_3_SAS/Code/images/Megan_Watson_Project.html#increase-percentage-of-admissions-receiving-physical-therapy",
    "href": "Project_3_Predictive_Modelling/Project_3_SAS/Code/images/Megan_Watson_Project.html#increase-percentage-of-admissions-receiving-physical-therapy",
    "title": "Data Checking - Statistical Consulting",
    "section": "Increase Percentage of Admissions Receiving Physical Therapy",
    "text": "Increase Percentage of Admissions Receiving Physical Therapy\n\n# Summarize and calculate percentages\nsummary_data &lt;- data_ex |&gt; \n  group_by(Admit_MonthYear, PT_YESNO) |&gt; \n  summarise(count = n()) |&gt; \n  mutate(total_count = sum(count)) |&gt; \n  ungroup() |&gt; \n  mutate(percentage = (count / total_count) * 100) |&gt; \n  filter(PT_YESNO == 1)\n\n`summarise()` has grouped output by 'Admit_MonthYear'. You can override using\nthe `.groups` argument.\n\n# Plot the data\nggplot(summary_data, aes(x = Admit_MonthYear, y = percentage)) +\n  geom_line() +\n  theme_minimal() +\n  geom_rect(data = highlight_periods, aes(xmin = start, xmax = end, ymin = -Inf, ymax = Inf), \n            inherit.aes = FALSE, fill = \"lightblue\", alpha = 0.2) +\n  annotate(\"text\", x = as.Date(\"2015-09-01\"), y = Inf, \n           label = \"QI-Initiative\", color = \"skyblue2\", size = 4, vjust = 1.5) +\n  labs(title = \"Percentage of Admissions Receiving Physical Therapy\",\n       x = \"Admit Date\",\n       y = \"Percentage\")\n\n\n\n\n\n\n\n\nThey also successfully increased the percentage of patients receiving PT from ~40% pre-initiative to ~80% during the initiative. This percentage then decreases to ~70% post-initiative.\n::::"
  },
  {
    "objectID": "Project_3_Predictive_Modelling/Project_3_SAS/Code/images/Megan_Watson_Project.html#mechanical-ventilation-1",
    "href": "Project_3_Predictive_Modelling/Project_3_SAS/Code/images/Megan_Watson_Project.html#mechanical-ventilation-1",
    "title": "Data Checking - Statistical Consulting",
    "section": "Mechanical Ventilation",
    "text": "Mechanical Ventilation\nThe researcher expressed that patients would be stratified on Mechanical ventilation v. NO mechanical ventilation. However, I do not see this variable. Let’s go ahead and create it, assuming that patients with NA for MV_TOTAL were simply never on mechanical ventilation.\n\n# Dummy code MV status \ndata_ex$MV_YN &lt;- ifelse(is.na(data_ex$MV_Total), 0, 1)\n\nLet’s check that worked as intended.\n\n# Filter down to variables of interest to verify dummy coding worked\ncheck &lt;- data_ex %&gt;%\n  select(id, MV_Total, MV_YN)\n\n# Pretty print\nkable(head(check), format = \"html\") %&gt;%\n  kable_styling(bootstrap_options = c(\"condensed\", \"stripe\", \"hover\"))\n\n\n\n\nid\nMV_Total\nMV_YN\n\n\n\n\n2\nNA\n0\n\n\n3\n13\n1\n\n\n4\nNA\n0\n\n\n5\n16\n1\n\n\n8\nNA\n0\n\n\n10\n2\n1\n\n\n\n\n\n\n\nLooks good."
  },
  {
    "objectID": "Project_3_Predictive_Modelling/Project_3_SAS/Code/images/Megan_Watson_Project.html#qi-initiative-time-period",
    "href": "Project_3_Predictive_Modelling/Project_3_SAS/Code/images/Megan_Watson_Project.html#qi-initiative-time-period",
    "title": "Data Checking - Statistical Consulting",
    "section": "QI-Initiative Time Period",
    "text": "QI-Initiative Time Period\nLet’s group participants into the time period that they were present during the initiative for.\n\n# Create new variable based on QI initiative time period.\ndata_ex &lt;- data_ex |&gt; \n  mutate(initiative = ifelse(Admit_MonthYear &lt; as.Date(\"2015-04-01\"), \"Pre\",\n                             ifelse(Admit_MonthYear &gt; as.Date(\"2015-12-01\"), \"Post\", \"During\")))\n\nLet’s double check we did that right.\n\n# Check counts\ntable(data_ex$initiative)\n\n\nDuring   Post    Pre \n   656   2860   2889 \n\n# Double check assigning\ndata_ex |&gt; \n  select(id, Admit_MonthYear, initiative) |&gt; \n  filter(Admit_MonthYear &lt; as.Date(\"2015-04-01\")) |&gt; \n  arrange(desc(Admit_MonthYear)) |&gt; \n  head() |&gt; \n  pretty_print()\n\n\n\n\nid\nAdmit_MonthYear\ninitiative\n\n\n\n\n5759\n2015-03-01\nPre\n\n\n5760\n2015-03-01\nPre\n\n\n5761\n2015-03-01\nPre\n\n\n5763\n2015-03-01\nPre\n\n\n5765\n2015-03-01\nPre\n\n\n5767\n2015-03-01\nPre\n\n\n\n\n\n\n# Double check assigning\ndata_ex |&gt; \n  select(id, Admit_MonthYear, initiative) |&gt; \n  filter(Admit_MonthYear &gt; as.Date(\"2015-04-01\")) |&gt; \n  arrange(Admit_MonthYear) |&gt; \n  head() |&gt; \n  pretty_print()\n\n\n\n\nid\nAdmit_MonthYear\ninitiative\n\n\n\n\n6094\n2015-05-01\nDuring\n\n\n6096\n2015-05-01\nDuring\n\n\n6099\n2015-05-01\nDuring\n\n\n6100\n2015-05-01\nDuring\n\n\n6105\n2015-05-01\nDuring\n\n\n6106\n2015-05-01\nDuring\n\n\n\n\n\n\n# Double check assigning\ndata_ex |&gt; \n  select(id, Admit_MonthYear, initiative) |&gt; \n  filter(Admit_MonthYear &gt;= as.Date(\"2015-12-01\")) |&gt; \n  arrange(Admit_MonthYear) |&gt; \n  head() |&gt; \n  pretty_print()\n\n\n\n\nid\nAdmit_MonthYear\ninitiative\n\n\n\n\n7167\n2015-12-01\nDuring\n\n\n7169\n2015-12-01\nDuring\n\n\n7171\n2015-12-01\nDuring\n\n\n7172\n2015-12-01\nDuring\n\n\n7174\n2015-12-01\nDuring\n\n\n7176\n2015-12-01\nDuring\n\n\n\n\n\n\n# Double check assigning\ndata_ex |&gt; \n  select(id, Admit_MonthYear, initiative) |&gt; \n  filter(Admit_MonthYear &gt; as.Date(\"2015-12-01\")) |&gt; \n  arrange(Admit_MonthYear) |&gt; \n  head() |&gt; \n  pretty_print()\n\n\n\n\nid\nAdmit_MonthYear\ninitiative\n\n\n\n\n7324\n2016-01-01\nPost\n\n\n7331\n2016-01-01\nPost\n\n\n7336\n2016-01-01\nPost\n\n\n7339\n2016-01-01\nPost\n\n\n7343\n2016-01-01\nPost\n\n\n7345\n2016-01-01\nPost\n\n\n\n\n\n\n\nLooks good.\n\nSome Graphing\n\ndata_ex &lt;- data_ex |&gt; \n  mutate(initiative = as.factor(initiative)) |&gt; \n  mutate(initiative = relevel(initiative, ref = \"Pre\"))\n\nggplot(data_ex, aes(x = PT_YESNO, y = Unit_LOS_log, fill = initiative)) +\n  geom_boxplot() +\n  theme_minimal() +\n  scale_fill_brewer(palette = \"Pastel2\")\n\n\n\n\n\n\n\n\nThis shows that MICU LOS decreased in the during and post QI initiative periods compared to the pre-iniatitive period.\n\ndata_ex |&gt; \n  filter(PT_YESNO == 1) |&gt; \n  ggplot(aes(x = initiative, y = Unit_LOS_log, fill = initiative)) +\n  geom_boxplot() +\n  theme_minimal() +\n  scale_fill_brewer(palette = \"Pastel2\")\n\n\n\n\n\n\n\n\n\ndata_ex |&gt; \n  filter(PT_YESNO == 1) |&gt; \n  ggplot(aes(x = initiative, y = Unit_LOS_log, fill = initiative)) +\n  geom_boxplot() +\n  theme_minimal() +\n  scale_fill_brewer(palette = \"Pastel2\")\n\n\n\n\n\n\n\n\nThere is an interaction here between PT and unit LOS and initiative.\nIn other words, you HAVE to filter based on PT\n\n\nMV Total\n\nggplot(data_ex, aes(x = PT_YESNO, y = MV_Total_log, fill = initiative)) +\n  geom_boxplot() +\n  theme_minimal() +\n  scale_fill_brewer(palette = \"Pastel2\")\n\nWarning: Removed 3149 rows containing non-finite outside the scale range\n(`stat_boxplot()`)."
  },
  {
    "objectID": "Project_3_Predictive_Modelling/Project_3_SAS/Code/images/Megan_Watson_Project.html#check-for-repeat-admissions",
    "href": "Project_3_Predictive_Modelling/Project_3_SAS/Code/images/Megan_Watson_Project.html#check-for-repeat-admissions",
    "title": "Data Checking - Statistical Consulting",
    "section": "Check for Repeat Admissions",
    "text": "Check for Repeat Admissions\n\n# Check for repeate admissions\nlength(unique(data_ex$id))\n\n[1] 6405\n\ndim(data_ex)\n\n[1] 6405   40\n\n\nThe number of unique patient id’s is the same number as the number of rows in our data set.\nSo we do not have repeate admissions in this data set and do not need to account for repeated measures! (We meet the assumption of independent observations)\n:::"
  },
  {
    "objectID": "Project_3_Predictive_Modelling/Project_3_SAS/Code/images/Megan_Watson_Project-01.html",
    "href": "Project_3_Predictive_Modelling/Project_3_SAS/Code/images/Megan_Watson_Project-01.html",
    "title": "Data Checking - Statistical Consulting",
    "section": "",
    "text": "This is the Quarto markdown for the live consultation in BIOS 6621: Statistical Consulting.\nThis is a research project encompassing the entire data analysis pipeline, from initial consultation to scope of work writing, execution of statistical analysis, and completion of project deliverables."
  },
  {
    "objectID": "Project_3_Predictive_Modelling/Project_3_SAS/Code/images/Megan_Watson_Project-01.html#general-information",
    "href": "Project_3_Predictive_Modelling/Project_3_SAS/Code/images/Megan_Watson_Project-01.html#general-information",
    "title": "Data Checking - Statistical Consulting",
    "section": "General Information",
    "text": "General Information\n\n\n\n\n\n\n\n\n\nInvestigator\nMegan Watson\nDate\nSeptember 30, 2024\n\n\nProject Title\nQuality Improvement into Standard Practice: Persistent Practice Changes and Decreased Length of Stay for 3 years following a Medical ICU Physical Therapy Quality Improvement Project"
  },
  {
    "objectID": "Project_3_Predictive_Modelling/Project_3_SAS/Code/images/Megan_Watson_Project-01.html#project-description",
    "href": "Project_3_Predictive_Modelling/Project_3_SAS/Code/images/Megan_Watson_Project-01.html#project-description",
    "title": "Data Checking - Statistical Consulting",
    "section": "Project Description",
    "text": "Project Description\n\nBackground\n             The objective of this study is to assess the impact of a MICU physical therapy (PT) quality improvement (QI) project on physical therapist practice, and quantify the changes in length of stay (LOS) and mechanical ventilation (MV) time. The goals of the QI Initiative were to increase percentage of MICU admissions receiving PT, decrease time from MICU admission to first PT, and increase the frequency of PT visits. The main question the researcher came to us with was: how to correctly model the longitudinal data in the study?\n\n\nStudy Design\n              This was a retrospective cohort study, comparing 3 main time periods: Pre-QI Initiative (before April 2015), During QI Initiative (April 2015-Dec 2015), and After-QI Initiative (Jan 2016 and later). The QI initiative was deployed during the 9-month period between April 2015 and December 2015, during which time there were education and staffing changes. Patients were stratified as either having mechanical ventilation or not having mechanical ventilation. The primary outcomes of interest were MICU LOS and time on MV. Secondary outcomes of interest were the non-ICU LOS, total hospital LOS, and discharge location.\n\n\nDescription of the Data\n              Data was received as a de-identified .csv. Data points consist of: Admit and Discharge Time, PT Consult Time, Admit to Consult Time, First Therapy Time, Consult to Therapy Time, Admit to Therapy Time, Total Therapy Visits, Days with Therapy Visits, Average, min, and Max Therapy Length, Admit and Discharge Date/Time, Total MV Days, Hospital LOS, Patient Age, Sex, “Problem List”, and Codes (presumably ICD9/10).\n\n\nAnticipated Sample Size / Study Population\n              The sample size is N = ~3000-4000, consisting of patients admitted to the MICU with a LOS between 2 and 30 days (I.e. exclude &lt; 2 days or &gt; 30 days). This was a single site study performed here at Colorado University.\n\n\nHypothesis\n              The hypothesis for the study was that LOS and time on MV would decrease when comparing the pre- and intra-initiative time periods, and when comparing the pre- and post-initiative time periods. Clinical significance for this outcome is considered as a 1-2 day change in LOS. The intended end product of this project is a publication in a PT journal.\n\n\nAnalysis Plan (Sketch)\n              Data should first be examined for missingness, as this has not been performed. Similarly, repeat MICU admissions should be explored and controlled for to ensure independence of observations. Death during MICU stay should also be examined and adjusted for if needed. The analysis plan discussed was to utilize splines to capture the trajectory of LOS or time on MV over time by fitting smooth curves to the data. Linear contrasts should be performed to assess if there is a significant difference in LOS or time on MV when comparing two timepoints (i.e. pre- and intra-, pre- and post- , and post- and intra-QI Initiative. Supplementary analyses should be performed to assess secondary outcomes of interest of non-ICU LOS and total hospital LOS."
  },
  {
    "objectID": "Project_3_Predictive_Modelling/Project_3_SAS/Code/images/Megan_Watson_Project-01.html#project-deliverables",
    "href": "Project_3_Predictive_Modelling/Project_3_SAS/Code/images/Megan_Watson_Project-01.html#project-deliverables",
    "title": "Data Checking - Statistical Consulting",
    "section": "Project Deliverables",
    "text": "Project Deliverables\n             At this stage, Megan has only asked for insight into what method to use to account for variance in time for her analysis. There are currently no other deliverables.\nTimeline / Deadlines\nAt this time, Megan has not asked for data analysis efforts from us.\nShould this change, the anticipated project start date will be 1 week after our next meeting with her, where revisions to the Comprehensive Analysis Report are also expected to be completed.  The assigned analysts will reach out to the investigator to confirm the project timeframe and the project start date may be altered if necessary. 2 weeks will be allowed for initial data analysis by the assigned analyst.\nA follow-up meeting is to occur no later than 1 month after the next meeting with Megan. During the follow-up meeting, the project team will discuss and preliminary results of the analysis, mockup the desired tables and figures (1-2 descriptive tables, 1-2 analysis results tables and up to 4 graphs), and discuss next steps. During the follow-up meeting, the team will establish a meeting schedule for the remainder of the project. \nEstimated Cost\nThis represents our best estimate of the effort needed to complete the project. Should the scope of work change substantially, the analyst(s) will discuss changes with the investigator and issue a new or amended project agreement.\n\n\n\n\nEffort\n\n\n\n\nMonths\nPhD Faculty\nMaster’s Faculty\nStudent\n\n\n\n\n\n\n\n\n1\n20%\n0%\n80%\n\n\n\n\nTotal Costs\n’Bout Tree Fiddy"
  },
  {
    "objectID": "Project_3_Predictive_Modelling/Project_3_SAS/Code/images/Megan_Watson_Project-01.html#Outcome_Variables",
    "href": "Project_3_Predictive_Modelling/Project_3_SAS/Code/images/Megan_Watson_Project-01.html#Outcome_Variables",
    "title": "Data Checking - Statistical Consulting",
    "section": "Outcome Variables",
    "text": "Outcome Variables\n\nMechanical VentilationMICU Length of StayHospital Length of StayNon-ICU Length of StayDischarge Location\n\n\nFirst we will begin with mechanical ventilation time (MV)\n\n# Create histogram and qqplot\ndistr_plots(data_ex, \"MV_Total\")\n\nWarning: `aes_string()` was deprecated in ggplot2 3.0.0.\nℹ Please use tidy evaluation idioms with `aes()`.\nℹ See also `vignette(\"ggplot2-in-packages\")` for more information.\n\n\nWarning: Removed 3166 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\nWarning: Removed 3166 rows containing non-finite outside the scale range\n(`stat_qq()`).\n\n\nWarning: Removed 3166 rows containing non-finite outside the scale range\n(`stat_qq_line()`).\n\n\n\n\n\n\n\n\n\nThat’s some severe right skewness! Is that logarithmic? Let’s try and transform and see what happens.\n\n# Perform a log transform of mechanical ventilation time and plot\ndata_ex$MV_Total_log &lt;- log(data_ex$MV_Total)\n\n# Create histogram and qqplot\ndistr_plots(data_ex, \"MV_Total_log\", 10)\n\nWarning: Removed 3166 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\nWarning: Removed 3166 rows containing non-finite outside the scale range\n(`stat_qq()`).\n\n\nWarning: Removed 3166 rows containing non-finite outside the scale range\n(`stat_qq_line()`).\n\n\n\n\n\n\n\n\n\nThat’s better. Maybe try a square root transformation?\n\n# Perform a sqrt transformation and plot\ndata_ex$MV_Total_sqrt &lt;- sqrt(data_ex$MV_Total)\n\n# Create histogram and qqplot\ndistr_plots(data_ex, \"MV_Total_sqrt\", 10)\n\nWarning: Removed 3166 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\nWarning: Removed 3166 rows containing non-finite outside the scale range\n(`stat_qq()`).\n\n\nWarning: Removed 3166 rows containing non-finite outside the scale range\n(`stat_qq_line()`).\n\n\n\n\n\n\n\n\n\nThat looks worse. Out of curiosity I’m going to try using the bestNormalize package I’ve been learning to see if it can recommend the best tranformation.\n\nBNobject &lt;- bestNormalize(data_ex$MV_Total)\n\nWarning: `progress_estimated()` was deprecated in dplyr 1.0.0.\nℹ The deprecated feature was likely used in the bestNormalize package.\n  Please report the issue to the authors.\n\nBNobject\n\nBest Normalizing transformation with 3264 Observations\n Estimated Normality Statistics (Pearson P / df, lower =&gt; more normal):\n - arcsinh(x): 24.5648\n - Box-Cox: 24.6417\n - Center+scale: 23.9277\n - Double Reversed Log_b(x+a): 25.5195\n - Exp(x): 343.7202\n - Log_b(x+a): 24.6037\n - orderNorm (ORQ): 24.2529\n - sqrt(x + a): 24.2553\n - Yeo-Johnson: 24.781\nEstimation method: Out-of-sample via CV with 10 folds and 5 repeats\n \nBased off these, bestNormalize chose:\ncenter_scale(x) Transformation with 3264 nonmissing obs.\n Estimated statistics:\n - mean (before standardization) = 6.102022 \n - sd (before standardization) = 5.111536 \n\n\nThe short explanation of how these values work is that the value closest to 1 is the best transformation, but these are all VERY far from one. So that didn’t help.\n\nBoxplot\n\n#Create boxplot to assess for outliers\nggplot(data_ex, aes(y = MV_Total_log)) +\n  geom_boxplot() +\n  labs(title = \"Boxplot of MV Total Log\",\n       y = \"MV Total Log\")\n\nWarning: Removed 3166 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\n\n\n\n\nThere are also no outliers for MV_Total_Log!\n\n\nSummary\nIt looks like the best option we have is a log transformation of MV TOTAL.\nAlternatively, we will likely end up running a Poisson or Negative Binomial regression to handle this data.\nTop of Tabset\n\n\n\n\n# Create histogram and qqplot\ndistr_plots(data_ex, \"Unit_LOS\", 10)\n\n\n\n\n\n\n\n\nThat’s super skewed as well! Let’s try a log transform.\n\n# Perform log transform of MICU LOS\ndata_ex$Unit_LOS_log &lt;- log(data_ex$Unit_LOS)\n\n# Create histogram and qqplot\ndistr_plots(data_ex, \"Unit_LOS_log\", 10)\n\n\n\n\n\n\n\n\nWe’ve got more of an S shape going there. I wonder if a BoxCox transformation is more appropriate?\n\nlibrary(bestNormalize)\n# Use bestNormalize to try to find best transformation\nBNobject &lt;- bestNormalize(data_ex$Unit_LOS)\nBNobject\n\nBest Normalizing transformation with 6430 Observations\n Estimated Normality Statistics (Pearson P / df, lower =&gt; more normal):\n - arcsinh(x): 9.2942\n - Box-Cox: 5.6373\n - Center+scale: 34.8156\n - Double Reversed Log_b(x+a): 57.3566\n - Log_b(x+a): 9.2942\n - orderNorm (ORQ): 1.3648\n - sqrt(x + a): 19.5539\n - Yeo-Johnson: 5.6698\nEstimation method: Out-of-sample via CV with 10 folds and 5 repeats\n \nBased off these, bestNormalize chose:\norderNorm Transformation with 6430 nonmissing obs and ties\n - 515 unique values \n - Original quantiles:\n    0%    25%    50%    75%   100% \n 48.00  66.00  95.00 166.75 720.00 \n\n# Perform boxcox transformation\ndata_ex$Unit_LOS_boxcox &lt;- boxcox(data_ex$Unit_LOS)$x.t\n\n# Create histogram and qqplot\ndistr_plots(data_ex, \"Unit_LOS_boxcox\", 25)\n\n\n\n\n\n\n\n\n\n# Create boxplot to assess for outliers\nggplot(data_ex, aes(y = Unit_LOS_boxcox)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\nThere are no outliers for boxcox Unit LOS.\nThat histogram looks better at least, and we don’t have any outliers. The bestNormalize package offers ways to back transform after you run your analysis, but I haven’t gotten that far in learning it yet. It looks like a boxcox transformation is a candidate however.\n\nSummary\nWill come back to this. I think there is a specific link function we use in this situation when we have an s-shaped qq plot like that.\nTop of Tabset\n\n\n\n\n# Create histogram and qqplot\ndistr_plots(data_ex, \"`HOSPITAL LOS`\", 25)\n\n\n\n\n\n\n\n\nHospital LOS also looks logarithmic.\nLet’s transform and assess.\n\n# Perform log transform \ndata_ex$HOSPITAL_LOS_log &lt;- log(data_ex$`HOSPITAL LOS`)\n\n# Create histogram and qqplot\ndistr_plots(data_ex, \"HOSPITAL_LOS_log\", 25)\n\n\n\n\n\n\n\nggplot(data_ex, aes(y = HOSPITAL_LOS_log)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\nLooks good! There are a handful of outliers in there, but the data looks normal and those values will likely be included.\n\nSummary\nHOSPITAL LOS is normal after a log transform. We will either perform that or a Poisson or Negative Binomial Top of Tabset\n\n\n\nWe must compute NON_ICU_LOS by subtracting Unit_LOS from HOSPITAL LOS.\n\n# Compute variable for non icu LOS.\ndata_ex &lt;- data_ex |&gt; \n  mutate(NON_ICU_LOS = `HOSPITAL LOS` - Unit_LOS)\n\n# Create histogram and qqplot\ndistr_plots(data_ex, \"NON_ICU_LOS\", 25)\n\n\n\n\n\n\n\n\nAs expected, that looks exponential.\n\n# Perform log transform\ndata_ex$NON_ICU_LOS_log &lt;- log(data_ex$NON_ICU_LOS)\n\n# Create histogram and qqplot\ndistr_plots(data_ex, \"NON_ICU_LOS_log\", 25)\n\nWarning: Removed 106 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\nWarning: Removed 106 rows containing non-finite outside the scale range\n(`stat_qq()`).\n\n\nWarning: Removed 106 rows containing non-finite outside the scale range\n(`stat_qq_line()`).\n\n\n\n\n\n\n\n\n\nThat’s bimodal. Odd.\nLet’s try a square root transform\n\n# Perform log transform\ndata_ex$NON_ICU_LOS_sqrt &lt;- sqrt(data_ex$NON_ICU_LOS)\n\n# Create histogram and qqplot\ndistr_plots(data_ex, \"NON_ICU_LOS_sqrt\", 25)\n\n\n\n\n\n\n\n\nLooks crummy.\nLet’s try bestNormalize\n\nBNobject &lt;- bestNormalize(data_ex$NON_ICU_LOS)\nBNobject\n\nBest Normalizing transformation with 6430 Observations\n Estimated Normality Statistics (Pearson P / df, lower =&gt; more normal):\n - arcsinh(x): 7.0628\n - Center+scale: 49.4811\n - Double Reversed Log_b(x+a): 59.8982\n - Log_b(x+a): 8.6584\n - orderNorm (ORQ): 1.542\n - sqrt(x + a): 10.833\n - Yeo-Johnson: 5.398\nEstimation method: Out-of-sample via CV with 10 folds and 5 repeats\n \nBased off these, bestNormalize chose:\norderNorm Transformation with 6430 nonmissing obs and ties\n - 1045 unique values \n - Original quantiles:\n    0%    25%    50%    75%   100% \n   0.0   32.0  110.5  284.0 9206.0 \n\n# Perform boxcox transformation\ndata_ex$NON_ICU_LOS_yeo &lt;- yeojohnson(data_ex$NON_ICU_LOS)$x.t\n\n# Create histogram and qqplot\ndistr_plots(data_ex, \"NON_ICU_LOS_yeo\", 25)\n\n\n\n\n\n\n\n\nLooks better! Still bimodal but much more normal.\n\nSummary\nLooks like Yeo-Johnson transformation may be the way to go.\nTop of Tabset\n\n\n\n\n# Examine discharge locations\npretty_print(table(data_ex$DISCH_DISP))\n\n\n\n\nVar1\nFreq\n\n\n\n\nAcute IP to RPCU\n5\n\n\nAdministrative Discharge\n1\n\n\nAdmitted as an Inpatient\n3\n\n\nAgainst Clinical Advice\n22\n\n\nAnother Health Care Institution Not Defined w/Planned Readmission\n1\n\n\nCourt/Law Enforcement\n22\n\n\nDischarge Lounge\n6\n\n\nDischarged to Other Facility\n23\n\n\nDischarged/transferred to a Designated Cancer Center or Children’s Hospital\n41\n\n\nDrug/Alch Detox D/C\n4\n\n\nExpired\n1137\n\n\nExpired in Medical Facility\n8\n\n\nExpired Readmit as Organ Donor\n7\n\n\nFederal Hospital\n43\n\n\nHome-Health Care Svc\n759\n\n\nHome or Self Care\n2417\n\n\nHome or Self Care w/Planned Readmission\n1\n\n\nHospice/Home\n213\n\n\nHospice/Medical Facility\n211\n\n\nIntermediate Care Facility\n14\n\n\nIV Therapy Provider\n1\n\n\nLeft Against Medical Advice\n49\n\n\nLong Term Care\n451\n\n\nMental Health Facility\n2\n\n\nNursing Facility\n37\n\n\nPsychiatric Hospital\n34\n\n\nRehab Facility\n249\n\n\nShort Term Hospital\n71\n\n\nSkilled Nursing Facility\n596\n\n\nSwing Bed\n1\n\n\nTRANSFER TO CANCER OR CHILDRENS\n1\n\n\n\n\n\n\n\nThere are a few options we have for to analyze discharge location.\n\nWe can spit on expired vs not expired\nWe can split on hom vs other\nWe can just look at all the groups with the largest N, and collapse the rest into a single variable of ‘other’. This would be\n\nExpired (1137), Home (3177), Hospice (424), long term care (451), rehab (249), skilled Nursing facility (596)\n\n\nWe will have to ask the researcher how she wants to group up these discharge locations! Keeping in mind that since we are looking at three different time periods, each group will essentially by a third of what is shown here."
  },
  {
    "objectID": "Project_3_Predictive_Modelling/Project_3_SAS/Code/images/Megan_Watson_Project-01.html#average-micu-los",
    "href": "Project_3_Predictive_Modelling/Project_3_SAS/Code/images/Megan_Watson_Project-01.html#average-micu-los",
    "title": "Data Checking - Statistical Consulting",
    "section": "Average MICU LOS",
    "text": "Average MICU LOS\n\n# Summarize and plot the data\ndata_ex |&gt; \n  group_by(Admit_MonthYear) |&gt; \n  summarise(avg_Unit_LOS = mean(Unit_LOS, na.rm = T)) |&gt; \n  ggplot(aes(x = Admit_MonthYear, y = avg_Unit_LOS)) +\n  geom_line() +\n  theme_minimal() +\n  geom_rect(data = highlight_periods, aes(xmin = start, xmax = end, ymin = -Inf, ymax = Inf), \n            inherit.aes = FALSE, fill = \"lightblue\", alpha = 0.2) +\n  labs(title = \"Average MICU LOS over Time\",\n       x = \"Admit Date\",\n       y = \"Avg MICU LOS\")\n\n\n\n\n\n\n\n\nIt does not appear that MICU_LOS has decreased across any of the time periods.\nThis is including patients that did not receive PT, so of course there’s no difference.\n\nMICU LOS over Time by Mechanical Ventilation\n\n# Dummy code MV status \ndata_ex$MV_YN &lt;- ifelse(is.na(data_ex$MV_Total), 0, 1)\n\n# Make it a factor for better plotting\ndata_ex &lt;- data_ex |&gt; \n  mutate(MV_YN = as.factor(MV_YN))\n\n# Summarize and plot the data\ndata_ex |&gt; \n  group_by(Admit_MonthYear, MV_YN) |&gt; \n  summarise(avg_Unit_LOS = mean(Unit_LOS, na.rm = T)) |&gt; \n  ggplot(aes(x = Admit_MonthYear, y = avg_Unit_LOS, color = MV_YN, group = MV_YN)) +\n  geom_line() +\n  theme_minimal() +\n  geom_rect(data = highlight_periods, aes(xmin = start, xmax = end, ymin = -Inf, ymax = Inf), \n            inherit.aes = FALSE, fill = \"lightblue\", alpha = 0.2) +\n  labs(title = \"Average MICU LOS over Time by Mechanical Ventilation\",\n       x = \"Admit Date\",\n       y = \"Avg MICU LOS\")\n\n`summarise()` has grouped output by 'Admit_MonthYear'. You can override using\nthe `.groups` argument.\n\n\n\n\n\n\n\n\n\nThis shows that those who were mechanically ventilated had a higher MICU length of stay. This does not appear to have differed in relation to the QI iniative.\nI am also including those that did not receive PT here, so that would explain the null finding.\n\ndata_ex &lt;- data_ex |&gt; \n  mutate(PT_YESNO = as.factor(PT_YESNO))\n\n# Summarize and plot the data\ndata_ex |&gt; \n  group_by(Admit_MonthYear, MV_YN, PT_YESNO) |&gt; \n  summarise(avg_Unit_LOS = mean(Unit_LOS, na.rm = T)) |&gt; \n  ggplot(aes(x = Admit_MonthYear, y = avg_Unit_LOS, color = PT_YESNO)) +\n  geom_point() +\n  geom_smooth(method = \"loess\", se = F) +\n  theme_minimal() +\n  geom_rect(data = highlight_periods, aes(xmin = start, xmax = end, ymin = -Inf, ymax = Inf), \n            inherit.aes = FALSE, fill = \"lightblue\", alpha = 0.2) +\n  labs(title = \"Average MICU LOS over Time by Mechanical Ventilation\",\n       x = \"Admit Date\",\n       y = \"Avg MICU LOS\") +\n  facet_wrap(~MV_YN)\n\n`summarise()` has grouped output by 'Admit_MonthYear', 'MV_YN'. You can\noverride using the `.groups` argument.\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nI think this might show an interaction. For those on mechanical ventilation, they seemed to decrease in MICU LOS more if they received PT.\nFor those not on MV, the PT did not seem to change their average LOS.\n\n\nAverage MICU LOS over Time by Physical Therapy\n\n# Make it a factor for better plotting\ndata_ex &lt;- data_ex |&gt; \n  mutate(PT_YESNO = as.factor(PT_YESNO))\n\n# Summarize and plot the data\ndata_ex |&gt; \n  group_by(Admit_MonthYear, PT_YESNO) |&gt; \n  summarise(avg_Unit_LOS = mean(Unit_LOS, na.rm = T)) |&gt; \n  filter(Admit_MonthYear &gt; as.Date(\"2012-01-01\")) |&gt; \n  ggplot(aes(x = Admit_MonthYear, y = avg_Unit_LOS, color = PT_YESNO, group = PT_YESNO)) +\n  geom_line() +\n  theme_minimal() +\n  geom_rect(data = highlight_periods, aes(xmin = start, xmax = end, ymin = -Inf, ymax = Inf), \n            inherit.aes = FALSE, fill = \"lightblue\", alpha = 0.2) +\n  labs(title = \"Average MICU LOS over Time by Physical Therapy\",\n       x = \"Admit Date\",\n       y = \"Avg MICU LOS\")\n\n`summarise()` has grouped output by 'Admit_MonthYear'. You can override using\nthe `.groups` argument.\n\n\n\n\n\n\n\n\n\n\n# Summarize and plot the data\ndata_ex |&gt; \n  group_by(Admit_MonthYear, PT_YESNO) |&gt; \n  summarise(avg_Unit_LOS = mean(Unit_LOS, na.rm = T)) |&gt; \n  filter(Admit_MonthYear &gt; as.Date(\"2012-01-01\")) |&gt; \n  ggplot(aes(x = Admit_MonthYear, y = avg_Unit_LOS, color = PT_YESNO, group = PT_YESNO)) +\n  geom_point() +\n  geom_smooth(method = \"loess\", se = F) +\n  theme_minimal() +\n  geom_rect(data = highlight_periods, aes(xmin = start, xmax = end, ymin = -Inf, ymax = Inf), \n            inherit.aes = FALSE, fill = \"lightblue\", alpha = 0.2) +\n  labs(title = \"Average MICU LOS over Time by Physical Therapy\",\n       x = \"Admit Date\",\n       y = \"Avg MICU LOS\")\n\n`summarise()` has grouped output by 'Admit_MonthYear'. You can override using\nthe `.groups` argument.\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nInteresting! It does look like average MICU length of stay decreased slighly immediately after the QI initiative. It also looks like it may be increasing slightly from then on.\nThat does appear that those received PT had a decrease before and after the iniative\nBut overall it looks like the QI may have successfuly decreased MICU LOS.\n\n\nFitting with Splines\n\nlibrary(splines)\n# Summarize and plot the data\ndata_ex |&gt; \n  group_by(Admit_MonthYear, PT_YESNO) |&gt; \n  summarise(avg_Unit_LOS = mean(Unit_LOS, na.rm = T)) |&gt; \n  filter(Admit_MonthYear &gt; as.Date(\"2012-01-01\")) |&gt; \n  ggplot(aes(x = Admit_MonthYear, y = avg_Unit_LOS, color = PT_YESNO, group = PT_YESNO)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", formula = y ~ ns(x, df = 4), se = F) +\n  theme_minimal() +\n  geom_rect(data = highlight_periods, aes(xmin = start, xmax = end, ymin = -Inf, ymax = Inf), \n            inherit.aes = FALSE, fill = \"lightblue\", alpha = 0.2) +\n  labs(title = \"Average MICU LOS over Time by Physical Therapy\",\n       x = \"Admit Date\",\n       y = \"Avg MICU LOS\")\n\n`summarise()` has grouped output by 'Admit_MonthYear'. You can override using\nthe `.groups` argument.\n\n\n\n\n\n\n\n\n\nLooking just at those who received PT\n\n# Make it a factor for better plotting\ndata_ex &lt;- data_ex |&gt; \n  mutate(PT_YESNO = as.factor(PT_YESNO))\n\n# Summarize and plot the data\ndata_ex |&gt; \n  group_by(Admit_MonthYear, PT_YESNO) |&gt; \n  summarise(avg_Unit_LOS = mean(Unit_LOS, na.rm = T)) |&gt; \n  filter(Admit_MonthYear &gt; as.Date(\"2012-01-01\")) |&gt; \n  filter(PT_YESNO == 1) |&gt; \n  ggplot(aes(x = Admit_MonthYear, y = avg_Unit_LOS, group = PT_YESNO)) +\n  geom_point() +\n  geom_smooth(method = \"loess\", se = F) +\n  theme_minimal() +\n  geom_rect(data = highlight_periods, aes(xmin = start, xmax = end, ymin = -Inf, ymax = Inf), \n            inherit.aes = FALSE, fill = \"lightblue\", alpha = 0.2) +\n  labs(title = \"Average MICU LOS over Time by Physical Therapy\",\n       x = \"Admit Date\",\n       y = \"Avg MICU LOS\")\n\n`summarise()` has grouped output by 'Admit_MonthYear'. You can override using\nthe `.groups` argument.\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nFitting a loess curve, it does appear that for those that receive PT, they had a decrease in average MICU LOS during the intervention compared to before. It also appears that they are increasing in average MICU following the interention, but this may still be lower compared to the pre-intervention period.\n:::"
  },
  {
    "objectID": "Project_3_Predictive_Modelling/Project_3_SAS/Code/images/Megan_Watson_Project-01.html#increase-frequency-of-pt-visits",
    "href": "Project_3_Predictive_Modelling/Project_3_SAS/Code/images/Megan_Watson_Project-01.html#increase-frequency-of-pt-visits",
    "title": "Data Checking - Statistical Consulting",
    "section": "Increase Frequency of PT Visits",
    "text": "Increase Frequency of PT Visits\n\n# Summarize and plot the data\ndata_ex |&gt; \n  group_by(Admit_MonthYear) |&gt; \n  summarise(avg_pct_therapy = mean(`PERCENT OF DAYS WITH THERAPY VISITS`, na.rm = T)) |&gt; \n  ggplot(aes(x = Admit_MonthYear, y = avg_pct_therapy)) +\n  geom_line() +\n  theme_minimal() +\n  geom_rect(data = highlight_periods, aes(xmin = start, xmax = end, ymin = -Inf, ymax = Inf), \n            inherit.aes = FALSE, fill = \"lightblue\", alpha = 0.2) +\n  labs(title = \"Average Percentage of Days with Physical Therapy Over Time\",\n       x = \"Admit Date\",\n       y = \"Average Days with Physical Therapy (%)\")\n\n\n\n\n\n\n\n\nInteresting! They successfully and drastically increased the percentage of days that patients had physical therapy (roughly from 10-20% pre-iniative to 50-60% post iniative). This percentage did seem to drop off over time during the post- initiative period however, though it is still higher than it was pre-initiative (now roughly 30-40%).\n\nFit with Loess\n\n# Summarize and plot the data\ndata_ex |&gt; \n  group_by(Admit_MonthYear) |&gt; \n  summarise(avg_pct_therapy = mean(`PERCENT OF DAYS WITH THERAPY VISITS`, na.rm = T)) |&gt; \n  ggplot(aes(x = Admit_MonthYear, y = avg_pct_therapy)) +\n  geom_point() +\n  geom_smooth(method = \"loess\", se = F) +\n  theme_minimal() +\n  geom_rect(data = highlight_periods, aes(xmin = start, xmax = end, ymin = -Inf, ymax = Inf), \n            inherit.aes = FALSE, fill = \"lightblue\", alpha = 0.2) +\n  labs(title = \"Average Percentage of Days with Physical Therapy Over Time\",\n       x = \"Admit Date\",\n       y = \"Average Days with Physical Therapy (%)\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\nFit with Splines\n\n# Summarize and plot the data\ndata_ex |&gt; \n  group_by(Admit_MonthYear) |&gt; \n  summarise(avg_pct_therapy = mean(`PERCENT OF DAYS WITH THERAPY VISITS`, na.rm = T)) |&gt; \n  ggplot(aes(x = Admit_MonthYear, y = avg_pct_therapy)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", formula = y ~ ns(x, df = 5), se = F) +\n  theme_minimal() +\n  geom_rect(data = highlight_periods, aes(xmin = start, xmax = end, ymin = -Inf, ymax = Inf), \n            inherit.aes = FALSE, fill = \"lightblue\", alpha = 0.2) +\n  labs(title = \"Average Percentage of Days with Physical Therapy Over Time\",\n       x = \"Admit Date\",\n       y = \"Average Days with Physical Therapy (%)\")"
  },
  {
    "objectID": "Project_3_Predictive_Modelling/Project_3_SAS/Code/images/Megan_Watson_Project-01.html#decrease-time-from-admit-to-therapy",
    "href": "Project_3_Predictive_Modelling/Project_3_SAS/Code/images/Megan_Watson_Project-01.html#decrease-time-from-admit-to-therapy",
    "title": "Data Checking - Statistical Consulting",
    "section": "Decrease Time from Admit to Therapy",
    "text": "Decrease Time from Admit to Therapy\n\n# Summarize and plot the data\ndata_ex |&gt; \n  group_by(Admit_MonthYear) |&gt; \n  summarise(avg_ADMIT_TO_TREAT = mean(`ADMIT TO TREAT`, na.rm = T)) |&gt; \n  ggplot(aes(x = Admit_MonthYear, y = avg_ADMIT_TO_TREAT)) +\n  geom_line() +\n  theme_minimal() +\n  geom_rect(data = highlight_periods, aes(xmin = start, xmax = end, ymin = -Inf, ymax = Inf), \n            inherit.aes = FALSE, fill = \"lightblue\", alpha = 0.2) +\n  annotate(\"text\", x = as.Date(\"2015-09-01\"), y = Inf, \n           label = \"QI-Initiative\", color = \"skyblue2\", size = 4, vjust = 1.5,) +\n  labs(title = \"Time from Admit to Therapy over Time\",\n       x = \"Admit Date\",\n       y = \"Time from Admit to Therapy\")\n\n\n\n\n\n\n\n\nThey successfully decreased the time from a patient’s first admit to their first physical therapy session.\nThis means that patients were receiving PT sooner.\n\nPlot with Loess\n\n# Summarize and plot the data\ndata_ex |&gt; \n  group_by(Admit_MonthYear) |&gt; \n  summarise(avg_ADMIT_TO_TREAT = mean(`ADMIT TO TREAT`, na.rm = T)) |&gt; \n  ggplot(aes(x = Admit_MonthYear, y = avg_ADMIT_TO_TREAT)) +\n  geom_point() +\n  geom_smooth(method = \"loess\", se = F) +\n  theme_minimal() +\n  geom_rect(data = highlight_periods, aes(xmin = start, xmax = end, ymin = -Inf, ymax = Inf), \n            inherit.aes = FALSE, fill = \"lightblue\", alpha = 0.2) +\n  annotate(\"text\", x = as.Date(\"2015-09-01\"), y = Inf, \n           label = \"QI-Initiative\", color = \"skyblue2\", size = 4, vjust = 1.5,) +\n  labs(title = \"Time from Admit to Therapy over Time\",\n       x = \"Admit Date\",\n       y = \"Time from Admit to Therapy\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nYeah looks crummier with loess, will likely delete."
  },
  {
    "objectID": "Project_3_Predictive_Modelling/Project_3_SAS/Code/images/Megan_Watson_Project-01.html#increase-percentage-of-admissions-receiving-physical-therapy",
    "href": "Project_3_Predictive_Modelling/Project_3_SAS/Code/images/Megan_Watson_Project-01.html#increase-percentage-of-admissions-receiving-physical-therapy",
    "title": "Data Checking - Statistical Consulting",
    "section": "Increase Percentage of Admissions Receiving Physical Therapy",
    "text": "Increase Percentage of Admissions Receiving Physical Therapy\n\n# Summarize and calculate percentages\nsummary_data &lt;- data_ex |&gt; \n  group_by(Admit_MonthYear, PT_YESNO) |&gt; \n  summarise(count = n()) |&gt; \n  mutate(total_count = sum(count)) |&gt; \n  ungroup() |&gt; \n  mutate(percentage = (count / total_count) * 100) |&gt; \n  filter(PT_YESNO == 1)\n\n`summarise()` has grouped output by 'Admit_MonthYear'. You can override using\nthe `.groups` argument.\n\n# Plot the data\nggplot(summary_data, aes(x = Admit_MonthYear, y = percentage)) +\n  geom_line() +\n  theme_minimal() +\n  geom_rect(data = highlight_periods, aes(xmin = start, xmax = end, ymin = -Inf, ymax = Inf), \n            inherit.aes = FALSE, fill = \"lightblue\", alpha = 0.2) +\n  annotate(\"text\", x = as.Date(\"2015-09-01\"), y = Inf, \n           label = \"QI-Initiative\", color = \"skyblue2\", size = 4, vjust = 1.5) +\n  labs(title = \"Percentage of Admissions Receiving Physical Therapy\",\n       x = \"Admit Date\",\n       y = \"Percentage\")\n\n\n\n\n\n\n\n\nThey also successfully increased the percentage of patients receiving PT from ~40% pre-initiative to ~80% during the initiative. This percentage then decreases to ~70% post-initiative.\n::::"
  },
  {
    "objectID": "Project_3_Predictive_Modelling/Project_3_SAS/Code/images/Megan_Watson_Project-01.html#mechanical-ventilation-1",
    "href": "Project_3_Predictive_Modelling/Project_3_SAS/Code/images/Megan_Watson_Project-01.html#mechanical-ventilation-1",
    "title": "Data Checking - Statistical Consulting",
    "section": "Mechanical Ventilation",
    "text": "Mechanical Ventilation\nThe researcher expressed that patients would be stratified on Mechanical ventilation v. NO mechanical ventilation. However, I do not see this variable. Let’s go ahead and create it, assuming that patients with NA for MV_TOTAL were simply never on mechanical ventilation.\n\n# Dummy code MV status \ndata_ex$MV_YN &lt;- ifelse(is.na(data_ex$MV_Total), 0, 1)\n\nLet’s check that worked as intended.\n\n# Filter down to variables of interest to verify dummy coding worked\ncheck &lt;- data_ex %&gt;%\n  select(id, MV_Total, MV_YN)\n\n# Pretty print\nkable(head(check), format = \"html\") %&gt;%\n  kable_styling(bootstrap_options = c(\"condensed\", \"stripe\", \"hover\"))\n\n\n\n\nid\nMV_Total\nMV_YN\n\n\n\n\n2\nNA\n0\n\n\n3\n13\n1\n\n\n4\nNA\n0\n\n\n5\n16\n1\n\n\n8\nNA\n0\n\n\n10\n2\n1\n\n\n\n\n\n\n\nLooks good."
  },
  {
    "objectID": "Project_3_Predictive_Modelling/Project_3_SAS/Code/images/Megan_Watson_Project-01.html#qi-initiative-time-period",
    "href": "Project_3_Predictive_Modelling/Project_3_SAS/Code/images/Megan_Watson_Project-01.html#qi-initiative-time-period",
    "title": "Data Checking - Statistical Consulting",
    "section": "QI-Initiative Time Period",
    "text": "QI-Initiative Time Period\nLet’s group participants into the time period that they were present during the initiative for.\n\n# Create new variable based on QI initiative time period.\ndata_ex &lt;- data_ex |&gt; \n  mutate(initiative = ifelse(Admit_MonthYear &lt; as.Date(\"2015-04-01\"), \"Pre\",\n                             ifelse(Admit_MonthYear &gt; as.Date(\"2015-12-01\"), \"Post\", \"During\")))\n\nLet’s double check we did that right.\n\n# Check counts\ntable(data_ex$initiative)\n\n\nDuring   Post    Pre \n   656   2860   2889 \n\n# Double check assigning\ndata_ex |&gt; \n  select(id, Admit_MonthYear, initiative) |&gt; \n  filter(Admit_MonthYear &lt; as.Date(\"2015-04-01\")) |&gt; \n  arrange(desc(Admit_MonthYear)) |&gt; \n  head() |&gt; \n  pretty_print()\n\n\n\n\nid\nAdmit_MonthYear\ninitiative\n\n\n\n\n5759\n2015-03-01\nPre\n\n\n5760\n2015-03-01\nPre\n\n\n5761\n2015-03-01\nPre\n\n\n5763\n2015-03-01\nPre\n\n\n5765\n2015-03-01\nPre\n\n\n5767\n2015-03-01\nPre\n\n\n\n\n\n\n# Double check assigning\ndata_ex |&gt; \n  select(id, Admit_MonthYear, initiative) |&gt; \n  filter(Admit_MonthYear &gt; as.Date(\"2015-04-01\")) |&gt; \n  arrange(Admit_MonthYear) |&gt; \n  head() |&gt; \n  pretty_print()\n\n\n\n\nid\nAdmit_MonthYear\ninitiative\n\n\n\n\n6094\n2015-05-01\nDuring\n\n\n6096\n2015-05-01\nDuring\n\n\n6099\n2015-05-01\nDuring\n\n\n6100\n2015-05-01\nDuring\n\n\n6105\n2015-05-01\nDuring\n\n\n6106\n2015-05-01\nDuring\n\n\n\n\n\n\n# Double check assigning\ndata_ex |&gt; \n  select(id, Admit_MonthYear, initiative) |&gt; \n  filter(Admit_MonthYear &gt;= as.Date(\"2015-12-01\")) |&gt; \n  arrange(Admit_MonthYear) |&gt; \n  head() |&gt; \n  pretty_print()\n\n\n\n\nid\nAdmit_MonthYear\ninitiative\n\n\n\n\n7167\n2015-12-01\nDuring\n\n\n7169\n2015-12-01\nDuring\n\n\n7171\n2015-12-01\nDuring\n\n\n7172\n2015-12-01\nDuring\n\n\n7174\n2015-12-01\nDuring\n\n\n7176\n2015-12-01\nDuring\n\n\n\n\n\n\n# Double check assigning\ndata_ex |&gt; \n  select(id, Admit_MonthYear, initiative) |&gt; \n  filter(Admit_MonthYear &gt; as.Date(\"2015-12-01\")) |&gt; \n  arrange(Admit_MonthYear) |&gt; \n  head() |&gt; \n  pretty_print()\n\n\n\n\nid\nAdmit_MonthYear\ninitiative\n\n\n\n\n7324\n2016-01-01\nPost\n\n\n7331\n2016-01-01\nPost\n\n\n7336\n2016-01-01\nPost\n\n\n7339\n2016-01-01\nPost\n\n\n7343\n2016-01-01\nPost\n\n\n7345\n2016-01-01\nPost\n\n\n\n\n\n\n\nLooks good.\n\nSome Graphing\n\ndata_ex &lt;- data_ex |&gt; \n  mutate(initiative = as.factor(initiative)) |&gt; \n  mutate(initiative = relevel(initiative, ref = \"Pre\"))\n\nggplot(data_ex, aes(x = PT_YESNO, y = Unit_LOS_log, fill = initiative)) +\n  geom_boxplot() +\n  theme_minimal() +\n  scale_fill_brewer(palette = \"Pastel2\")\n\n\n\n\n\n\n\n\nThis shows that MICU LOS decreased in the during and post QI initiative periods compared to the pre-iniatitive period.\n\ndata_ex |&gt; \n  filter(PT_YESNO == 1) |&gt; \n  ggplot(aes(x = initiative, y = Unit_LOS_log, fill = initiative)) +\n  geom_boxplot() +\n  theme_minimal() +\n  scale_fill_brewer(palette = \"Pastel2\")\n\n\n\n\n\n\n\n\n\ndata_ex |&gt; \n  filter(PT_YESNO == 1) |&gt; \n  ggplot(aes(x = initiative, y = Unit_LOS_log, fill = initiative)) +\n  geom_boxplot() +\n  theme_minimal() +\n  scale_fill_brewer(palette = \"Pastel2\")\n\n\n\n\n\n\n\n\nThere is an interaction here between PT and unit LOS and initiative.\nIn other words, you HAVE to filter based on PT\n\n\nMV Total\n\nggplot(data_ex, aes(x = PT_YESNO, y = MV_Total_log, fill = initiative)) +\n  geom_boxplot() +\n  theme_minimal() +\n  scale_fill_brewer(palette = \"Pastel2\")\n\nWarning: Removed 3149 rows containing non-finite outside the scale range\n(`stat_boxplot()`)."
  },
  {
    "objectID": "Project_3_Predictive_Modelling/Project_3_SAS/Code/images/Megan_Watson_Project-01.html#check-for-repeat-admissions",
    "href": "Project_3_Predictive_Modelling/Project_3_SAS/Code/images/Megan_Watson_Project-01.html#check-for-repeat-admissions",
    "title": "Data Checking - Statistical Consulting",
    "section": "Check for Repeat Admissions",
    "text": "Check for Repeat Admissions\n\n# Check for repeate admissions\nlength(unique(data_ex$id))\n\n[1] 6405\n\ndim(data_ex)\n\n[1] 6405   40\n\n\nThe number of unique patient id’s is the same number as the number of rows in our data set.\nSo we do not have repeate admissions in this data set and do not need to account for repeated measures! (We meet the assumption of independent observations)\n:::"
  },
  {
    "objectID": "Project_3_Predictive_Modelling/Project_3_R/Code/Project_3_Predictive_Modelling.html",
    "href": "Project_3_Predictive_Modelling/Project_3_R/Code/Project_3_Predictive_Modelling.html",
    "title": "Predictive Modelling",
    "section": "",
    "text": "The aim of the current study is to investigate whether MRI image features extracted from baseline kidney images can enhance the prediction of disease progression in young Autosomal Dominant Polycystic Kidney Disease (ADPKD) patients. This study is important because recent literature has demonstrated the inclusion of MRI image features improves the prognostic accuracy of models in adults, but this has not yet been demonstrated in children. Improving prediction models could greatly assist in diagnosing and predicting kidney disease outcomes in children, which is more challenging than in adults and could lead to improved treatment.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe two clinical hypotheses for this study are that including MRI image features will improve model performance compared to using baseline kidney volume alone in models predicting\n\nPercentage change of total kidney volume growth\nClassification of a patient as having fast or slow progression of the disease."
  },
  {
    "objectID": "Project_3_Predictive_Modelling/Project_3_R/Code/Project_3_Predictive_Modelling.html#gabor-transform",
    "href": "Project_3_Predictive_Modelling/Project_3_R/Code/Project_3_Predictive_Modelling.html#gabor-transform",
    "title": "Predictive Modelling",
    "section": "Gabor Transform",
    "text": "Gabor Transform\n\nWhat is it\nThe Gabor Transform, named after Dennis Gabor, is a powerful technique for analyzing the texture of MRI images. By convolving the image with a Gabor filter, it becomes possible to discriminate between features based on intensity differences. This allows the target region and background to be differentiated if they possess distinct features, as they will exhibit different intensity levels. Moreover, the Gabor Transform has tunable parameters, such as the frequency of the sinusoidal wave, which can be adjusted to extract specific textures from the images. Higher frequencies are ideal for capturing fine textures, while lower frequencies are better suited for coarse textures.\n\n\nHow it Works\nThe Gabor Filter first applies a Gaussian Envelope to focus on a small region of the image. It then applies a sinusoidal wave that oscillates at a specific frequency and captures the variation in intensities at that frequency and orientation within that region.\n\n\nExample of Gabor Transform\n\n\n\nExample of applying a Gaussian, then Gabor, and Laplacian Filter (GGL) for differentiation between two different textures (+’s vs L’s) (2).\n\n\n\n\nImage Features Provided by the Gabor Transform\nIn general, Gabor functions can easily extract features of:\n\nSpatial Frequency (e.g. how often pixel intensity changes in a given area)\nDensity (e.g. concentration of features within a certain area)\nOrientation (e.g. Textures or edges at 0°, 45°, 90°, 135°)\nPhase (e.g. alignment/distance of features)\nEnergy (e.g. overall intensity)\n\nSources: 1, 2"
  },
  {
    "objectID": "Project_3_Predictive_Modelling/Project_3_R/Code/Project_3_Predictive_Modelling.html#gray-level-co-occurrence-matrix",
    "href": "Project_3_Predictive_Modelling/Project_3_R/Code/Project_3_Predictive_Modelling.html#gray-level-co-occurrence-matrix",
    "title": "Predictive Modelling",
    "section": "Gray Level Co-Occurrence Matrix",
    "text": "Gray Level Co-Occurrence Matrix\n\nWhat is it\nThe Gray Level Co-Occurrence Matrix (GLCM) is another powerful tool for extracting texture features from an MRI image. GLCM works under the assumption that the textural information in an image is contained in the “average” spatial relationship that the gray tones in the image have to each other. For example, when a small patch of the picture has little variation of features, the dominant property is tone; When a small patch of a picture has high variation, the dominant property is texture (3).\n\n\nHow it Works\nGLCM examines the spatial relationship between pairs of pixels in an image and calculates how often pairs of pixel values occur at a specified distance and orientation. It does this by computing a set of gray-tone spacial-dependence matrices for various angular relationships and distances between neighboring resolution cell pairs on the image (3).\n\n\nExample of GLCM\n\n\n\nExample of textural features extracted from two different land-use category images (3).\n\n\n\n\nImage Features Provided by GLCM\nIn general, GLCM provides information on the following features:\n\nHomogeneity\nLinear Structure\nContrast\nNumber and Nature of Boundaries\nComplexity of the Image"
  },
  {
    "objectID": "Project_3_Predictive_Modelling/Project_3_R/Code/Project_3_Predictive_Modelling.html#local-binary-pattern",
    "href": "Project_3_Predictive_Modelling/Project_3_R/Code/Project_3_Predictive_Modelling.html#local-binary-pattern",
    "title": "Predictive Modelling",
    "section": "Local Binary Pattern",
    "text": "Local Binary Pattern\n\nWhat is it\nThe Local Binary Pattern (LBP) is a third powerful method for extracting texture features from an image. An important and fundamental property of texture is how uniform the patterns are, and LBP captures this by detecting these uniform patterns in circular neighborhoods at any rotation and spatial resolution. LBP is rotation invariant, meaning it does not matter what rotation the image is at; it will always extract very similar features) (4).\n\n\nHow it Works\nLBP works by comparing the intensity of a central pixel with its neighboring pixels and encoding this relationship into a binary pattern. For each neighbor, if it’s intensity is greater or equal to the central pixel, it gets assigned a 1 (otherwise a 0). The binary values of all neighbors are then concatenated to form a binary number, and this number is converted into a decimal that represent the LBP for the central pixel (4).\n\n\nExample of LBP\n\n\n\nExample of the 36 unique comparisons that can be made between neighboring pixels (4).\n\n\n\n\nCode Information\n\n\n\n\n\n\n\nImage Features Provided by the LBP\nIn general, the LBP provides image features on:\n\nUniformity\nLocal Contrast\nTexture Description\nSpatial Patterns\nGray Level Distribution"
  },
  {
    "objectID": "Project_3_Predictive_Modelling/Project_3_R/Code/Project_3_Predictive_Modelling.html#study-design",
    "href": "Project_3_Predictive_Modelling/Project_3_R/Code/Project_3_Predictive_Modelling.html#study-design",
    "title": "Predictive Modelling",
    "section": "Study Design",
    "text": "Study Design\nThe investigators recruited 71 young patients with ADPKD and collected MRI data at baseline and after 3 years. Additionally, the height corrected kidney volume for each patient was collected at baseline and 3 years by a physician, and the percentage change calculated. Patients were classified as having slow or fast progression of the disease based on this percentage change. Image features were extracted from the baseline MRI images including 2 image features on kidney geometric information, 5 features based on Gabor transform, 2 features based on gray level co-occurrence matrix, 5 features based on image textures, and 5 features based on local binary pattern.\n\nStatistical Hypotheses\n\nA linear regression model predicting percentage change of total kidney volume growth including MRI image features and baseline kidney volume will have better performance than a model with baseline kidney volume alone, as determined by specificity, sensitivity, PPV,  NPV,  accuracy, and AUC.\nA logistic regression model predicting classification of disease progression as slow or fast including MRI image features and baseline kidney volume will have better performance than a model with baseline kidney volume alone, as determined by specificity, sensitivity, PPV,  NPV,  accuracy, and AUC."
  },
  {
    "objectID": "Project_3_Predictive_Modelling/Project_3_R/Code/Project_3_Predictive_Modelling.html#summary-2",
    "href": "Project_3_Predictive_Modelling/Project_3_R/Code/Project_3_Predictive_Modelling.html#summary-2",
    "title": "Predictive Modelling",
    "section": "Summary",
    "text": "Summary\n\nKidney Volume Change\nkidvol_change is approximately normally distributed following a log transform, with 2 potential outliers we will keep an eye on.\n\n\nSlow vs Fast Disease Progression\nWe have an even count between patients who had slow vs fast disease progression."
  },
  {
    "objectID": "Project_3_Predictive_Modelling/Project_3_R/Code/Project_3_Predictive_Modelling.html#glcm1",
    "href": "Project_3_Predictive_Modelling/Project_3_R/Code/Project_3_Predictive_Modelling.html#glcm1",
    "title": "Predictive Modelling",
    "section": "GLCM1",
    "text": "GLCM1\n\n# Make plots\ndistr_plots(data1_train, \"glcm1\", 10)\n\n\n\n\n\n\n\n\nglcm1 is approximately normally distributed.\nTop of Tabset"
  },
  {
    "objectID": "Project_3_Predictive_Modelling/Project_3_R/Code/Project_3_Predictive_Modelling.html#glcm2",
    "href": "Project_3_Predictive_Modelling/Project_3_R/Code/Project_3_Predictive_Modelling.html#glcm2",
    "title": "Predictive Modelling",
    "section": "GLCM2",
    "text": "GLCM2\n\n# Make plots\ndistr_plots(data1_train, \"glcm2\", 10)\n\n\n\n\n\n\n\n\nglcm2 may be logarithmic.\n\nLog Transform\n\n# Create log variable\ndata1_train &lt;- data1_train |&gt; \n  mutate(glcm2_log = log(glcm2))\n\n#| fig-height: 3.5\n#| fig-width: 8\n# Make plots\ndistr_plots(data1_train, \"glcm2_log\", 10)\n\n\n\n\n\n\n\n\n\n# Use bestNormalize R package to select the best transformation\nBNObject &lt;- bestNormalize(data1_train$glcm2)\nBNObject\n\nBest Normalizing transformation with 56 Observations\n Estimated Normality Statistics (Pearson P / df, lower =&gt; more normal):\n - arcsinh(x): 1.5713\n - Box-Cox: 1.4393\n - Center+scale: 2.778\n - Double Reversed Log_b(x+a): 3.388\n - Log_b(x+a): 1.5713\n - orderNorm (ORQ): 1.7147\n - sqrt(x + a): 1.6827\n - Yeo-Johnson: 1.4393\nEstimation method: Out-of-sample via CV with 10 folds and 5 repeats\n \nBased off these, bestNormalize chose:\nStandardized Box Cox Transformation with 56 nonmissing obs.:\n Estimated statistics:\n - lambda = 0.245653 \n - mean (before standardization) = 105.7341 \n - sd (before standardization) = 37.69292 \n\n\nIt appears that this is the best transformation we will get.\n\n\nSummary\nThe best transformation for glcm2 is a log transform.\nTop of Tabset"
  },
  {
    "objectID": "Project_3_Predictive_Modelling/Project_3_R/Code/Project_3_Predictive_Modelling.html#correlation-matrix-i",
    "href": "Project_3_Predictive_Modelling/Project_3_R/Code/Project_3_Predictive_Modelling.html#correlation-matrix-i",
    "title": "Predictive Modelling",
    "section": "Correlation Matrix I",
    "text": "Correlation Matrix I\nTo assess for the best engineering of features and select the most promising covariates for this predictive model, I will create the following correlation matrices.\n\nWith unaltered features & engineered features\nWith averages and interactions to aggregate features from the same class\nWith squared features\n\nThis process should uncover hidden relationships between features that are not immediately obvious (such as a squared feature being a significant predictor, but not in its original form).\nThis first matrix will contain the unaltered features in their original form, as well as the engineered features as determined by examination of distributions.\n\n# Control matrix for better output\nmatrix_order &lt;- c(\"kidvol_base\", \"kidvol_change\", \"kidvol_change_log\", \"progression\", \"geom1\", \"geom1_yeo\", \"geom2\", \"geom2_yeo\", \"gabor1\", \"gabor2\", \"gabor2_yeo\", \"gabor3\", \"gabor4\", \"gabor4_log\", \"gabor5\", \"glcm1\", \"glcm2\", \"glcm2_log\", \"txti1\", \"txti2\", \"txti3\", \"txti4\", \"txti4_log\", \"txti5\", \"txti5_log\", \"lbp1\", \"lbp2\", \"lbp3\", \"lbp4\", \"lbp4_log\", \"lbp5\", \"lbp5_log\")\n \n# Clean the output by making a trimmed dataset excluding extaneous variables\ndata_for_matrix &lt;- data1_train |&gt; \n  select(matrix_order)\n\nWarning: Using an external vector in selections was deprecated in tidyselect 1.1.0.\nℹ Please use `all_of()` or `any_of()` instead.\n  # Was:\n  data %&gt;% select(matrix_order)\n\n  # Now:\n  data %&gt;% select(all_of(matrix_order))\n\nSee &lt;https://tidyselect.r-lib.org/reference/faq-external-vector.html&gt;.\n\n# We factored our variables at the start. To make a correlation matrix we must reconvert those back to numeric\ndata_for_matrix &lt;- data.frame(lapply(data_for_matrix, function(x) if (is.factor(x)) as.numeric(x) else x))\n\n# Make a correlation matrix with all variables of the trimmed data set\ncorrelation_matrix &lt;- cor(data_for_matrix, use = \"complete.obs\")\n\n# Plot correlation matrix\ncorrplot(correlation_matrix, method = \"color\", addCoef.col = \"black\", number.cex = 0.4, insig = \"blank\")\n\n\n\n\n\n\n\n\nFrom this plot we can see a few relationships between our original and transformed features."
  },
  {
    "objectID": "Project_3_Predictive_Modelling/Project_3_R/Code/Project_3_Predictive_Modelling.html#Corr_One",
    "href": "Project_3_Predictive_Modelling/Project_3_R/Code/Project_3_Predictive_Modelling.html#Corr_One",
    "title": "Predictive Modelling",
    "section": "Main Observations",
    "text": "Main Observations\n\nGeometryGabor TransformGray Level Co-Occurence MatrixTxtiLocal Binary Pattern\n\n\ngeom1 (r = -0.17) and geom2_yeo (r = -0.15) have about equal correlation with kidvol_change\nTop of Tabset\n\n\nNone of the original or transformed gabor features are correlated to change in kidvol_change_log.\nInterestingly, gabor3 appears to have a strong correlation to the untransformed kidvol_change, but not with kidvol_change_log. Let’s assess.\n\n# Create plot\nggplot(data1_train, aes(x = gabor3, y = kidvol_change)) +\n  geom_smooth(method = \"lm\") +\n  geom_point() +\n  labs(title = \"Change in Kidney Volume by Gabor3\",\n       x = \"Gabor3\",\n       y = \"Change in Kidney Volume (%)\") +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nThis appears to be driven entirely by that outlier patient on the far left.\nWe previously determined that patients 37 and 51 were outliers on the boxplots on kidvol_change.\nLet’s assess how this relationship changes after removing them.\n\n# Remove outlier patients on kidvol change\ndata_out_rem &lt;- data1_train |&gt; \n  filter(!Subject_ID %in% c(37,51))\n\n# Plot\nggplot(data_out_rem, aes(x = gabor3, y = kidvol_change)) +\n  geom_smooth(method = \"lm\") +\n  geom_point() +\n  labs(title = \"Change in Kidney Volume by Gabor3\",\n       x = \"Gabor3\",\n       y = \"Change in Kidney Volume (%)\") +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nThe relationship now looks appropriately linear when these outlier patients are removed.\nWe can also see that taking the log of the change in kidney volume reduces the influence of these outliers and makes the correlation non significant\n\n# Create plot\nggplot(data1_train, aes(x = gabor3, y = kidvol_change_log)) +\n  geom_smooth(method = \"lm\") +\n  geom_point() +\n  labs(title = \"Change in Log Kidney Volume by Gabor3\",\n       x = \"Gabor3\",\n       y = \"Change in Log Kidney Volume (%)\") +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 1 row containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nNote: Patients 37 and 51 may be outliers and will need to be assessed using leverage and influence after the final model is fit!\n\n\nTop of Tabset\n\n\nNone of the MRI features from the gray level co-occurence matrix were strongly correlated with kidvol_change.\nTop of Tabset\n\n\ntxti2 is a strong predictor of change in kidvol regardless if whether kidvol is transformed or not.\n\n# Create plot\nggplot(data1_train, aes(x = txti2, y = kidvol_change)) +\n  geom_smooth(method = \"lm\") +\n  geom_point() +\n  labs(title = \"Change in Log Kidney Volume by Txti2\",\n       x = \"Txti2\",\n       y = \"Change in Log Kidney Volume (%)\") +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\ntxti2 is a strong predictor of change in kidney volume!\nTop of Tabset\n\n\n\nLbp2\nlbp2 is weakly correlated to change in kidney volume.\n\n# Create plot\nggplot(data1_train, aes(x = lbp2, y = kidvol_change)) +\n  geom_smooth(method = \"lm\") +\n  geom_point() +\n  labs(title = \"Change in Log Kidney Volume by lbp2\",\n       x = \"lbp2\",\n       y = \"Change in Log Kidney Volume (%)\") +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nmodel &lt;- lm(kidvol_change ~ lbp2, data = data1_train)\nsummary(model)\n\n\nCall:\nlm(formula = kidvol_change ~ lbp2, data = data1_train)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-11.584  -5.786  -1.331   2.369  28.519 \n\nCoefficients:\n            Estimate Std. Error t value         Pr(&gt;|t|)    \n(Intercept)    9.502      1.085   8.754 0.00000000000837 ***\nlbp2           2.343      1.945   1.205            0.234    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.926 on 52 degrees of freedom\nMultiple R-squared:  0.02716,   Adjusted R-squared:  0.008455 \nF-statistic: 1.452 on 1 and 52 DF,  p-value: 0.2337\n\n\n\n\nLbp4\nlbp4 after log transforming is weakly associated with change in kidney volume, but not strong enough to be significant.\n\n# Create plot\nggplot(data1_train, aes(x = lbp4_log, y = kidvol_change)) +\n  geom_smooth(method = \"lm\") +\n  geom_point() +\n  labs(title = \"Change in Log Kidney Volume by Lbp4\",\n       x = \"Log Lbp4\",\n       y = \"Change in Kidney Volume (%)\") +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\nLbp Log\nlbp5_log is strongly correlated to change in kidney volume.\n\n# Create plot\nggplot(data1_train, aes(x = lbp5_log, y = kidvol_change)) +\n  geom_smooth(method = \"lm\") +\n  geom_point() +\n  labs(title = \"Change in Log Kidney Volume by Log Lbp5\",\n       x = \"Log Lbp5\",\n       y = \"Change in Log Kidney Volume (%)\") +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nWe have a pretty strong negative relationship here, looks like it is slightly driven by those two outlier patients.\nTop of Tabset"
  },
  {
    "objectID": "Project_3_Predictive_Modelling/Project_3_R/Code/Project_3_Predictive_Modelling.html#summary-18",
    "href": "Project_3_Predictive_Modelling/Project_3_R/Code/Project_3_Predictive_Modelling.html#summary-18",
    "title": "Predictive Modelling",
    "section": "Summary",
    "text": "Summary\nThe features that are most promising as covariates at this point are:\n\ngeom2_yeo\ngabor3\ntxt12\nlbp5_log"
  },
  {
    "objectID": "Project_3_Predictive_Modelling/Project_3_R/Code/Project_3_Predictive_Modelling.html#averages-and-interaction-terms",
    "href": "Project_3_Predictive_Modelling/Project_3_R/Code/Project_3_Predictive_Modelling.html#averages-and-interaction-terms",
    "title": "Predictive Modelling",
    "section": "Averages and Interaction Terms",
    "text": "Averages and Interaction Terms\nWe saw in the correlation matrix that both geom1 and geom2 had pretty even correlations with change in kidney volume.\nIn the interest of discovering underlying patterns (and minimizing the number of features we have, since we have a limit of 5 based on our sample size), I will attempt to collapse these two variables by\n\nTaking their average\nTaking their interaction term\n\nI will also do the same for lbp2, and lbp5_log, which were correlated with the outcome.\n\nCreate Average and Interaction Terms\n\n# Create average and interaction terms\ndata1_train &lt;- data1_train |&gt; \n  mutate(geom_avg = (geom1_yeo + geom2_yeo)/2,\n         geom_int = geom1_yeo*geom2_yeo,\n         lbp_avg  = (lbp2 + lbp5_log),\n         lbp_int  = lbp2*lbp5_log)"
  },
  {
    "objectID": "Project_3_Predictive_Modelling/Project_3_R/Code/Project_3_Predictive_Modelling.html#correlation-matrix-ii",
    "href": "Project_3_Predictive_Modelling/Project_3_R/Code/Project_3_Predictive_Modelling.html#correlation-matrix-ii",
    "title": "Predictive Modelling",
    "section": "Correlation Matrix II",
    "text": "Correlation Matrix II\nNow we can rerun the correlation matrix and examine how those relationships changed\n\n# Control matrix for better output\nmatrix_order &lt;- c(\"kidvol_base\", \"kidvol_change\", \"kidvol_change_log\", \"progression\", \"geom1\", \"geom1_yeo\", \"geom2\", \"geom2_yeo\", \"geom_avg\", \"geom_int\", \"lbp1\", \"lbp2\", \"lbp3\", \"lbp4\", \"lbp4_log\", \"lbp5\", \"lbp5_log\", \"lbp_avg\", \"lbp_int\")\n \n# Clean the output by making a trimmed dataset excluding extaneous variables\ndata_for_matrix &lt;- data1_train |&gt; \n  select(matrix_order)\n\n# We factored our variables at the start. To make a correlation matrix we must reconvert those back to numeric\ndata_for_matrix &lt;- data.frame(lapply(data_for_matrix, function(x) if (is.factor(x)) as.numeric(x) else x))\n\n# Make a correlation matrix with all variables of the trimmed data set\ncorrelation_matrix &lt;- cor(data_for_matrix, use = \"complete.obs\")\n\n# Plot correlation matrix\ncorrplot(correlation_matrix, method = \"color\", addCoef.col = \"black\", number.cex = 0.6)"
  },
  {
    "objectID": "Project_3_Predictive_Modelling/Project_3_R/Code/Project_3_Predictive_Modelling.html#Corr_Two",
    "href": "Project_3_Predictive_Modelling/Project_3_R/Code/Project_3_Predictive_Modelling.html#Corr_Two",
    "title": "Predictive Modelling",
    "section": "Main Observations",
    "text": "Main Observations\n\nGeometryLocal Binary Pattern\n\n\ngeom_avg (r = 0.20) has a higher correlation coefficient than geom1_yeo and geom2_yeo alone, and will thus be used as an aggregate covariate to capture the geometry of the MRIs going forward.\n\n# Create plot\nggplot(data1_train, aes(x = geom_avg, y = kidvol_change)) +\n  geom_smooth(method = \"lm\") +\n  geom_point() +\n  labs(title = \"Change in Kidney Volume by geom_avg\",\n       x = \"geom_avg\",\n       y = \"Change in Kidney Volume (%)\") +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nNotably, this could be driven by those two outliers as mentioned previously.\nTop of Tabset\n\n\nThe average and interaction terms do not perform better than lbp5_log alone.\nTop of Tabset\n\n\n\n\nSummary\ngeom_avg increases the r by 0.03 over the individual non-combined features, and will be chosen as a candidate covariate.\nlbp_avg does not perform better than lbp5_log alone, and thus lbp5_log will be chosen as a candidate covariate in the final model."
  },
  {
    "objectID": "Project_3_Predictive_Modelling/Project_3_R/Code/Project_3_Predictive_Modelling.html#quadratic-terms",
    "href": "Project_3_Predictive_Modelling/Project_3_R/Code/Project_3_Predictive_Modelling.html#quadratic-terms",
    "title": "Predictive Modelling",
    "section": "Quadratic Terms",
    "text": "Quadratic Terms\nFinally, we will consider quadratic terms for each feature.\n\n# Create function to square specified columns and rename the new variables\nsquare_selected_variables &lt;- function(df, columns) {\n  df &lt;- df %&gt;%\n    mutate(across(all_of(columns), ~ .^2, .names = \"{.col}_square\"))\n  return(df)\n}\n\n# Choose columns to get squared\ncolumns &lt;- data1_train |&gt;\n  select(geom1, geom1_yeo, geom2, geom2_yeo, geom_avg, geom_int, gabor1, gabor2, gabor2_yeo, gabor3, gabor4, gabor4_log, gabor5, glcm1, glcm2, glcm2_log, txti1, txti2, txti3, txti4, txti4_log, txti5, txti5_log, lbp1, lbp2, lbp3, lbp4, lbp4_log, lbp5, lbp5_log, lbp_avg, lbp_int) |&gt;\n  colnames()\n\n\n# Square selected variables\ndata_square &lt;- square_selected_variables(data1_train, columns)"
  },
  {
    "objectID": "Project_3_Predictive_Modelling/Project_3_R/Code/Project_3_Predictive_Modelling.html#correlation-matrix-iii",
    "href": "Project_3_Predictive_Modelling/Project_3_R/Code/Project_3_Predictive_Modelling.html#correlation-matrix-iii",
    "title": "Predictive Modelling",
    "section": "Correlation Matrix III",
    "text": "Correlation Matrix III\n\n# Control matrix for better output\nmatrix_order &lt;- c(\"kidvol_base\", \"kidvol_change\", \"kidvol_change_log\", \"progression\", \"geom1_square\", \"geom1_yeo_square\", \"geom2_square\", \"geom2_yeo_square\", \"geom_avg_square\", \"geom_int_square\", \"gabor1_square\", \"gabor2_square\", \"gabor2_yeo_square\", \"gabor3_square\", \"gabor4_square\", \"gabor4_log_square\", \"gabor5_square\", \"glcm1_square\", \"glcm2_square\", \"glcm2_log_square\", \"txti1_square\", \"txti2_square\", \"txti3_square\", \"txti4_square\", \"txti4_log_square\", \"txti5_square\", \"txti5_log_square\", \"lbp1_square\", \"lbp2_square\", \"lbp3_square\", \"lbp4_square\", \"lbp4_log_square\", \"lbp5_square\", \"lbp5_log_square\", \"lbp_avg_square\", \"lbp_int_square\")\n \n# Clean the output by making a trimmed dataset excluding extraneous variables\ndata_for_matrix2 &lt;- data_square |&gt; \n  select(matrix_order)\n\n# We factored our variables at the start. To make a correlation matrix we must reconvert those back to numeric\ndata_for_matrix2 &lt;- data.frame(lapply(data_for_matrix2, function(x) if (is.factor(x)) as.numeric(x) else x))\n\n# Make a correlation matrix with all variables of the trimmed data set\ncorrelation_matrix &lt;- cor(data_for_matrix2, use = \"complete.obs\")\n\n# Plot correlation matrix\ncorrplot(correlation_matrix, method = \"color\", addCoef.col = \"black\", number.cex = 0.2)"
  },
  {
    "objectID": "Project_3_Predictive_Modelling/Project_3_R/Code/Project_3_Predictive_Modelling.html#Corr_Three",
    "href": "Project_3_Predictive_Modelling/Project_3_R/Code/Project_3_Predictive_Modelling.html#Corr_Three",
    "title": "Predictive Modelling",
    "section": "Main Observations",
    "text": "Main Observations\n\nGabor TransformGray Level Co-Occurence Matrix\n\n\n\nGabor3\nThe correlation coefficient for gabor3_square is higher than gabor3.\n\nggplot(data_square, aes(x = gabor3_square, y = kidvol_change)) +\n  geom_point() +\n  geom_smooth(method = \"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nLet’s compare if we remove those outliers\n\nggplot(data_out_rem, aes(x = gabor3**2, y = kidvol_change)) +\n  geom_point() +\n  geom_smooth(method = \"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nThis appears to be driven by that outlier patient.\nTop of Tabset\n\n\n\nglcm2_square has a strong correlation (r = 0.31). Let’s examine.\n\n# Create the plot\nggplot(data_square, aes(x = glcm2_square, y = kidvol_change)) +\n  geom_point() +\n  geom_smooth(method = \"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n# Check the model\nmodel &lt;- lm(kidvol_change ~ glcm2**2, data = data1_train)\nsummary(model)\n\n\nCall:\nlm(formula = kidvol_change ~ glcm2^2, data = data1_train)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-11.657  -5.766  -1.591   3.782  29.290 \n\nCoefficients:\n                Estimate   Std. Error t value    Pr(&gt;|t|)    \n(Intercept) 7.8715892217 1.4117081716   5.576 0.000000892 ***\nglcm2       0.0000013538 0.0000008435   1.605       0.115    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.844 on 52 degrees of freedom\nMultiple R-squared:  0.0472,    Adjusted R-squared:  0.02888 \nF-statistic: 2.576 on 1 and 52 DF,  p-value: 0.1146\n\n\nThis could be driven by the outlier, but we will include glcm2_square as a covariate during model selection.\nTop of Tabset\n\n\n\n\nSummary\nGLCM2_square will be considered as a potential covariate during model selection."
  },
  {
    "objectID": "Project_3_Predictive_Modelling/Project_3_R/Code/Project_3_Predictive_Modelling.html#perform-scaling",
    "href": "Project_3_Predictive_Modelling/Project_3_R/Code/Project_3_Predictive_Modelling.html#perform-scaling",
    "title": "Predictive Modelling",
    "section": "Perform Scaling",
    "text": "Perform Scaling\nWe will perform z norm scaling using caret, which will appropriately scale the test set of each fold using the mean and standard deviation of each respective training set."
  },
  {
    "objectID": "Project_3_Predictive_Modelling/Project_3_R/Code/Project_3_Predictive_Modelling.html#cost-function",
    "href": "Project_3_Predictive_Modelling/Project_3_R/Code/Project_3_Predictive_Modelling.html#cost-function",
    "title": "Predictive Modelling",
    "section": "Cost Function",
    "text": "Cost Function\nThe cost function in a linear regression is Root Mean Square Error (RMSE):\n\\[ \\text{RMSE} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2} \\]\nWhere y is the actual change in outcome variable and y-hat is the predicted change, and n is the number of observations.\n\nMapping Function\nIn the case of linear regression, the mapping function is essentially each beta in the model.\n\n\nGoal\nThe goal is to select a mapping function that minimizes the cost function and thereby produces the best predictions for the outcome variable.\nWe will be using the caret package to perform model training using 5-fold cross validation and Ordinary Least Squares (OLS). Another option is to perform gradient descent."
  },
  {
    "objectID": "Project_3_Predictive_Modelling/Project_3_R/Code/Project_3_Predictive_Modelling.html#1A",
    "href": "Project_3_Predictive_Modelling/Project_3_R/Code/Project_3_Predictive_Modelling.html#1A",
    "title": "Predictive Modelling",
    "section": "Model 1A: Baseline Kidney Volume",
    "text": "Model 1A: Baseline Kidney Volume\n\nAnalysisVisualizationSummary\n\n\nTo answer the researcher’s first question for Task 1, we will perform a predictive model that uses the baseline height-corrected total kidney volume to predict percent change in kidney volume.\n\\[ Kidney Volume Change = 𝛽_0 + 𝛽_1*Log Kidney Volume Baseline + e \\]\n\nset.seed(123)\n\n# Set up a 5-fold cross validation\ntc &lt;- trainControl(method = \"cv\", number = 5)\n\n# Perform the linear regression using 5-fold CV\nmodel1a &lt;- train(kidvol_change ~ log(kidvol_base), \n                 data = data, \n                 method = \"lm\",\n                 trControl = tc)\n# Get RMSE\nmodel1a\n\nLinear Regression \n\n71 samples\n 1 predictor\n\nNo pre-processing\nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 56, 56, 58, 58, 56 \nResampling results:\n\n  RMSE      Rsquared    MAE     \n  8.096326  0.05257111  6.192124\n\nTuning parameter 'intercept' was held constant at a value of TRUE\n\n# Get model coefficients\nsummary(model1a)\n\n\nCall:\nlm(formula = .outcome ~ ., data = dat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-16.166  -5.741  -1.240   3.758  27.000 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept)          26.582     11.698   2.272   0.0262 *\n`log(kidvol_base)`   -2.987      2.028  -1.473   0.1453  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.176 on 69 degrees of freedom\nMultiple R-squared:  0.03049,   Adjusted R-squared:  0.01644 \nF-statistic:  2.17 on 1 and 69 DF,  p-value: 0.1453\n\n# Examine RSME and R Squared of each fold\npretty_print(model1a$resample)\n\n\n\n\nRMSE\nRsquared\nMAE\nResample\n\n\n\n\n9.164071\n0.1296555\n6.966941\nFold1\n\n\n8.938726\n0.1208880\n6.144274\nFold2\n\n\n9.002214\n0.0078560\n7.279776\nFold3\n\n\n6.777802\n0.0009846\n5.651400\nFold4\n\n\n6.598816\n0.0034716\n4.918228\nFold5\n\n\n\n\n\n\n\nThe model has an RMSE of 8.10.\nTop of Tabset\n\n\n\nActual vs Predicted Values\n\n# Save predicted values to data set\ndata$predictions1a &lt;- predict(model1a, newdata = data)\n\n# Visualize predicted vs actual values\nggplot(data, aes(x = kidvol_change, y = predictions1a)) +\n  geom_point() +\n  geom_abline(intercept = 0, slope = 1, color = \"blue\", linetype = \"dashed\") +\n  theme_minimal() +\n  labs(title = \"Predicted vs Actual Values for Baseline Kidney Volume\",\n       x = \"Actual Values\",\n       y = \"Predicted Values\")\n\n\n\n\n\n\n\n\nIn a well performing model, the dots are close to a straight line (the blue dashed line), which would indicate perfect overlap between predicted and actual values.\n\n\nResiduals\n\n# Calculate the residuals\ndata$residuals1a &lt;- data$kidvol_change - data$predictions1a\n\n# Plot\nggplot(data, aes(x = predictions1a, y = residuals1a)) +\n  geom_point() +\n  geom_hline(yintercept = 0, color = \"blue\", linetype = \"dashed\") +\n  theme_minimal() +\n  labs(title = \"Residuals for Baseline Kidney Volume\",\n       x = \"Predicted Values\",\n       y = \"Residuals\")\n\n\n\n\n\n\n\n\nThere’s some pretty large differences in the predictions here, with some patients having predicted values that are 15% off or greater!\n\n\nQQ Plot\n\n# Plot QQ Plot\nggplot(data, aes(sample = residuals1a)) +\n  geom_qq() +\n  geom_qq_line() +\n  theme_minimal() +\n  labs(title = \"QQ Plot of Model 1A\")\n\n\n\n\n\n\n\n\nThe QQ plot is almost normal, but not quite.\n\n\nHistogram of Residuals\n\n# Create histogram of residuals\nggplot(data, aes(x = residuals1a)) + \n  geom_histogram(bins = 20) +\n  labs(title = \"Histogram of Residuals for Model 1A\", \n       x = \"Residuals\", \n       y = \"Count\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThe residuals are almost normally distributed, but not quite.\nTop of Tabset\n\n\n\nthe RMSE of the model including kidvol_base alone is 8.11\nThe model does not perform too well, when looking at the plots of the predicted vs actual values.\nThe assumption of normality is almost, but not quite, met.\nTop of Tabset"
  },
  {
    "objectID": "Project_3_Predictive_Modelling/Project_3_R/Code/Project_3_Predictive_Modelling.html#1B",
    "href": "Project_3_Predictive_Modelling/Project_3_R/Code/Project_3_Predictive_Modelling.html#1B",
    "title": "Predictive Modelling",
    "section": "Model 1B: MRI Features",
    "text": "Model 1B: MRI Features\nTo answer the researcher’s second question for Task 1, we will run a predictive model that uses only MRI image features to predict percent change in kidney volume at year 3.\n\\[ Kidney Volume Change = 𝛽_0 + 𝛽_1*Txti2 + e \\]\n\nModel Selection\nModel selection will be performed on Fold 1 to avoid data snooping on the test set.\nThe potential scaled covariates for our final model after feature engineering and interactive variable selection are:\n\ngeom_avg\ngabor3\ntxti2\nlbp5_log\nglcm2_square\n\nWe will perform model selection using backwards elimination and BIC.\n\nBackwards Elimination IAnalysis IVisualization ILeverage and InfluenceBackwards Elimination IIAnalysis IIVisualization IISummary\n\n\n\n# Create variables\ndata1_train &lt;- data1_train |&gt; \n  mutate(geom_avg_scale = scale((geom1_yeo+geom2_yeo)/2),\n         gabor3_scale = scale(gabor3),\n         txti2_scale = scale(txti2),\n         lbp5_log_scale = scale(log(lbp5)),\n         glcm2_square_scale = scale(glcm2**2))\n\n# First build the model with all variables\nmodel1b &lt;- lm(kidvol_change ~ geom_avg_scale + gabor3_scale + txti2_scale + lbp5_log_scale + glcm2_square_scale, data = data1_train)\nsummary(model1b)\n\n\nCall:\nlm(formula = kidvol_change ~ geom_avg_scale + gabor3_scale + \n    txti2_scale + lbp5_log_scale + glcm2_square_scale, data = data1_train)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-17.458  -4.477  -1.072   2.927  18.913 \n\nCoefficients:\n                   Estimate Std. Error t value         Pr(&gt;|t|)    \n(Intercept)          9.3544     0.9681   9.662 0.00000000000077 ***\ngeom_avg_scale      -0.5476     1.0569  -0.518           0.6067    \ngabor3_scale        -1.8407     1.0054  -1.831           0.0733 .  \ntxti2_scale         -2.0236     1.0275  -1.970           0.0547 .  \nlbp5_log_scale      -1.0410     1.0918  -0.953           0.3451    \nglcm2_square_scale   1.9041     1.0320   1.845           0.0712 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.114 on 48 degrees of freedom\nMultiple R-squared:  0.2765,    Adjusted R-squared:  0.2011 \nF-statistic: 3.668 on 5 and 48 DF,  p-value: 0.006844\n\n# Perform backward elimination \nols_step_backward_sbc(model1b)\n\n\n                               Stepwise Summary                                \n-----------------------------------------------------------------------------\nStep    Variable            AIC        SBC       SBIC        R2       Adj. R2 \n-----------------------------------------------------------------------------\n 0      Full Model        372.792    386.715    221.016    0.27647    0.20110 \n 1      geom_avg_scale    371.093    383.027    218.848    0.27242    0.21303 \n 2      lbp5_log_scale    370.716    380.661    218.098    0.25023    0.20524 \n-----------------------------------------------------------------------------\n\nFinal Model Output \n------------------\n\n                         Model Summary                           \n----------------------------------------------------------------\nR                        0.500       RMSE                 6.828 \nR-Squared                0.250       MSE                 46.621 \nAdj. R-Squared           0.205       Coef. Var           75.855 \nPred R-Squared          -0.235       AIC                370.716 \nMAE                      5.016       SBC                380.661 \n----------------------------------------------------------------\n RMSE: Root Mean Square Error \n MSE: Mean Square Error \n MAE: Mean Absolute Error \n AIC: Akaike Information Criteria \n SBC: Schwarz Bayesian Criteria \n\n                               ANOVA                                \n-------------------------------------------------------------------\n                Sum of                                             \n               Squares        DF    Mean Square      F        Sig. \n-------------------------------------------------------------------\nRegression     840.194         3        280.065    5.562    0.0023 \nResidual      2517.532        50         50.351                    \nTotal         3357.727        53                                   \n-------------------------------------------------------------------\n\n                                      Parameter Estimates                                       \n-----------------------------------------------------------------------------------------------\n             model      Beta    Std. Error    Std. Beta      t        Sig      lower     upper \n-----------------------------------------------------------------------------------------------\n       (Intercept)     9.354         0.966                  9.688    0.000     7.415    11.294 \n      gabor3_scale    -2.114         0.981       -0.266    -2.155    0.036    -4.084    -0.143 \n       txti2_scale    -2.200         1.016       -0.276    -2.165    0.035    -4.240    -0.159 \nglcm2_square_scale     2.076         1.018        0.261     2.040    0.047     0.032     4.121 \n-----------------------------------------------------------------------------------------------\n\n\ngeom_avg and lbp5_log are removed based on BIC. AIC prefers the model with lbp5_log\nHowever, we must note that we suspect patients 37 and 51 to be outliers on total kidney volume change, and we saw previously that the correlations for gabor3 and glcm2_square may be being driven by these outlier patients.\nTop of Tabset\n\n\n\nPerform 5-Fold Cross Validation\nThe final model selected via backwards elimination includes gabor3, txti2, and glcm2_square\n\nset.seed(123)\n# Create variables for original data set\ndata &lt;- data |&gt; \n  mutate(gabor3_scale = scale(gabor3),\n         txti2_scale = scale(txti2),\n         glcm2_square = glcm2**2,\n         glcm2_square_scale = scale(glcm2**2),\n         lbp5_log_scale = scale(log(lbp5)),\n         geom_avg_scale = scale(geom1+geom2)/2)\n\n# Perform the linear regression using 5-fold CV\nmodel1b &lt;- train(kidvol_change ~ gabor3 + txti2 + glcm2_square, \n                 data = data, \n                 method = \"lm\",\n                 trControl = tc,\n                 preProc = \"scale\")\n# Get RMSE\nmodel1b\n\nLinear Regression \n\n71 samples\n 3 predictor\n\nPre-processing: scaled (3) \nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 56, 56, 58, 58, 56 \nResampling results:\n\n  RMSE      Rsquared   MAE     \n  9.032055  0.1690167  6.528863\n\nTuning parameter 'intercept' was held constant at a value of TRUE\n\n# Get model coefficients\nsummary(model1b)\n\n\nCall:\nlm(formula = .outcome ~ ., data = dat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-16.630  -4.574  -1.414   3.490  28.925 \n\nCoefficients:\n             Estimate Std. Error t value          Pr(&gt;|t|)    \n(Intercept)    9.0641     1.0337   8.768 0.000000000000996 ***\ngabor3         0.1500     0.9800   0.153            0.8788    \ntxti2         -2.2213     0.9844  -2.256            0.0273 *  \nglcm2_square   0.8494     0.9690   0.877            0.3838    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.049 on 67 degrees of freedom\nMultiple R-squared:  0.08767,   Adjusted R-squared:  0.04681 \nF-statistic: 2.146 on 3 and 67 DF,  p-value: 0.1026\n\n\nThe RMSE is 9.03, worse than the model with kidvol_base alone.\nTop of Tabset\n\n\n\n\nActual vs Predicted Values\n\n# Save predicted valuest to data set\ndata$predictions1b &lt;- predict(model1b, newdata = data)\n\n# Visualize predicted vs actual values\nggplot(data, aes(x = kidvol_change, y = predictions1b)) +\n  geom_point() +\n  geom_abline(intercept = 0, slope = 1, color = \"blue\", linetype = \"dashed\") +\n  theme_minimal() +\n  labs(title = \"Predicted vs Actual Values for MRI Features\",\n       x = \"Actual Values\",\n       y = \"Predicted Values\")\n\n\n\n\n\n\n\n\n\n\nResiduals\n\n# Calculate the residuals\ndata$residuals1b &lt;- data$kidvol_change - data$predictions1b\n\n# Plot\nggplot(data, aes(x = predictions1a, y = residuals1b)) +\n  geom_point() +\n  geom_hline(yintercept = 0, color = \"blue\", linetype = \"dashed\") +\n  theme_minimal() +\n  labs(title = \"Residuals for MRI Features\",\n       x = \"Predicted Values\",\n       y = \"Residuals\")\n\n\n\n\n\n\n\n\nOur residuals are comparable in this model compared to modela 1A using kidvol_base alone. Some It appears that more residuals are closer to 0, but some predictions are as large as 30% off!\n\n\nQQ Plot\n\n# Plot QQ Plot\nggplot(data, aes(sample = residuals1b)) +\n  geom_qq() +\n  geom_qq_line() +\n  theme_minimal() +\n  labs(title = \"QQ Plot of Model 1B\")\n\n\n\n\n\n\n\n\n\n\nHistogram of Residuals\n\n# Create histogram of residuals\nggplot(data, aes(x = residuals1b)) + \n  geom_histogram(bins = 20) +\n  labs(title = \"Histogram of Residuals for Model 1B\", \n       x = \"Residuals\", \n       y = \"Count\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nTop of Tabset\n\n\n\nThe cutoff for leverage (hat-value) is 2(p+1)/n, where p is the number of variables in the model, or 2(3+1)/71 = 0.112.\nThe cut off for Cook’s D &gt; 1.0.\n\n# Extract model\nmodel1b_ext &lt;- model1b$finalModel\n\n# Influence plot\ninfluence_measures &lt;- influencePlot(model1b_ext, main = \"Influence Plot\",\n              sub = \"Circle size is proportional to Cook's Distance\")\n\n\n\n\n\n\n\n# Get row numbers of outliers\ninfluence_measures\n\n      StudRes       Hat     CookD\nX2   3.205536 0.3120826 1.0236809\nX8   4.481013 0.1736179 0.8208826\nX9   3.131995 0.3212338 1.0257341\nX35 -1.208289 0.4497401 0.2962813\n\n\nWe have four clear outliers that are points of high leverage and influence.\nLet’s identify them\n\n# Identify outlier patients\noutliers &lt;- data[c(2, 8, 9, 35),]\n\n# Pretty print\npretty_print(head(outliers))\n\n\n\n\nSubject_ID\ngeom1\ngeom2\ngabor1\ngabor2\ngabor3\ngabor4\ngabor5\nglcm1\nglcm2\ntxti1\ntxti2\ntxti3\ntxti4\ntxti5\nlbp1\nlbp2\nlbp3\nlbp4\nlbp5\nkidvol_base\nkidvol_visit2\nprogression\nkidvol_change\nfold_number\npredictions1a\nresiduals1a\ngabor3_scale\ntxti2_scale\nglcm2_square\nglcm2_square_scale\nlbp5_log_scale\ngeom_avg_scale\npredictions1b\nresiduals1b\n\n\n\n\n51\n-43.393038\n1882.95573\n0.0047377\n0.0544573\n0.0002580\n0.0000224\n0.0029656\n2454.8065\n6026075.1\n-14.67151\n-9.355575\n137.26039\n215.2532\n87.526780\n0.1577583\n0.1676594\n0.0264497\n0.0248877\n0.0281097\n185\n387\nFast\n36.28\nFold2\n10.989456\n25.290543\n0.01648347\n-1.3536710\n36313581159049\n4.4785466\n-0.4385074\n-0.06753985\n16.224288\n20.055712\n\n\n37\n-3.860771\n14.90555\n0.2621932\n-0.1983384\n-0.0520030\n0.0687453\n0.0393381\n871.6238\n759728.0\n-14.72259\n-1.969424\n28.99502\n216.7546\n3.878631\n-0.0557547\n0.0722141\n-0.0040263\n0.0031086\n0.0052149\n173\n372\nFast\n38.19\nFold3\n11.189771\n27.000229\n-3.32238849\n-0.2849587\n577186627450\n-0.3302081\n-1.1025159\n-0.22901595\n9.264656\n28.925344\n\n\n52\n77.054904\n5937.45816\n0.4598489\n0.1556082\n0.0715563\n0.2114611\n0.0242139\n-638.4451\n407612.2\n20.26785\n9.686888\n196.33234\n410.7855\n93.835797\n9.7525713\n1.1625327\n11.3376835\n95.1126464\n1.3514824\n358\n640\nFast\n26.18\nFold4\n9.017566\n17.162434\n4.57161897\n1.4016092\n166147673143\n-0.3855183\n1.0880277\n0.30114927\n6.655437\n19.524563\n\n\n48\n-124.266792\n15442.23561\n-0.1630575\n0.0017660\n-0.0002880\n0.0265878\n0.0000031\n2545.1960\n6478022.6\n6.41584\n7.649503\n49.07799\n41.1630\n58.514902\n-0.1337633\n3.0589420\n-0.4091741\n0.0178926\n9.3571262\n290\n327\nSlow\n4.21\nFold5\n9.646765\n-5.436765\n-0.01839687\n1.1068172\n41964776184221\n5.2389818\n1.8507097\n1.12273578\n11.399413\n-7.189413\n\n\n\n\n\n\n\nAs predicted, patients 37 and 52 are points of high leverage and influence.\nAlso as identified before, patients 48 and 52 are points of high leverage and influence, which can be denoted by their absurdly high values for lbp1 and lbp2.\nSince we previously identified that certain correlations such as with gabor3 and glcm2_square were being driven by these outlier patients, we will re-run model selection again using backwards elimination, with these patients removed.\nTop of Tabset\n\n\n\nRemove Outliers and Re-Run Variable Selection\nWe will perform backwards elimination again with outlier patients removed.\nWe can expect gabor3 and glcm2_square to be removed since their correlations were driven by these outliers, as identified in earlier plots.\n\n# Remove outlier patients from data set\ndata1_train &lt;- data1_train |&gt; \n  filter(!Subject_ID %in% c(37, 48, 51, 52))\n\n# Create model\nmodel1b2 &lt;- lm(kidvol_change ~ geom_avg_scale + gabor3_scale + txti2_scale + lbp5_log_scale + glcm2_square_scale, data = data1_train)\n\n# Examine Model Summary\nsummary(model1b2)\n\n\nCall:\nlm(formula = kidvol_change ~ geom_avg_scale + gabor3_scale + \n    txti2_scale + lbp5_log_scale + glcm2_square_scale, data = data1_train)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-9.3899 -3.2977 -0.9786  2.4046 15.9736 \n\nCoefficients:\n                   Estimate Std. Error t value           Pr(&gt;|t|)    \n(Intercept)          8.1922     0.7809  10.491 0.0000000000000867 ***\ngeom_avg_scale      -0.1844     0.8252  -0.223            0.82416    \ngabor3_scale         0.9018     0.9673   0.932            0.35609    \ntxti2_scale         -2.1787     0.8004  -2.722            0.00913 ** \nlbp5_log_scale      -0.9964     0.8565  -1.163            0.25073    \nglcm2_square_scale  -1.2907     1.1787  -1.095            0.27918    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.538 on 46 degrees of freedom\nMultiple R-squared:  0.1898,    Adjusted R-squared:  0.1017 \nF-statistic: 2.155 on 5 and 46 DF,  p-value: 0.07558\n\n# Perform backward elimination \nols_step_forward_sbc(model1b2)\n\n\n                              Stepwise Summary                              \n--------------------------------------------------------------------------\nStep    Variable         AIC        SBC       SBIC        R2       Adj. R2 \n--------------------------------------------------------------------------\n 0      Base Model     334.154    338.056    186.402    0.00000    0.00000 \n 1      txti2_scale    328.649    334.503    181.305    0.13439    0.11708 \n--------------------------------------------------------------------------\n\nFinal Model Output \n------------------\n\n                         Model Summary                          \n---------------------------------------------------------------\nR                       0.367       RMSE                 5.384 \nR-Squared               0.134       MSE                 28.989 \nAdj. R-Squared          0.117       Coef. Var           66.297 \nPred R-Squared          0.067       AIC                328.649 \nMAE                     4.093       SBC                334.503 \n---------------------------------------------------------------\n RMSE: Root Mean Square Error \n MSE: Mean Square Error \n MAE: Mean Absolute Error \n AIC: Akaike Information Criteria \n SBC: Schwarz Bayesian Criteria \n\n                               ANOVA                                \n-------------------------------------------------------------------\n                Sum of                                             \n               Squares        DF    Mean Square      F        Sig. \n-------------------------------------------------------------------\nRegression     234.037         1        234.037    7.763    0.0075 \nResidual      1507.420        50         30.148                    \nTotal         1741.457        51                                   \n-------------------------------------------------------------------\n\n                                  Parameter Estimates                                    \n----------------------------------------------------------------------------------------\n      model      Beta    Std. Error    Std. Beta      t        Sig      lower     upper \n----------------------------------------------------------------------------------------\n(Intercept)     8.344         0.762                 10.954    0.000     6.814     9.874 \ntxti2_scale    -2.136         0.767       -0.367    -2.786    0.008    -3.677    -0.596 \n----------------------------------------------------------------------------------------\n\n\nAs predicted, when excluding these outlier patients, gabor3 and glcm2_square are no longer significant.\nThe only selected now using BIC is txti2!\nLet’s perform 5-fold CV again using this model.\nTop of Tabset\n\n\n\n\nRemove Outliers\n\n# Remove outlier patients from data set\ndata &lt;- data |&gt; \n  filter(!Subject_ID %in% c(37, 48, 51, 52))\n\n\n\nPerform Regression with 5-Fold Cross Validation\n\nset.seed(123)\n\n# Set up a 5-fold cross validation\ntc &lt;- trainControl(method = \"cv\", number = 5)\n\n# Perform the linear regression using 5-fold CV\nmodel1b2 &lt;- train(kidvol_change ~ txti2, \n                 data = data, \n                 method = \"lm\",\n                 trControl = tc,\n                 preProc = \"scale\")\n\n# Get RMSE\nmodel1b2\n\nLinear Regression \n\n67 samples\n 1 predictor\n\nPre-processing: scaled (1) \nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 54, 55, 53, 55, 51 \nResampling results:\n\n  RMSE      Rsquared  MAE    \n  6.153724  0.171239  4.68434\n\nTuning parameter 'intercept' was held constant at a value of TRUE\n\n# Get model coefficients\nsummary(model1b2)\n\n\nCall:\nlm(formula = .outcome ~ ., data = dat)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.6726  -3.5604  -0.5467   3.0722  18.8813 \n\nCoefficients:\n            Estimate Std. Error t value             Pr(&gt;|t|)    \n(Intercept)   8.3805     0.7661  10.939 0.000000000000000222 ***\ntxti2        -2.0443     0.7718  -2.649               0.0101 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.27 on 65 degrees of freedom\nMultiple R-squared:  0.09742,   Adjusted R-squared:  0.08353 \nF-statistic: 7.016 on 1 and 65 DF,  p-value: 0.01013\n\n\nThe RMSE is now 6.15, drastically lower than with kidvol_base_log alone (but we need to rerun that model without outliers).\nThis model predicts 17.0% of the variance in kidvol_change.\nTop of Tabset\n\n\n\n\nActual vs Predicted Values\n\n# Save predicted valuest to data set\ndata$predictions1b2 &lt;- predict(model1b2, newdata = data)\n\n# Visualize predicted vs actual values\nggplot(data, aes(x = kidvol_change, y = predictions1b2)) +\n  geom_point() +\n  geom_abline(intercept = 0, slope = 1, color = \"blue\", linetype = \"dashed\") +\n  theme_minimal() +\n  labs(title = \"Predicted vs Actual Values for TXTI2\",\n       x = \"Actual Values\",\n       y = \"Predicted Values\")\n\n\n\n\n\n\n\n\nThe predicted models are still not that close to the actual values.\n\n\nResiduals\n\n# Calculate the residuals\ndata$residuals1b2 &lt;- data$kidvol_change - data$predictions1b2\n\n# Plot residuals\nggplot(data, aes(x = predictions1b2, y = residuals1b2)) +\n  geom_point() +\n  geom_hline(yintercept = 0, color = \"blue\", linetype = \"dashed\") +\n  theme_minimal() +\n  labs(title = \"Residuals for TXTI2\",\n       x = \"Predicted Values\",\n       y = \"Residuals\")\n\n\n\n\n\n\n\n\nThe residuals still look pretty large.\n\n\nQQ Plot\n\n# Plot QQ Plot\nggplot(data, aes(sample = residuals1b2)) +\n  geom_qq() +\n  geom_qq_line() +\n  theme_minimal() +\n  labs(title = \"QQ Plot of TXTI2\")\n\n\n\n\n\n\n\n\n\n\nHistogram of Residuals\n\n# Create histogram of residuals\nggplot(data, aes(x = residuals1b2)) + \n  geom_histogram(bins = 20) +\n  labs(title = \"Histogram of Residuals for TXTI2\", \n       x = \"Residuals\", \n       y = \"Count\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThe residuals are now approximately normally distributed, and would be more so with a larger sample size.\n\n\nLeverage and Influence\n\n# Extract model\nmodel1b2_ext &lt;- model1b2$finalModel\n\n# Influence plot\ninfluence_measures &lt;- influencePlot(model1b2_ext, main = \"Influence Plot\",\n              sub = \"Circle size is proportional to Cook's Distance\")\n\n\n\n\n\n\n\n# Get row numbers of outliers\ninfluence_measures\n\n       StudRes        Hat       CookD\nX5   2.7790166 0.01496267 0.053157442\nX27 -0.5734689 0.10123968 0.018715616\nX35 -1.7314354 0.05744408 0.088628443\nX53  0.2718257 0.13375256 0.005786868\nX60  3.2804634 0.03089234 0.149126512\n\n\nWe no longer have outliers in this model as assessed by cutoffs using leverage and influence.\nTop of Tabset\n\n\n\nIn this step we discovered that patiens 37, 48, 51, and 52 were outliers with high leverage and influence in our model.\nAfter removing them, only txti2 was selected as an MRI image feature predicting change in kidney volume.\nThe RMSE for model 1B after removing outliers was 6.15"
  },
  {
    "objectID": "Project_3_Predictive_Modelling/Project_3_R/Code/Project_3_Predictive_Modelling.html#1C",
    "href": "Project_3_Predictive_Modelling/Project_3_R/Code/Project_3_Predictive_Modelling.html#1C",
    "title": "Predictive Modelling",
    "section": "Model 1C: Baseline Kidney Volume and TXTI2",
    "text": "Model 1C: Baseline Kidney Volume and TXTI2\n\nAnalysisVisualizationSummary\n\n\nTo answer the researcher’s third question, we will run a predictive model that uses the baseline height-corrected total kidney volume AND MRI image features to predict percent change in kidney volume.\n\\[ Kidney Volume Change = 𝛽_0 + 𝛽_{1}*Baseline Kidney Volume + 𝛽_{2}*Txti2 +  e \\]\n\nset.seed(123)\n\n# Perform the linear regression using 5-fold CV\nmodel1c &lt;- train(kidvol_change ~ log(kidvol_base) + txti2, \n                 data = data, \n                 method = \"lm\",\n                 trControl = tc,\n                 preProc = \"scale\",\n                 metric = \"RMSE\")\n# Get RMSE\nmodel1c\n\nLinear Regression \n\n67 samples\n 2 predictor\n\nPre-processing: scaled (2) \nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 54, 55, 53, 55, 51 \nResampling results:\n\n  RMSE      Rsquared   MAE     \n  6.140875  0.2383243  4.643702\n\nTuning parameter 'intercept' was held constant at a value of TRUE\n\n# Get model coefficients\nsummary(model1c)\n\n\nCall:\nlm(formula = .outcome ~ ., data = dat)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-16.3316  -3.5730  -0.3356   3.2914  17.0199 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)         21.2149     9.3480   2.269  0.02662 * \n`log(kidvol_base)`  -1.0820     0.7855  -1.378  0.17315   \ntxti2               -2.2803     0.7855  -2.903  0.00506 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.228 on 64 degrees of freedom\nMultiple R-squared:  0.1234,    Adjusted R-squared:  0.09601 \nF-statistic: 4.505 on 2 and 64 DF,  p-value: 0.01477\n\n\nThe RMSE for the combined model is 6.14, about the same as the model using just txti2 alone.\nTop of Tabset\n\n\n\nActual Vs Predicted\n\n# Save predicted valuest to data set\ndata$predictions1c &lt;- predict(model1c, newdata = data)\n\n# Visualize predicted vs actual values\nggplot(data, aes(x = kidvol_change, y = predictions1c)) +\n  geom_point() +\n  geom_abline(intercept = 0, slope = 1, color = \"blue\", linetype = \"dashed\") +\n  theme_minimal() +\n  labs(title = \"Predicted vs Actual Values for Baseline Kidney Volumne and TXTI2\",\n       x = \"Actual Values\",\n       y = \"Predicted Values\")\n\n\n\n\n\n\n\n\nThe model still does not have the best prediction.\n\n\nResiduals\n\n# Calculate the residuals\ndata$residuals1c &lt;- data$kidvol_change - data$predictions1c\n\n# Plot residuals\nggplot(data, aes(x = predictions1c, y = residuals1c)) +\n  geom_point() +\n  geom_hline(yintercept = 0, color = \"blue\", linetype = \"dashed\") +\n  theme_minimal() +\n  labs(title = \"Residuals for Baseline Kidney Volume and TXTI2\",\n       x = \"Predicted Values\",\n       y = \"Residuals\")\n\n\n\n\n\n\n\n\nThe residuals are slightly better than the model with kidvol_base alone.\n\n# Plot QQ Plot\nggplot(data, aes(sample = residuals1c)) +\n  geom_qq() +\n  geom_qq_line() +\n  theme_minimal() +\n  labs(title = \"QQ Plot of Baseline Kidney Volume and  TXTI2\")\n\n\n\n\n\n\n\n\nThis is the best QQ plot we have had so far.\n\n\nHistogram of Residuals\n\n# Create histogram of residuals\nggplot(data, aes(x = residuals1c)) + \n  geom_histogram(bins = 20) +\n  labs(title = \"Histogram of Residuals for Baseline Kidney Volume and TXTI2\", \n       x = \"Residuals\", \n       y = \"Count\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nLeverage and Influence\n\n# Extract model\nmodel1c_ext &lt;- model1c$finalModel\n\n# Influence plot\ninfluence_measures &lt;- influencePlot(model1c_ext, main = \"Influence Plot\",\n              sub = \"Circle size is proportional to Cook's Distance\")\n\n\n\n\n\n\n\n# Get row numbers of outliers\ninfluence_measures\n\n      StudRes        Hat       CookD\nX2  0.7595324 0.12618822 0.027954583\nX5  2.8250641 0.01514097 0.036876777\nX29 2.3152733 0.04010315 0.069889503\nX53 0.2032994 0.13606032 0.002202691\nX60 3.0216117 0.07797505 0.228366413\n\n\nThere are no outliers (some edge cases but we will retain them)\n\n\nPlot Final Model\n\n# Plot final model\nggplot(data, aes(x = log(kidvol_base), y = kidvol_change)) +\n  geom_point() +\n  geom_smooth(method = \"lm\") +\n  labs(title = \"Model 1C: Baseline Kidney Volume and Txti2\",\n       x = \"Baseline Kidney Volume (log)\",\n       y = \"Change in Kidney Volume (%)\") +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nThis plot shows that generally, the larger your kidney volume at baseline, the less of an increase there was in size over 3 years.\nTop of Tabset\n\n\n\nThe RMSE for model 1C is 6.14, this is similar performance to model 1B.\nModel 1C meets the assumptions of a linear regression (normality) the best.\nTop of Tabset"
  },
  {
    "objectID": "Project_3_Predictive_Modelling/Project_3_R/Code/Project_3_Predictive_Modelling.html#model-comparison",
    "href": "Project_3_Predictive_Modelling/Project_3_R/Code/Project_3_Predictive_Modelling.html#model-comparison",
    "title": "Predictive Modelling",
    "section": "Model Comparison",
    "text": "Model Comparison\nHere we will compare how models 1A, 1B, and 1C performed.\nNote: We removed outliers over the course of our model inspection. Thus we will re-run each analysis to get the true RMSE for each model.\n\nModel 1A\n\nset.seed(123)\n##### Model 1A\n# Perform the linear regression using 5-fold CV\nmodel1a &lt;- train(kidvol_change ~ log(kidvol_base), \n                 data = data, \n                 method = \"lm\",\n                 trControl = tc)\n# Get RMSE\nmodel1a\n\nLinear Regression \n\n67 samples\n 1 predictor\n\nNo pre-processing\nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 54, 55, 53, 55, 51 \nResampling results:\n\n  RMSE      Rsquared    MAE     \n  6.519605  0.04277987  5.194627\n\nTuning parameter 'intercept' was held constant at a value of TRUE\n\n\n\n\nModel 1B\n\nset.seed(123)\n##### Model 1B\n# Perform the linear regression using 5-fold CV\nmodel1b2 &lt;- train(kidvol_change ~ txti2, \n                 data = data, \n                 method = \"lm\",\n                 trControl = tc,\n                 preProc = \"scale\")\n# Get RMSE\nmodel1b2\n\nLinear Regression \n\n67 samples\n 1 predictor\n\nPre-processing: scaled (1) \nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 54, 55, 53, 55, 51 \nResampling results:\n\n  RMSE      Rsquared  MAE    \n  6.153724  0.171239  4.68434\n\nTuning parameter 'intercept' was held constant at a value of TRUE\n\n\n\n\nModel 1C\n\nset.seed(123)\n# Perform the linear regression using 5-fold CV\nmodel1c &lt;- train(kidvol_change ~ log(kidvol_base) + txti2_scale, \n                 data = data, \n                 method = \"lm\",\n                 trControl = tc)\n# Get RMSE\nmodel1c\n\nLinear Regression \n\n67 samples\n 2 predictor\n\nNo pre-processing\nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 54, 55, 53, 55, 51 \nResampling results:\n\n  RMSE      Rsquared   MAE     \n  6.140875  0.2383243  4.643702\n\nTuning parameter 'intercept' was held constant at a value of TRUE\n\n\n\n\nFinal Average RMSE\n\n\n\nModel\nRMSE\n\n\n\n\n1A: Baseline Kidney Volume\n6.52\n\n\n1B: Txti2\n6.15\n\n\n1C Baseline Kidney Volume and Txti2\n6.14\n\n\n\n\n\nCompare Model Performance\nWe can also use the caret package to compare perfomance between these three models.\n\nUpper Diagonal Values: These are the differences in the metric values between models. Positive values mean the first model has a higher metric value, while negative values mean the first model has a lower metric value.\nLower Diagonal Values: These are the p-values from hypothesis tests comparing the metric values between models. Small p-values (typically &lt; 0.05) indicate significant differences between the models for that metric.\n\n\n# Create list of models to compare\nmodel_list &lt;- list(`Kidney Volume at Baseline` = model1a, `Txti2` = model1b2, `Kidney Volume at Baseline and Txti2` = model1c)\n\n# Compare models\nresults &lt;- resamples(model_list)\nsummary(results)\n\n\nCall:\nsummary.resamples(object = results)\n\nModels: Kidney Volume at Baseline, Txti2, Kidney Volume at Baseline and Txti2 \nNumber of resamples: 5 \n\nMAE \n                                        Min.  1st Qu.   Median     Mean\nKidney Volume at Baseline           4.296977 5.268043 5.312707 5.194627\nTxti2                               3.442303 4.553295 4.891441 4.684340\nKidney Volume at Baseline and Txti2 3.282857 4.494957 4.952172 4.643702\n                                     3rd Qu.     Max. NA's\nKidney Volume at Baseline           5.438730 5.656678    0\nTxti2                               5.203527 5.331133    0\nKidney Volume at Baseline and Txti2 5.112327 5.376196    0\n\nRMSE \n                                        Min.  1st Qu.   Median     Mean\nKidney Volume at Baseline           5.426090 6.382835 6.398772 6.519605\nTxti2                               4.084323 6.119866 6.382093 6.153724\nKidney Volume at Baseline and Txti2 4.117591 5.917561 6.331809 6.140875\n                                     3rd Qu.     Max. NA's\nKidney Volume at Baseline           6.857043 7.533286    0\nTxti2                               6.536207 7.646131    0\nKidney Volume at Baseline and Txti2 6.550354 7.787060    0\n\nRsquared \n                                            Min.    1st Qu.     Median\nKidney Volume at Baseline           0.0003237652 0.02525581 0.03695319\nTxti2                               0.0177893816 0.04359540 0.10855879\nKidney Volume at Baseline and Txti2 0.0163224093 0.05880114 0.19970450\n                                          Mean    3rd Qu.      Max. NA's\nKidney Volume at Baseline           0.04277987 0.05047119 0.1008954    0\nTxti2                               0.17123905 0.26687362 0.4193780    0\nKidney Volume at Baseline and Txti2 0.23832433 0.43143200 0.4853616    0\n\n# Compare the RMSEs \ndiffs &lt;- diff(results) \nsummary(diffs)\n\n\nCall:\nsummary.diff.resamples(object = diffs)\n\np-value adjustment: bonferroni \nUpper diagonal: estimates of the difference\nLower diagonal: p-value for H0: difference = 0\n\nMAE \n                                    Kidney Volume at Baseline Txti2  \nKidney Volume at Baseline                                     0.51029\nTxti2                               0.05817                          \nKidney Volume at Baseline and Txti2 0.06738                   1.00000\n                                    Kidney Volume at Baseline and Txti2\nKidney Volume at Baseline           0.55092                            \nTxti2                               0.04064                            \nKidney Volume at Baseline and Txti2                                    \n\nRMSE \n                                    Kidney Volume at Baseline Txti2  \nKidney Volume at Baseline                                     0.36588\nTxti2                               0.6808                           \nKidney Volume at Baseline and Txti2 0.7453                    1.0000 \n                                    Kidney Volume at Baseline and Txti2\nKidney Volume at Baseline           0.37873                            \nTxti2                               0.01285                            \nKidney Volume at Baseline and Txti2                                    \n\nRsquared \n                                    Kidney Volume at Baseline Txti2   \nKidney Volume at Baseline                                     -0.12846\nTxti2                               0.3766                            \nKidney Volume at Baseline and Txti2 0.2353                    0.2573  \n                                    Kidney Volume at Baseline and Txti2\nKidney Volume at Baseline           -0.19554                           \nTxti2                               -0.06709                           \nKidney Volume at Baseline and Txti2                                    \n\n# Plot RMSEs of each model\ndotplot(results, metric = \"RMSE\", main = \"Comparison of RMSE Across Models\")\n\n\n\n\n\n\n\n\nModel A had an RMSE that was 0.37 points higher than model B (p = 0.68), and 0.74 points higher than model C (p = 0.75)"
  },
  {
    "objectID": "Project_3_Predictive_Modelling/Project_3_R/Code/Project_3_Predictive_Modelling.html#results",
    "href": "Project_3_Predictive_Modelling/Project_3_R/Code/Project_3_Predictive_Modelling.html#results",
    "title": "Predictive Modelling",
    "section": "Results",
    "text": "Results\nTo evaluate whether differences in model performance were statistically significant, RMSE for each model were compared using pairwise t-tests with Bonferroni p-value correction. The differences in model performance were not statistically significant. Model A had a 0.37 higher RMSE compared to model B (p = 0.68), and a 0.38 higher RMSE compared to model C (p = 0.75).\n\n\n\nModel\nRMSE\nP-Adjusted\n\n\n\n\n\n1A: Baseline Kidney Volume\n6.52\n–\n\n\n\n1B: Txti2\n6.15\n0.68\n—\n\n\n1C Baseline Kidney Volume and Txti2\n6.14\n0.75\n1.0000"
  },
  {
    "objectID": "Project_3_Predictive_Modelling/Project_3_R/Code/Project_3_Predictive_Modelling.html#conclusion",
    "href": "Project_3_Predictive_Modelling/Project_3_R/Code/Project_3_Predictive_Modelling.html#conclusion",
    "title": "Predictive Modelling",
    "section": "Conclusion",
    "text": "Conclusion\nThe differences in model performance for task one were not statistically significant. Model A had an RMSE that was 0.37 points higher than model B (p = 0.68), and 0.74 points higher than model C (p = 0.75). This may be due to small sample size however.\nThus, including MRI image features into the predictive model did not increase predictive capability above and beyond that of just using kidney volume measurements at baseline.\nOn the other hand, if it is true that a model with baseline kidney volume, a model with txti2 alone, and a model with both included all perform similarly at predicting percent change in kidney volume after 3 years, then this could provide support for predicting change in kidney volume percent by EITHER MRI image features or baseline kidney volume.\nThat is, utilizing MRI image features alone may offer similar predictive capabilities to using kidney volume measurements taken by a practiced physician. Therefore, if one is easier or cheaper than the other to acquire, the easier alternative could be used in place of the harder alternative and still achieve similar predictive power. For example, if there are MRI records but perhaps kidney volume measurements were not taken, then the MRI images can be used instead for making predictions."
  },
  {
    "objectID": "Project_3_Predictive_Modelling/Project_3_R/Code/Project_3_Predictive_Modelling.html#2A",
    "href": "Project_3_Predictive_Modelling/Project_3_R/Code/Project_3_Predictive_Modelling.html#2A",
    "title": "Predictive Modelling",
    "section": "Model 2A",
    "text": "Model 2A\n\nBaseline Kidney Volume\nTo answer the researcher’s first question for this task, we will run a predictive model that uses the baseline height-corrected total kidney volume to predict whether a patient had fast or slow disease progression.\n\\[ logit(Progression) = 𝛽_0 + 𝛽_{Baseline Kidney Volume} + e \\]\n\nAnalysisVisualizationSummary\n\n\n\nlibrary(plotROC)\n# Convert progression into a factor\ndata &lt;- data |&gt;\n  mutate(progression_num = as.numeric(progression)-1)\ndata$progression\n\n [1] Slow Fast Slow Slow Fast Fast Slow Fast Slow Slow Fast Slow Fast Fast Slow\n[16] Slow Slow Fast Slow Fast Fast Fast Fast Fast Slow Fast Fast Slow Fast Slow\n[31] Slow Slow Slow Fast Slow Slow Slow Slow Slow Fast Fast Fast Slow Slow Fast\n[46] Slow Fast Slow Fast Fast Fast Slow Slow Slow Fast Fast Fast Fast Slow Fast\n[61] Slow Slow Fast Fast Slow Fast Fast\nLevels: Slow Fast\n\nset.seed(123)\n# Create a training control object with 5-fold cross-validation and class probabilities \ntc &lt;- trainControl(method = \"cv\", number = 5, classProbs = TRUE, summaryFunction = twoClassSummary)\n\n# Perform the linear regression using 5-fold CV\nmodel2a &lt;- train(progression~ log(kidvol_base), \n                 data = data, \n                 method = \"glm\",\n                 family = \"binomial\",\n                 trControl = tc,\n                 metric = \"ROC\",\n                 preProc = \"scale\")\n\n# Examine model\nmodel2a\n\nGeneralized Linear Model \n\n67 samples\n 1 predictor\n 2 classes: 'Slow', 'Fast' \n\nPre-processing: scaled (1) \nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 54, 53, 53, 55, 53 \nResampling results:\n\n  ROC        Sens  Spec     \n  0.4746032  0.3   0.7142857\n\nsummary(model2a)\n\n\nCall:\nNULL\n\nCoefficients:\n                   Estimate Std. Error z value Pr(&gt;|z|)\n(Intercept)         0.96887    2.93675   0.330    0.741\n`log(kidvol_base)` -0.07914    0.24665  -0.321    0.748\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 92.867  on 66  degrees of freedom\nResidual deviance: 92.764  on 65  degrees of freedom\nAIC: 96.764\n\nNumber of Fisher Scoring iterations: 3\n\n\nkidvol_base_log is not a significant predictor of change in kidney volume (p = 0.75).\nThe AUC for this model is 0.47, which is near guessing. The sensitivity is 0.3 and the specificity is 0.71\nTop of Tabset\n\n\n\nConfusion Matrix\n\n# Save predictions\ndata$predictions2a &lt;- predict(model2a, newdata = data)\ndata$predictions2a_prob &lt;- predict(model2a, newdata = data, type = \"prob\")[,2]\ndata$predictions2a_num &lt;- as.numeric(data$predictions2a_prob)-1\n\n# Create confusion matrix\ncm &lt;- table(Predicted = data$predictions2a, Actual = data$progression)\n\n# Examine confusion matrix and performance\ncm\n\n         Actual\nPredicted Slow Fast\n     Slow   11   11\n     Fast   22   23\n\n\nWe can see the poor performance clearly in the confusion matrix.\n\n\nROC Curve\n\n# Plot ROC Curve\nroc2a &lt;- ggplot(data, aes(m = predictions2a_num, d = progression_num)) +\n  geom_roc(n.cuts = 10, labels = F, labelround = 4) +\n  theme_minimal() +\n  labs(title = \"ROC Curve Model 2A: Baseline Kidney Volume\",\n       x = \"False Positive Rate\",\n       y = \"True Positive Rate\") +\n  geom_abline(intercept = 0, slope = 1, linetype = \"dashed\", color = \"blue\")\n\nWe can see that the ROC curve is very poor, near guessing.\n\n\n\nThe model using kidvol_base_log alone performed poorly.\nAUC: 0.47\nSensitivity: 0.3\nSpecificity: 0.71\nTop of Tabset"
  },
  {
    "objectID": "Project_3_Predictive_Modelling/Project_3_R/Code/Project_3_Predictive_Modelling.html#2B",
    "href": "Project_3_Predictive_Modelling/Project_3_R/Code/Project_3_Predictive_Modelling.html#2B",
    "title": "Predictive Modelling",
    "section": "Model 2B: MRI Image Features",
    "text": "Model 2B: MRI Image Features\nTo answer the researcher’s second question for this task, we will run a predictive model that uses the MRI features to predict whether a patient had fast or slow disease progression.\n\\[ logit(Progression) = 𝛽_0 + 𝛽_{MRI Feature 1} + 𝛽_{MRI Feature 2} + ... + e \\]\nAs the correlation matrices revealed similar relationship between the MRI image features and progression as they did with kidvol_change, we will select the same candidate covariates for Task 2 as we did for Task 1.\nThese are:\n\ngeom_avg\ngabor3\ntxti2\nlbp5_log\nglcm2_square\n\n\nModel SelectionAnalysisVisualizationMessing around with saturated modelSummary\n\n\n\n# Create variable\ndata1_train &lt;- data1_train |&gt; \n  mutate(progression_num = as.numeric(progression)-1)\n\n# Create model\nmodel2b &lt;- glm(progression_num ~ geom_avg_scale + gabor3_scale + txti2_scale + lbp5_log_scale + glcm2_square_scale, data = data1_train, family = \"binomial\")\n\n# Examine Model Summary\nsummary(model2b)\n\n\nCall:\nglm(formula = progression_num ~ geom_avg_scale + gabor3_scale + \n    txti2_scale + lbp5_log_scale + glcm2_square_scale, family = \"binomial\", \n    data = data1_train)\n\nCoefficients:\n                   Estimate Std. Error z value Pr(&gt;|z|)   \n(Intercept)         -0.1231     0.3313  -0.372  0.71018   \ngeom_avg_scale      -0.1339     0.3567  -0.375  0.70737   \ngabor3_scale         0.8120     0.4575   1.775  0.07593 . \ntxti2_scale         -1.2378     0.4499  -2.751  0.00594 **\nlbp5_log_scale      -0.7129     0.3976  -1.793  0.07295 . \nglcm2_square_scale  -0.6465     0.4903  -1.318  0.18738   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 72.010  on 51  degrees of freedom\nResidual deviance: 55.253  on 46  degrees of freedom\nAIC: 67.253\n\nNumber of Fisher Scoring iterations: 5\n\n# Perform backward elimination \nols_step_forward_sbc(model2b)\n\n\n                              Stepwise Summary                               \n---------------------------------------------------------------------------\nStep    Variable        AIC       SBC         SBIC         R2       Adj. R2 \n---------------------------------------------------------------------------\n 0      Base Model     79.405    83.308    -66879.305    0.00000    0.00000 \n 1      txti2_scale    71.935    77.789    -96068.990    0.16650    0.14983 \n---------------------------------------------------------------------------\n\nFinal Model Output \n------------------\n\n                        Model Summary                          \n--------------------------------------------------------------\nR                       0.408       RMSE                0.456 \nR-Squared               0.167       MSE                 0.208 \nAdj. R-Squared          0.150       Coef. Var          96.757 \nPred R-Squared          0.113       AIC                71.935 \nMAE                     0.419       SBC                77.789 \n--------------------------------------------------------------\n RMSE: Root Mean Square Error \n MSE: Mean Square Error \n MAE: Mean Absolute Error \n AIC: Akaike Information Criteria \n SBC: Schwarz Bayesian Criteria \n\n                              ANOVA                                \n------------------------------------------------------------------\n               Sum of                                             \n              Squares        DF    Mean Square      F        Sig. \n------------------------------------------------------------------\nRegression      2.161         1          2.161    9.988    0.0027 \nResidual       10.819        50          0.216                    \nTotal          12.981        51                                   \n------------------------------------------------------------------\n\n                                  Parameter Estimates                                    \n----------------------------------------------------------------------------------------\n      model      Beta    Std. Error    Std. Beta      t        Sig      lower     upper \n----------------------------------------------------------------------------------------\n(Intercept)     0.487         0.065                  7.542    0.000     0.357     0.616 \ntxti2_scale    -0.205         0.065       -0.408    -3.160    0.003    -0.336    -0.075 \n----------------------------------------------------------------------------------------\n\n\nThe model with the smallest BIC is with txti2 alone, thus that will be our final model.\nTop of Tabset\n\n\n\nset.seed(123)\n\n# Perform the linear regression using 5-fold CV\nmodel2b &lt;- train(progression ~ txti2, \n                 data = data, \n                 method = \"glm\",\n                 family = \"binomial\",\n                 trControl = tc,\n                 metric = \"ROC\",\n                 preProc = \"scale\")\n\n# Examine model\nmodel2b\n\nGeneralized Linear Model \n\n67 samples\n 1 predictor\n 2 classes: 'Slow', 'Fast' \n\nPre-processing: scaled (1) \nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 54, 53, 53, 55, 53 \nResampling results:\n\n  ROC        Sens       Spec     \n  0.7376417  0.5666667  0.6428571\n\nsummary(model2b)\n\n\nCall:\nNULL\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)   \n(Intercept)  0.01522    0.26188   0.058  0.95364   \ntxti2       -0.82651    0.30174  -2.739  0.00616 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 92.867  on 66  degrees of freedom\nResidual deviance: 83.741  on 65  degrees of freedom\nAIC: 87.741\n\nNumber of Fisher Scoring iterations: 4\n\n\ntxti2 is a significant predictor of kidvol_change (p = 0.0062).\nThe AUC is 0.74, the senstivity is 0.57, and the specificity is 0.64\nTop of Tabset\n\n\n\nConfusion Matrix\n\n# Save predictions\ndata$predictions2b &lt;- predict(model2b, newdata = data)\ndata$predictions2b_prob &lt;- predict(model2b, newdata = data, type = \"prob\")[,2]\ndata$predictions2b_num &lt;- as.numeric(data$predictions2b_prob)-1\n\n# Create confusion matrix\ncm &lt;- table(Predicted = data$predictions2b, Actual = data$progression)\n\n# Examine confusion matrix and performance\ncm\n\n         Actual\nPredicted Slow Fast\n     Slow   20   12\n     Fast   13   22\n\n\nWe can see that we are getting more correct hits in the confusion matrix compared to using baseline kidney volume alone.\nTop of Tabset\n\n\nROC Curve\n\n# Plot ROC Curve\nroc2b &lt;- ggplot(data, aes(m = predictions2b_num, d = progression_num)) +\n  geom_roc(n.cuts = 10, labels = F, labelround = 4) +\n  theme_minimal() +\n  labs(title = \"ROC Curve Model 2B: Txti2\",\n       x = \"False Positive Rate\",\n       y = \"True Positive Rate\") +\n  geom_abline(intercept = 0, slope = 1, linetype = \"dashed\", color = \"blue\")\n\n# Visualize Plot\nroc2b\n\n\n\n\n\n\n\n\nThat’s not a bad curve!\nTop of Tabset\n\n\n\n\nset.seed(123)\n\n# Create variable\ndata &lt;- data |&gt; \n  mutate(geom_avg = (geom1+geom2/2))\n\n# Perform the linear regression using 5-fold CV\nmodel2b_sat &lt;- train(progression ~ txti2 + geom_avg + log(lbp5) + gabor3, \n                 data = data, \n                 method = \"glm\",\n                 family = \"binomial\",\n                 trControl = tc,\n                 metric = \"ROC\",\n                 preProc = \"scale\")\n\n# Examine model\nmodel2b_sat\n\nGeneralized Linear Model \n\n67 samples\n 4 predictor\n 2 classes: 'Slow', 'Fast' \n\nPre-processing: scaled (4) \nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 54, 53, 53, 55, 53 \nResampling results:\n\n  ROC        Sens       Spec     \n  0.6654195  0.6047619  0.6190476\n\nsummary(model2b_sat)\n\n\nCall:\nNULL\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)   \n(Intercept) -0.28058    0.42210  -0.665  0.50622   \ntxti2       -0.81344    0.31123  -2.614  0.00896 **\ngeom_avg    -0.01038    0.26837  -0.039  0.96915   \n`log(lbp5)` -0.30809    0.29114  -1.058  0.28996   \ngabor3       0.10950    0.26274   0.417  0.67686   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 92.867  on 66  degrees of freedom\nResidual deviance: 82.263  on 62  degrees of freedom\nAIC: 92.263\n\nNumber of Fisher Scoring iterations: 4\n\n# Save predictions\ndata$predictions2b_sat &lt;- predict(model2b_sat, newdata = data)\ndata$predictions2b_prob_sat &lt;- predict(model2b_sat, newdata = data, type = \"prob\")[,2]\ndata$predictions2b_num_sat &lt;- as.numeric(data$predictions2b_prob_sat)-1\n\n# Create confusion matrix\ncm &lt;- table(Predicted = data$predictions2b, Actual = data$progression)\n\n# Examine confusion matrix and performance\ncm\n\n         Actual\nPredicted Slow Fast\n     Slow   20   12\n     Fast   13   22\n\n# Plot ROC Curve\nroc2b &lt;- ggplot(data, aes(m = predictions2b_num, d = progression_num)) +\n  geom_roc(n.cuts = 10, labels = F, labelround = 4) +\n  theme_minimal() +\n  labs(title = \"ROC Curve Model 2B: Txti2\",\n       x = \"False Positive Rate\",\n       y = \"True Positive Rate\") +\n  geom_abline(intercept = 0, slope = 1, linetype = \"dashed\", color = \"blue\")\n\n# Visualize Plot\nroc2b\n\n\n\n\n\n\n\n# Calculate area under the curve\ncalc_auc(roc2b)$AUC\n\n[1] 0.6934046\n\n\nNot that different from just txti2. I just wanted to check because I was getting the BIC backwards selection choosing this model.\n\n\nModel 2B performs much better than model 2A. We can especially see this when we look at the ROC curve.\nAUC: 0.74\nSensitivity: 0.57\nSpecificity: 0.64\nTop of Tabset"
  },
  {
    "objectID": "Project_3_Predictive_Modelling/Project_3_R/Code/Project_3_Predictive_Modelling.html#2C",
    "href": "Project_3_Predictive_Modelling/Project_3_R/Code/Project_3_Predictive_Modelling.html#2C",
    "title": "Predictive Modelling",
    "section": "Model 2C: Baseline Kidney Volume and MRI Image Features",
    "text": "Model 2C: Baseline Kidney Volume and MRI Image Features\nTo answer the researcher’s third question, we will run a predictive model that uses the baseline height-corrected total kidney volume AND MRI image features to predict slow vs fast disease progression.\n\\[ logit(Progression) = 𝛽_0 + 𝛽_{Baseline Kidney Volume} + 𝛽_{Txti2} + ... + e \\]\n\nAnalysisVisualizationSummary\n\n\n\nset.seed(123)\n\n# Perform the linear regression using 5-fold CV\nmodel2c &lt;- train(progression ~ log(kidvol_base) + txti2, \n                 data = data, \n                 method = \"glm\",\n                 family = \"binomial\",\n                 trControl = tc,\n                 metric = \"ROC\",\n                 preProc = \"scale\")\n\n# Examine model\nmodel2c\n\nGeneralized Linear Model \n\n67 samples\n 2 predictor\n 2 classes: 'Slow', 'Fast' \n\nPre-processing: scaled (2) \nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 54, 53, 53, 55, 53 \nResampling results:\n\n  ROC        Sens       Spec     \n  0.7064626  0.5666667  0.5857143\n\nsummary(model2c)\n\n\nCall:\nNULL\n\nCoefficients:\n                   Estimate Std. Error z value Pr(&gt;|z|)   \n(Intercept)          3.2899     3.2933   0.999  0.31781   \n`log(kidvol_base)`  -0.2757     0.2768  -0.996  0.31912   \ntxti2               -0.8901     0.3106  -2.866  0.00416 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 92.867  on 66  degrees of freedom\nResidual deviance: 82.711  on 64  degrees of freedom\nAIC: 88.711\n\nNumber of Fisher Scoring iterations: 4\n\n\nThe model performs similarly to using txti2 alone.\nThe AUC is 0.71, the sensitivity is 0.57, and the specificity is 0.59\nTop of Tabset\n\n\n\nConfusion Matrix\n\n# Save predictions\ndata$predictions2c &lt;- predict(model2c, newdata = data)\ndata$predictions2c_prob &lt;- predict(model2c, newdata = data, type = \"prob\")[,2]\ndata$predictions2c_num &lt;- as.numeric(data$predictions2c_prob)-1\n\n# Create confusion matrix\ncm &lt;- table(Predicted = data$predictions2c, Actual = data$progression)\n\n# Examine confusion matrix and performance\ncm\n\n         Actual\nPredicted Slow Fast\n     Slow   22   13\n     Fast   11   21\n\n\nNot much different than model 2.\n\n\nROC Curve\n\n# Plot ROC Curve\nroc2c &lt;- ggplot(data, aes(m = predictions2c_num, d = progression_num)) +\n  geom_roc(n.cuts = 10, labels = F, labelround = 4) +\n  theme_minimal() +\n  labs(title = \"ROC Curve Model 2C: Baseline Kidney Volume and Txti2\",\n       x = \"False Positive Rate\",\n       y = \"True Positive Rate\") +\n  geom_abline(intercept = 0, slope = 1, linetype = \"dashed\", color = \"blue\")\n\n# Visualize Plot\nroc2c\n\n\n\n\n\n\n\n\nNot much different than just using txti2 alone.\nTop of Tabset\n\n\n\nModel C does not appear to perform much differently than model B.\nAUC: 0.71\nSensitivity: 0.57\nSpecificity is 0.59\nTop of Tabset"
  },
  {
    "objectID": "Project_3_Predictive_Modelling/Project_3_R/Code/Project_3_Predictive_Modelling.html#model-comparison-1",
    "href": "Project_3_Predictive_Modelling/Project_3_R/Code/Project_3_Predictive_Modelling.html#model-comparison-1",
    "title": "Predictive Modelling",
    "section": "Model Comparison",
    "text": "Model Comparison\n\n# Create list of models to compare\nmodel_list &lt;- list(`Kidney Volume at Baseline` = model2a, Txti2 = model2b, `Kidney Volume at Baseline and Txti2` = model2c)\n\n# Compare models\nresults &lt;- resamples(model_list)\nsummary(results)\n\n\nCall:\nsummary.resamples(object = results)\n\nModels: Kidney Volume at Baseline, Txti2, Kidney Volume at Baseline and Txti2 \nNumber of resamples: 5 \n\nROC \n                                         Min.   1st Qu.    Median      Mean\nKidney Volume at Baseline           0.3469388 0.4081633 0.4761905 0.4746032\nTxti2                               0.5555556 0.6428571 0.6938776 0.7376417\nKidney Volume at Baseline and Txti2 0.5833333 0.6428571 0.6530612 0.7064626\n                                      3rd Qu.      Max. NA's\nKidney Volume at Baseline           0.5306122 0.6111111    0\nTxti2                               0.7959184 1.0000000    0\nKidney Volume at Baseline and Txti2 0.6734694 0.9795918    0\n\nSens \n                                         Min.   1st Qu.    Median      Mean\nKidney Volume at Baseline           0.0000000 0.2857143 0.2857143 0.3000000\nTxti2                               0.1428571 0.1666667 0.6666667 0.5666667\nKidney Volume at Baseline and Txti2 0.1666667 0.2857143 0.6666667 0.5666667\n                                      3rd Qu. Max. NA's\nKidney Volume at Baseline           0.4285714  0.5    0\nTxti2                               0.8571429  1.0    0\nKidney Volume at Baseline and Txti2 0.7142857  1.0    0\n\nSpec \n                                         Min.   1st Qu.    Median      Mean\nKidney Volume at Baseline           0.5714286 0.5714286 0.7142857 0.7142857\nTxti2                               0.4285714 0.5000000 0.7142857 0.6428571\nKidney Volume at Baseline and Txti2 0.4285714 0.4285714 0.5000000 0.5857143\n                                      3rd Qu.      Max. NA's\nKidney Volume at Baseline           0.7142857 1.0000000    0\nTxti2                               0.7142857 0.8571429    0\nKidney Volume at Baseline and Txti2 0.7142857 0.8571429    0\n\nresults$models\n\n[1] \"Kidney Volume at Baseline\"           \"Txti2\"                              \n[3] \"Kidney Volume at Baseline and Txti2\"\n\n# Compare the AUCs \ndiffs &lt;- diff(results) \nsummary(diffs)\n\n\nCall:\nsummary.diff.resamples(object = diffs)\n\np-value adjustment: bonferroni \nUpper diagonal: estimates of the difference\nLower diagonal: p-value for H0: difference = 0\n\nROC \n                                    Kidney Volume at Baseline Txti2   \nKidney Volume at Baseline                                     -0.26304\nTxti2                               0.2809                            \nKidney Volume at Baseline and Txti2 0.3134                    0.8648  \n                                    Kidney Volume at Baseline and Txti2\nKidney Volume at Baseline           -0.23186                           \nTxti2                                0.03118                           \nKidney Volume at Baseline and Txti2                                    \n\nSens \n                                    Kidney Volume at Baseline\nKidney Volume at Baseline                                    \nTxti2                               0.4126                   \nKidney Volume at Baseline and Txti2 0.2756                   \n                                    Txti2                 \nKidney Volume at Baseline           -0.2666666666666666630\nTxti2                                                     \nKidney Volume at Baseline and Txti2 1.0000                \n                                    Kidney Volume at Baseline and Txti2\nKidney Volume at Baseline           -0.2666666666666666630             \nTxti2                               -0.0000000000000000111             \nKidney Volume at Baseline and Txti2                                    \n\nSpec \n                                    Kidney Volume at Baseline Txti2  \nKidney Volume at Baseline                                     0.07143\nTxti2                               1                                \nKidney Volume at Baseline and Txti2 1                         1      \n                                    Kidney Volume at Baseline and Txti2\nKidney Volume at Baseline           0.12857                            \nTxti2                               0.05714                            \nKidney Volume at Baseline and Txti2                                    \n\n# Plot RMSEs of each model\ndotplot(results, metric = \"ROC\",\n        main = \"Comparison of AUC Across Models\")\n\n\n\n\n\n\n\ndotplot(results, metric = \"Sens\",\n        main = \"Comparison of Sensitivity Across Models\")\n\n\n\n\n\n\n\ndotplot(results, metric = \"Spec\",\n        main = \"Comparison of Specificity Across Models\")\n\n\n\n\n\n\n\n\nThe difference in performance between models was not statistically significant (report AUC’s and 95% CI’s here)"
  },
  {
    "objectID": "Project_3_Predictive_Modelling/Project_3_R/Code/Project_3_Predictive_Modelling.html#results-1",
    "href": "Project_3_Predictive_Modelling/Project_3_R/Code/Project_3_Predictive_Modelling.html#results-1",
    "title": "Predictive Modelling",
    "section": "Results",
    "text": "Results\nTable 2. Results of Pairwise T-Tests Comparing Model Performance Predicting Disease Progression using AUC\n\n\n\n\n\n\n\n\n\n\nModel\nAUC\nSensitivity\nSpecificity\n  P-Adjusted\n\n\nBaseline Kidney Volume\n0.47\n0.30\n0.71\n—\n\n\nTxti2\n0.74\n0.57\n0.64\n0.28\n\n\nBaseline Kidney Volume and Txti2\n0.71\n0.57\n0.59\n0.31"
  },
  {
    "objectID": "Project_3_Predictive_Modelling/Project_3_R/Code/Project_3_Predictive_Modelling.html#summary-27",
    "href": "Project_3_Predictive_Modelling/Project_3_R/Code/Project_3_Predictive_Modelling.html#summary-27",
    "title": "Predictive Modelling",
    "section": "Summary",
    "text": "Summary\nThe model using txti2 alone performed better at predicting disease progression than the model using kidvol_base alone. However, this difference was not statistically significant.\nAdditionally, the model incorporating both txti2 and kidvol_base did not perform better than the model using just txti.\nHowever, examining the ROC curves and the 95% CIs for all of the models, it does appear that there is a trend towards these difference being significant. It is possible that these may have not been statistically significant due to a small sample size (N = 67)."
  },
  {
    "objectID": "Project_3_Predictive_Modelling/Project_3_R/Code/Project_3_Predictive_Modelling.html#roc-curves",
    "href": "Project_3_Predictive_Modelling/Project_3_R/Code/Project_3_Predictive_Modelling.html#roc-curves",
    "title": "Predictive Modelling",
    "section": "ROC Curves",
    "text": "ROC Curves\nSource\n\n# Example Data: Combine predictions into a single data frame\ndata_for_ROC &lt;- data.frame(\n  progression_num = data$progression_num,  # Your actual outcomes\n  `Baseline Kidney Volume` = data$predictions2a_num,\n  `Txti2` = data$predictions2b_num,\n  `Baseline Kidney Volume and Txti2` = data$predictions2c_num\n)\n\n# Melt the data frame for ggplot\ndata_melted &lt;- melt(data_for_ROC, id.vars = \"progression_num\", variable.name = \"model\", value.name = \"predictions\")\n\n# Plot ROC Curves\nroc_plot &lt;- ggplot(data_melted, aes(m = predictions, d = progression_num, color = model)) +\n  geom_roc(n.cuts = 10, labels = FALSE, labelround = 4) +\n  theme_minimal() +\n  labs(title = \"ROC Curves for Models Predicting Kidney Disease Progression\",\n       x = \"False Positive Rate\",\n       y = \"True Positive Rate\",\n       color = \"Model\") +\n  geom_abline(intercept = 0, slope = 1, linetype = \"dashed\", color = \"black\") +\n  scale_color_brewer(palette = \"Set2\") +\n  theme( text = element_text(size = 15))\n\n# Visualize Plot\nroc_plot"
  },
  {
    "objectID": "Project_3_Predictive_Modelling/Project_3_R/Code/images/Megan_Watson_Project-02.html",
    "href": "Project_3_Predictive_Modelling/Project_3_R/Code/images/Megan_Watson_Project-02.html",
    "title": "Data Checking - Statistical Consulting",
    "section": "",
    "text": "This is the Quarto markdown for the live consultation in BIOS 6621: Statistical Consulting.\nThis is a research project encompassing the entire data analysis pipeline, from initial consultation to scope of work writing, execution of statistical analysis, and completion of project deliverables."
  },
  {
    "objectID": "Project_3_Predictive_Modelling/Project_3_R/Code/images/Megan_Watson_Project-02.html#general-information",
    "href": "Project_3_Predictive_Modelling/Project_3_R/Code/images/Megan_Watson_Project-02.html#general-information",
    "title": "Data Checking - Statistical Consulting",
    "section": "General Information",
    "text": "General Information\n\n\n\n\n\n\n\n\n\nInvestigator\nMegan Watson\nDate\nSeptember 30, 2024\n\n\nProject Title\nQuality Improvement into Standard Practice: Persistent Practice Changes and Decreased Length of Stay for 3 years following a Medical ICU Physical Therapy Quality Improvement Project"
  },
  {
    "objectID": "Project_3_Predictive_Modelling/Project_3_R/Code/images/Megan_Watson_Project-02.html#project-description",
    "href": "Project_3_Predictive_Modelling/Project_3_R/Code/images/Megan_Watson_Project-02.html#project-description",
    "title": "Data Checking - Statistical Consulting",
    "section": "Project Description",
    "text": "Project Description\n\nBackground\n             The objective of this study is to assess the impact of a MICU physical therapy (PT) quality improvement (QI) project on physical therapist practice, and quantify the changes in length of stay (LOS) and mechanical ventilation (MV) time. The goals of the QI Initiative were to increase percentage of MICU admissions receiving PT, decrease time from MICU admission to first PT, and increase the frequency of PT visits. The main question the researcher came to us with was: how to correctly model the longitudinal data in the study?\n\n\nStudy Design\n              This was a retrospective cohort study, comparing 3 main time periods: Pre-QI Initiative (before April 2015), During QI Initiative (April 2015-Dec 2015), and After-QI Initiative (Jan 2016 and later). The QI initiative was deployed during the 9-month period between April 2015 and December 2015, during which time there were education and staffing changes. Patients were stratified as either having mechanical ventilation or not having mechanical ventilation. The primary outcomes of interest were MICU LOS and time on MV. Secondary outcomes of interest were the non-ICU LOS, total hospital LOS, and discharge location.\n\n\nDescription of the Data\n              Data was received as a de-identified .csv. Data points consist of: Admit and Discharge Time, PT Consult Time, Admit to Consult Time, First Therapy Time, Consult to Therapy Time, Admit to Therapy Time, Total Therapy Visits, Days with Therapy Visits, Average, min, and Max Therapy Length, Admit and Discharge Date/Time, Total MV Days, Hospital LOS, Patient Age, Sex, “Problem List”, and Codes (presumably ICD9/10).\n\n\nAnticipated Sample Size / Study Population\n              The sample size is N = ~3000-4000, consisting of patients admitted to the MICU with a LOS between 2 and 30 days (I.e. exclude &lt; 2 days or &gt; 30 days). This was a single site study performed here at Colorado University.\n\n\nHypothesis\n              The hypothesis for the study was that LOS and time on MV would decrease when comparing the pre- and intra-initiative time periods, and when comparing the pre- and post-initiative time periods. Clinical significance for this outcome is considered as a 1-2 day change in LOS. The intended end product of this project is a publication in a PT journal.\n\n\nAnalysis Plan (Sketch)\n              Data should first be examined for missingness, as this has not been performed. Similarly, repeat MICU admissions should be explored and controlled for to ensure independence of observations. Death during MICU stay should also be examined and adjusted for if needed. The analysis plan discussed was to utilize splines to capture the trajectory of LOS or time on MV over time by fitting smooth curves to the data. Linear contrasts should be performed to assess if there is a significant difference in LOS or time on MV when comparing two timepoints (i.e. pre- and intra-, pre- and post- , and post- and intra-QI Initiative. Supplementary analyses should be performed to assess secondary outcomes of interest of non-ICU LOS and total hospital LOS."
  },
  {
    "objectID": "Project_3_Predictive_Modelling/Project_3_R/Code/images/Megan_Watson_Project-02.html#project-deliverables",
    "href": "Project_3_Predictive_Modelling/Project_3_R/Code/images/Megan_Watson_Project-02.html#project-deliverables",
    "title": "Data Checking - Statistical Consulting",
    "section": "Project Deliverables",
    "text": "Project Deliverables\n             At this stage, Megan has only asked for insight into what method to use to account for variance in time for her analysis. There are currently no other deliverables.\nTimeline / Deadlines\nAt this time, Megan has not asked for data analysis efforts from us.\nShould this change, the anticipated project start date will be 1 week after our next meeting with her, where revisions to the Comprehensive Analysis Report are also expected to be completed.  The assigned analysts will reach out to the investigator to confirm the project timeframe and the project start date may be altered if necessary. 2 weeks will be allowed for initial data analysis by the assigned analyst.\nA follow-up meeting is to occur no later than 1 month after the next meeting with Megan. During the follow-up meeting, the project team will discuss and preliminary results of the analysis, mockup the desired tables and figures (1-2 descriptive tables, 1-2 analysis results tables and up to 4 graphs), and discuss next steps. During the follow-up meeting, the team will establish a meeting schedule for the remainder of the project. \nEstimated Cost\nThis represents our best estimate of the effort needed to complete the project. Should the scope of work change substantially, the analyst(s) will discuss changes with the investigator and issue a new or amended project agreement.\n\n\n\n\nEffort\n\n\n\n\nMonths\nPhD Faculty\nMaster’s Faculty\nStudent\n\n\n\n\n\n\n\n\n1\n20%\n0%\n80%\n\n\n\n\nTotal Costs\n’Bout Tree Fiddy"
  },
  {
    "objectID": "Project_3_Predictive_Modelling/Project_3_R/Code/images/Megan_Watson_Project-02.html#Outcome_Variables",
    "href": "Project_3_Predictive_Modelling/Project_3_R/Code/images/Megan_Watson_Project-02.html#Outcome_Variables",
    "title": "Data Checking - Statistical Consulting",
    "section": "Outcome Variables",
    "text": "Outcome Variables\n\nMechanical VentilationMICU Length of StayHospital Length of StayNon-ICU Length of StayDischarge Location\n\n\nFirst we will begin with mechanical ventilation time (MV)\n\n# Create histogram and qqplot\ndistr_plots(data_ex, \"MV_Total\")\n\nWarning: `aes_string()` was deprecated in ggplot2 3.0.0.\nℹ Please use tidy evaluation idioms with `aes()`.\nℹ See also `vignette(\"ggplot2-in-packages\")` for more information.\n\n\nWarning: Removed 3166 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\nWarning: Removed 3166 rows containing non-finite outside the scale range\n(`stat_qq()`).\n\n\nWarning: Removed 3166 rows containing non-finite outside the scale range\n(`stat_qq_line()`).\n\n\n\n\n\n\n\n\n\nThat’s some severe right skewness! Is that logarithmic? Let’s try and transform and see what happens.\n\n# Perform a log transform of mechanical ventilation time and plot\ndata_ex$MV_Total_log &lt;- log(data_ex$MV_Total)\n\n# Create histogram and qqplot\ndistr_plots(data_ex, \"MV_Total_log\", 10)\n\nWarning: Removed 3166 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\nWarning: Removed 3166 rows containing non-finite outside the scale range\n(`stat_qq()`).\n\n\nWarning: Removed 3166 rows containing non-finite outside the scale range\n(`stat_qq_line()`).\n\n\n\n\n\n\n\n\n\nThat’s better. Maybe try a square root transformation?\n\n# Perform a sqrt transformation and plot\ndata_ex$MV_Total_sqrt &lt;- sqrt(data_ex$MV_Total)\n\n# Create histogram and qqplot\ndistr_plots(data_ex, \"MV_Total_sqrt\", 10)\n\nWarning: Removed 3166 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\nWarning: Removed 3166 rows containing non-finite outside the scale range\n(`stat_qq()`).\n\n\nWarning: Removed 3166 rows containing non-finite outside the scale range\n(`stat_qq_line()`).\n\n\n\n\n\n\n\n\n\nThat looks worse. Out of curiosity I’m going to try using the bestNormalize package I’ve been learning to see if it can recommend the best tranformation.\n\nBNobject &lt;- bestNormalize(data_ex$MV_Total)\n\nWarning: `progress_estimated()` was deprecated in dplyr 1.0.0.\nℹ The deprecated feature was likely used in the bestNormalize package.\n  Please report the issue to the authors.\n\nBNobject\n\nBest Normalizing transformation with 3264 Observations\n Estimated Normality Statistics (Pearson P / df, lower =&gt; more normal):\n - arcsinh(x): 24.3829\n - Box-Cox: 24.6082\n - Center+scale: 23.8767\n - Double Reversed Log_b(x+a): 26.4455\n - Exp(x): 342.5592\n - Log_b(x+a): 24.4146\n - orderNorm (ORQ): 24.1343\n - sqrt(x + a): 24.253\n - Yeo-Johnson: 24.8498\nEstimation method: Out-of-sample via CV with 10 folds and 5 repeats\n \nBased off these, bestNormalize chose:\ncenter_scale(x) Transformation with 3264 nonmissing obs.\n Estimated statistics:\n - mean (before standardization) = 6.102022 \n - sd (before standardization) = 5.111536 \n\n\nThe short explanation of how these values work is that the value closest to 1 is the best transformation, but these are all VERY far from one. So that didn’t help.\n\nBoxplot\n\n#Create boxplot to assess for outliers\nggplot(data_ex, aes(y = MV_Total_log)) +\n  geom_boxplot() +\n  labs(title = \"Boxplot of MV Total Log\",\n       y = \"MV Total Log\")\n\nWarning: Removed 3166 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\n\n\n\n\nThere are also no outliers for MV_Total_Log!\n\n\nSummary\nIt looks like the best option we have is a log transformation of MV TOTAL.\nAlternatively, we will likely end up running a Poisson or Negative Binomial regression to handle this data.\nTop of Tabset\n\n\n\n\n# Create histogram and qqplot\ndistr_plots(data_ex, \"Unit_LOS\", 10)\n\n\n\n\n\n\n\n\nThat’s super skewed as well! Let’s try a log transform.\n\n# Perform log transform of MICU LOS\ndata_ex$Unit_LOS_log &lt;- log(data_ex$Unit_LOS)\n\n# Create histogram and qqplot\ndistr_plots(data_ex, \"Unit_LOS_log\", 10)\n\n\n\n\n\n\n\n\nWe’ve got more of an S shape going there. I wonder if a BoxCox transformation is more appropriate?\n\nlibrary(bestNormalize)\n# Use bestNormalize to try to find best transformation\nBNobject &lt;- bestNormalize(data_ex$Unit_LOS)\nBNobject\n\nBest Normalizing transformation with 6430 Observations\n Estimated Normality Statistics (Pearson P / df, lower =&gt; more normal):\n - arcsinh(x): 9.1701\n - Box-Cox: 5.7665\n - Center+scale: 33.825\n - Double Reversed Log_b(x+a): 56.9738\n - Log_b(x+a): 9.1701\n - orderNorm (ORQ): 1.3419\n - sqrt(x + a): 19.9464\n - Yeo-Johnson: 5.7659\nEstimation method: Out-of-sample via CV with 10 folds and 5 repeats\n \nBased off these, bestNormalize chose:\norderNorm Transformation with 6430 nonmissing obs and ties\n - 515 unique values \n - Original quantiles:\n    0%    25%    50%    75%   100% \n 48.00  66.00  95.00 166.75 720.00 \n\n# Perform boxcox transformation\ndata_ex$Unit_LOS_boxcox &lt;- boxcox(data_ex$Unit_LOS)$x.t\n\n# Create histogram and qqplot\ndistr_plots(data_ex, \"Unit_LOS_boxcox\", 25)\n\n\n\n\n\n\n\n\n\n# Create boxplot to assess for outliers\nggplot(data_ex, aes(y = Unit_LOS_boxcox)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\nThere are no outliers for boxcox Unit LOS.\nThat histogram looks better at least, and we don’t have any outliers. The bestNormalize package offers ways to back transform after you run your analysis, but I haven’t gotten that far in learning it yet. It looks like a boxcox transformation is a candidate however.\n\nSummary\nWill come back to this. I think there is a specific link function we use in this situation when we have an s-shaped qq plot like that.\nTop of Tabset\n\n\n\n\n# Create histogram and qqplot\ndistr_plots(data_ex, \"`HOSPITAL LOS`\", 25)\n\n\n\n\n\n\n\n\nHospital LOS also looks logarithmic.\nLet’s transform and assess.\n\n# Perform log transform \ndata_ex$HOSPITAL_LOS_log &lt;- log(data_ex$`HOSPITAL LOS`)\n\n# Create histogram and qqplot\ndistr_plots(data_ex, \"HOSPITAL_LOS_log\", 25)\n\n\n\n\n\n\n\nggplot(data_ex, aes(y = HOSPITAL_LOS_log)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\nLooks good! There are a handful of outliers in there, but the data looks normal and those values will likely be included.\n\nSummary\nHOSPITAL LOS is normal after a log transform. We will either perform that or a Poisson or Negative Binomial Top of Tabset\n\n\n\nWe must compute NON_ICU_LOS by subtracting Unit_LOS from HOSPITAL LOS.\n\n# Compute variable for non icu LOS.\ndata_ex &lt;- data_ex |&gt; \n  mutate(NON_ICU_LOS = `HOSPITAL LOS` - Unit_LOS)\n\n# Create histogram and qqplot\ndistr_plots(data_ex, \"NON_ICU_LOS\", 25)\n\n\n\n\n\n\n\n\nAs expected, that looks exponential.\n\n# Perform log transform\ndata_ex$NON_ICU_LOS_log &lt;- log(data_ex$NON_ICU_LOS)\n\n# Create histogram and qqplot\ndistr_plots(data_ex, \"NON_ICU_LOS_log\", 25)\n\nWarning: Removed 106 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\nWarning: Removed 106 rows containing non-finite outside the scale range\n(`stat_qq()`).\n\n\nWarning: Removed 106 rows containing non-finite outside the scale range\n(`stat_qq_line()`).\n\n\n\n\n\n\n\n\n\nThat’s bimodal. Odd.\nLet’s try a square root transform\n\n# Perform log transform\ndata_ex$NON_ICU_LOS_sqrt &lt;- sqrt(data_ex$NON_ICU_LOS)\n\n# Create histogram and qqplot\ndistr_plots(data_ex, \"NON_ICU_LOS_sqrt\", 25)\n\n\n\n\n\n\n\n\nLooks crummy.\nLet’s try bestNormalize\n\nBNobject &lt;- bestNormalize(data_ex$NON_ICU_LOS)\nBNobject\n\nBest Normalizing transformation with 6430 Observations\n Estimated Normality Statistics (Pearson P / df, lower =&gt; more normal):\n - arcsinh(x): 7.1573\n - Center+scale: 54.5975\n - Double Reversed Log_b(x+a): 57.6372\n - Log_b(x+a): 8.7927\n - orderNorm (ORQ): 1.5657\n - sqrt(x + a): 10.3512\n - Yeo-Johnson: 5.4506\nEstimation method: Out-of-sample via CV with 10 folds and 5 repeats\n \nBased off these, bestNormalize chose:\norderNorm Transformation with 6430 nonmissing obs and ties\n - 1045 unique values \n - Original quantiles:\n    0%    25%    50%    75%   100% \n   0.0   32.0  110.5  284.0 9206.0 \n\n# Perform boxcox transformation\ndata_ex$NON_ICU_LOS_yeo &lt;- yeojohnson(data_ex$NON_ICU_LOS)$x.t\n\n# Create histogram and qqplot\ndistr_plots(data_ex, \"NON_ICU_LOS_yeo\", 25)\n\n\n\n\n\n\n\n\nLooks better! Still bimodal but much more normal.\n\nSummary\nLooks like Yeo-Johnson transformation may be the way to go.\nTop of Tabset\n\n\n\n\n# Examine discharge locations\npretty_print(table(data_ex$DISCH_DISP))\n\n\n\n\nVar1\nFreq\n\n\n\n\nAcute IP to RPCU\n5\n\n\nAdministrative Discharge\n1\n\n\nAdmitted as an Inpatient\n3\n\n\nAgainst Clinical Advice\n22\n\n\nAnother Health Care Institution Not Defined w/Planned Readmission\n1\n\n\nCourt/Law Enforcement\n22\n\n\nDischarge Lounge\n6\n\n\nDischarged to Other Facility\n23\n\n\nDischarged/transferred to a Designated Cancer Center or Children’s Hospital\n41\n\n\nDrug/Alch Detox D/C\n4\n\n\nExpired\n1137\n\n\nExpired in Medical Facility\n8\n\n\nExpired Readmit as Organ Donor\n7\n\n\nFederal Hospital\n43\n\n\nHome-Health Care Svc\n759\n\n\nHome or Self Care\n2417\n\n\nHome or Self Care w/Planned Readmission\n1\n\n\nHospice/Home\n213\n\n\nHospice/Medical Facility\n211\n\n\nIntermediate Care Facility\n14\n\n\nIV Therapy Provider\n1\n\n\nLeft Against Medical Advice\n49\n\n\nLong Term Care\n451\n\n\nMental Health Facility\n2\n\n\nNursing Facility\n37\n\n\nPsychiatric Hospital\n34\n\n\nRehab Facility\n249\n\n\nShort Term Hospital\n71\n\n\nSkilled Nursing Facility\n596\n\n\nSwing Bed\n1\n\n\nTRANSFER TO CANCER OR CHILDRENS\n1\n\n\n\n\n\n\n\nThere are a few options we have for to analyze discharge location.\n\nWe can spit on expired vs not expired\nWe can split on hom vs other\nWe can just look at all the groups with the largest N, and collapse the rest into a single variable of ‘other’. This would be\n\nExpired (1137), Home (3177), Hospice (424), long term care (451), rehab (249), skilled Nursing facility (596)\n\n\nWe will have to ask the researcher how she wants to group up these discharge locations! Keeping in mind that since we are looking at three different time periods, each group will essentially by a third of what is shown here."
  },
  {
    "objectID": "Project_3_Predictive_Modelling/Project_3_R/Code/images/Megan_Watson_Project-02.html#average-micu-los",
    "href": "Project_3_Predictive_Modelling/Project_3_R/Code/images/Megan_Watson_Project-02.html#average-micu-los",
    "title": "Data Checking - Statistical Consulting",
    "section": "Average MICU LOS",
    "text": "Average MICU LOS\n\n# Summarize and plot the data\ndata_ex |&gt; \n  group_by(Admit_MonthYear) |&gt; \n  summarise(avg_Unit_LOS = mean(Unit_LOS, na.rm = T)) |&gt; \n  ggplot(aes(x = Admit_MonthYear, y = avg_Unit_LOS)) +\n  geom_line() +\n  theme_minimal() +\n  geom_rect(data = highlight_periods, aes(xmin = start, xmax = end, ymin = -Inf, ymax = Inf), \n            inherit.aes = FALSE, fill = \"lightblue\", alpha = 0.2) +\n  labs(title = \"Average MICU LOS over Time\",\n       x = \"Admit Date\",\n       y = \"Avg MICU LOS\")\n\n\n\n\n\n\n\n\nIt does not appear that MICU_LOS has decreased across any of the time periods.\nThis is including patients that did not receive PT, so of course there’s no difference.\n\nMICU LOS over Time by Mechanical Ventilation\n\n# Dummy code MV status \ndata_ex$MV_YN &lt;- ifelse(is.na(data_ex$MV_Total), 0, 1)\n\n# Make it a factor for better plotting\ndata_ex &lt;- data_ex |&gt; \n  mutate(MV_YN = as.factor(MV_YN))\n\n# Summarize and plot the data\ndata_ex |&gt; \n  group_by(Admit_MonthYear, MV_YN) |&gt; \n  summarise(avg_Unit_LOS = mean(Unit_LOS, na.rm = T)) |&gt; \n  ggplot(aes(x = Admit_MonthYear, y = avg_Unit_LOS, color = MV_YN, group = MV_YN)) +\n  geom_line() +\n  theme_minimal() +\n  geom_rect(data = highlight_periods, aes(xmin = start, xmax = end, ymin = -Inf, ymax = Inf), \n            inherit.aes = FALSE, fill = \"lightblue\", alpha = 0.2) +\n  labs(title = \"Average MICU LOS over Time by Mechanical Ventilation\",\n       x = \"Admit Date\",\n       y = \"Avg MICU LOS\")\n\n`summarise()` has grouped output by 'Admit_MonthYear'. You can override using\nthe `.groups` argument.\n\n\n\n\n\n\n\n\n\nThis shows that those who were mechanically ventilated had a higher MICU length of stay. This does not appear to have differed in relation to the QI iniative.\nI am also including those that did not receive PT here, so that would explain the null finding.\n\ndata_ex &lt;- data_ex |&gt; \n  mutate(PT_YESNO = as.factor(PT_YESNO))\n\n# Summarize and plot the data\ndata_ex |&gt; \n  group_by(Admit_MonthYear, MV_YN, PT_YESNO) |&gt; \n  summarise(avg_Unit_LOS = mean(Unit_LOS, na.rm = T)) |&gt; \n  ggplot(aes(x = Admit_MonthYear, y = avg_Unit_LOS, color = PT_YESNO)) +\n  geom_point() +\n  geom_smooth(method = \"loess\", se = F) +\n  theme_minimal() +\n  geom_rect(data = highlight_periods, aes(xmin = start, xmax = end, ymin = -Inf, ymax = Inf), \n            inherit.aes = FALSE, fill = \"lightblue\", alpha = 0.2) +\n  labs(title = \"Average MICU LOS over Time by Mechanical Ventilation\",\n       x = \"Admit Date\",\n       y = \"Avg MICU LOS\") +\n  facet_wrap(~MV_YN)\n\n`summarise()` has grouped output by 'Admit_MonthYear', 'MV_YN'. You can\noverride using the `.groups` argument.\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nI think this might show an interaction. For those on mechanical ventilation, they seemed to decrease in MICU LOS more if they received PT.\nFor those not on MV, the PT did not seem to change their average LOS.\n\n\nAverage MICU LOS over Time by Physical Therapy\n\n# Make it a factor for better plotting\ndata_ex &lt;- data_ex |&gt; \n  mutate(PT_YESNO = as.factor(PT_YESNO))\n\n# Summarize and plot the data\ndata_ex |&gt; \n  group_by(Admit_MonthYear, PT_YESNO) |&gt; \n  summarise(avg_Unit_LOS = mean(Unit_LOS, na.rm = T)) |&gt; \n  filter(Admit_MonthYear &gt; as.Date(\"2012-01-01\")) |&gt; \n  ggplot(aes(x = Admit_MonthYear, y = avg_Unit_LOS, color = PT_YESNO, group = PT_YESNO)) +\n  geom_line() +\n  theme_minimal() +\n  geom_rect(data = highlight_periods, aes(xmin = start, xmax = end, ymin = -Inf, ymax = Inf), \n            inherit.aes = FALSE, fill = \"lightblue\", alpha = 0.2) +\n  labs(title = \"Average MICU LOS over Time by Physical Therapy\",\n       x = \"Admit Date\",\n       y = \"Avg MICU LOS\")\n\n`summarise()` has grouped output by 'Admit_MonthYear'. You can override using\nthe `.groups` argument.\n\n\n\n\n\n\n\n\n\n\n# Summarize and plot the data\ndata_ex |&gt; \n  group_by(Admit_MonthYear, PT_YESNO) |&gt; \n  summarise(avg_Unit_LOS = mean(Unit_LOS, na.rm = T)) |&gt; \n  filter(Admit_MonthYear &gt; as.Date(\"2012-01-01\")) |&gt; \n  ggplot(aes(x = Admit_MonthYear, y = avg_Unit_LOS, color = PT_YESNO, group = PT_YESNO)) +\n  geom_point() +\n  geom_smooth(method = \"loess\", se = F) +\n  theme_minimal() +\n  geom_rect(data = highlight_periods, aes(xmin = start, xmax = end, ymin = -Inf, ymax = Inf), \n            inherit.aes = FALSE, fill = \"lightblue\", alpha = 0.2) +\n  labs(title = \"Average MICU LOS over Time by Physical Therapy\",\n       x = \"Admit Date\",\n       y = \"Avg MICU LOS\")\n\n`summarise()` has grouped output by 'Admit_MonthYear'. You can override using\nthe `.groups` argument.\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nInteresting! It does look like average MICU length of stay decreased slighly immediately after the QI initiative. It also looks like it may be increasing slightly from then on.\nThat does appear that those received PT had a decrease before and after the iniative\nBut overall it looks like the QI may have successfuly decreased MICU LOS.\n\n\nFitting with Splines\n\nlibrary(splines)\n# Summarize and plot the data\ndata_ex |&gt; \n  group_by(Admit_MonthYear, PT_YESNO) |&gt; \n  summarise(avg_Unit_LOS = mean(Unit_LOS, na.rm = T)) |&gt; \n  filter(Admit_MonthYear &gt; as.Date(\"2012-01-01\")) |&gt; \n  ggplot(aes(x = Admit_MonthYear, y = avg_Unit_LOS, color = PT_YESNO, group = PT_YESNO)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", formula = y ~ ns(x, df = 4), se = F) +\n  theme_minimal() +\n  geom_rect(data = highlight_periods, aes(xmin = start, xmax = end, ymin = -Inf, ymax = Inf), \n            inherit.aes = FALSE, fill = \"lightblue\", alpha = 0.2) +\n  labs(title = \"Average MICU LOS over Time by Physical Therapy\",\n       x = \"Admit Date\",\n       y = \"Avg MICU LOS\")\n\n`summarise()` has grouped output by 'Admit_MonthYear'. You can override using\nthe `.groups` argument.\n\n\n\n\n\n\n\n\n\nLooking just at those who received PT\n\n# Make it a factor for better plotting\ndata_ex &lt;- data_ex |&gt; \n  mutate(PT_YESNO = as.factor(PT_YESNO))\n\n# Summarize and plot the data\ndata_ex |&gt; \n  group_by(Admit_MonthYear, PT_YESNO) |&gt; \n  summarise(avg_Unit_LOS = mean(Unit_LOS, na.rm = T)) |&gt; \n  filter(Admit_MonthYear &gt; as.Date(\"2012-01-01\")) |&gt; \n  filter(PT_YESNO == 1) |&gt; \n  ggplot(aes(x = Admit_MonthYear, y = avg_Unit_LOS, group = PT_YESNO)) +\n  geom_point() +\n  geom_smooth(method = \"loess\", se = F) +\n  theme_minimal() +\n  geom_rect(data = highlight_periods, aes(xmin = start, xmax = end, ymin = -Inf, ymax = Inf), \n            inherit.aes = FALSE, fill = \"lightblue\", alpha = 0.2) +\n  labs(title = \"Average MICU LOS over Time by Physical Therapy\",\n       x = \"Admit Date\",\n       y = \"Avg MICU LOS\")\n\n`summarise()` has grouped output by 'Admit_MonthYear'. You can override using\nthe `.groups` argument.\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nFitting a loess curve, it does appear that for those that receive PT, they had a decrease in average MICU LOS during the intervention compared to before. It also appears that they are increasing in average MICU following the interention, but this may still be lower compared to the pre-intervention period.\n:::"
  },
  {
    "objectID": "Project_3_Predictive_Modelling/Project_3_R/Code/images/Megan_Watson_Project-02.html#increase-frequency-of-pt-visits",
    "href": "Project_3_Predictive_Modelling/Project_3_R/Code/images/Megan_Watson_Project-02.html#increase-frequency-of-pt-visits",
    "title": "Data Checking - Statistical Consulting",
    "section": "Increase Frequency of PT Visits",
    "text": "Increase Frequency of PT Visits\n\n# Summarize and plot the data\ndata_ex |&gt; \n  group_by(Admit_MonthYear) |&gt; \n  summarise(avg_pct_therapy = mean(`PERCENT OF DAYS WITH THERAPY VISITS`, na.rm = T)) |&gt; \n  ggplot(aes(x = Admit_MonthYear, y = avg_pct_therapy)) +\n  geom_line() +\n  theme_minimal() +\n  geom_rect(data = highlight_periods, aes(xmin = start, xmax = end, ymin = -Inf, ymax = Inf), \n            inherit.aes = FALSE, fill = \"lightblue\", alpha = 0.2) +\n  labs(title = \"Average Percentage of Days with Physical Therapy Over Time\",\n       x = \"Admit Date\",\n       y = \"Average Days with Physical Therapy (%)\")\n\n\n\n\n\n\n\n\nInteresting! They successfully and drastically increased the percentage of days that patients had physical therapy (roughly from 10-20% pre-iniative to 50-60% post iniative). This percentage did seem to drop off over time during the post- initiative period however, though it is still higher than it was pre-initiative (now roughly 30-40%).\n\nFit with Loess\n\n# Summarize and plot the data\ndata_ex |&gt; \n  group_by(Admit_MonthYear) |&gt; \n  summarise(avg_pct_therapy = mean(`PERCENT OF DAYS WITH THERAPY VISITS`, na.rm = T)) |&gt; \n  ggplot(aes(x = Admit_MonthYear, y = avg_pct_therapy)) +\n  geom_point() +\n  geom_smooth(method = \"loess\", se = F) +\n  theme_minimal() +\n  geom_rect(data = highlight_periods, aes(xmin = start, xmax = end, ymin = -Inf, ymax = Inf), \n            inherit.aes = FALSE, fill = \"lightblue\", alpha = 0.2) +\n  labs(title = \"Average Percentage of Days with Physical Therapy Over Time\",\n       x = \"Admit Date\",\n       y = \"Average Days with Physical Therapy (%)\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\nFit with Splines\n\n# Summarize and plot the data\ndata_ex |&gt; \n  group_by(Admit_MonthYear) |&gt; \n  summarise(avg_pct_therapy = mean(`PERCENT OF DAYS WITH THERAPY VISITS`, na.rm = T)) |&gt; \n  ggplot(aes(x = Admit_MonthYear, y = avg_pct_therapy)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", formula = y ~ ns(x, df = 5), se = F) +\n  theme_minimal() +\n  geom_rect(data = highlight_periods, aes(xmin = start, xmax = end, ymin = -Inf, ymax = Inf), \n            inherit.aes = FALSE, fill = \"lightblue\", alpha = 0.2) +\n  labs(title = \"Average Percentage of Days with Physical Therapy Over Time\",\n       x = \"Admit Date\",\n       y = \"Average Days with Physical Therapy (%)\")"
  },
  {
    "objectID": "Project_3_Predictive_Modelling/Project_3_R/Code/images/Megan_Watson_Project-02.html#decrease-time-from-admit-to-therapy",
    "href": "Project_3_Predictive_Modelling/Project_3_R/Code/images/Megan_Watson_Project-02.html#decrease-time-from-admit-to-therapy",
    "title": "Data Checking - Statistical Consulting",
    "section": "Decrease Time from Admit to Therapy",
    "text": "Decrease Time from Admit to Therapy\n\n# Summarize and plot the data\ndata_ex |&gt; \n  group_by(Admit_MonthYear) |&gt; \n  summarise(avg_ADMIT_TO_TREAT = mean(`ADMIT TO TREAT`, na.rm = T)) |&gt; \n  ggplot(aes(x = Admit_MonthYear, y = avg_ADMIT_TO_TREAT)) +\n  geom_line() +\n  theme_minimal() +\n  geom_rect(data = highlight_periods, aes(xmin = start, xmax = end, ymin = -Inf, ymax = Inf), \n            inherit.aes = FALSE, fill = \"lightblue\", alpha = 0.2) +\n  annotate(\"text\", x = as.Date(\"2015-09-01\"), y = Inf, \n           label = \"QI-Initiative\", color = \"skyblue2\", size = 4, vjust = 1.5,) +\n  labs(title = \"Time from Admit to Therapy over Time\",\n       x = \"Admit Date\",\n       y = \"Time from Admit to Therapy\")\n\n\n\n\n\n\n\n\nThey successfully decreased the time from a patient’s first admit to their first physical therapy session.\nThis means that patients were receiving PT sooner.\n\nPlot with Loess\n\n# Summarize and plot the data\ndata_ex |&gt; \n  group_by(Admit_MonthYear) |&gt; \n  summarise(avg_ADMIT_TO_TREAT = mean(`ADMIT TO TREAT`, na.rm = T)) |&gt; \n  ggplot(aes(x = Admit_MonthYear, y = avg_ADMIT_TO_TREAT)) +\n  geom_point() +\n  geom_smooth(method = \"loess\", se = F) +\n  theme_minimal() +\n  geom_rect(data = highlight_periods, aes(xmin = start, xmax = end, ymin = -Inf, ymax = Inf), \n            inherit.aes = FALSE, fill = \"lightblue\", alpha = 0.2) +\n  annotate(\"text\", x = as.Date(\"2015-09-01\"), y = Inf, \n           label = \"QI-Initiative\", color = \"skyblue2\", size = 4, vjust = 1.5,) +\n  labs(title = \"Time from Admit to Therapy over Time\",\n       x = \"Admit Date\",\n       y = \"Time from Admit to Therapy\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nYeah looks crummier with loess, will likely delete."
  },
  {
    "objectID": "Project_3_Predictive_Modelling/Project_3_R/Code/images/Megan_Watson_Project-02.html#increase-percentage-of-admissions-receiving-physical-therapy",
    "href": "Project_3_Predictive_Modelling/Project_3_R/Code/images/Megan_Watson_Project-02.html#increase-percentage-of-admissions-receiving-physical-therapy",
    "title": "Data Checking - Statistical Consulting",
    "section": "Increase Percentage of Admissions Receiving Physical Therapy",
    "text": "Increase Percentage of Admissions Receiving Physical Therapy\n\n# Summarize and calculate percentages\nsummary_data &lt;- data_ex |&gt; \n  group_by(Admit_MonthYear, PT_YESNO) |&gt; \n  summarise(count = n()) |&gt; \n  mutate(total_count = sum(count)) |&gt; \n  ungroup() |&gt; \n  mutate(percentage = (count / total_count) * 100) |&gt; \n  filter(PT_YESNO == 1)\n\n`summarise()` has grouped output by 'Admit_MonthYear'. You can override using\nthe `.groups` argument.\n\n# Plot the data\nggplot(summary_data, aes(x = Admit_MonthYear, y = percentage)) +\n  geom_line() +\n  theme_minimal() +\n  geom_rect(data = highlight_periods, aes(xmin = start, xmax = end, ymin = -Inf, ymax = Inf), \n            inherit.aes = FALSE, fill = \"lightblue\", alpha = 0.2) +\n  annotate(\"text\", x = as.Date(\"2015-09-01\"), y = Inf, \n           label = \"QI-Initiative\", color = \"skyblue2\", size = 4, vjust = 1.5) +\n  labs(title = \"Percentage of Admissions Receiving Physical Therapy\",\n       x = \"Admit Date\",\n       y = \"Percentage\")\n\n\n\n\n\n\n\n\nThey also successfully increased the percentage of patients receiving PT from ~40% pre-initiative to ~80% during the initiative. This percentage then decreases to ~70% post-initiative.\n::::"
  },
  {
    "objectID": "Project_3_Predictive_Modelling/Project_3_R/Code/images/Megan_Watson_Project-02.html#mechanical-ventilation-1",
    "href": "Project_3_Predictive_Modelling/Project_3_R/Code/images/Megan_Watson_Project-02.html#mechanical-ventilation-1",
    "title": "Data Checking - Statistical Consulting",
    "section": "Mechanical Ventilation",
    "text": "Mechanical Ventilation\nThe researcher expressed that patients would be stratified on Mechanical ventilation v. NO mechanical ventilation. However, I do not see this variable. Let’s go ahead and create it, assuming that patients with NA for MV_TOTAL were simply never on mechanical ventilation.\n\n# Dummy code MV status \ndata_ex$MV_YN &lt;- ifelse(is.na(data_ex$MV_Total), 0, 1)\n\nLet’s check that worked as intended.\n\n# Filter down to variables of interest to verify dummy coding worked\ncheck &lt;- data_ex %&gt;%\n  select(id, MV_Total, MV_YN)\n\n# Pretty print\nkable(head(check), format = \"html\") %&gt;%\n  kable_styling(bootstrap_options = c(\"condensed\", \"stripe\", \"hover\"))\n\n\n\n\nid\nMV_Total\nMV_YN\n\n\n\n\n2\nNA\n0\n\n\n3\n13\n1\n\n\n4\nNA\n0\n\n\n5\n16\n1\n\n\n8\nNA\n0\n\n\n10\n2\n1\n\n\n\n\n\n\n\nLooks good."
  },
  {
    "objectID": "Project_3_Predictive_Modelling/Project_3_R/Code/images/Megan_Watson_Project-02.html#qi-initiative-time-period",
    "href": "Project_3_Predictive_Modelling/Project_3_R/Code/images/Megan_Watson_Project-02.html#qi-initiative-time-period",
    "title": "Data Checking - Statistical Consulting",
    "section": "QI-Initiative Time Period",
    "text": "QI-Initiative Time Period\nLet’s group participants into the time period that they were present during the initiative for.\n\n# Create new variable based on QI initiative time period.\ndata_ex &lt;- data_ex |&gt; \n  mutate(initiative = ifelse(Admit_MonthYear &lt; as.Date(\"2015-04-01\"), \"Pre\",\n                             ifelse(Admit_MonthYear &gt; as.Date(\"2015-12-01\"), \"Post\", \"During\")))\n\nLet’s double check we did that right.\n\n# Check counts\ntable(data_ex$initiative)\n\n\nDuring   Post    Pre \n   656   2860   2889 \n\n# Double check assigning\ndata_ex |&gt; \n  select(id, Admit_MonthYear, initiative) |&gt; \n  filter(Admit_MonthYear &lt; as.Date(\"2015-04-01\")) |&gt; \n  arrange(desc(Admit_MonthYear)) |&gt; \n  head() |&gt; \n  pretty_print()\n\n\n\n\nid\nAdmit_MonthYear\ninitiative\n\n\n\n\n5759\n2015-03-01\nPre\n\n\n5760\n2015-03-01\nPre\n\n\n5761\n2015-03-01\nPre\n\n\n5763\n2015-03-01\nPre\n\n\n5765\n2015-03-01\nPre\n\n\n5767\n2015-03-01\nPre\n\n\n\n\n\n\n# Double check assigning\ndata_ex |&gt; \n  select(id, Admit_MonthYear, initiative) |&gt; \n  filter(Admit_MonthYear &gt; as.Date(\"2015-04-01\")) |&gt; \n  arrange(Admit_MonthYear) |&gt; \n  head() |&gt; \n  pretty_print()\n\n\n\n\nid\nAdmit_MonthYear\ninitiative\n\n\n\n\n6094\n2015-05-01\nDuring\n\n\n6096\n2015-05-01\nDuring\n\n\n6099\n2015-05-01\nDuring\n\n\n6100\n2015-05-01\nDuring\n\n\n6105\n2015-05-01\nDuring\n\n\n6106\n2015-05-01\nDuring\n\n\n\n\n\n\n# Double check assigning\ndata_ex |&gt; \n  select(id, Admit_MonthYear, initiative) |&gt; \n  filter(Admit_MonthYear &gt;= as.Date(\"2015-12-01\")) |&gt; \n  arrange(Admit_MonthYear) |&gt; \n  head() |&gt; \n  pretty_print()\n\n\n\n\nid\nAdmit_MonthYear\ninitiative\n\n\n\n\n7167\n2015-12-01\nDuring\n\n\n7169\n2015-12-01\nDuring\n\n\n7171\n2015-12-01\nDuring\n\n\n7172\n2015-12-01\nDuring\n\n\n7174\n2015-12-01\nDuring\n\n\n7176\n2015-12-01\nDuring\n\n\n\n\n\n\n# Double check assigning\ndata_ex |&gt; \n  select(id, Admit_MonthYear, initiative) |&gt; \n  filter(Admit_MonthYear &gt; as.Date(\"2015-12-01\")) |&gt; \n  arrange(Admit_MonthYear) |&gt; \n  head() |&gt; \n  pretty_print()\n\n\n\n\nid\nAdmit_MonthYear\ninitiative\n\n\n\n\n7324\n2016-01-01\nPost\n\n\n7331\n2016-01-01\nPost\n\n\n7336\n2016-01-01\nPost\n\n\n7339\n2016-01-01\nPost\n\n\n7343\n2016-01-01\nPost\n\n\n7345\n2016-01-01\nPost\n\n\n\n\n\n\n\nLooks good.\n\nSome Graphing\n\ndata_ex &lt;- data_ex |&gt; \n  mutate(initiative = as.factor(initiative)) |&gt; \n  mutate(initiative = relevel(initiative, ref = \"Pre\"))\n\nggplot(data_ex, aes(x = PT_YESNO, y = Unit_LOS_log, fill = initiative)) +\n  geom_boxplot() +\n  theme_minimal() +\n  scale_fill_brewer(palette = \"Pastel2\")\n\n\n\n\n\n\n\n\nThis shows that MICU LOS decreased in the during and post QI initiative periods compared to the pre-iniatitive period.\n\ndata_ex |&gt; \n  filter(PT_YESNO == 1) |&gt; \n  ggplot(aes(x = initiative, y = Unit_LOS_log, fill = initiative)) +\n  geom_boxplot() +\n  theme_minimal() +\n  scale_fill_brewer(palette = \"Pastel2\")\n\n\n\n\n\n\n\n\n\ndata_ex |&gt; \n  filter(PT_YESNO == 1) |&gt; \n  ggplot(aes(x = initiative, y = Unit_LOS_log, fill = initiative)) +\n  geom_boxplot() +\n  theme_minimal() +\n  scale_fill_brewer(palette = \"Pastel2\")\n\n\n\n\n\n\n\n\nThere is an interaction here between PT and unit LOS and initiative.\nIn other words, you HAVE to filter based on PT\n\n\nMV Total\n\nggplot(data_ex, aes(x = PT_YESNO, y = MV_Total_log, fill = initiative)) +\n  geom_boxplot() +\n  theme_minimal() +\n  scale_fill_brewer(palette = \"Pastel2\")\n\nWarning: Removed 3149 rows containing non-finite outside the scale range\n(`stat_boxplot()`)."
  },
  {
    "objectID": "Project_3_Predictive_Modelling/Project_3_R/Code/images/Megan_Watson_Project-02.html#check-for-repeat-admissions",
    "href": "Project_3_Predictive_Modelling/Project_3_R/Code/images/Megan_Watson_Project-02.html#check-for-repeat-admissions",
    "title": "Data Checking - Statistical Consulting",
    "section": "Check for Repeat Admissions",
    "text": "Check for Repeat Admissions\n\n# Check for repeate admissions\nlength(unique(data_ex$id))\n\n[1] 6405\n\ndim(data_ex)\n\n[1] 6405   40\n\n\nThe number of unique patient id’s is the same number as the number of rows in our data set.\nSo we do not have repeate admissions in this data set and do not need to account for repeated measures! (We meet the assumption of independent observations)\n:::"
  },
  {
    "objectID": "Project_2/Project_2_SAS/Code/Linear_Mixed_Model_SAS.html",
    "href": "Project_2/Project_2_SAS/Code/Linear_Mixed_Model_SAS.html",
    "title": "Linear Mixed Model (SAS)",
    "section": "",
    "text": "The objective of this project is to gain experience in the application of Linear Mixed Models for statistical analysis. We are replicating this analysis in SAS.\nThis is the exact same data set as Project 2. Please see Project 2 - MLR with Confounding and Interaction for a detailed description of the data cleaning process.\n\n\nThe aim of the current study is to assess how treatment response differs for HIV+ patients 2 years after initiating Highly Active Antiretroviral Therapy (HAART) based on hard drug usage (such as heroin or cocaine). This study is of particular scientific interest because it is unclear whether the use of hard drugs inhibits the immune system in humans; treatment strategies may differ based on these results. The researchers are interested in comparing subjects who never used hard drugs to current hard drug users (those that use hard drugs at year 2) or previous hard drug users (those who used drugs at year 0 or 1). Outcomes of interest are: viral load (HIV copies in a mL of blood), CD4+ T cell count (a measure of immunologic health), and aggregate physical and quality of life scores from the SF-36.\nThe clinical hypothesis is that, if hard drugs inhibit the immune system in humans, subjects who currently or previously used hard drugs will have higher viral load and lower CD4+ T cell counts than those who never used hard drugs. Additionally, the researchers are interested in knowing if potential differences between the drug use groups can be explained by differences in adherence to the treatment regimen. The researchers are agnostic on how quality of life changes after treatment, since side effects of the treatment are significant.\nThe project description provided by the PI is available below:"
  },
  {
    "objectID": "Project_2/Project_2_SAS/Code/Linear_Mixed_Model_SAS.html#project-description",
    "href": "Project_2/Project_2_SAS/Code/Linear_Mixed_Model_SAS.html#project-description",
    "title": "Linear Mixed Model (SAS)",
    "section": "",
    "text": "The aim of the current study is to assess how treatment response differs for HIV+ patients 2 years after initiating Highly Active Antiretroviral Therapy (HAART) based on hard drug usage (such as heroin or cocaine). This study is of particular scientific interest because it is unclear whether the use of hard drugs inhibits the immune system in humans; treatment strategies may differ based on these results. The researchers are interested in comparing subjects who never used hard drugs to current hard drug users (those that use hard drugs at year 2) or previous hard drug users (those who used drugs at year 0 or 1). Outcomes of interest are: viral load (HIV copies in a mL of blood), CD4+ T cell count (a measure of immunologic health), and aggregate physical and quality of life scores from the SF-36.\nThe clinical hypothesis is that, if hard drugs inhibit the immune system in humans, subjects who currently or previously used hard drugs will have higher viral load and lower CD4+ T cell counts than those who never used hard drugs. Additionally, the researchers are interested in knowing if potential differences between the drug use groups can be explained by differences in adherence to the treatment regimen. The researchers are agnostic on how quality of life changes after treatment, since side effects of the treatment are significant.\nThe project description provided by the PI is available below:"
  },
  {
    "objectID": "Project_2/Project_2_SAS/Code/Linear_Mixed_Model_SAS.html#connect-to-sas",
    "href": "Project_2/Project_2_SAS/Code/Linear_Mixed_Model_SAS.html#connect-to-sas",
    "title": "Linear Mixed Model (SAS)",
    "section": "Connect to SAS",
    "text": "Connect to SAS\n\n# This code connects to SAS On Demand\nlibrary(configSAS)\nconfigSAS::set_sas_engine()\n\nUsing SAS Config named: oda\nSAS Connection established. Subprocess id is 28544\n\n#  This code allows you to run ```{sas}``` chunks\nsas = knitr::opts_chunk$get(\"sas\")"
  },
  {
    "objectID": "Project_2/Project_2_SAS/Code/Linear_Mixed_Model_SAS.html#create-library",
    "href": "Project_2/Project_2_SAS/Code/Linear_Mixed_Model_SAS.html#create-library",
    "title": "Linear Mixed Model (SAS)",
    "section": "Create Library",
    "text": "Create Library\n\n* Create library;\n%LET CourseRoot = /home/u63376223/sasuser.v94/Advanced Data Analysis;\nLIBNAME Proj2LMM \"&CourseRoot/Project 2 LMM\";"
  },
  {
    "objectID": "Project_2/Project_2_SAS/Code/Linear_Mixed_Model_SAS.html#read-in-data",
    "href": "Project_2/Project_2_SAS/Code/Linear_Mixed_Model_SAS.html#read-in-data",
    "title": "Linear Mixed Model (SAS)",
    "section": "Read in Data",
    "text": "Read in Data\n\n* Import the dataset;\nPROC IMPORT\n    DATAFILE = \"&CourseRoot/Project 2 LMM/Proj2_data_2_cleaned.csv\"\n    OUT = Proj2LMM.data\n    REPLACE;\n    GETNAMES = YES;\n    RUN;\n    \n* Drop the extraneous VAR1 column;\nDATA Proj2LMM.data;\n    SET Proj2LMM.data;\n    DROP VAR1;\n    RUN;"
  },
  {
    "objectID": "Project_2/Project_2_SAS/Code/Linear_Mixed_Model_SAS.html#handle-na-values",
    "href": "Project_2/Project_2_SAS/Code/Linear_Mixed_Model_SAS.html#handle-na-values",
    "title": "Linear Mixed Model (SAS)",
    "section": "Handle NA Values",
    "text": "Handle NA Values\n\n* Fix missing values;\nDATA Proj2LMM.data;\n    SET Proj2LMM.data;\n    if hard_drugs = \"NA\" then hard_drugs = \"\";\n        else hard_drugs = hard_drugs;\n    if ADH = \"NA\" then ADH = \"\";\n        ELSE ADH = ADH;\n    IF FRP = \"NA\" THEN FRP = \"\";\n        ELSE FRP = FRP;\n    RUN;"
  },
  {
    "objectID": "Project_2/Project_2_SAS/Code/Linear_Mixed_Model_SAS.html#examine-data-set",
    "href": "Project_2/Project_2_SAS/Code/Linear_Mixed_Model_SAS.html#examine-data-set",
    "title": "Linear Mixed Model (SAS)",
    "section": "Examine Data Set",
    "text": "Examine Data Set\n\n* Examine header;\nPROC PRINT DATA = Proj2LMM.data (OBS = 6);\n    RUN;\n\nlstlog\n\n\n&lt;!DOCTYPE html&gt;\n\n\n\n\n\nSAS Output\n\n\n\n\n\n\n\nThe SAS System\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObs\n\n\nnewid\n\n\nAGG_MENT\n\n\nAGG_PHYS\n\n\nHASHV\n\n\nHASHF\n\n\nBMI\n\n\nLIV34\n\n\nFRP\n\n\nCESD\n\n\nDKGRP\n\n\nLEU3N\n\n\nVLOAD\n\n\nADH\n\n\nEDUCBAS\n\n\nhivpos\n\n\nage\n\n\nART\n\n\neverART\n\n\nyears\n\n\nhard_drugs\n\n\nVLOAD_log\n\n\nLEU3N_log\n\n\nLEU3N_standard\n\n\nLEU3N_orderNorm\n\n\nLEU3N_boxcox\n\n\nLEU3N_yeojohnson\n\n\nAGG_MENT_orderNorm\n\n\nAGG_PHYS_orderNorm\n\n\nBMI_outlier\n\n\nCESD_sqrt\n\n\n\n\n\n\n1\n\n\n1\n\n\n44.90709633\n\n\n52.52556921\n\n\nNo\n\n\nNA\n\n\n24.71755909\n\n\nNo\n\n\nNo\n\n\n14\n\n\nNone\n\n\n104.1659454\n\n\n102013\n\n\n \n\n\nAt least one year college but no degree\n\n\n1\n\n\n52\n\n\n0\n\n\n0\n\n\n0\n\n\nYes\n\n\n11.532855535\n\n\n4.6459852563\n\n\n-1.42534825\n\n\n-1.564570348\n\n\n-1.6864492\n\n\n-1.791855452\n\n\n-0.325883903\n\n\n-0.026750748\n\n\nFALSE\n\n\n3.7416573868\n\n\n\n\n2\n\n\n1\n\n\n58.20753602\n\n\n41.29346502\n\n\nNo\n\n\nLess Often\n\n\n26.06800717\n\n\nNo\n\n\nNo\n\n\n2\n\n\n&gt;13 drinks/week\n\n\n262.0060502\n\n\n27\n\n\n95-99%\n\n\nAt least one year college but no degree\n\n\n1\n\n\n53\n\n\n1\n\n\n1\n\n\n1\n\n\nNo\n\n\n3.295836866\n\n\n5.5683675959\n\n\n-0.803739256\n\n\n-0.80590283\n\n\n-0.751743565\n\n\n-0.690930883\n\n\n0.9992307421\n\n\n-0.858070277\n\n\nFALSE\n\n\n1.4142135624\n\n\n\n\n3\n\n\n1\n\n\n59.65135974\n\n\n48.54453213\n\n\nNo\n\n\nNever\n\n\n27.16420923\n\n\nNo\n\n\nNo\n\n\n1\n\n\nNone\n\n\n345.4009918\n\n\n60\n\n\n100%\n\n\nAt least one year college but no degree\n\n\n1\n\n\n54\n\n\n1\n\n\n1\n\n\n2\n\n\nNo\n\n\n4.0943445622\n\n\n5.8447060372\n\n\n-0.475311662\n\n\n-0.42189608\n\n\n-0.364691713\n\n\n-0.288979274\n\n\n1.326252431\n\n\n-0.371364781\n\n\nFALSE\n\n\n1\n\n\n\n\n4\n\n\n2\n\n\n46.34189863\n\n\n27.92331491\n\n\nNo\n\n\nNever\n\n\n26.66936015\n\n\nNo\n\n\nYes\n\n\n20\n\n\nNone\n\n\n257.8277832\n\n\n8121\n\n\n \n\n\n9,10, or 11th grade\n\n\n1\n\n\n54\n\n\n0\n\n\n0\n\n\n0\n\n\nYes\n\n\n9.0022085783\n\n\n5.5522918551\n\n\n-0.820194189\n\n\n-0.821113591\n\n\n-0.772479637\n\n\n-0.713187968\n\n\n-0.247826962\n\n\n-1.765742707\n\n\nFALSE\n\n\n4.472135955\n\n\n\n\n5\n\n\n2\n\n\n48.71790642\n\n\n38.03806704\n\n\nYes\n\n\nNever\n\n\n25.96576195\n\n\nNo\n\n\nNo\n\n\n18\n\n\n&gt;13 drinks/week\n\n\n459.4561691\n\n\n21\n\n\n100%\n\n\n9,10, or 11th grade\n\n\n1\n\n\n55\n\n\n1\n\n\n1\n\n\n1\n\n\nNo\n\n\n3.0445224377\n\n\n6.130043549\n\n\n-0.026137328\n\n\n0.0992729637\n\n\n0.1026213079\n\n\n0.1669827951\n\n\n-0.112577084\n\n\n-1.105593651\n\n\nFALSE\n\n\n4.2426406871\n\n\n\n\n6\n\n\n2\n\n\n45.41482533\n\n\n37.32203958\n\n\nNo\n\n\nNever\n\n\n26.96036793\n\n\nNo\n\n\nNo\n\n\n18\n\n\n4-13 drinks/week\n\n\n263.0692533\n\n\n48\n\n\n100%\n\n\n9,10, or 11th grade\n\n\n1\n\n\n56\n\n\n1\n\n\n1\n\n\n2\n\n\nNo\n\n\n3.8712010109\n\n\n5.572417318\n\n\n-0.799552128\n\n\n-0.799440785\n\n\n-0.746490508\n\n\n-0.68530504\n\n\n-0.295407078\n\n\n-1.175901733\n\n\nFALSE\n\n\n4.2426406871\n\n\n\n\n\n\n\n\n\n\n\u001411                                                         The SAS System                    Saturday, December 28, 2024 08:34:00 PM\n\n185        ods listing close;ods html5 (id=saspy_internal) file=_tomods1 options(bitmap_mode='inline') device=svg style=HTMLBlue;\n185      ! ods graphics on / outputfmt=png;\n186        \n187        * Examine header;\n188        PROC PRINT DATA = Proj2LMM.data (OBS = 6);\n189            RUN;\n190        \n191        \n192        ods html5 (id=saspy_internal) close;ods listing;\n193        \n\u001412                                                         The SAS System                    Saturday, December 28, 2024 08:34:00 PM\n\n194"
  },
  {
    "objectID": "Project_2/Project_2_SAS/Code/Linear_Mixed_Model_SAS.html#hard-drug-use-group",
    "href": "Project_2/Project_2_SAS/Code/Linear_Mixed_Model_SAS.html#hard-drug-use-group",
    "title": "Linear Mixed Model (SAS)",
    "section": "Hard Drug Use Group",
    "text": "Hard Drug Use Group\n\nNever User: Did not use hard drugs at any time point\nPrevious User: Used hard drugs at time points 0 or 1, but not 2.\nCurrent User: Used hard drugs at time point 2.\n\n\nCreate Hard Drug Use Group\n\n* Create hard drugs use group;\nPROC SQL;\n    CREATE TABLE data_2_grouped AS\n    SELECt newid,\n           hard_drugs,\n           years,\n           CASE\n               WHEN max(CASE WHEN hard_drugs = 'Yes' AND years = 2 THEN 1 ELSE 0 END) &gt; 0 THEN 'Current User'\n               WHEN max(CASE WHEN hard_drugs = 'Yes' AND (years = 0 OR years = 1) THEN 1 ELSE 0 END) &gt; 0 AND \n                    max(case when hard_drugs = 'No' AND years = 2 THEN 1 ELSE 0 END) &gt; 0 THEN 'Previous User'\n               ELSE 'Never User'\n           END AS hard_drugs_grp\n    FROM PROJ2LMM.data\n    GROUP BY newid\n    ORDER BY newid, years;\nQUIT;\n    \n\n\n\nAppend to Original Data Set\n\n\n* Append to original data set;\nDATA Proj2LMM.Data;\n    MERGE Proj2LMM.Data(IN = A) data_2_grouped(IN = B);\n    BY newid;\n    IF A;\n    RUN;\n\n\n\nDouble Check Classification\n\n* Double check classification;\n* This matches the previous analysis in R;\nPROC TABULATE DATA = Proj2LMM.data;\n    CLASS hard_drugs_grp;\n    TABLE hard_drugs_grp;\n    RUN;\n\nlstlog\n\n\n&lt;!DOCTYPE html&gt;\n\n\n\n\n\nSAS Output\n\n\n\n\n\n\n\nThe SAS System\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nhard_drugs_grp\n\n\n\n\nCurrent User\n\n\nNever User\n\n\nPrevious User\n\n\n\n\nN\n\n\nN\n\n\nN\n\n\n\n\n\n\n180\n\n\n1332\n\n\n138\n\n\n\n\n\n\n\n\n\n\n\n\u001417                                                         The SAS System                    Saturday, December 28, 2024 08:34:00 PM\n\n239        ods listing close;ods html5 (id=saspy_internal) file=_tomods1 options(bitmap_mode='inline') device=svg style=HTMLBlue;\n239      ! ods graphics on / outputfmt=png;\n240        \n241        * Double check classification;\n242        * This matches the previous analysis in R;\n243        PROC TABULATE DATA = Proj2LMM.data;\n244         CLASS hard_drugs_grp;\n245         TABLE hard_drugs_grp;\n246         RUN;\n247        \n248        \n249        ods html5 (id=saspy_internal) close;ods listing;\n250        \n\u001418                                                         The SAS System                    Saturday, December 28, 2024 08:34:00 PM\n\n251"
  },
  {
    "objectID": "Project_2/Project_2_SAS/Code/Linear_Mixed_Model_SAS.html#adherence-high-vs.-low",
    "href": "Project_2/Project_2_SAS/Code/Linear_Mixed_Model_SAS.html#adherence-high-vs.-low",
    "title": "Linear Mixed Model (SAS)",
    "section": "Adherence High vs. Low",
    "text": "Adherence High vs. Low\n\nHigh Adherence: &gt;=95% Adherence to protocol\nLow Adherence: &lt; 95 Adherence to protocol\n\n\nCreate Adherence Group\n\n* Create adherence group;\nPROC SQL;\n    CREATE TABLE data_2_grouped AS\n    SELECT newid,\n           ADH,\n           years,\n           CASE\n               WHEN max(case when years = 2 AND (ADH = '100%' or ADH = '95-99%') THEN 1 ELSE 0 END) = 1 THEN 'High Adherence' \n               ELSE 'Low Adherence'\n           END AS ADH_HIGHVSLOW\n    FROM PROJ2LMM.data\n    GROUP BY newid\n    ORDER BY newid, years;\nQUIT;\n\n\n\nAppend to Original Data Set\n\n* Append to original data set;\nDATA Proj2LMM.Data;\n    MERGE Proj2LMM.Data(IN = A) data_2_grouped(IN = B);\n    BY newid;\n    IF A;\n    RUN;\n\n\n\nDouble Check Classification\n\n* Double check classification;\n* This matches the previous analysis in R;\nPROC TABULATE DATA = Proj2LMM.data;\n    CLASS ADH_HIGHVSLOW;\n    TABLE ADH_HIGHVSLOW;\n    RUN;\n\nlstlog\n\n\n&lt;!DOCTYPE html&gt;\n\n\n\n\n\nSAS Output\n\n\n\n\n\n\n\nThe SAS System\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nADH_HIGHVSLOW\n\n\n\n\nHigh Adherence\n\n\nLow Adherence\n\n\n\n\nN\n\n\nN\n\n\n\n\n\n\n1485\n\n\n165\n\n\n\n\n\n\n\n\n\n\n\n\u001423                                                         The SAS System                    Saturday, December 28, 2024 08:34:00 PM\n\n292        ods listing close;ods html5 (id=saspy_internal) file=_tomods1 options(bitmap_mode='inline') device=svg style=HTMLBlue;\n292      ! ods graphics on / outputfmt=png;\n293        \n294        * Double check classification;\n295        * This matches the previous analysis in R;\n296        PROC TABULATE DATA = Proj2LMM.data;\n297         CLASS ADH_HIGHVSLOW;\n298         TABLE ADH_HIGHVSLOW;\n299         RUN;\n300        \n301        \n302        ods html5 (id=saspy_internal) close;ods listing;\n303        \n\u001424                                                         The SAS System                    Saturday, December 28, 2024 08:34:00 PM\n\n304"
  },
  {
    "objectID": "Project_2/Project_2_SAS/Code/Linear_Mixed_Model_SAS.html#create-college-education-variable",
    "href": "Project_2/Project_2_SAS/Code/Linear_Mixed_Model_SAS.html#create-college-education-variable",
    "title": "Linear Mixed Model (SAS)",
    "section": "Create College Education Variable",
    "text": "Create College Education Variable\n\nCreate College Level\n\n* Create college level variable;\nPROC SQL;\n    CREATE TABLE data_2_grouped AS\n    SELECT newid,\n           EDUCBAS,\n           years,\n           CASE\n               WHEN max(case when years = 2 AND (EDUCBAS IN (\"At least one year college but no degree\",\n                                                             \"Four years college or got degree\",\n                                                             \"Some graduate work\",\n                                                             \"Post-graduate degree\"))\n                                   THEN 1 ELSE 0 END) = 1 \n               THEN 'College' \n               ELSE 'No College'\n           END AS EDUC_COLLEGE\n    FROM PROJ2LMM.data\n    GROUP BY newid\n    ORDER BY newid, years;\nQUIT;\n\n\n\nAppend to Original Data Set\n\n* Append to original data set;\nDATA Proj2LMM.Data;\n    MERGE Proj2LMM.Data(IN = A) data_2_grouped(IN = B);\n    BY newid;\n    IF A;\n    RUN;\n\n\n\nDouble Check Classification\n\n* Double check classification;\n* This matches the previous analysis in R;\nPROC TABULATE DATA = Proj2LMM.data;\n    CLASS EDUC_COLLEGE;\n    TABLE EDUC_COLLEGE;\n    RUN;\n\nlstlog\n\n\n&lt;!DOCTYPE html&gt;\n\n\n\n\n\nSAS Output\n\n\n\n\n\n\n\nThe SAS System\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEDUC_COLLEGE\n\n\n\n\nCollege\n\n\nNo College\n\n\n\n\nN\n\n\nN\n\n\n\n\n\n\n1275\n\n\n375\n\n\n\n\n\n\n\n\n\n\n\n\u001429                                                         The SAS System                    Saturday, December 28, 2024 08:34:00 PM\n\n350        ods listing close;ods html5 (id=saspy_internal) file=_tomods1 options(bitmap_mode='inline') device=svg style=HTMLBlue;\n350      ! ods graphics on / outputfmt=png;\n351        \n352        * Double check classification;\n353        * This matches the previous analysis in R;\n354        PROC TABULATE DATA = Proj2LMM.data;\n355         CLASS EDUC_COLLEGE;\n356         TABLE EDUC_COLLEGE;\n357         RUN;\n358        \n359        \n360        ods html5 (id=saspy_internal) close;ods listing;\n361        \n\u001430                                                         The SAS System                    Saturday, December 28, 2024 08:34:00 PM\n\n362"
  },
  {
    "objectID": "Project_2/Project_2_SAS/Code/Linear_Mixed_Model_SAS.html#bivariate-relationships",
    "href": "Project_2/Project_2_SAS/Code/Linear_Mixed_Model_SAS.html#bivariate-relationships",
    "title": "Linear Mixed Model (SAS)",
    "section": "Bivariate Relationships",
    "text": "Bivariate Relationships\nHere we will plot the average of each outcome group by our PEVs.\nWe will use SQL to calculate the averages across years (similiar to how we use the tidyverse in R)."
  },
  {
    "objectID": "Project_2/Project_2_SAS/Code/Linear_Mixed_Model_SAS.html#Main_Outcomes",
    "href": "Project_2/Project_2_SAS/Code/Linear_Mixed_Model_SAS.html#Main_Outcomes",
    "title": "Linear Mixed Model (SAS)",
    "section": "Main Outcome Variables",
    "text": "Main Outcome Variables\n\nViral LoadCD4+ T Cell CountMental QOLPhysical QOLSummary\n\n\n\nBy Hard Drug Use\n\nCalculate Average Viral Load for Each Year\n\n*** By hard drug use and year;\nPROC SQL;\n    CREATE TABLE summary_data as\n    SELECT hard_drugs_grp,\n           years,\n           mean(VLOAD_log) as avg_VLOAD_log\n    FROM Proj2LMM.data\n    GROUP BY hard_drugs_grp, years;\n    QUIT;\n\n\n\nCreate Plot\n\n    \n* Plot;\nPROC SGPLOT DATA = summary_data;\n    SERIES x = years y = avg_VLOAD_log / GROUP = hard_drugs_grp;\n    XAXIS LABEL = \"Years\";\n    YAXIS LABEL = \"Average Viral Load\";\n    KEYLEGEND / POSITION = right ACROSS = 1 LOCATION = outside;\n    TITLE \"Average Viral Load by Year and Hard Drug Use Group\";\n    RUN;\n\n\nlstlog\n\n\n&lt;!DOCTYPE html&gt;\n\n\n\n\n\nSAS Output\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001433                                                         The SAS System                    Saturday, December 28, 2024 08:34:00 PM\n\n383        ods listing close;ods html5 (id=saspy_internal) file=_tomods1 options(bitmap_mode='inline') device=svg style=HTMLBlue;\n383      ! ods graphics on / outputfmt=png;\n384        \n385         \n386        * Plot;\n387        PROC SGPLOT DATA = summary_data;\n388         SERIES x = years y = avg_VLOAD_log / GROUP = hard_drugs_grp;\n389         XAXIS LABEL = \"Years\";\n390         YAXIS LABEL = \"Average Viral Load\";\n391         KEYLEGEND / POSITION = right ACROSS = 1 LOCATION = outside;\n392         TITLE \"Average Viral Load by Year and Hard Drug Use Group\";\n393         RUN;\n394        \n395        \n396        \n397        ods html5 (id=saspy_internal) close;ods listing;\n398        \n\u001434                                                         The SAS System                    Saturday, December 28, 2024 08:34:00 PM\n\n399        \n\n\n\n\n\n\n\nBy Adherence\n\nCalculate Average Viral Load for Each Year\n\n* Calculate Averages by Adherence;\nPROC SQL;\n    CREATE TABLE summary_data as\n    SELECT ADH_HIGHVSLOW,\n           years,\n           mean(VLOAD_log) as avg_VLOAD_log\n    FROM Proj2LMM.data\n    GROUP BY ADH_HIGHVSLOW, years;\n    QUIT;\n\n\n\nCreate Plot\n\n* Plot;\nPROC SGPLOT DATA = summary_data;\n    SERIES x = years y = avg_VLOAD_log / GROUP = ADH_HIGHVSLOW;\n    XAXIS LABEL = \"Years\";\n    YAXIS LABEL = \"Average Viral Load\";\n    KEYLEGEND / POSITION = right ACROSS = 1 LOCATION = outside;\n    TITLE \"Average Viral Load by Year and Adherence\";\n    RUN;\n\nlstlog\n\n\n&lt;!DOCTYPE html&gt;\n\n\n\n\n\nSAS Output\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001437                                                         The SAS System                    Saturday, December 28, 2024 08:34:00 PM\n\n420        ods listing close;ods html5 (id=saspy_internal) file=_tomods1 options(bitmap_mode='inline') device=svg style=HTMLBlue;\n420      ! ods graphics on / outputfmt=png;\n421        \n422        * Plot;\n423        PROC SGPLOT DATA = summary_data;\n424         SERIES x = years y = avg_VLOAD_log / GROUP = ADH_HIGHVSLOW;\n425         XAXIS LABEL = \"Years\";\n426         YAXIS LABEL = \"Average Viral Load\";\n427         KEYLEGEND / POSITION = right ACROSS = 1 LOCATION = outside;\n428         TITLE \"Average Viral Load by Year and Adherence\";\n429         RUN;\n430        \n431        \n432        ods html5 (id=saspy_internal) close;ods listing;\n433        \n\u001438                                                         The SAS System                    Saturday, December 28, 2024 08:34:00 PM\n\n434        \n\n\n\n\nTop of Tabset\n\n\n\n\n\nBy Hard Drug Use\n\nCalculate Average CD4+ T Cell Count for Each Year\n\n*** By hard drug use;\nPROC SQL;\n    CREATE TABLE summary_data as\n    SELECT hard_drugs_grp,\n           years,\n           mean(LEU3N) as avg_LEU3N\n    FROM Proj2LMM.data\n    GROUP BY hard_drugs_grp, years;\n    QUIT;\n\n\n\nCreate Plot\n\n* Plot;\nPROC SGPLOT DATA = summary_data;\n    SERIES x = years y = avg_LEU3N / GROUP = hard_drugs_grp;\n    XAXIS LABEL = \"Years\";\n    YAXIS LABEL = \"Average CD4+ T Cell Count\";\n    KEYLEGEND / POSITION = right ACROSS = 1 LOCATION = outside;\n    TITLE \"Average CD4+ T Cell Count by Year and Hard Drug Use Group\";\n    RUN;\n\nlstlog\n\n\n&lt;!DOCTYPE html&gt;\n\n\n\n\n\nSAS Output\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001441                                                         The SAS System                    Saturday, December 28, 2024 08:34:00 PM\n\n455        ods listing close;ods html5 (id=saspy_internal) file=_tomods1 options(bitmap_mode='inline') device=svg style=HTMLBlue;\n455      ! ods graphics on / outputfmt=png;\n456        \n457        * Plot;\n458        PROC SGPLOT DATA = summary_data;\n459         SERIES x = years y = avg_LEU3N / GROUP = hard_drugs_grp;\n460         XAXIS LABEL = \"Years\";\n461         YAXIS LABEL = \"Average CD4+ T Cell Count\";\n462         KEYLEGEND / POSITION = right ACROSS = 1 LOCATION = outside;\n463         TITLE \"Average CD4+ T Cell Count by Year and Hard Drug Use Group\";\n464         RUN;\n465        \n466        \n467        ods html5 (id=saspy_internal) close;ods listing;\n468        \n\u001442                                                         The SAS System                    Saturday, December 28, 2024 08:34:00 PM\n\n469        \n\n\n\n\n\n\n\nBy Adherence\n\nCalculate Average CD4+ T Cell Count for Each Year\n\n* Calculate averages for each year;\nPROC SQL;\n    CREATE TABLE summary_data as\n    SELECT ADH_HIGHVSLOW,\n           years,\n           mean(LEU3N) as avg_LEU3N\n    FROM Proj2LMM.data\n    GROUP BY ADH_HIGHVSLOW, years;\n    QUIT;\n\n\n\nCreate Plot\n\n* Plot;\nPROC SGPLOT DATA = summary_data;\n    SERIES x = years y = avg_LEU3N / GROUP = ADH_HIGHVSLOW;\n    XAXIS LABEL = \"Years\";\n    YAXIS LABEL = \"Average CD4+ T Cell Count\";\n    KEYLEGEND / POSITION = right ACROSS = 1 LOCATION = outside;\n    TITLE \"Average CD4+ T Cell Count by Year and Adherence\";\n    RUN;\n\nlstlog\n\n\n&lt;!DOCTYPE html&gt;\n\n\n\n\n\nSAS Output\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001445                                                         The SAS System                    Saturday, December 28, 2024 08:34:00 PM\n\n490        ods listing close;ods html5 (id=saspy_internal) file=_tomods1 options(bitmap_mode='inline') device=svg style=HTMLBlue;\n490      ! ods graphics on / outputfmt=png;\n491        \n492        * Plot;\n493        PROC SGPLOT DATA = summary_data;\n494         SERIES x = years y = avg_LEU3N / GROUP = ADH_HIGHVSLOW;\n495         XAXIS LABEL = \"Years\";\n496         YAXIS LABEL = \"Average CD4+ T Cell Count\";\n497         KEYLEGEND / POSITION = right ACROSS = 1 LOCATION = outside;\n498         TITLE \"Average CD4+ T Cell Count by Year and Adherence\";\n499         RUN;\n500        \n501        \n502        ods html5 (id=saspy_internal) close;ods listing;\n503        \n\u001446                                                         The SAS System                    Saturday, December 28, 2024 08:34:00 PM\n\n504        \n\n\n\n\nTop of Tabset\n\n\n\n\n\nBy Hard Drug Use\n\nCalculate Average Mental QOL Each Year\n\n* Calculate the average AGG_MENT for each hard_drugs_grp and year;\nPROC SQL;\n    CREATE TABLE summary_data as\n    SELECT hard_drugs_grp,\n           years,\n           mean(AGG_MENT) as avg_AGG_MENT\n    FROM Proj2LMM.data\n    GROUP BY hard_drugs_grp, years;\n    QUIT;\n\n\n\nCreate Plot\n\n* Plot;\nPROC SGPLOT DATA = summary_data;\n    SERIES x = years y = avg_AGG_MENT / GROUP = hard_drugs_grp;\n    XAXIS LABEL = \"Years\";\n    YAXIS LABEL = \"Average Mental QOL\";\n    KEYLEGEND / POSITION = right ACROSS = 1 LOCATION = outside;\n    TITLE \"Average Mental Quality of Life by Year and Hard Drug Use Group\";\n    RUN;\n\nlstlog\n\n\n&lt;!DOCTYPE html&gt;\n\n\n\n\n\nSAS Output\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001449                                                         The SAS System                    Saturday, December 28, 2024 08:34:00 PM\n\n525        ods listing close;ods html5 (id=saspy_internal) file=_tomods1 options(bitmap_mode='inline') device=svg style=HTMLBlue;\n525      ! ods graphics on / outputfmt=png;\n526        \n527        * Plot;\n528        PROC SGPLOT DATA = summary_data;\n529         SERIES x = years y = avg_AGG_MENT / GROUP = hard_drugs_grp;\n530         XAXIS LABEL = \"Years\";\n531         YAXIS LABEL = \"Average Mental QOL\";\n532         KEYLEGEND / POSITION = right ACROSS = 1 LOCATION = outside;\n533         TITLE \"Average Mental Quality of Life by Year and Hard Drug Use Group\";\n534         RUN;\n535        \n536        \n537        ods html5 (id=saspy_internal) close;ods listing;\n538        \n\u001450                                                         The SAS System                    Saturday, December 28, 2024 08:34:00 PM\n\n539        \n\n\n\n\n\n\n\nBy Adherence\n\nCalculate Average Mental QOL Each Year\n\n* Calculate the average for each year;\nPROC SQL;\n    CREATE TABLE summary_data AS\n    SELECT ADH_HIGHVSLOW,\n           years,\n           mean(AGG_MENT) AS avg_AGG_MENT\n    FROM Proj2LMM.data\n    GROUP BY ADH_HIGHVSLOW, years;\n    QUIT;\n\n\n\nCreate Plot\n\n* Plot;\nPROC SGPLOT DATA = summary_data;\n    SERIES x = years y = avg_AGG_MENT / GROUP = ADH_HIGHVSLOW;\n    XAXIS LABEL = \"Years\";\n    YAXIS LABEL = \"Average Mental QOL\";\n    KEYLEGEND / POSITION = right ACROSS = 1 LOCATION = outside;\n    TITLE \"Average Mental Quality of Life by Year and Adherence\";\n    RUN;\n\nlstlog\n\n\n&lt;!DOCTYPE html&gt;\n\n\n\n\n\nSAS Output\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001453                                                         The SAS System                    Saturday, December 28, 2024 08:34:00 PM\n\n560        ods listing close;ods html5 (id=saspy_internal) file=_tomods1 options(bitmap_mode='inline') device=svg style=HTMLBlue;\n560      ! ods graphics on / outputfmt=png;\n561        \n562        * Plot;\n563        PROC SGPLOT DATA = summary_data;\n564         SERIES x = years y = avg_AGG_MENT / GROUP = ADH_HIGHVSLOW;\n565         XAXIS LABEL = \"Years\";\n566         YAXIS LABEL = \"Average Mental QOL\";\n567         KEYLEGEND / POSITION = right ACROSS = 1 LOCATION = outside;\n568         TITLE \"Average Mental Quality of Life by Year and Adherence\";\n569         RUN;\n570        \n571        \n572        ods html5 (id=saspy_internal) close;ods listing;\n573        \n\u001454                                                         The SAS System                    Saturday, December 28, 2024 08:34:00 PM\n\n574        \n\n\n\n\nTop of Tabset\n\n\n\n\n\nBy Hard Drug Use\n\nCalculate Average Physical QOL Each Year\n\n* Calculate the average AGG_PHYS for each hard_drugs_grp and year;\nPROC SQL;\n    CREATE TABLE summary_data as\n    SELECT hard_drugs_grp,\n           years,\n           mean(AGG_PHYS) as avg_AGG_PHYS\n    FROM Proj2LMM.data\n    GROUP BY hard_drugs_grp, years;\n    QUIT;\n\n\n\nCreate Plot\n\n* Plot;\nPROC SGPLOT DATA = summary_data;\n    SERIES x = years y = avg_AGG_PHYS / GROUP = hard_drugs_grp;\n    XAXIS LABEL = \"Years\";\n    YAXIS LABEL = \"Average Physical QOL\";\n    KEYLEGEND / POSITION = right ACROSS = 1 LOCATION = outside;\n    TITLE \"Average Physical Quality of Life by Year and Hard Drug Use Group\";\n    RUN;\n\nlstlog\n\n\n&lt;!DOCTYPE html&gt;\n\n\n\n\n\nSAS Output\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001457                                                         The SAS System                    Saturday, December 28, 2024 08:34:00 PM\n\n595        ods listing close;ods html5 (id=saspy_internal) file=_tomods1 options(bitmap_mode='inline') device=svg style=HTMLBlue;\n595      ! ods graphics on / outputfmt=png;\n596        \n597        * Plot;\n598        PROC SGPLOT DATA = summary_data;\n599         SERIES x = years y = avg_AGG_PHYS / GROUP = hard_drugs_grp;\n600         XAXIS LABEL = \"Years\";\n601         YAXIS LABEL = \"Average Physical QOL\";\n602         KEYLEGEND / POSITION = right ACROSS = 1 LOCATION = outside;\n603         TITLE \"Average Physical Quality of Life by Year and Hard Drug Use Group\";\n604         RUN;\n605        \n606        \n607        ods html5 (id=saspy_internal) close;ods listing;\n608        \n\u001458                                                         The SAS System                    Saturday, December 28, 2024 08:34:00 PM\n\n609        \n\n\n\n\n\n\n\nBy Adherence\n\nCalculate Average Physical QOL Each Year\n\n* Calculate the average for each year;\nPROC SQL;\n    CREATE TABLE summary_data AS\n    SELECT ADH_HIGHVSLOW,\n           years,\n           mean(AGG_PHYS) AS avg_AGG_PHYS\n    FROM Proj2LMM.data\n    GROUP BY ADH_HIGHVSLOW, years;\n    QUIT;\n\n\n\nCreate Plot\n\n* Plot;\nPROC SGPLOT DATA = summary_data;\n    SERIES x = years y = avg_AGG_PHYS / GROUP = ADH_HIGHVSLOW;\n    XAXIS LABEL = \"Years\";\n    YAXIS LABEL = \"Average Physical QOL\";\n    KEYLEGEND / POSITION = right ACROSS = 1 LOCATION = outside;\n    TITLE \"Average Physical Quality of Life by Year and Adherence\";\n    RUN;\n\nlstlog\n\n\n&lt;!DOCTYPE html&gt;\n\n\n\n\n\nSAS Output\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001461                                                         The SAS System                    Saturday, December 28, 2024 08:34:00 PM\n\n630        ods listing close;ods html5 (id=saspy_internal) file=_tomods1 options(bitmap_mode='inline') device=svg style=HTMLBlue;\n630      ! ods graphics on / outputfmt=png;\n631        \n632        * Plot;\n633        PROC SGPLOT DATA = summary_data;\n634         SERIES x = years y = avg_AGG_PHYS / GROUP = ADH_HIGHVSLOW;\n635         XAXIS LABEL = \"Years\";\n636         YAXIS LABEL = \"Average Physical QOL\";\n637         KEYLEGEND / POSITION = right ACROSS = 1 LOCATION = outside;\n638         TITLE \"Average Physical Quality of Life by Year and Adherence\";\n639         RUN;\n640        \n641        \n642        ods html5 (id=saspy_internal) close;ods listing;\n643        \n\u001462                                                         The SAS System                    Saturday, December 28, 2024 08:34:00 PM\n\n644        \n\n\n\n\nTop of Tabset\n\n\n\n\nFor hard drug use group, previous users have:\n\nThe lowest average log viral load\nThe lowest leukocyte count, and highest mental and physical QOL.\n\nFor adherence, those with low adherence have:\n\nThe highest average log viral load\nThe lowest leukocyte count, and lowest mental and physical QOL.\n\nTop of Tabset"
  },
  {
    "objectID": "Project_2/Project_2_SAS/Code/Linear_Mixed_Model_SAS.html#Covariates",
    "href": "Project_2/Project_2_SAS/Code/Linear_Mixed_Model_SAS.html#Covariates",
    "title": "Linear Mixed Model (SAS)",
    "section": "Covariates",
    "text": "Covariates\n\nDepressionFrailty Related PhenotypeCollege Education\n\n\nDepression was a confounder in the model for mental QOL and must be accounted for.\n\nBy Hard Drugs\n\nCalculate Average Depression Scores for Each Year\n\n* Calculate the average for each year;\nPROC SQL;\n    CREATE TABLE summary_data AS\n    SELECT hard_drugs_grp,\n           years,\n           mean(CESD) AS avg_CESD\n    FROM Proj2LMM.data\n    GROUP BY hard_drugs_grp, years;\n    QUIT;\n\n\n\nCreate Plot\n\n* Plot;\nPROC SGPLOT DATA = summary_data;\n    SERIES x = years y = avg_CESD / GROUP = hard_drugs_grp;\n    XAXIS LABEL = \"Years\";\n    YAXIS LABEL = \"Average Depresion Score\";\n    KEYLEGEND / POSITION = right ACROSS = 1 LOCATION = outside;\n    TITLE \"Average Depression Score by Year and Hard Drugs Use\";\n    RUN;\n\nlstlog\n\n\n&lt;!DOCTYPE html&gt;\n\n\n\n\n\nSAS Output\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001465                                                         The SAS System                    Saturday, December 28, 2024 08:34:00 PM\n\n665        ods listing close;ods html5 (id=saspy_internal) file=_tomods1 options(bitmap_mode='inline') device=svg style=HTMLBlue;\n665      ! ods graphics on / outputfmt=png;\n666        \n667        * Plot;\n668        PROC SGPLOT DATA = summary_data;\n669         SERIES x = years y = avg_CESD / GROUP = hard_drugs_grp;\n670         XAXIS LABEL = \"Years\";\n671         YAXIS LABEL = \"Average Depresion Score\";\n672         KEYLEGEND / POSITION = right ACROSS = 1 LOCATION = outside;\n673         TITLE \"Average Depression Score by Year and Hard Drugs Use\";\n674         RUN;\n675        \n676        \n677        ods html5 (id=saspy_internal) close;ods listing;\n678        \n\u001466                                                         The SAS System                    Saturday, December 28, 2024 08:34:00 PM\n\n679        \n\n\n\n\n\n\n\nBy Adherence\n\nCalculate Average Depression Score for Each Year\n\n* Calculate the average for each year;\nPROC SQL;\n    CREATE TABLE summary_data AS\n    SELECT ADH_HIGHVSLOW,\n           years,\n           mean(CESD) AS avg_CESD\n    FROM Proj2LMM.data\n    GROUP BY ADH_HIGHVSLOW, years;\n    QUIT;\n\n\n\nCreate Plot\n\nPROC SGPLOT DATA = summary_data;\n    SERIES x = years y = avg_CESD / GROUP = ADH_HIGHVSLOW;\n    XAXIS LABEL = \"Years\";\n    YAXIS LABEL = \"Average Depresion Score\";\n    KEYLEGEND / POSITION = right ACROSS = 1 LOCATION = outside;\n    TITLE \"Average Depression Score by Year and Adherence\";\n    RUN;\n\nlstlog\n\n\n&lt;!DOCTYPE html&gt;\n\n\n\n\n\nSAS Output\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001469                                                         The SAS System                    Saturday, December 28, 2024 08:34:00 PM\n\n700        ods listing close;ods html5 (id=saspy_internal) file=_tomods1 options(bitmap_mode='inline') device=svg style=HTMLBlue;\n700      ! ods graphics on / outputfmt=png;\n701        \n702        PROC SGPLOT DATA = summary_data;\n703         SERIES x = years y = avg_CESD / GROUP = ADH_HIGHVSLOW;\n704         XAXIS LABEL = \"Years\";\n705         YAXIS LABEL = \"Average Depresion Score\";\n706         KEYLEGEND / POSITION = right ACROSS = 1 LOCATION = outside;\n707         TITLE \"Average Depression Score by Year and Adherence\";\n708         RUN;\n709        \n710        \n711        ods html5 (id=saspy_internal) close;ods listing;\n712        \n\u001470                                                         The SAS System                    Saturday, December 28, 2024 08:34:00 PM\n\n713        \n\n\n\n\nTop of Tabset\n\n\n\n\nFrailty Related Phenotype is a precision variable for physical QOL, and was previously significant for CD4+ T Cell Count.\n\nCD4+ T Cell Count\n\nCalculate Averages for Each Year\n\n*** CD4+ T Cell Count;\n* Calculate the average for each year;\nPROC SQL;\n    CREATE TABLE summary_data AS\n    SELECT FRP,\n           years,\n           mean(LEU3N) AS avg_LEU3N\n    FROM Proj2LMM.data\n    WHERE FRP IS NOT MISSING\n    GROUP BY FRP, years;\n    QUIT;\n\n\n\nCreate Plot\n\n* Plot;\nPROC SGPLOT DATA = summary_data;\n    SERIES x = years y = avg_LEU3N / GROUP = frp;\n    XAXIS LABEL = \"Years\";\n    YAXIS LABEL = \"Average CD4+ T Cell Count\";\n    KEYLEGEND / POSITION = right ACROSS = 1 LOCATION = outside;\n    TITLE \"Average CD4+ T Cell Count by Year and Frailty Related Phenotype\";\n    RUN;\n\nlstlog\n\n\n&lt;!DOCTYPE html&gt;\n\n\n\n\n\nSAS Output\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001473                                                         The SAS System                    Saturday, December 28, 2024 08:34:00 PM\n\n736        ods listing close;ods html5 (id=saspy_internal) file=_tomods1 options(bitmap_mode='inline') device=svg style=HTMLBlue;\n736      ! ods graphics on / outputfmt=png;\n737        \n738        * Plot;\n739        PROC SGPLOT DATA = summary_data;\n740         SERIES x = years y = avg_LEU3N / GROUP = frp;\n741         XAXIS LABEL = \"Years\";\n742         YAXIS LABEL = \"Average CD4+ T Cell Count\";\n743         KEYLEGEND / POSITION = right ACROSS = 1 LOCATION = outside;\n744         TITLE \"Average CD4+ T Cell Count by Year and Frailty Related Phenotype\";\n745         RUN;\n746        \n747        \n748        ods html5 (id=saspy_internal) close;ods listing;\n749        \n\u001474                                                         The SAS System                    Saturday, December 28, 2024 08:34:00 PM\n\n750        \n\n\n\n\n\n\n\nPhysical QOL\n\n*** Physical QOL;\nPROC SQL;\n    CREATE TABLE summary_data AS\n    SELECT FRP,\n           years,\n           mean(AGG_PHYS) AS avg_AGG_PHYS\n    FROM Proj2LMM.data\n    WHERE FRP IS NOT MISSING\n    GROUP BY FRP, years;\n    QUIT;\n\n\nCreate Plot\n\n* Plot;\nPROC SGPLOT DATA = summary_data;\n    SERIES x = years y = avg_AGG_PHYS / GROUP = frp;\n    XAXIS LABEL = \"Years\";\n    YAXIS LABEL = \"Average Physical QOL\";\n    KEYLEGEND / POSITION = right ACROSS = 1 LOCATION = outside;\n    TITLE \"Average Physical QOL by Year and Frailty Related Phenotype\";\n    RUN;\n\nlstlog\n\n\n&lt;!DOCTYPE html&gt;\n\n\n\n\n\nSAS Output\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001477                                                         The SAS System                    Saturday, December 28, 2024 08:34:00 PM\n\n772        ods listing close;ods html5 (id=saspy_internal) file=_tomods1 options(bitmap_mode='inline') device=svg style=HTMLBlue;\n772      ! ods graphics on / outputfmt=png;\n773        \n774        * Plot;\n775        PROC SGPLOT DATA = summary_data;\n776         SERIES x = years y = avg_AGG_PHYS / GROUP = frp;\n777         XAXIS LABEL = \"Years\";\n778         YAXIS LABEL = \"Average Physical QOL\";\n779         KEYLEGEND / POSITION = right ACROSS = 1 LOCATION = outside;\n780         TITLE \"Average Physical QOL by Year and Frailty Related Phenotype\";\n781         RUN;\n782        \n783        \n784        ods html5 (id=saspy_internal) close;ods listing;\n785        \n\u001478                                                         The SAS System                    Saturday, December 28, 2024 08:34:00 PM\n\n786        \n\n\n\n\nTop of Tabset\n\n\n\n\nThis was significant for the model with VLOAD_log. Let’s assess.\n\nViral Load\n\nCalculate Average Viral Load\n\n*** Viral Load;\nPROC SQL;\n    CREATE TABLE summary_data AS\n    SELECT EDUC_COLLEGE,\n           years,\n           mean(VLOAD_log) AS avg_VLOAD_log\n    FROM Proj2LMM.data\n    GROUP BY EDUC_COLLEGE, years;\n    QUIT;\n\n\n\nCreate Plot\n\n* Plot;\nPROC SGPLOT DATA = summary_data;\n    SERIES x = years y = avg_VLOAD_log / GROUP = EDUC_COLLEGE;\n    XAXIS LABEL = \"Years\";\n    YAXIS LABEL = \"Average Viral Load\";\n    KEYLEGEND / POSITION = right ACROSS = 1 LOCATION = outside;\n    TITLE \"Average Viral Load by Year and College Education\";\n    RUN;\n\nlstlog\n\n\n&lt;!DOCTYPE html&gt;\n\n\n\n\n\nSAS Output\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001481                                                         The SAS System                    Saturday, December 28, 2024 08:34:00 PM\n\n807        ods listing close;ods html5 (id=saspy_internal) file=_tomods1 options(bitmap_mode='inline') device=svg style=HTMLBlue;\n807      ! ods graphics on / outputfmt=png;\n808        \n809        * Plot;\n810        PROC SGPLOT DATA = summary_data;\n811         SERIES x = years y = avg_VLOAD_log / GROUP = EDUC_COLLEGE;\n812         XAXIS LABEL = \"Years\";\n813         YAXIS LABEL = \"Average Viral Load\";\n814         KEYLEGEND / POSITION = right ACROSS = 1 LOCATION = outside;\n815         TITLE \"Average Viral Load by Year and College Education\";\n816         RUN;\n817        \n818        \n819        ods html5 (id=saspy_internal) close;ods listing;\n820        \n\u001482                                                         The SAS System                    Saturday, December 28, 2024 08:34:00 PM\n\n821        \n\n\n\n\n\n\nCalculate Average CD4+ T Cell Count\n\n*** CD4+ T Cell Count;\nPROC SQL;\n    CREATE TABLE summary_data AS\n    SELECT EDUC_COLLEGE,\n           years,\n           mean(LEU3N) AS avg_LEU3N\n    FROM Proj2LMM.data\n    GROUP BY EDUC_COLLEGE, years;\n    QUIT;\n\n\n\nCreate Plot\n\n* Plot;\nPROC SGPLOT DATA = summary_data;\n    SERIES x = years y = avg_LEU3N / GROUP = EDUC_COLLEGE;\n    XAXIS LABEL = \"Years\";\n    YAXIS LABEL = \"Average CD4+ T Cell Count\";\n    KEYLEGEND / POSITION = right ACROSS = 1 LOCATION = outside;\n    TITLE \"Average CD4+ T Cell Count by Year and College Education\";\n    RUN;\n\nlstlog\n\n\n&lt;!DOCTYPE html&gt;\n\n\n\n\n\nSAS Output\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001485                                                         The SAS System                    Saturday, December 28, 2024 08:34:00 PM\n\n842        ods listing close;ods html5 (id=saspy_internal) file=_tomods1 options(bitmap_mode='inline') device=svg style=HTMLBlue;\n842      ! ods graphics on / outputfmt=png;\n843        \n844        * Plot;\n845        PROC SGPLOT DATA = summary_data;\n846         SERIES x = years y = avg_LEU3N / GROUP = EDUC_COLLEGE;\n847         XAXIS LABEL = \"Years\";\n848         YAXIS LABEL = \"Average CD4+ T Cell Count\";\n849         KEYLEGEND / POSITION = right ACROSS = 1 LOCATION = outside;\n850         TITLE \"Average CD4+ T Cell Count by Year and College Education\";\n851         RUN;\n852        \n853        \n854        ods html5 (id=saspy_internal) close;ods listing;\n855        \n\u001486                                                         The SAS System                    Saturday, December 28, 2024 08:34:00 PM\n\n856        \n\n\n\n\n\n\n\nMental Quality of Life\n\nCalculate Average Mental Quality of Life\n\n*** Mental QOL;\nPROC SQL;\n    CREATE TABLE summary_data AS\n    SELECT EDUC_COLLEGE,\n           years,\n           mean(AGG_MENT) AS avg_AGG_MENT\n    FROM Proj2LMM.data\n    GROUP BY EDUC_COLLEGE, years;\n    QUIT;\n\n\n\nCreate Plot\n\n* Plot;\nPROC SGPLOT DATA = summary_data;\n    SERIES x = years y = avg_AGG_MENT / GROUP = EDUC_COLLEGE;\n    XAXIS LABEL = \"Years\";\n    YAXIS LABEL = \"Average Mental QOL\";\n    KEYLEGEND / POSITION = right ACROSS = 1 LOCATION = outside;\n    TITLE \"Average Mental QOL by Year and College Education\";\n    RUN;\n\nlstlog\n\n\n&lt;!DOCTYPE html&gt;\n\n\n\n\n\nSAS Output\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001489                                                         The SAS System                    Saturday, December 28, 2024 08:34:00 PM\n\n877        ods listing close;ods html5 (id=saspy_internal) file=_tomods1 options(bitmap_mode='inline') device=svg style=HTMLBlue;\n877      ! ods graphics on / outputfmt=png;\n878        \n879        * Plot;\n880        PROC SGPLOT DATA = summary_data;\n881         SERIES x = years y = avg_AGG_MENT / GROUP = EDUC_COLLEGE;\n882         XAXIS LABEL = \"Years\";\n883         YAXIS LABEL = \"Average Mental QOL\";\n884         KEYLEGEND / POSITION = right ACROSS = 1 LOCATION = outside;\n885         TITLE \"Average Mental QOL by Year and College Education\";\n886         RUN;\n887        \n888        \n889        ods html5 (id=saspy_internal) close;ods listing;\n890        \n\u001490                                                         The SAS System                    Saturday, December 28, 2024 08:34:00 PM\n\n891        \n\n\n\n\n\n\n\nPhysical Quality of Life\n\nCalculate Average Quality of Life\n\n*** Physical QOL;\nPROC SQL;\n    CREATE TABLE summary_data AS\n    SELECT EDUC_COLLEGE,\n           years,\n           mean(AGG_PHYS) AS avg_AGG_PHYS\n    FROM Proj2LMM.data\n    GROUP BY EDUC_COLLEGE, years;\n    QUIT;\n\n\n\nCreate Plot\n\n* Plot;\nPROC SGPLOT DATA = summary_data;\n    SERIES x = years y = avg_AGG_PHYS / GROUP = EDUC_COLLEGE;\n    XAXIS LABEL = \"Years\";\n    YAXIS LABEL = \"Average Physical QOL\";\n    KEYLEGEND / POSITION = right ACROSS = 1 LOCATION = outside;\n    TITLE \"Average Physical QOL by Year and College Education\";\n    RUN;\n\nlstlog\n\n\n&lt;!DOCTYPE html&gt;\n\n\n\n\n\nSAS Output\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001493                                                         The SAS System                    Saturday, December 28, 2024 08:34:00 PM\n\n912        ods listing close;ods html5 (id=saspy_internal) file=_tomods1 options(bitmap_mode='inline') device=svg style=HTMLBlue;\n912      ! ods graphics on / outputfmt=png;\n913        \n914        * Plot;\n915        PROC SGPLOT DATA = summary_data;\n916         SERIES x = years y = avg_AGG_PHYS / GROUP = EDUC_COLLEGE;\n917         XAXIS LABEL = \"Years\";\n918         YAXIS LABEL = \"Average Physical QOL\";\n919         KEYLEGEND / POSITION = right ACROSS = 1 LOCATION = outside;\n920         TITLE \"Average Physical QOL by Year and College Education\";\n921         RUN;\n922        \n923        \n924        ods html5 (id=saspy_internal) close;ods listing;\n925        \n\u001494                                                         The SAS System                    Saturday, December 28, 2024 08:34:00 PM\n\n926        \n\n\n\n\nTop of Tabset"
  },
  {
    "objectID": "Project_2/Project_2_SAS/Code/Linear_Mixed_Model_SAS.html#VLOAD",
    "href": "Project_2/Project_2_SAS/Code/Linear_Mixed_Model_SAS.html#VLOAD",
    "title": "Linear Mixed Model (SAS)",
    "section": "Log Viral Load",
    "text": "Log Viral Load\nBased on the previous interactive model selection in Project 2, the final covariates for this model were hard_drugs_grp, ADH, and EDUC_COLLEGE.\n\nModel SelectionAnalysisInterpretationVisualization\n\n\nTo answer the researcher’s main question, we will run a model of hard_drugs_grp, ADH_HIGHVSLOW, years, and an interaction between hard_drugs_grp*years to see if the slopes differ based on hard drug usage.\nNote: An unstructured covariance matrix is not needed here since we are only using random intercepts.\n\nPerform Saturated Model\n\n*** Log Viral Load;\nPROC MIXED DATA = PROJ2LMM.DATA METHOD = REML;\n    CLASS hard_drugs_grp (REF='Never User') ADH_HIGHVSLOW;\n    MODEL VLOAD_log = hard_drugs_grp ADH_HIGHVSLOW years hard_drugs_grp*years / SOLUTION CL;\n    RANDOM INTERCEPT / SUBJECT = newid VCORR;\n    ODS OUTPUT FitStatistics = Model_full;\n  ODS SELECT SolutionF Tests3; * Output only these specific tables;\n    RUN;\n\nlstlog\n\n\n&lt;!DOCTYPE html&gt;\n\n\n\n\n\nSAS Output\n\n\n\n\n\n\n\nAverage Physical QOL by Year and College Education\n\n\n\n\nThe Mixed Procedure\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolution for Fixed Effects\n\n\n\n\nEffect\n\n\nhard_drugs_grp\n\n\nADH_HIGHVSLOW\n\n\nEstimate\n\n\nStandardError\n\n\nDF\n\n\nt Value\n\n\nPr &gt; |t|\n\n\nAlpha\n\n\nLower\n\n\nUpper\n\n\n\n\n\n\nIntercept\n\n\n \n\n\n \n\n\n10.2501\n\n\n0.3108\n\n\n537\n\n\n32.97\n\n\n&lt;.0001\n\n\n0.05\n\n\n9.6395\n\n\n10.8608\n\n\n\n\nhard_drugs_grp\n\n\nCurrent User\n\n\n \n\n\n-0.4207\n\n\n0.3838\n\n\n1057\n\n\n-1.10\n\n\n0.2733\n\n\n0.05\n\n\n-1.1739\n\n\n0.3325\n\n\n\n\nhard_drugs_grp\n\n\nPrevious User\n\n\n \n\n\n-0.6969\n\n\n0.4320\n\n\n1057\n\n\n-1.61\n\n\n0.1070\n\n\n0.05\n\n\n-1.5446\n\n\n0.1508\n\n\n\n\nhard_drugs_grp\n\n\nNever User\n\n\n \n\n\n0\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n\n\nADH_HIGHVSLOW\n\n\n \n\n\nHigh Adherence\n\n\n-0.4302\n\n\n0.3131\n\n\n1057\n\n\n-1.37\n\n\n0.1698\n\n\n0.05\n\n\n-1.0446\n\n\n0.1843\n\n\n\n\nADH_HIGHVSLOW\n\n\n \n\n\nLow Adherence\n\n\n0\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n\n\nyears\n\n\n \n\n\n \n\n\n-3.1379\n\n\n0.08277\n\n\n1057\n\n\n-37.91\n\n\n&lt;.0001\n\n\n0.05\n\n\n-3.3003\n\n\n-2.9755\n\n\n\n\nyears*hard_drugs_grp\n\n\nCurrent User\n\n\n \n\n\n0.3940\n\n\n0.2364\n\n\n1057\n\n\n1.67\n\n\n0.0958\n\n\n0.05\n\n\n-0.06982\n\n\n0.8578\n\n\n\n\nyears*hard_drugs_grp\n\n\nPrevious User\n\n\n \n\n\n0.4877\n\n\n0.2661\n\n\n1057\n\n\n1.83\n\n\n0.0671\n\n\n0.05\n\n\n-0.03438\n\n\n1.0098\n\n\n\n\nyears*hard_drugs_grp\n\n\nNever User\n\n\n \n\n\n0\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nType 3 Tests of Fixed Effects\n\n\n\n\nEffect\n\n\nNum DF\n\n\nDen DF\n\n\nF Value\n\n\nPr &gt; F\n\n\n\n\n\n\nhard_drugs_grp\n\n\n2\n\n\n1057\n\n\n1.73\n\n\n0.1774\n\n\n\n\nADH_HIGHVSLOW\n\n\n1\n\n\n1057\n\n\n1.89\n\n\n0.1698\n\n\n\n\nyears\n\n\n1\n\n\n1057\n\n\n607.53\n\n\n&lt;.0001\n\n\n\n\nyears*hard_drugs_grp\n\n\n2\n\n\n1057\n\n\n2.77\n\n\n0.0632\n\n\n\n\n\n\n\n\n\n\n\u001495                                                         The SAS System                    Saturday, December 28, 2024 08:34:00 PM\n\n929        ods listing close;ods html5 (id=saspy_internal) file=_tomods1 options(bitmap_mode='inline') device=svg style=HTMLBlue;\n929      ! ods graphics on / outputfmt=png;\n930        \n931        *** Log Viral Load;\n932        PROC MIXED DATA = PROJ2LMM.DATA METHOD = REML;\n933         CLASS hard_drugs_grp (REF='Never User') ADH_HIGHVSLOW;\n934         MODEL VLOAD_log = hard_drugs_grp ADH_HIGHVSLOW years hard_drugs_grp*years / SOLUTION CL;\n935         RANDOM INTERCEPT / SUBJECT = newid VCORR;\n936         ODS OUTPUT FitStatistics = Model_full;\n937          ODS SELECT SolutionF Tests3; * Output only these specific tables;\n938         RUN;\n939        \n940        \n941        ods html5 (id=saspy_internal) close;ods listing;\n942        \n\u001496                                                         The SAS System                    Saturday, December 28, 2024 08:34:00 PM\n\n943        \n\n\n\n\nThe interaction is not significant. Let’s re-run without it.\n\n\nRe-Run Model Without Interaction Term\n\nPROC MIXED DATA = PROJ2LMM.DATA METHOD = REML;\n    CLASS hard_drugs_grp (REF='Never User') ADH_HIGHVSLOW;\n    MODEL VLOAD_log = hard_drugs_grp ADH_HIGHVSLOW years / SOLUTION CL;\n    RANDOM INTERCEPT / SUBJECT = newid VCORR;\n    ODS OUTPUT FitStatistics = Model_Reduced1;\n  ODS SELECT SolutionF Tests3; * Output only these specific tables;\n    RUN;\n\nlstlog\n\n\n&lt;!DOCTYPE html&gt;\n\n\n\n\n\nSAS Output\n\n\n\n\n\n\n\nAverage Physical QOL by Year and College Education\n\n\n\n\nThe Mixed Procedure\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolution for Fixed Effects\n\n\n\n\nEffect\n\n\nhard_drugs_grp\n\n\nADH_HIGHVSLOW\n\n\nEstimate\n\n\nStandardError\n\n\nDF\n\n\nt Value\n\n\nPr &gt; |t|\n\n\nAlpha\n\n\nLower\n\n\nUpper\n\n\n\n\n\n\nIntercept\n\n\n \n\n\n \n\n\n10.1648\n\n\n0.3088\n\n\n537\n\n\n32.92\n\n\n&lt;.0001\n\n\n0.05\n\n\n9.5583\n\n\n10.7714\n\n\n\n\nhard_drugs_grp\n\n\nCurrent User\n\n\n \n\n\n-0.02746\n\n\n0.3027\n\n\n1059\n\n\n-0.09\n\n\n0.9277\n\n\n0.05\n\n\n-0.6213\n\n\n0.5664\n\n\n\n\nhard_drugs_grp\n\n\nPrevious User\n\n\n \n\n\n-0.2099\n\n\n0.3406\n\n\n1059\n\n\n-0.62\n\n\n0.5378\n\n\n0.05\n\n\n-0.8782\n\n\n0.4583\n\n\n\n\nhard_drugs_grp\n\n\nNever User\n\n\n \n\n\n0\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n\n\nADH_HIGHVSLOW\n\n\n \n\n\nHigh Adherence\n\n\n-0.4302\n\n\n0.3132\n\n\n1059\n\n\n-1.37\n\n\n0.1698\n\n\n0.05\n\n\n-1.0447\n\n\n0.1843\n\n\n\n\nADH_HIGHVSLOW\n\n\n \n\n\nLow Adherence\n\n\n0\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n\n\nyears\n\n\n \n\n\n \n\n\n-3.0519\n\n\n0.07425\n\n\n1059\n\n\n-41.10\n\n\n&lt;.0001\n\n\n0.05\n\n\n-3.1975\n\n\n-2.9062\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nType 3 Tests of Fixed Effects\n\n\n\n\nEffect\n\n\nNum DF\n\n\nDen DF\n\n\nF Value\n\n\nPr &gt; F\n\n\n\n\n\n\nhard_drugs_grp\n\n\n2\n\n\n1059\n\n\n0.19\n\n\n0.8268\n\n\n\n\nADH_HIGHVSLOW\n\n\n1\n\n\n1059\n\n\n1.89\n\n\n0.1698\n\n\n\n\nyears\n\n\n1\n\n\n1059\n\n\n1689.61\n\n\n&lt;.0001\n\n\n\n\n\n\n\n\n\n\n\u001497                                                         The SAS System                    Saturday, December 28, 2024 08:34:00 PM\n\n946        ods listing close;ods html5 (id=saspy_internal) file=_tomods1 options(bitmap_mode='inline') device=svg style=HTMLBlue;\n946      ! ods graphics on / outputfmt=png;\n947        \n948        PROC MIXED DATA = PROJ2LMM.DATA METHOD = REML;\n949         CLASS hard_drugs_grp (REF='Never User') ADH_HIGHVSLOW;\n950         MODEL VLOAD_log = hard_drugs_grp ADH_HIGHVSLOW years / SOLUTION CL;\n951         RANDOM INTERCEPT / SUBJECT = newid VCORR;\n952         ODS OUTPUT FitStatistics = Model_Reduced1;\n953          ODS SELECT SolutionF Tests3; * Output only these specific tables;\n954         RUN;\n955        \n956        \n957        ods html5 (id=saspy_internal) close;ods listing;\n958        \n\u001498                                                         The SAS System                    Saturday, December 28, 2024 08:34:00 PM\n\n959        \n\n\n\n\nThat looks pretty good, however, we did see that high adherence vs low adherence had differing slopes when we plotted them. Let’s include an interaction term for adherence.\n\n\nAdd Adherence Interaction Term\n\n** Add Adherence interaction term;\nPROC MIXED DATA = PROJ2LMM.DATA METHOD = REML;\n    CLASS hard_drugs_grp (REF='Never User') ADH_HIGHVSLOW;\n    MODEL VLOAD_log = hard_drugs_grp ADH_HIGHVSLOW years ADH_HIGHVSLOW*years / SOLUTION CL;\n    RANDOM INTERCEPT / SUBJECT = newid VCORR;\n    ODS OUTPUT FitStatistics = Model_Reduced2;\n  ODS SELECT SolutionF Tests3; * Output only these specific tables;\n    RUN;\n\nlstlog\n\n\n&lt;!DOCTYPE html&gt;\n\n\n\n\n\nSAS Output\n\n\n\n\n\n\n\nAverage Physical QOL by Year and College Education\n\n\n\n\nThe Mixed Procedure\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolution for Fixed Effects\n\n\n\n\nEffect\n\n\nhard_drugs_grp\n\n\nADH_HIGHVSLOW\n\n\nEstimate\n\n\nStandardError\n\n\nDF\n\n\nt Value\n\n\nPr &gt; |t|\n\n\nAlpha\n\n\nLower\n\n\nUpper\n\n\n\n\n\n\nIntercept\n\n\n \n\n\n \n\n\n9.2825\n\n\n0.3777\n\n\n537\n\n\n24.58\n\n\n&lt;.0001\n\n\n0.05\n\n\n8.5406\n\n\n10.0244\n\n\n\n\nhard_drugs_grp\n\n\nCurrent User\n\n\n \n\n\n-0.02743\n\n\n0.3028\n\n\n1058\n\n\n-0.09\n\n\n0.9278\n\n\n0.05\n\n\n-0.6215\n\n\n0.5666\n\n\n\n\nhard_drugs_grp\n\n\nPrevious User\n\n\n \n\n\n-0.2104\n\n\n0.3407\n\n\n1058\n\n\n-0.62\n\n\n0.5370\n\n\n0.05\n\n\n-0.8788\n\n\n0.4581\n\n\n\n\nhard_drugs_grp\n\n\nNever User\n\n\n \n\n\n0\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n\n\nADH_HIGHVSLOW\n\n\n \n\n\nHigh Adherence\n\n\n0.5520\n\n\n0.3959\n\n\n1058\n\n\n1.39\n\n\n0.1635\n\n\n0.05\n\n\n-0.2249\n\n\n1.3290\n\n\n\n\nADH_HIGHVSLOW\n\n\n \n\n\nLow Adherence\n\n\n0\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n\n\nyears\n\n\n \n\n\n \n\n\n-2.1626\n\n\n0.2313\n\n\n1058\n\n\n-9.35\n\n\n&lt;.0001\n\n\n0.05\n\n\n-2.6164\n\n\n-1.7089\n\n\n\n\nyears*ADH_HIGHVSLOW\n\n\n \n\n\nHigh Adherence\n\n\n-0.9897\n\n\n0.2440\n\n\n1058\n\n\n-4.06\n\n\n&lt;.0001\n\n\n0.05\n\n\n-1.4684\n\n\n-0.5109\n\n\n\n\nyears*ADH_HIGHVSLOW\n\n\n \n\n\nLow Adherence\n\n\n0\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nType 3 Tests of Fixed Effects\n\n\n\n\nEffect\n\n\nNum DF\n\n\nDen DF\n\n\nF Value\n\n\nPr &gt; F\n\n\n\n\n\n\nhard_drugs_grp\n\n\n2\n\n\n1058\n\n\n0.19\n\n\n0.8262\n\n\n\n\nADH_HIGHVSLOW\n\n\n1\n\n\n1058\n\n\n1.94\n\n\n0.1635\n\n\n\n\nyears\n\n\n1\n\n\n1058\n\n\n474.57\n\n\n&lt;.0001\n\n\n\n\nyears*ADH_HIGHVSLOW\n\n\n1\n\n\n1058\n\n\n16.45\n\n\n&lt;.0001\n\n\n\n\n\n\n\n\n\n\n\u001499                                                         The SAS System                    Saturday, December 28, 2024 08:34:00 PM\n\n962        ods listing close;ods html5 (id=saspy_internal) file=_tomods1 options(bitmap_mode='inline') device=svg style=HTMLBlue;\n962      ! ods graphics on / outputfmt=png;\n963        \n964        ** Add Adherence interaction term;\n965        PROC MIXED DATA = PROJ2LMM.DATA METHOD = REML;\n966         CLASS hard_drugs_grp (REF='Never User') ADH_HIGHVSLOW;\n967         MODEL VLOAD_log = hard_drugs_grp ADH_HIGHVSLOW years ADH_HIGHVSLOW*years / SOLUTION CL;\n968         RANDOM INTERCEPT / SUBJECT = newid VCORR;\n969         ODS OUTPUT FitStatistics = Model_Reduced2;\n970          ODS SELECT SolutionF Tests3; * Output only these specific tables;\n971         RUN;\n972        \n973        \n974        ods html5 (id=saspy_internal) close;ods listing;\n975        \n\u0014100                                                        The SAS System                    Saturday, December 28, 2024 08:34:00 PM\n\n976        \n\n\n\n\nWe have a significant interaciton. Let’s perform model selection based on AIC and BIC.\n\n\nCompare BIC of Each Model\n\n** Perform Model Selection Comparing BIC;\nDATA COMPARE_BIC;\n    SET Model_Full(in=full) Model_Reduced1(in = reduced1) Model_Reduced2(in = reduced2);\n    IF full THEN Model = \"Model 1\";\n    ELSE IF reduced1 THEN MODEL = \"Model 2\";\n    ELSE Model = \"Model 3\";\n    \n    IF Descr = \"BIC (Smaller is Better)\";\n    RUN;\n    \n* Print BICs for each model;\nPROC PRINT DATA = COMPARE_BIC;\n    RUN;\n\nlstlog\n\n\n&lt;!DOCTYPE html&gt;\n\n\n\n\n\nSAS Output\n\n\n\n\n\n\n\nAverage Physical QOL by Year and College Education\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObs\n\n\nDescr\n\n\nValue\n\n\nModel\n\n\n\n\n\n\n1\n\n\nBIC (Smaller is Better)\n\n\n7876.9\n\n\nModel 1\n\n\n\n\n2\n\n\nBIC (Smaller is Better)\n\n\n7880.5\n\n\nModel 2\n\n\n\n\n3\n\n\nBIC (Smaller is Better)\n\n\n7865.2\n\n\nModel 3\n\n\n\n\n\n\n\n\n\n\n\u0014101                                                        The SAS System                    Saturday, December 28, 2024 08:34:00 PM\n\n979        ods listing close;ods html5 (id=saspy_internal) file=_tomods1 options(bitmap_mode='inline') device=svg style=HTMLBlue;\n979      ! ods graphics on / outputfmt=png;\n980        \n981        ** Perform Model Selection Comparing BIC;\n982        DATA COMPARE_BIC;\n983         SET Model_Full(in=full) Model_Reduced1(in = reduced1) Model_Reduced2(in = reduced2);\n984         IF full THEN Model = \"Model 1\";\n985         ELSE IF reduced1 THEN MODEL = \"Model 2\";\n986         ELSE Model = \"Model 3\";\n987         \n988         IF Descr = \"BIC (Smaller is Better)\";\n989         RUN;\n990         \n991        * Print BICs for each model;\n992        PROC PRINT DATA = COMPARE_BIC;\n993         RUN;\n994        \n995        \n996        ods html5 (id=saspy_internal) close;ods listing;\n997        \n\u0014102                                                        The SAS System                    Saturday, December 28, 2024 08:34:00 PM\n\n998        \n\n\n\n\nBIC prefers model 3, with the ADH_HIGHVSLOW*years interaction. Thus that is our final model.\nTop of Tabset\n\n\n\nBased on model selection, the final model for log viral load includes hard_drugs_grp, ADH_HIGHVSLOW, years, and an interaction between ADH_HIGHVSLOW*years.\n\nRun Final Model\n\n* Run Final Model;\nPROC MIXED DATA = PROJ2LMM.DATA METHOD = REML;\n    CLASS hard_drugs_grp (REF='Never User') ADH_HIGHVSLOW;\n    MODEL VLOAD_log = hard_drugs_grp ADH_HIGHVSLOW years ADH_HIGHVSLOW*years / SOLUTION CL;\n    RANDOM INTERCEPT / SUBJECT = newid VCORR;\n    ODS OUTPUT FitStatistics = Model_Reduced2;\n  STORE OUT=Model_VLOAD; *Store the model information for later use;\n    RUN;\n\nlstlog\n\n\n&lt;!DOCTYPE html&gt;\n\n\n\n\n\nSAS Output\n\n\n\n\n\n\n\nAverage Physical QOL by Year and College Education\n\n\n\n\nThe Mixed Procedure\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel Information\n\n\n\n\n\n\nData Set\n\n\nPROJ2LMM.DATA\n\n\n\n\nDependent Variable\n\n\nVLOAD_log\n\n\n\n\nCovariance Structure\n\n\nVariance Components\n\n\n\n\nSubject Effect\n\n\nnewid\n\n\n\n\nEstimation Method\n\n\nREML\n\n\n\n\nResidual Variance Method\n\n\nProfile\n\n\n\n\nFixed Effects SE Method\n\n\nModel-Based\n\n\n\n\nDegrees of Freedom Method\n\n\nContainment\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nClass Level Information\n\n\n\n\nClass\n\n\nLevels\n\n\nValues\n\n\n\n\n\n\nhard_drugs_grp\n\n\n3\n\n\nCurrent User Previous User Never User\n\n\n\n\nADH_HIGHVSLOW\n\n\n2\n\n\nHigh Adherence Low Adherence\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDimensions\n\n\n\n\n\n\nCovariance Parameters\n\n\n2\n\n\n\n\nColumns in X\n\n\n9\n\n\n\n\nColumns in Z per Subject\n\n\n1\n\n\n\n\nSubjects\n\n\n550\n\n\n\n\nMax Obs per Subject\n\n\n3\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNumber of Observations\n\n\n\n\n\n\nNumber of Observations Read\n\n\n1650\n\n\n\n\nNumber of Observations Used\n\n\n1601\n\n\n\n\nNumber of Observations Not Used\n\n\n49\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIteration History\n\n\n\n\nIteration\n\n\nEvaluations\n\n\n-2 Res Log Like\n\n\nCriterion\n\n\n\n\n\n\n0\n\n\n1\n\n\n8003.57222850\n\n\n \n\n\n\n\n1\n\n\n2\n\n\n7852.56115225\n\n\n0.00000014\n\n\n\n\n2\n\n\n1\n\n\n7852.56079665\n\n\n0.00000000\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nConvergence criteria met.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEstimated V Correlation Matrix for Subject 1\n\n\n\n\nRow\n\n\nCol1\n\n\nCol2\n\n\nCol3\n\n\n\n\n\n\n1\n\n\n1.0000\n\n\n0.3314\n\n\n0.3314\n\n\n\n\n2\n\n\n0.3314\n\n\n1.0000\n\n\n0.3314\n\n\n\n\n3\n\n\n0.3314\n\n\n0.3314\n\n\n1.0000\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCovariance Parameter Estimates\n\n\n\n\nCov Parm\n\n\nSubject\n\n\nEstimate\n\n\n\n\n\n\nIntercept\n\n\nnewid\n\n\n2.8822\n\n\n\n\nResidual\n\n\n \n\n\n5.8156\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFit Statistics\n\n\n\n\n\n\n-2 Res Log Likelihood\n\n\n7852.6\n\n\n\n\nAIC (Smaller is Better)\n\n\n7856.6\n\n\n\n\nAICC (Smaller is Better)\n\n\n7856.6\n\n\n\n\nBIC (Smaller is Better)\n\n\n7865.2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolution for Fixed Effects\n\n\n\n\nEffect\n\n\nhard_drugs_grp\n\n\nADH_HIGHVSLOW\n\n\nEstimate\n\n\nStandardError\n\n\nDF\n\n\nt Value\n\n\nPr &gt; |t|\n\n\nAlpha\n\n\nLower\n\n\nUpper\n\n\n\n\n\n\nIntercept\n\n\n \n\n\n \n\n\n9.2825\n\n\n0.3777\n\n\n537\n\n\n24.58\n\n\n&lt;.0001\n\n\n0.05\n\n\n8.5406\n\n\n10.0244\n\n\n\n\nhard_drugs_grp\n\n\nCurrent User\n\n\n \n\n\n-0.02743\n\n\n0.3028\n\n\n1058\n\n\n-0.09\n\n\n0.9278\n\n\n0.05\n\n\n-0.6215\n\n\n0.5666\n\n\n\n\nhard_drugs_grp\n\n\nPrevious User\n\n\n \n\n\n-0.2104\n\n\n0.3407\n\n\n1058\n\n\n-0.62\n\n\n0.5370\n\n\n0.05\n\n\n-0.8788\n\n\n0.4581\n\n\n\n\nhard_drugs_grp\n\n\nNever User\n\n\n \n\n\n0\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n\n\nADH_HIGHVSLOW\n\n\n \n\n\nHigh Adherence\n\n\n0.5520\n\n\n0.3959\n\n\n1058\n\n\n1.39\n\n\n0.1635\n\n\n0.05\n\n\n-0.2249\n\n\n1.3290\n\n\n\n\nADH_HIGHVSLOW\n\n\n \n\n\nLow Adherence\n\n\n0\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n\n\nyears\n\n\n \n\n\n \n\n\n-2.1626\n\n\n0.2313\n\n\n1058\n\n\n-9.35\n\n\n&lt;.0001\n\n\n0.05\n\n\n-2.6164\n\n\n-1.7089\n\n\n\n\nyears*ADH_HIGHVSLOW\n\n\n \n\n\nHigh Adherence\n\n\n-0.9897\n\n\n0.2440\n\n\n1058\n\n\n-4.06\n\n\n&lt;.0001\n\n\n0.05\n\n\n-1.4684\n\n\n-0.5109\n\n\n\n\nyears*ADH_HIGHVSLOW\n\n\n \n\n\nLow Adherence\n\n\n0\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nType 3 Tests of Fixed Effects\n\n\n\n\nEffect\n\n\nNum DF\n\n\nDen DF\n\n\nF Value\n\n\nPr &gt; F\n\n\n\n\n\n\nhard_drugs_grp\n\n\n2\n\n\n1058\n\n\n0.19\n\n\n0.8262\n\n\n\n\nADH_HIGHVSLOW\n\n\n1\n\n\n1058\n\n\n1.94\n\n\n0.1635\n\n\n\n\nyears\n\n\n1\n\n\n1058\n\n\n474.57\n\n\n&lt;.0001\n\n\n\n\nyears*ADH_HIGHVSLOW\n\n\n1\n\n\n1058\n\n\n16.45\n\n\n&lt;.0001\n\n\n\n\n\n\n\n\n\n\n\u0014103                                                        The SAS System                    Saturday, December 28, 2024 08:34:00 PM\n\n1001       ods listing close;ods html5 (id=saspy_internal) file=_tomods1 options(bitmap_mode='inline') device=svg style=HTMLBlue;\n1001     ! ods graphics on / outputfmt=png;\n1002       \n1003       * Run Final Model;\n1004       PROC MIXED DATA = PROJ2LMM.DATA METHOD = REML;\n1005        CLASS hard_drugs_grp (REF='Never User') ADH_HIGHVSLOW;\n1006        MODEL VLOAD_log = hard_drugs_grp ADH_HIGHVSLOW years ADH_HIGHVSLOW*years / SOLUTION CL;\n1007        RANDOM INTERCEPT / SUBJECT = newid VCORR;\n1008        ODS OUTPUT FitStatistics = Model_Reduced2;\n1009         STORE OUT=Model_VLOAD; *Store the model information for later use;\n1010        RUN;\n1011       \n1012       \n1013       ods html5 (id=saspy_internal) close;ods listing;\n1014       \n\u0014104                                                        The SAS System                    Saturday, December 28, 2024 08:34:00 PM\n\n1015       \n\n\n\n\nAfter controlling for adherence and between subject means (intercepts), there was no difference in average log viral load for current (p = 0.93, 95% CI: -0.62, 0.56) and previous (p = 0.54, 95% CI: -0.88, 0.46) hard drug users compared to never users.\nAfter controlling for hard drug use and between subject means, those with low adherence did not differ significantly in average log viral load compared to those with high adherence (p = 0.16, 95% CI: -1.32, 0.22).\nThere was a significant decrease in log viral load over time while controlling for subject-specific means, hard drug use, and adherence, with an average decrease of 3.15 log copies/mL per year (p &lt; 0.0001, 95% CI: -3.30, -3.00).\nSubject ID contributes greatly to log viral load, as it accounts for 2.882 / (2.882 + 5.816) = 0.331 or 33.1% of the variance in log viral load.\nThe slope for viral load over time differed significantly between those with low and high adherence. Those with low adherence had on average a 0.99 increase in log viral load each year compared to those with high adherence (p &lt; 0.0001, 95% CI: 0.51, 1.47).\n\n\nChange Reference Group\n\n* Change reference group;\nPROC MIXED DATA = PROJ2LMM.DATA METHOD = REML;\n    CLASS hard_drugs_grp (REF='Previous User') ADH_HIGHVSLOW;\n    MODEL VLOAD_log = hard_drugs_grp ADH_HIGHVSLOW years ADH_HIGHVSLOW*years / SOLUTION CL;\n    RANDOM INTERCEPT / SUBJECT = newid VCORR;\n    ODS OUTPUT FitStatistics = Model_Reduced2;\n  ODS SELECT SolutionF Tests3; * Output only these specific tables;\n    RUN;\n\nlstlog\n\n\n&lt;!DOCTYPE html&gt;\n\n\n\n\n\nSAS Output\n\n\n\n\n\n\n\nAverage Physical QOL by Year and College Education\n\n\n\n\nThe Mixed Procedure\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolution for Fixed Effects\n\n\n\n\nEffect\n\n\nhard_drugs_grp\n\n\nADH_HIGHVSLOW\n\n\nEstimate\n\n\nStandardError\n\n\nDF\n\n\nt Value\n\n\nPr &gt; |t|\n\n\nAlpha\n\n\nLower\n\n\nUpper\n\n\n\n\n\n\nIntercept\n\n\n \n\n\n \n\n\n9.0721\n\n\n0.4814\n\n\n537\n\n\n18.84\n\n\n&lt;.0001\n\n\n0.05\n\n\n8.1264\n\n\n10.0179\n\n\n\n\nhard_drugs_grp\n\n\nCurrent User\n\n\n \n\n\n0.1829\n\n\n0.4307\n\n\n1058\n\n\n0.42\n\n\n0.6711\n\n\n0.05\n\n\n-0.6623\n\n\n1.0281\n\n\n\n\nhard_drugs_grp\n\n\nNever User\n\n\n \n\n\n0.2104\n\n\n0.3407\n\n\n1058\n\n\n0.62\n\n\n0.5370\n\n\n0.05\n\n\n-0.4581\n\n\n0.8788\n\n\n\n\nhard_drugs_grp\n\n\nPrevious User\n\n\n \n\n\n0\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n\n\nADH_HIGHVSLOW\n\n\n \n\n\nHigh Adherence\n\n\n0.5520\n\n\n0.3959\n\n\n1058\n\n\n1.39\n\n\n0.1635\n\n\n0.05\n\n\n-0.2249\n\n\n1.3290\n\n\n\n\nADH_HIGHVSLOW\n\n\n \n\n\nLow Adherence\n\n\n0\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n\n\nyears\n\n\n \n\n\n \n\n\n-2.1626\n\n\n0.2313\n\n\n1058\n\n\n-9.35\n\n\n&lt;.0001\n\n\n0.05\n\n\n-2.6164\n\n\n-1.7089\n\n\n\n\nyears*ADH_HIGHVSLOW\n\n\n \n\n\nHigh Adherence\n\n\n-0.9897\n\n\n0.2440\n\n\n1058\n\n\n-4.06\n\n\n&lt;.0001\n\n\n0.05\n\n\n-1.4684\n\n\n-0.5109\n\n\n\n\nyears*ADH_HIGHVSLOW\n\n\n \n\n\nLow Adherence\n\n\n0\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nType 3 Tests of Fixed Effects\n\n\n\n\nEffect\n\n\nNum DF\n\n\nDen DF\n\n\nF Value\n\n\nPr &gt; F\n\n\n\n\n\n\nhard_drugs_grp\n\n\n2\n\n\n1058\n\n\n0.19\n\n\n0.8262\n\n\n\n\nADH_HIGHVSLOW\n\n\n1\n\n\n1058\n\n\n1.94\n\n\n0.1635\n\n\n\n\nyears\n\n\n1\n\n\n1058\n\n\n474.57\n\n\n&lt;.0001\n\n\n\n\nyears*ADH_HIGHVSLOW\n\n\n1\n\n\n1058\n\n\n16.45\n\n\n&lt;.0001\n\n\n\n\n\n\n\n\n\n\n\u0014105                                                        The SAS System                    Saturday, December 28, 2024 08:34:00 PM\n\n1018       ods listing close;ods html5 (id=saspy_internal) file=_tomods1 options(bitmap_mode='inline') device=svg style=HTMLBlue;\n1018     ! ods graphics on / outputfmt=png;\n1019       \n1020       * Change reference group;\n1021       PROC MIXED DATA = PROJ2LMM.DATA METHOD = REML;\n1022        CLASS hard_drugs_grp (REF='Previous User') ADH_HIGHVSLOW;\n1023        MODEL VLOAD_log = hard_drugs_grp ADH_HIGHVSLOW years ADH_HIGHVSLOW*years / SOLUTION CL;\n1024        RANDOM INTERCEPT / SUBJECT = newid VCORR;\n1025        ODS OUTPUT FitStatistics = Model_Reduced2;\n1026         ODS SELECT SolutionF Tests3; * Output only these specific tables;\n1027        RUN;\n1028       \n1029       \n1030       ods html5 (id=saspy_internal) close;ods listing;\n1031       \n\u0014106                                                        The SAS System                    Saturday, December 28, 2024 08:34:00 PM\n\n1032       \n\n\n\n\nPrevious hard drug users did not differ significantly in average log viral load compared to current users (p = 0.67, 95% CI: -0.66, 1.03).\nTop of Tabset\n\n\n\n\nHard Drug Use Group\nAfter controlling for adherence and between subject means (intercepts), there was no difference in average log viral load for current (p = 0.93, 95% CI: -0.62, 0.56) and previous (p = 0.54, 95% CI: -0.88, 0.46) hard drug users compared to never users. Previous hard drug users did not differ significantly in average log viral load compared to current users (p = 0.67, 95% CI: -0.66, 1.03).\n\n\nAdherence\nAfter controlling for hard drug use and between subject means, those with low adherence did not differ significantly in average log viral load compared to those with high adherence (p = 0.16, 95% CI: -1.32, 0.22).\n\n\nTime\nThere was a significant decrease in log viral load over time while controlling for subject-specific means, hard drug use, and adherence, with an average decrease of 3.15 log copies/mL per year (p &lt; 0.0001, 95% CI: -3.30, -3.00).\n\n\nViral Load over Time by Adherence\nThe slope for viral load over time differed significantly between those with low and high adherence. Those with low adherence had on average a 0.99 increase in log viral load each year compared to those with high adherence (p &lt; 0.0001, 95% CI: 0.51, 1.47). The other in group comparisons were not significant (p &gt; 0.05).\n\n\nWithin Subject Means\nSubject ID contributes greatly to log viral load, as it accounts for 2.882 / (2.882 + 5.816) = 0.331 or 33.1% of the variance in log viral load.\nTop of Tabset\n\n\n\nWe can plot the predicted values of our model in order to visualize the interaction and understand it better.\n\nGenerate Predicted Values with PROC PLM\n\n* Use PROC PLM to generate the predicted values under the final model;\nPROC PLM RESTORE = Model_VLOAD;\n    SCORE DATA = Proj2LMM.Data OUT = Proj2LMM.Data PREDICTED = Predicted_VLOAD_log;\n    RUN;\n\n\n\nVisualize Predicted Values\n\n* Visualize the predicted values under the final model;\nPROC SGPLOT DATA = Proj2LMM.Data;\n    REG X = Years Y = Predicted_VLOAD_log / GROUP = ADH_HIGHVSLOW LINEATTRS = (THICKNESS=2) NOMARKERS;\n    XAXIS LABEL = \"Years\";\n    YAXIS LABEL = \"Average Log Viral Load\";\n    TITLE \"Predicted Log Viral Load by Adherence Level\";\n    RUN;\n\nlstlog\n\n\n&lt;!DOCTYPE html&gt;\n\n\n\n\n\nSAS Output\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u0014109                                                        The SAS System                    Saturday, December 28, 2024 08:34:00 PM\n\n1048       ods listing close;ods html5 (id=saspy_internal) file=_tomods1 options(bitmap_mode='inline') device=svg style=HTMLBlue;\n1048     ! ods graphics on / outputfmt=png;\n1049       \n1050       * Visualize the predicted values under the final model;\n1051       PROC SGPLOT DATA = Proj2LMM.Data;\n1052        REG X = Years Y = Predicted_VLOAD_log / GROUP = ADH_HIGHVSLOW LINEATTRS = (THICKNESS=2) NOMARKERS;\n1053        XAXIS LABEL = \"Years\";\n1054        YAXIS LABEL = \"Average Log Viral Load\";\n1055        TITLE \"Predicted Log Viral Load by Adherence Level\";\n1056        RUN;\n1057       \n1058       \n1059       ods html5 (id=saspy_internal) close;ods listing;\n1060       \n\u0014110                                                        The SAS System                    Saturday, December 28, 2024 08:34:00 PM\n\n1061       \n\n\n\n\nTop of Tabset"
  },
  {
    "objectID": "Project_2/Project_2_SAS/Code/Linear_Mixed_Model_SAS.html#CD4",
    "href": "Project_2/Project_2_SAS/Code/Linear_Mixed_Model_SAS.html#CD4",
    "title": "Linear Mixed Model (SAS)",
    "section": "CD4+ T Cell Count",
    "text": "CD4+ T Cell Count\n\nModel SelectionAnalysisInterpretationVisualization\n\n\nNote: It appears that if we run the analysis with random slopes for newid in SAS, the program is unable to run due to insufficient memory. This may because I am using the unpaid version of SAS. We will therefore run these analyses without the random slopes. The results are similiar to how we originally ran this in R.\n\nRun Saturated Model with Unstructured Covariance Matrix\nFirst let us fit the full model of interest using a structured covariance matrix, and hard_drugs_grp, ADH_HIGHVSLOW, years, and the hard_drugs_grp*years interaction term.\n\n* Run with unstructured covariance matrix;\nPROC MIXED DATA = PROJ2LMM.DATA METHOD = REML;\n    CLASS newid hard_drugs_grp ADH_HIGHVSLOW;\n    MODEL LEU3N = hard_drugs_grp ADH_HIGHVSLOW years hard_drugs_grp*years / CL;\n    RANDOM intercept / SUBJECT = newid TYPE = UN VCORR;\n    ODS OUTPUT FitStatistics = Full_Model_Un;\n  ODS SELECT SolutionF Tests3; * Output only these specific tables;\n    RUN;\n\nlstlog\n\n\n&lt;!DOCTYPE html&gt;\n\n\n\n\n\nSAS Output\n\n\n\n\n\n\n\nPredicted Log Viral Load by Adherence Level\n\n\n\n\nThe Mixed Procedure\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolution for Fixed Effects\n\n\n\n\nEffect\n\n\nhard_drugs_grp\n\n\nADH_HIGHVSLOW\n\n\nEstimate\n\n\nStandardError\n\n\nDF\n\n\nt Value\n\n\nPr &gt; |t|\n\n\nAlpha\n\n\nLower\n\n\nUpper\n\n\n\n\n\n\nIntercept\n\n\n \n\n\n \n\n\n299.95\n\n\n44.9793\n\n\n538\n\n\n6.67\n\n\n&lt;.0001\n\n\n0.05\n\n\n211.59\n\n\n388.30\n\n\n\n\nhard_drugs_grp\n\n\nCurrent User\n\n\n \n\n\n163.73\n\n\n46.9672\n\n\n1061\n\n\n3.49\n\n\n0.0005\n\n\n0.05\n\n\n71.5748\n\n\n255.89\n\n\n\n\nhard_drugs_grp\n\n\nNever User\n\n\n \n\n\n47.3576\n\n\n37.1302\n\n\n1061\n\n\n1.28\n\n\n0.2024\n\n\n0.05\n\n\n-25.4993\n\n\n120.21\n\n\n\n\nhard_drugs_grp\n\n\nPrevious User\n\n\n \n\n\n0\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n\n\nADH_HIGHVSLOW\n\n\n \n\n\nHigh Adherence\n\n\n33.6629\n\n\n32.0532\n\n\n1061\n\n\n1.05\n\n\n0.2939\n\n\n0.05\n\n\n-29.2320\n\n\n96.5579\n\n\n\n\nADH_HIGHVSLOW\n\n\n \n\n\nLow Adherence\n\n\n0\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n\n\nyears\n\n\n \n\n\n \n\n\n57.1508\n\n\n12.0511\n\n\n1061\n\n\n4.74\n\n\n&lt;.0001\n\n\n0.05\n\n\n33.5041\n\n\n80.7975\n\n\n\n\nyears*hard_drugs_grp\n\n\nCurrent User\n\n\n \n\n\n-27.0024\n\n\n16.0179\n\n\n1061\n\n\n-1.69\n\n\n0.0921\n\n\n0.05\n\n\n-58.4327\n\n\n4.4279\n\n\n\n\nyears*hard_drugs_grp\n\n\nNever User\n\n\n \n\n\n32.3086\n\n\n12.6813\n\n\n1061\n\n\n2.55\n\n\n0.0110\n\n\n0.05\n\n\n7.4254\n\n\n57.1919\n\n\n\n\nyears*hard_drugs_grp\n\n\nPrevious User\n\n\n \n\n\n0\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nType 3 Tests of Fixed Effects\n\n\n\n\nEffect\n\n\nNum DF\n\n\nDen DF\n\n\nF Value\n\n\nPr &gt; F\n\n\n\n\n\n\nhard_drugs_grp\n\n\n2\n\n\n1061\n\n\n7.60\n\n\n0.0005\n\n\n\n\nADH_HIGHVSLOW\n\n\n1\n\n\n1061\n\n\n1.10\n\n\n0.2939\n\n\n\n\nyears\n\n\n1\n\n\n1061\n\n\n114.80\n\n\n&lt;.0001\n\n\n\n\nyears*hard_drugs_grp\n\n\n2\n\n\n1061\n\n\n15.83\n\n\n&lt;.0001\n\n\n\n\n\n\n\n\n\n\n\u0014111                                                        The SAS System                    Saturday, December 28, 2024 08:34:00 PM\n\n1064       ods listing close;ods html5 (id=saspy_internal) file=_tomods1 options(bitmap_mode='inline') device=svg style=HTMLBlue;\n1064     ! ods graphics on / outputfmt=png;\n1065       \n1066       * Run with unstructured covariance matrix;\n1067       PROC MIXED DATA = PROJ2LMM.DATA METHOD = REML;\n1068        CLASS newid hard_drugs_grp ADH_HIGHVSLOW;\n1069        MODEL LEU3N = hard_drugs_grp ADH_HIGHVSLOW years hard_drugs_grp*years / CL;\n1070        RANDOM intercept / SUBJECT = newid TYPE = UN VCORR;\n1071        ODS OUTPUT FitStatistics = Full_Model_Un;\n1072         ODS SELECT SolutionF Tests3; * Output only these specific tables;\n1073        RUN;\n1074       \n1075       \n1076       ods html5 (id=saspy_internal) close;ods listing;\n1077       \n\u0014112                                                        The SAS System                    Saturday, December 28, 2024 08:34:00 PM\n\n1078       \n\n\n\n\nThe interaction is significant and thus this is our final model.\nTop of Tabset\n\n\n\nHere we perform the analysis with the final selected model, using an unstructured covariance matrix.\n\n* Run with unstructured covariance matrix;\nPROC MIXED DATA = PROJ2LMM.DATA METHOD = REML;\n    CLASS newid hard_drugs_grp (REF = \"Never User\") ADH_HIGHVSLOW;\n    MODEL LEU3N = hard_drugs_grp ADH_HIGHVSLOW years hard_drugs_grp*years / CL;\n    RANDOM intercept / SUBJECT = newid TYPE = UN VCORR;\n    ODS OUTPUT FitStatistics = Full_Model_Un;\n  STORE OUT=Model_LEU3N; *Store the model information for later use;\n    RUN;\n\nlstlog\n\n\n&lt;!DOCTYPE html&gt;\n\n\n\n\n\nSAS Output\n\n\n\n\n\n\n\nPredicted Log Viral Load by Adherence Level\n\n\n\n\nThe Mixed Procedure\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel Information\n\n\n\n\n\n\nData Set\n\n\nPROJ2LMM.DATA\n\n\n\n\nDependent Variable\n\n\nLEU3N\n\n\n\n\nCovariance Structure\n\n\nUnstructured\n\n\n\n\nSubject Effect\n\n\nnewid\n\n\n\n\nEstimation Method\n\n\nREML\n\n\n\n\nResidual Variance Method\n\n\nProfile\n\n\n\n\nFixed Effects SE Method\n\n\nModel-Based\n\n\n\n\nDegrees of Freedom Method\n\n\nContainment\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nClass Level Information\n\n\n\n\nClass\n\n\nLevels\n\n\nValues\n\n\n\n\n\n\nnewid\n\n\n550\n\n\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550\n\n\n\n\nhard_drugs_grp\n\n\n3\n\n\nCurrent User Previous User Never User\n\n\n\n\nADH_HIGHVSLOW\n\n\n2\n\n\nHigh Adherence Low Adherence\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDimensions\n\n\n\n\n\n\nCovariance Parameters\n\n\n2\n\n\n\n\nColumns in X\n\n\n10\n\n\n\n\nColumns in Z per Subject\n\n\n1\n\n\n\n\nSubjects\n\n\n550\n\n\n\n\nMax Obs per Subject\n\n\n3\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNumber of Observations\n\n\n\n\n\n\nNumber of Observations Read\n\n\n1650\n\n\n\n\nNumber of Observations Used\n\n\n1606\n\n\n\n\nNumber of Observations Not Used\n\n\n44\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIteration History\n\n\n\n\nIteration\n\n\nEvaluations\n\n\n-2 Res Log Like\n\n\nCriterion\n\n\n\n\n\n\n0\n\n\n1\n\n\n22144.63962092\n\n\n \n\n\n\n\n1\n\n\n2\n\n\n21067.28949930\n\n\n0.00000014\n\n\n\n\n2\n\n\n1\n\n\n21067.28822306\n\n\n0.00000000\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nConvergence criteria met.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEstimated V Correlation Matrix for newid 1\n\n\n\n\nRow\n\n\nCol1\n\n\nCol2\n\n\nCol3\n\n\n\n\n\n\n1\n\n\n1.0000\n\n\n0.7757\n\n\n0.7757\n\n\n\n\n2\n\n\n0.7757\n\n\n1.0000\n\n\n0.7757\n\n\n\n\n3\n\n\n0.7757\n\n\n0.7757\n\n\n1.0000\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCovariance Parameter Estimates\n\n\n\n\nCov Parm\n\n\nSubject\n\n\nEstimate\n\n\n\n\n\n\nUN(1,1)\n\n\nnewid\n\n\n46194\n\n\n\n\nResidual\n\n\n \n\n\n13361\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFit Statistics\n\n\n\n\n\n\n-2 Res Log Likelihood\n\n\n21067.3\n\n\n\n\nAIC (Smaller is Better)\n\n\n21071.3\n\n\n\n\nAICC (Smaller is Better)\n\n\n21071.3\n\n\n\n\nBIC (Smaller is Better)\n\n\n21079.9\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNull Model Likelihood Ratio Test\n\n\n\n\nDF\n\n\nChi-Square\n\n\nPr &gt; ChiSq\n\n\n\n\n\n\n1\n\n\n1077.35\n\n\n&lt;.0001\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolution for Fixed Effects\n\n\n\n\nEffect\n\n\nhard_drugs_grp\n\n\nADH_HIGHVSLOW\n\n\nEstimate\n\n\nStandardError\n\n\nDF\n\n\nt Value\n\n\nPr &gt; |t|\n\n\nAlpha\n\n\nLower\n\n\nUpper\n\n\n\n\n\n\nIntercept\n\n\n \n\n\n \n\n\n347.31\n\n\n30.9484\n\n\n538\n\n\n11.22\n\n\n&lt;.0001\n\n\n0.05\n\n\n286.51\n\n\n408.10\n\n\n\n\nhard_drugs_grp\n\n\nCurrent User\n\n\n \n\n\n116.38\n\n\n32.9920\n\n\n1061\n\n\n3.53\n\n\n0.0004\n\n\n0.05\n\n\n51.6394\n\n\n181.11\n\n\n\n\nhard_drugs_grp\n\n\nPrevious User\n\n\n \n\n\n-47.3576\n\n\n37.1302\n\n\n1061\n\n\n-1.28\n\n\n0.2024\n\n\n0.05\n\n\n-120.21\n\n\n25.4993\n\n\n\n\nhard_drugs_grp\n\n\nNever User\n\n\n \n\n\n0\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n\n\nADH_HIGHVSLOW\n\n\n \n\n\nHigh Adherence\n\n\n33.6629\n\n\n32.0532\n\n\n1061\n\n\n1.05\n\n\n0.2939\n\n\n0.05\n\n\n-29.2320\n\n\n96.5579\n\n\n\n\nADH_HIGHVSLOW\n\n\n \n\n\nLow Adherence\n\n\n0\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n\n\nyears\n\n\n \n\n\n \n\n\n89.4594\n\n\n3.9479\n\n\n1061\n\n\n22.66\n\n\n&lt;.0001\n\n\n0.05\n\n\n81.7129\n\n\n97.2059\n\n\n\n\nyears*hard_drugs_grp\n\n\nCurrent User\n\n\n \n\n\n-59.3110\n\n\n11.2662\n\n\n1061\n\n\n-5.26\n\n\n&lt;.0001\n\n\n0.05\n\n\n-81.4176\n\n\n-37.2043\n\n\n\n\nyears*hard_drugs_grp\n\n\nPrevious User\n\n\n \n\n\n-32.3086\n\n\n12.6813\n\n\n1061\n\n\n-2.55\n\n\n0.0110\n\n\n0.05\n\n\n-57.1919\n\n\n-7.4254\n\n\n\n\nyears*hard_drugs_grp\n\n\nNever User\n\n\n \n\n\n0\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nType 3 Tests of Fixed Effects\n\n\n\n\nEffect\n\n\nNum DF\n\n\nDen DF\n\n\nF Value\n\n\nPr &gt; F\n\n\n\n\n\n\nhard_drugs_grp\n\n\n2\n\n\n1061\n\n\n7.60\n\n\n0.0005\n\n\n\n\nADH_HIGHVSLOW\n\n\n1\n\n\n1061\n\n\n1.10\n\n\n0.2939\n\n\n\n\nyears\n\n\n1\n\n\n1061\n\n\n114.80\n\n\n&lt;.0001\n\n\n\n\nyears*hard_drugs_grp\n\n\n2\n\n\n1061\n\n\n15.83\n\n\n&lt;.0001\n\n\n\n\n\n\n\n\n\n\n\u0014113                                                        The SAS System                    Saturday, December 28, 2024 08:34:00 PM\n\n1081       ods listing close;ods html5 (id=saspy_internal) file=_tomods1 options(bitmap_mode='inline') device=svg style=HTMLBlue;\n1081     ! ods graphics on / outputfmt=png;\n1082       \n1083       * Run with unstructured covariance matrix;\n1084       PROC MIXED DATA = PROJ2LMM.DATA METHOD = REML;\n1085        CLASS newid hard_drugs_grp (REF = \"Never User\") ADH_HIGHVSLOW;\n1086        MODEL LEU3N = hard_drugs_grp ADH_HIGHVSLOW years hard_drugs_grp*years / CL;\n1087        RANDOM intercept / SUBJECT = newid TYPE = UN VCORR;\n1088        ODS OUTPUT FitStatistics = Full_Model_Un;\n1089         STORE OUT=Model_LEU3N; *Store the model information for later use;\n1090        RUN;\n1091       \n1092       \n1093       ods html5 (id=saspy_internal) close;ods listing;\n1094       \n\u0014114                                                        The SAS System                    Saturday, December 28, 2024 08:34:00 PM\n\n1095       \n\n\n\n\nAfter controlling for adherence and between subject means (intercepts), there was no difference in average CD4+ T Cell count for previous users compared to never hard drug users (p = 0.13, 95% CI: -109.54, 14.08). However, current users had on average a 116.91 cell higher CD4+ T Cell count compared to never users (p &lt; 0.0001, 95% CI: 61.97, 171.85).\nAfter controlling for hard drug use and between subject means, those with low adherence did not differ significantly in average CD4+ T Cell count compared to those with high adherence (p = 0.51, 95% CI: -75.92, 37.59).\nThere was a significant increase in CD4+ T Cell count over time while controlling for subject-specific means, hard drug use, and adherence, with an average increase of 89.30 log copies/mL per year (p &lt; 0.0001, 95% CI: -3.30, -3.00).\nThe slope for CD4+ T Cell count over time differed significantly based on hard drug use. On average, previous hard drug users had a 32.15 decrease in CD4+ T Cell count per year compared to never drug users (p = 0.025, 95% CI: -60.24, -4.06), and current users had a 59.15 decrease in CD4+ T Cell count each year compared to never hard drug users (p &lt; 0.0001, 95% CI: -84.11, -34.20).\n\nChange Reference Category\n\n* Change reference Category;\nPROC MIXED DATA = PROJ2LMM.DATA METHOD = REML;\n    CLASS newid hard_drugs_grp (REF = \"Previous User\") ADH_HIGHVSLOW;\n    MODEL LEU3N = hard_drugs_grp ADH_HIGHVSLOW years hard_drugs_grp*years / CL;\n    RANDOM intercept / SUBJECT = newid TYPE = UN VCORR;\n    ODS OUTPUT FitStatistics = Full_Model_Un;\n  ODS SELECT SolutionF Tests3; * Output only these specific tables;\n    RUN;\n\nlstlog\n\n\n&lt;!DOCTYPE html&gt;\n\n\n\n\n\nSAS Output\n\n\n\n\n\n\n\nPredicted Log Viral Load by Adherence Level\n\n\n\n\nThe Mixed Procedure\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolution for Fixed Effects\n\n\n\n\nEffect\n\n\nhard_drugs_grp\n\n\nADH_HIGHVSLOW\n\n\nEstimate\n\n\nStandardError\n\n\nDF\n\n\nt Value\n\n\nPr &gt; |t|\n\n\nAlpha\n\n\nLower\n\n\nUpper\n\n\n\n\n\n\nIntercept\n\n\n \n\n\n \n\n\n299.95\n\n\n44.9793\n\n\n538\n\n\n6.67\n\n\n&lt;.0001\n\n\n0.05\n\n\n211.59\n\n\n388.30\n\n\n\n\nhard_drugs_grp\n\n\nCurrent User\n\n\n \n\n\n163.73\n\n\n46.9672\n\n\n1061\n\n\n3.49\n\n\n0.0005\n\n\n0.05\n\n\n71.5748\n\n\n255.89\n\n\n\n\nhard_drugs_grp\n\n\nNever User\n\n\n \n\n\n47.3576\n\n\n37.1302\n\n\n1061\n\n\n1.28\n\n\n0.2024\n\n\n0.05\n\n\n-25.4993\n\n\n120.21\n\n\n\n\nhard_drugs_grp\n\n\nPrevious User\n\n\n \n\n\n0\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n\n\nADH_HIGHVSLOW\n\n\n \n\n\nHigh Adherence\n\n\n33.6629\n\n\n32.0532\n\n\n1061\n\n\n1.05\n\n\n0.2939\n\n\n0.05\n\n\n-29.2320\n\n\n96.5579\n\n\n\n\nADH_HIGHVSLOW\n\n\n \n\n\nLow Adherence\n\n\n0\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n\n\nyears\n\n\n \n\n\n \n\n\n57.1508\n\n\n12.0511\n\n\n1061\n\n\n4.74\n\n\n&lt;.0001\n\n\n0.05\n\n\n33.5041\n\n\n80.7975\n\n\n\n\nyears*hard_drugs_grp\n\n\nCurrent User\n\n\n \n\n\n-27.0024\n\n\n16.0179\n\n\n1061\n\n\n-1.69\n\n\n0.0921\n\n\n0.05\n\n\n-58.4327\n\n\n4.4279\n\n\n\n\nyears*hard_drugs_grp\n\n\nNever User\n\n\n \n\n\n32.3086\n\n\n12.6813\n\n\n1061\n\n\n2.55\n\n\n0.0110\n\n\n0.05\n\n\n7.4254\n\n\n57.1919\n\n\n\n\nyears*hard_drugs_grp\n\n\nPrevious User\n\n\n \n\n\n0\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nType 3 Tests of Fixed Effects\n\n\n\n\nEffect\n\n\nNum DF\n\n\nDen DF\n\n\nF Value\n\n\nPr &gt; F\n\n\n\n\n\n\nhard_drugs_grp\n\n\n2\n\n\n1061\n\n\n7.60\n\n\n0.0005\n\n\n\n\nADH_HIGHVSLOW\n\n\n1\n\n\n1061\n\n\n1.10\n\n\n0.2939\n\n\n\n\nyears\n\n\n1\n\n\n1061\n\n\n114.80\n\n\n&lt;.0001\n\n\n\n\nyears*hard_drugs_grp\n\n\n2\n\n\n1061\n\n\n15.83\n\n\n&lt;.0001\n\n\n\n\n\n\n\n\n\n\n\u0014115                                                        The SAS System                    Saturday, December 28, 2024 08:34:00 PM\n\n1098       ods listing close;ods html5 (id=saspy_internal) file=_tomods1 options(bitmap_mode='inline') device=svg style=HTMLBlue;\n1098     ! ods graphics on / outputfmt=png;\n1099       \n1100       * Change reference Category;\n1101       PROC MIXED DATA = PROJ2LMM.DATA METHOD = REML;\n1102        CLASS newid hard_drugs_grp (REF = \"Previous User\") ADH_HIGHVSLOW;\n1103        MODEL LEU3N = hard_drugs_grp ADH_HIGHVSLOW years hard_drugs_grp*years / CL;\n1104        RANDOM intercept / SUBJECT = newid TYPE = UN VCORR;\n1105        ODS OUTPUT FitStatistics = Full_Model_Un;\n1106         ODS SELECT SolutionF Tests3; * Output only these specific tables;\n1107        RUN;\n1108       \n1109       \n1110       ods html5 (id=saspy_internal) close;ods listing;\n1111       \n\u0014116                                                        The SAS System                    Saturday, December 28, 2024 08:34:00 PM\n\n1112       \n\n\n\n\nTop of Tabset\n\n\n\nPrevious hard drug users had on average a 164.66 lower CD4+ T Cell count compared to current users (p &lt; 0.0001, 95% CI: 86.44, 242.87).\nThe slope for CD4+ T Cell count over time did not differ between previous and current hard drug users (p = 0.14, 95% CI: -62.49, 8.49).\nTop of Tabset\n\n\n\nGenerate Predicted Values with PROC PLM\n\n* Use PROC PLM to generate the predicted values under the final model;\nPROC PLM RESTORE = Model_LEU3N;\n    SCORE DATA = Proj2LMM.Data OUT = Proj2LMM.Data PREDICTED = Predicted_LEU3N;\n    RUN;\n\n\n\nVisualize Predicted Values\n\n* Visualize the predicted values under the final model;\nPROC SGPLOT DATA = Proj2LMM.Data;\n    REG X = Years Y = Predicted_LEU3N / GROUP = hard_drugs_grp LINEATTRS = (THICKNESS=2) NOMARKERS;\n    XAXIS LABEL = \"Years\";\n    YAXIS LABEL = \"Predicted CD4+ T Cell Count\";\n    TITLE \"Predicted CD4+ T Cell Count by Hard Drug Use\";\n    RUN;\n\nlstlog\n\n\n&lt;!DOCTYPE html&gt;\n\n\n\n\n\nSAS Output\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u0014119                                                        The SAS System                    Saturday, December 28, 2024 08:34:00 PM\n\n1128       ods listing close;ods html5 (id=saspy_internal) file=_tomods1 options(bitmap_mode='inline') device=svg style=HTMLBlue;\n1128     ! ods graphics on / outputfmt=png;\n1129       \n1130       * Visualize the predicted values under the final model;\n1131       PROC SGPLOT DATA = Proj2LMM.Data;\n1132        REG X = Years Y = Predicted_LEU3N / GROUP = hard_drugs_grp LINEATTRS = (THICKNESS=2) NOMARKERS;\n1133        XAXIS LABEL = \"Years\";\n1134        YAXIS LABEL = \"Predicted CD4+ T Cell Count\";\n1135        TITLE \"Predicted CD4+ T Cell Count by Hard Drug Use\";\n1136        RUN;\n1137       \n1138       \n1139       ods html5 (id=saspy_internal) close;ods listing;\n1140       \n\u0014120                                                        The SAS System                    Saturday, December 28, 2024 08:34:00 PM\n\n1141       \n\n\n\n\nTop of Tabset"
  },
  {
    "objectID": "Project_2/Project_2_SAS/Code/Linear_Mixed_Model_SAS.html#Mental",
    "href": "Project_2/Project_2_SAS/Code/Linear_Mixed_Model_SAS.html#Mental",
    "title": "Linear Mixed Model (SAS)",
    "section": "Mental QOL",
    "text": "Mental QOL\nWe will be fitting the full model with hard_drugs_grp, ADH_HIGHVSLOW, years, CESD, and the hard_drugs_grp*years interaction.\nWe will also use an unstructured covariance matrix to account for unequal variances.\n\nModel SelectionAnalysisInterpretationVisualization\n\n\n\nRun the Full Model with Unstructured Covariance Matrix\n\n* Run with unstructured covariance matrix;\nPROC MIXED DATA = PROJ2LMM.DATA METHOD = REML;\n    CLASS hard_drugs_grp (REF = \"Never User\") ADH_HIGHVSLOW;\n    MODEL AGG_MENT = hard_drugs_grp ADH_HIGHVSLOW years hard_drugs_grp*years cesd / CL;\n    RANDOM intercept newid/ SUBJECT = newid TYPE = UN VCORR;\n    ODS OUTPUT FitStatistics = Full_Model_Un;\n  ODS SELECT SolutionF Tests3; * Output only these specific tables;\n    RUN;\n\nlstlog\n\n\n&lt;!DOCTYPE html&gt;\n\n\n\n\n\nSAS Output\n\n\n\n\n\n\n\nPredicted CD4+ T Cell Count by Hard Drug Use\n\n\n\n\nThe Mixed Procedure\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolution for Fixed Effects\n\n\n\n\nEffect\n\n\nhard_drugs_grp\n\n\nADH_HIGHVSLOW\n\n\nEstimate\n\n\nStandardError\n\n\nDF\n\n\nt Value\n\n\nPr &gt; |t|\n\n\nAlpha\n\n\nLower\n\n\nUpper\n\n\n\n\n\n\nIntercept\n\n\n \n\n\n \n\n\n58.5820\n\n\n0.7907\n\n\n546\n\n\n74.09\n\n\n&lt;.0001\n\n\n0.05\n\n\n57.0289\n\n\n60.1351\n\n\n\n\nhard_drugs_grp\n\n\nCurrent User\n\n\n \n\n\n-2.2698\n\n\n0.9874\n\n\n1059\n\n\n-2.30\n\n\n0.0217\n\n\n0.05\n\n\n-4.2073\n\n\n-0.3323\n\n\n\n\nhard_drugs_grp\n\n\nPrevious User\n\n\n \n\n\n0.7127\n\n\n1.0862\n\n\n1059\n\n\n0.66\n\n\n0.5119\n\n\n0.05\n\n\n-1.4186\n\n\n2.8440\n\n\n\n\nhard_drugs_grp\n\n\nNever User\n\n\n \n\n\n0\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n\n\nADH_HIGHVSLOW\n\n\n \n\n\nHigh Adherence\n\n\n-0.7240\n\n\n0.7440\n\n\n1059\n\n\n-0.97\n\n\n0.3308\n\n\n0.05\n\n\n-2.1839\n\n\n0.7360\n\n\n\n\nADH_HIGHVSLOW\n\n\n \n\n\nLow Adherence\n\n\n0\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n\n\nyears\n\n\n \n\n\n \n\n\n0.3094\n\n\n0.2068\n\n\n1059\n\n\n1.50\n\n\n0.1349\n\n\n0.05\n\n\n-0.09641\n\n\n0.7152\n\n\n\n\nyears*hard_drugs_grp\n\n\nCurrent User\n\n\n \n\n\n1.6862\n\n\n0.6090\n\n\n1059\n\n\n2.77\n\n\n0.0057\n\n\n0.05\n\n\n0.4913\n\n\n2.8812\n\n\n\n\nyears*hard_drugs_grp\n\n\nPrevious User\n\n\n \n\n\n-0.3602\n\n\n0.6633\n\n\n1059\n\n\n-0.54\n\n\n0.5872\n\n\n0.05\n\n\n-1.6617\n\n\n0.9414\n\n\n\n\nyears*hard_drugs_grp\n\n\nNever User\n\n\n \n\n\n0\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n\n\nCESD\n\n\n \n\n\n \n\n\n-0.9088\n\n\n0.01778\n\n\n1059\n\n\n-51.12\n\n\n&lt;.0001\n\n\n0.05\n\n\n-0.9437\n\n\n-0.8740\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nType 3 Tests of Fixed Effects\n\n\n\n\nEffect\n\n\nNum DF\n\n\nDen DF\n\n\nF Value\n\n\nPr &gt; F\n\n\n\n\n\n\nhard_drugs_grp\n\n\n2\n\n\n1059\n\n\n3.02\n\n\n0.0492\n\n\n\n\nADH_HIGHVSLOW\n\n\n1\n\n\n1059\n\n\n0.95\n\n\n0.3308\n\n\n\n\nyears\n\n\n1\n\n\n1059\n\n\n6.64\n\n\n0.0101\n\n\n\n\nyears*hard_drugs_grp\n\n\n2\n\n\n1059\n\n\n4.18\n\n\n0.0155\n\n\n\n\nCESD\n\n\n1\n\n\n1059\n\n\n2613.16\n\n\n&lt;.0001\n\n\n\n\n\n\n\n\n\n\n\u0014121                                                        The SAS System                    Saturday, December 28, 2024 08:34:00 PM\n\n1144       ods listing close;ods html5 (id=saspy_internal) file=_tomods1 options(bitmap_mode='inline') device=svg style=HTMLBlue;\n1144     ! ods graphics on / outputfmt=png;\n1145       \n1146       * Run with unstructured covariance matrix;\n1147       PROC MIXED DATA = PROJ2LMM.DATA METHOD = REML;\n1148        CLASS hard_drugs_grp (REF = \"Never User\") ADH_HIGHVSLOW;\n1149        MODEL AGG_MENT = hard_drugs_grp ADH_HIGHVSLOW years hard_drugs_grp*years cesd / CL;\n1150        RANDOM intercept newid/ SUBJECT = newid TYPE = UN VCORR;\n1151        ODS OUTPUT FitStatistics = Full_Model_Un;\n1152         ODS SELECT SolutionF Tests3; * Output only these specific tables;\n1153        RUN;\n1154       \n1155       \n1156       ods html5 (id=saspy_internal) close;ods listing;\n1157       \n\u0014122                                                        The SAS System                    Saturday, December 28, 2024 08:34:00 PM\n\n1158       \n\n\n\n\nThe interaction is significant.\n\n\nRun Model without Interaction\n\n* Run without the interaction term;\nPROC MIXED DATA = PROJ2LMM.DATA METHOD = REML;\n    CLASS hard_drugs_grp (REF = \"Never User\") ADH_HIGHVSLOW;\n    MODEL AGG_MENT = hard_drugs_grp ADH_HIGHVSLOW years cesd / CL;\n    RANDOM intercept newid/ SUBJECT = newid TYPE = UN VCORR;\n    ODS OUTPUT FitStatistics = Red_Model;\n  ODS SELECT SolutionF Tests3; * Output only these specific tables;\n    RUN;\n\nlstlog\n\n\n&lt;!DOCTYPE html&gt;\n\n\n\n\n\nSAS Output\n\n\n\n\n\n\n\nPredicted CD4+ T Cell Count by Hard Drug Use\n\n\n\n\nThe Mixed Procedure\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolution for Fixed Effects\n\n\n\n\nEffect\n\n\nhard_drugs_grp\n\n\nADH_HIGHVSLOW\n\n\nEstimate\n\n\nStandardError\n\n\nDF\n\n\nt Value\n\n\nPr &gt; |t|\n\n\nAlpha\n\n\nLower\n\n\nUpper\n\n\n\n\n\n\nIntercept\n\n\n \n\n\n \n\n\n58.3585\n\n\n0.7846\n\n\n546\n\n\n74.38\n\n\n&lt;.0001\n\n\n0.05\n\n\n56.8173\n\n\n59.8997\n\n\n\n\nhard_drugs_grp\n\n\nCurrent User\n\n\n \n\n\n-0.5282\n\n\n0.7638\n\n\n1061\n\n\n-0.69\n\n\n0.4894\n\n\n0.05\n\n\n-2.0269\n\n\n0.9706\n\n\n\n\nhard_drugs_grp\n\n\nPrevious User\n\n\n \n\n\n0.3239\n\n\n0.8617\n\n\n1061\n\n\n0.38\n\n\n0.7071\n\n\n0.05\n\n\n-1.3669\n\n\n2.0147\n\n\n\n\nhard_drugs_grp\n\n\nNever User\n\n\n \n\n\n0\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n\n\nADH_HIGHVSLOW\n\n\n \n\n\nHigh Adherence\n\n\n-0.7059\n\n\n0.7454\n\n\n1061\n\n\n-0.95\n\n\n0.3438\n\n\n0.05\n\n\n-2.1685\n\n\n0.7566\n\n\n\n\nADH_HIGHVSLOW\n\n\n \n\n\nLow Adherence\n\n\n0\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n\n\nyears\n\n\n \n\n\n \n\n\n0.4595\n\n\n0.1860\n\n\n1061\n\n\n2.47\n\n\n0.0137\n\n\n0.05\n\n\n0.09442\n\n\n0.8245\n\n\n\n\nCESD\n\n\n \n\n\n \n\n\n-0.9042\n\n\n0.01774\n\n\n1061\n\n\n-50.97\n\n\n&lt;.0001\n\n\n0.05\n\n\n-0.9390\n\n\n-0.8694\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nType 3 Tests of Fixed Effects\n\n\n\n\nEffect\n\n\nNum DF\n\n\nDen DF\n\n\nF Value\n\n\nPr &gt; F\n\n\n\n\n\n\nhard_drugs_grp\n\n\n2\n\n\n1061\n\n\n0.34\n\n\n0.7137\n\n\n\n\nADH_HIGHVSLOW\n\n\n1\n\n\n1061\n\n\n0.90\n\n\n0.3438\n\n\n\n\nyears\n\n\n1\n\n\n1061\n\n\n6.10\n\n\n0.0137\n\n\n\n\nCESD\n\n\n1\n\n\n1061\n\n\n2598.01\n\n\n&lt;.0001\n\n\n\n\n\n\n\n\n\n\n\u0014123                                                        The SAS System                    Saturday, December 28, 2024 08:34:00 PM\n\n1161       ods listing close;ods html5 (id=saspy_internal) file=_tomods1 options(bitmap_mode='inline') device=svg style=HTMLBlue;\n1161     ! ods graphics on / outputfmt=png;\n1162       \n1163       * Run without the interaction term;\n1164       PROC MIXED DATA = PROJ2LMM.DATA METHOD = REML;\n1165        CLASS hard_drugs_grp (REF = \"Never User\") ADH_HIGHVSLOW;\n1166        MODEL AGG_MENT = hard_drugs_grp ADH_HIGHVSLOW years cesd / CL;\n1167        RANDOM intercept newid/ SUBJECT = newid TYPE = UN VCORR;\n1168        ODS OUTPUT FitStatistics = Red_Model;\n1169         ODS SELECT SolutionF Tests3; * Output only these specific tables;\n1170        RUN;\n1171       \n1172       \n1173       ods html5 (id=saspy_internal) close;ods listing;\n1174       \n\u0014124                                                        The SAS System                    Saturday, December 28, 2024 08:34:00 PM\n\n1175       \n\n\n\n\n\n\nCompare BICs\n\n* Print BICs;\nPROC PRINT DATA = Compare_bic;\n    RUN;\n\nlstlog\n\n\n&lt;!DOCTYPE html&gt;\n\n\n\n\n\nSAS Output\n\n\n\n\n\n\n\nPredicted CD4+ T Cell Count by Hard Drug Use\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObs\n\n\nDescr\n\n\nValue\n\n\nModel\n\n\n\n\n\n\n1\n\n\nBIC (Smaller is Better)\n\n\n7876.9\n\n\nModel 1\n\n\n\n\n2\n\n\nBIC (Smaller is Better)\n\n\n7880.5\n\n\nModel 2\n\n\n\n\n3\n\n\nBIC (Smaller is Better)\n\n\n7865.2\n\n\nModel 3\n\n\n\n\n\n\n\n\n\n\n\u0014125                                                        The SAS System                    Saturday, December 28, 2024 08:34:00 PM\n\n1178       ods listing close;ods html5 (id=saspy_internal) file=_tomods1 options(bitmap_mode='inline') device=svg style=HTMLBlue;\n1178     ! ods graphics on / outputfmt=png;\n1179       \n1180       * Print BICs;\n1181       PROC PRINT DATA = Compare_bic;\n1182        RUN;\n1183       \n1184       \n1185       ods html5 (id=saspy_internal) close;ods listing;\n1186       \n\u0014126                                                        The SAS System                    Saturday, December 28, 2024 08:34:00 PM\n\n1187       \n\n\n\n\nThe final model preferred by BIC is the model which includes the interaction term.\nTop of Tabset\n\n\n\n\n* Run with unstructured covariance matrix;\nPROC MIXED DATA = PROJ2LMM.DATA METHOD = REML;\n    CLASS hard_drugs_grp (REF = \"Never User\") ADH_HIGHVSLOW;\n    MODEL AGG_MENT = hard_drugs_grp ADH_HIGHVSLOW years hard_drugs_grp*years CESD / CL;\n    RANDOM intercept newid/ SUBJECT = newid TYPE = UN VCORR;\n    ODS OUTPUT FitStatistics = Full_Model_Un;\n  STORE OUT=Model_AGG_MENT; *Store the model information for later use;\n    RUN;\n\nlstlog\n\n\n&lt;!DOCTYPE html&gt;\n\n\n\n\n\nSAS Output\n\n\n\n\n\n\n\nPredicted CD4+ T Cell Count by Hard Drug Use\n\n\n\n\nThe Mixed Procedure\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel Information\n\n\n\n\n\n\nData Set\n\n\nPROJ2LMM.DATA\n\n\n\n\nDependent Variable\n\n\nAGG_MENT\n\n\n\n\nCovariance Structure\n\n\nUnstructured\n\n\n\n\nSubject Effect\n\n\nnewid\n\n\n\n\nEstimation Method\n\n\nREML\n\n\n\n\nResidual Variance Method\n\n\nProfile\n\n\n\n\nFixed Effects SE Method\n\n\nModel-Based\n\n\n\n\nDegrees of Freedom Method\n\n\nContainment\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nClass Level Information\n\n\n\n\nClass\n\n\nLevels\n\n\nValues\n\n\n\n\n\n\nhard_drugs_grp\n\n\n3\n\n\nCurrent User Previous User Never User\n\n\n\n\nADH_HIGHVSLOW\n\n\n2\n\n\nHigh Adherence Low Adherence\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDimensions\n\n\n\n\n\n\nCovariance Parameters\n\n\n4\n\n\n\n\nColumns in X\n\n\n11\n\n\n\n\nColumns in Z per Subject\n\n\n2\n\n\n\n\nSubjects\n\n\n550\n\n\n\n\nMax Obs per Subject\n\n\n3\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNumber of Observations\n\n\n\n\n\n\nNumber of Observations Read\n\n\n1650\n\n\n\n\nNumber of Observations Used\n\n\n1613\n\n\n\n\nNumber of Observations Not Used\n\n\n37\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIteration History\n\n\n\n\nIteration\n\n\nEvaluations\n\n\n-2 Res Log Like\n\n\nCriterion\n\n\n\n\n\n\n0\n\n\n1\n\n\n10950.77671696\n\n\n \n\n\n\n\n1\n\n\n2\n\n\n10814.72936283\n\n\n0.00000017\n\n\n\n\n2\n\n\n1\n\n\n10814.72869730\n\n\n0.00000000\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nConvergence criteria met.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEstimated V Correlation Matrix for Subject 1\n\n\n\n\nRow\n\n\nCol1\n\n\nCol2\n\n\nCol3\n\n\n\n\n\n\n1\n\n\n1.0000\n\n\n0.3392\n\n\n0.3392\n\n\n\n\n2\n\n\n0.3392\n\n\n1.0000\n\n\n0.3392\n\n\n\n\n3\n\n\n0.3392\n\n\n0.3392\n\n\n1.0000\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCovariance Parameter Estimates\n\n\n\n\nCov Parm\n\n\nSubject\n\n\nEstimate\n\n\n\n\n\n\nUN(1,1)\n\n\nnewid\n\n\n18.8576\n\n\n\n\nUN(2,1)\n\n\nnewid\n\n\n-0.04362\n\n\n\n\nUN(2,2)\n\n\nnewid\n\n\n0.000207\n\n\n\n\nResidual\n\n\n \n\n\n36.5706\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFit Statistics\n\n\n\n\n\n\n-2 Res Log Likelihood\n\n\n10814.7\n\n\n\n\nAIC (Smaller is Better)\n\n\n10822.7\n\n\n\n\nAICC (Smaller is Better)\n\n\n10822.8\n\n\n\n\nBIC (Smaller is Better)\n\n\n10840.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNull Model Likelihood Ratio Test\n\n\n\n\nDF\n\n\nChi-Square\n\n\nPr &gt; ChiSq\n\n\n\n\n\n\n3\n\n\n136.05\n\n\n&lt;.0001\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolution for Fixed Effects\n\n\n\n\nEffect\n\n\nhard_drugs_grp\n\n\nADH_HIGHVSLOW\n\n\nEstimate\n\n\nStandardError\n\n\nDF\n\n\nt Value\n\n\nPr &gt; |t|\n\n\nAlpha\n\n\nLower\n\n\nUpper\n\n\n\n\n\n\nIntercept\n\n\n \n\n\n \n\n\n58.5820\n\n\n0.7907\n\n\n546\n\n\n74.09\n\n\n&lt;.0001\n\n\n0.05\n\n\n57.0289\n\n\n60.1351\n\n\n\n\nhard_drugs_grp\n\n\nCurrent User\n\n\n \n\n\n-2.2698\n\n\n0.9874\n\n\n1059\n\n\n-2.30\n\n\n0.0217\n\n\n0.05\n\n\n-4.2073\n\n\n-0.3323\n\n\n\n\nhard_drugs_grp\n\n\nPrevious User\n\n\n \n\n\n0.7127\n\n\n1.0862\n\n\n1059\n\n\n0.66\n\n\n0.5119\n\n\n0.05\n\n\n-1.4186\n\n\n2.8440\n\n\n\n\nhard_drugs_grp\n\n\nNever User\n\n\n \n\n\n0\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n\n\nADH_HIGHVSLOW\n\n\n \n\n\nHigh Adherence\n\n\n-0.7240\n\n\n0.7440\n\n\n1059\n\n\n-0.97\n\n\n0.3308\n\n\n0.05\n\n\n-2.1839\n\n\n0.7360\n\n\n\n\nADH_HIGHVSLOW\n\n\n \n\n\nLow Adherence\n\n\n0\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n\n\nyears\n\n\n \n\n\n \n\n\n0.3094\n\n\n0.2068\n\n\n1059\n\n\n1.50\n\n\n0.1349\n\n\n0.05\n\n\n-0.09641\n\n\n0.7152\n\n\n\n\nyears*hard_drugs_grp\n\n\nCurrent User\n\n\n \n\n\n1.6862\n\n\n0.6090\n\n\n1059\n\n\n2.77\n\n\n0.0057\n\n\n0.05\n\n\n0.4913\n\n\n2.8812\n\n\n\n\nyears*hard_drugs_grp\n\n\nPrevious User\n\n\n \n\n\n-0.3602\n\n\n0.6633\n\n\n1059\n\n\n-0.54\n\n\n0.5872\n\n\n0.05\n\n\n-1.6617\n\n\n0.9414\n\n\n\n\nyears*hard_drugs_grp\n\n\nNever User\n\n\n \n\n\n0\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n\n\nCESD\n\n\n \n\n\n \n\n\n-0.9088\n\n\n0.01778\n\n\n1059\n\n\n-51.12\n\n\n&lt;.0001\n\n\n0.05\n\n\n-0.9437\n\n\n-0.8740\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nType 3 Tests of Fixed Effects\n\n\n\n\nEffect\n\n\nNum DF\n\n\nDen DF\n\n\nF Value\n\n\nPr &gt; F\n\n\n\n\n\n\nhard_drugs_grp\n\n\n2\n\n\n1059\n\n\n3.02\n\n\n0.0492\n\n\n\n\nADH_HIGHVSLOW\n\n\n1\n\n\n1059\n\n\n0.95\n\n\n0.3308\n\n\n\n\nyears\n\n\n1\n\n\n1059\n\n\n6.64\n\n\n0.0101\n\n\n\n\nyears*hard_drugs_grp\n\n\n2\n\n\n1059\n\n\n4.18\n\n\n0.0155\n\n\n\n\nCESD\n\n\n1\n\n\n1059\n\n\n2613.16\n\n\n&lt;.0001\n\n\n\n\n\n\n\n\n\n\n\u0014127                                                        The SAS System                    Saturday, December 28, 2024 08:34:00 PM\n\n1190       ods listing close;ods html5 (id=saspy_internal) file=_tomods1 options(bitmap_mode='inline') device=svg style=HTMLBlue;\n1190     ! ods graphics on / outputfmt=png;\n1191       \n1192       * Run with unstructured covariance matrix;\n1193       PROC MIXED DATA = PROJ2LMM.DATA METHOD = REML;\n1194        CLASS hard_drugs_grp (REF = \"Never User\") ADH_HIGHVSLOW;\n1195        MODEL AGG_MENT = hard_drugs_grp ADH_HIGHVSLOW years hard_drugs_grp*years CESD / CL;\n1196        RANDOM intercept newid/ SUBJECT = newid TYPE = UN VCORR;\n1197        ODS OUTPUT FitStatistics = Full_Model_Un;\n1198         STORE OUT=Model_AGG_MENT; *Store the model information for later use;\n1199        RUN;\n1200       \n1201       \n1202       ods html5 (id=saspy_internal) close;ods listing;\n1203       \n\u0014128                                                        The SAS System                    Saturday, December 28, 2024 08:34:00 PM\n\n1204       \n\n\n\n\n\nChange the Reference Level\n\n* Change the reference level;\nPROC MIXED DATA = PROJ2LMM.DATA METHOD = REML;\n    CLASS hard_drugs_grp (REF = \"Previous User\") ADH_HIGHVSLOW;\n    MODEL AGG_MENT = hard_drugs_grp ADH_HIGHVSLOW years CESD hard_drugs_grp*years / CL;\n    RANDOM intercept newid/ SUBJECT = newid TYPE = UN VCORR;\n    ODS OUTPUT FitStatistics = Full_Model_Un;\n  ODS SELECT SolutionF Tests3; * Output only these specific tables;\n    RUN;\n\nlstlog\n\n\n&lt;!DOCTYPE html&gt;\n\n\n\n\n\nSAS Output\n\n\n\n\n\n\n\nPredicted CD4+ T Cell Count by Hard Drug Use\n\n\n\n\nThe Mixed Procedure\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolution for Fixed Effects\n\n\n\n\nEffect\n\n\nhard_drugs_grp\n\n\nADH_HIGHVSLOW\n\n\nEstimate\n\n\nStandardError\n\n\nDF\n\n\nt Value\n\n\nPr &gt; |t|\n\n\nAlpha\n\n\nLower\n\n\nUpper\n\n\n\n\n\n\nIntercept\n\n\n \n\n\n \n\n\n59.2947\n\n\n1.2809\n\n\n546\n\n\n46.29\n\n\n&lt;.0001\n\n\n0.05\n\n\n56.7786\n\n\n61.8108\n\n\n\n\nhard_drugs_grp\n\n\nCurrent User\n\n\n \n\n\n-2.9825\n\n\n1.3993\n\n\n1059\n\n\n-2.13\n\n\n0.0333\n\n\n0.05\n\n\n-5.7282\n\n\n-0.2368\n\n\n\n\nhard_drugs_grp\n\n\nNever User\n\n\n \n\n\n-0.7127\n\n\n1.0862\n\n\n1059\n\n\n-0.66\n\n\n0.5119\n\n\n0.05\n\n\n-2.8440\n\n\n1.4186\n\n\n\n\nhard_drugs_grp\n\n\nPrevious User\n\n\n \n\n\n0\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n\n\nADH_HIGHVSLOW\n\n\n \n\n\nHigh Adherence\n\n\n-0.7240\n\n\n0.7440\n\n\n1059\n\n\n-0.97\n\n\n0.3308\n\n\n0.05\n\n\n-2.1839\n\n\n0.7360\n\n\n\n\nADH_HIGHVSLOW\n\n\n \n\n\nLow Adherence\n\n\n0\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n\n\nyears\n\n\n \n\n\n \n\n\n-0.05081\n\n\n0.6310\n\n\n1059\n\n\n-0.08\n\n\n0.9358\n\n\n0.05\n\n\n-1.2890\n\n\n1.1874\n\n\n\n\nCESD\n\n\n \n\n\n \n\n\n-0.9088\n\n\n0.01778\n\n\n1059\n\n\n-51.12\n\n\n&lt;.0001\n\n\n0.05\n\n\n-0.9437\n\n\n-0.8740\n\n\n\n\nyears*hard_drugs_grp\n\n\nCurrent User\n\n\n \n\n\n2.0464\n\n\n0.8525\n\n\n1059\n\n\n2.40\n\n\n0.0165\n\n\n0.05\n\n\n0.3736\n\n\n3.7192\n\n\n\n\nyears*hard_drugs_grp\n\n\nNever User\n\n\n \n\n\n0.3602\n\n\n0.6633\n\n\n1059\n\n\n0.54\n\n\n0.5872\n\n\n0.05\n\n\n-0.9414\n\n\n1.6617\n\n\n\n\nyears*hard_drugs_grp\n\n\nPrevious User\n\n\n \n\n\n0\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nType 3 Tests of Fixed Effects\n\n\n\n\nEffect\n\n\nNum DF\n\n\nDen DF\n\n\nF Value\n\n\nPr &gt; F\n\n\n\n\n\n\nhard_drugs_grp\n\n\n2\n\n\n1059\n\n\n3.02\n\n\n0.0492\n\n\n\n\nADH_HIGHVSLOW\n\n\n1\n\n\n1059\n\n\n0.95\n\n\n0.3308\n\n\n\n\nyears\n\n\n1\n\n\n1059\n\n\n6.64\n\n\n0.0101\n\n\n\n\nCESD\n\n\n1\n\n\n1059\n\n\n2613.16\n\n\n&lt;.0001\n\n\n\n\nyears*hard_drugs_grp\n\n\n2\n\n\n1059\n\n\n4.18\n\n\n0.0155\n\n\n\n\n\n\n\n\n\n\n\u0014129                                                        The SAS System                    Saturday, December 28, 2024 08:34:00 PM\n\n1207       ods listing close;ods html5 (id=saspy_internal) file=_tomods1 options(bitmap_mode='inline') device=svg style=HTMLBlue;\n1207     ! ods graphics on / outputfmt=png;\n1208       \n1209       * Change the reference level;\n1210       PROC MIXED DATA = PROJ2LMM.DATA METHOD = REML;\n1211        CLASS hard_drugs_grp (REF = \"Previous User\") ADH_HIGHVSLOW;\n1212        MODEL AGG_MENT = hard_drugs_grp ADH_HIGHVSLOW years CESD hard_drugs_grp*years / CL;\n1213        RANDOM intercept newid/ SUBJECT = newid TYPE = UN VCORR;\n1214        ODS OUTPUT FitStatistics = Full_Model_Un;\n1215         ODS SELECT SolutionF Tests3; * Output only these specific tables;\n1216        RUN;\n1217       \n1218       \n1219       ods html5 (id=saspy_internal) close;ods listing;\n1220       \n\u0014130                                                        The SAS System                    Saturday, December 28, 2024 08:34:00 PM\n\n1221       \n\n\n\n\nTop of Tabset\n\n\n\nAfter controlling for adherence and between subject means (intercepts), there was no difference in average mental QOL between previous and current hard drug users (p = 0.47, 95% CI: -1.36, 2.94). On average, current hard drug users had a mental QOL score that was 2.28 points lower than never drug users (p = 0.023, 95% CI: -4.24, -0.32).\nAfter controlling for hard drug use and between subject means, those with low adherence did not differ significantly in average mental QOL compared to those with high adherence (p = 0.26, 95% CI: -0.64, 2.35).\nThere was a significant no change in mental QOL over time while controlling for subject-specific means, hard drug use, and adherence, with an average increase of 0.31 points per year (p 0.14, 95% CI: -0.099, 0.71).\nThe slope for mental QOL over time differed significantly based on hard drug use. On average, current hard drug users had a 1.69 point increase in physical QOL per year compared to never drug users (p = 0.056, 95% CI: 0.50, 2.89). The slope for physical QOL over time did not differ between never and previous users (p = 0.58, 95% CI: -1.67, 0.94).\nControlling for adherence, hard drug use, and time, depression was a significant predictor of mental QOL. On average, mental QOL score decreased by 0.91 points for every 1 point increase in depression score (p &lt; 0.0001, 95% CI: -0.95, -0.88).\nTop of Tabset\n\n\n\nGenerate Predicted Values with PROC PLM\n\n* Use PROC PLM to generate the predicted values under the final model;\nPROC PLM RESTORE = Model_AGG_MENT;\n    SCORE DATA = Proj2LMM.Data OUT = Proj2LMM.Data PREDICTED = Predicted_AGG_MENT;\n    RUN;\n\n\n\nVisualize Predicted Values\n\n* Visualize the predicted values under the final model;\nPROC SGPLOT DATA = Proj2LMM.Data;\n    REG X = Years Y = Predicted_AGG_MENT / GROUP = hard_drugs_grp LINEATTRS = (THICKNESS=2) NOMARKERS;\n    XAXIS LABEL = \"Years\";\n    YAXIS LABEL = \"Predicted Mental QOL\";\n    TITLE \"Predicted Mental QOL by Hard Drug Use\";\n    RUN;\n\nlstlog\n\n\n&lt;!DOCTYPE html&gt;\n\n\n\n\n\nSAS Output\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u0014133                                                        The SAS System                    Saturday, December 28, 2024 08:34:00 PM\n\n1237       ods listing close;ods html5 (id=saspy_internal) file=_tomods1 options(bitmap_mode='inline') device=svg style=HTMLBlue;\n1237     ! ods graphics on / outputfmt=png;\n1238       \n1239       * Visualize the predicted values under the final model;\n1240       PROC SGPLOT DATA = Proj2LMM.Data;\n1241        REG X = Years Y = Predicted_AGG_MENT / GROUP = hard_drugs_grp LINEATTRS = (THICKNESS=2) NOMARKERS;\n1242        XAXIS LABEL = \"Years\";\n1243        YAXIS LABEL = \"Predicted Mental QOL\";\n1244        TITLE \"Predicted Mental QOL by Hard Drug Use\";\n1245        RUN;\n1246       \n1247       \n1248       ods html5 (id=saspy_internal) close;ods listing;\n1249       \n\u0014134                                                        The SAS System                    Saturday, December 28, 2024 08:34:00 PM\n\n1250       \n\n\n\n\nTop of Tabset"
  },
  {
    "objectID": "Project_2/Project_2_SAS/Code/Linear_Mixed_Model_SAS.html#Phys",
    "href": "Project_2/Project_2_SAS/Code/Linear_Mixed_Model_SAS.html#Phys",
    "title": "Linear Mixed Model (SAS)",
    "section": "Physical QOL",
    "text": "Physical QOL\nWe will begin by fitting the full model predicting physical QOL, including as covariates: hard_drugs_grp, ADH_HIGHVSLOW, FRP, years, and the hard_drugs_grp*years interaction term.\nWe know that we do not meet the assumption of equality of variances (as pictured below), so we will employ an unstructured covariance matrix to account for this.\n\nModel SelectionAnalysisInterpretationVisualization\n\n\n\nRun Model With Drug Usage Interaction\n\n* Run with unstructured covariance matrix;\nPROC MIXED DATA = PROJ2LMM.DATA METHOD = REML;\n    CLASS hard_drugs_grp (REF = \"Never User\") ADH_HIGHVSLOW FRP;\n    MODEL AGG_PHYS = hard_drugs_grp ADH_HIGHVSLOW years hard_drugs_grp*years FRP/ CL;\n    RANDOM intercept newid/ SUBJECT = newid TYPE = UN VCORR;\n    ODS OUTPUT FitStatistics = Full_Model_Phys;\n  ODS SELECT SolutionF Tests3; * Output only these specific tables;\n    RUN;\n\nlstlog\n\n\n&lt;!DOCTYPE html&gt;\n\n\n\n\n\nSAS Output\n\n\n\n\n\n\n\nPredicted Mental QOL by Hard Drug Use\n\n\n\n\nThe Mixed Procedure\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolution for Fixed Effects\n\n\n\n\nEffect\n\n\nhard_drugs_grp\n\n\nADH_HIGHVSLOW\n\n\nFRP\n\n\nEstimate\n\n\nStandardError\n\n\nDF\n\n\nt Value\n\n\nPr &gt; |t|\n\n\nAlpha\n\n\nLower\n\n\nUpper\n\n\n\n\n\n\nIntercept\n\n\n \n\n\n \n\n\n \n\n\n35.4001\n\n\n1.2422\n\n\n546\n\n\n28.50\n\n\n&lt;.0001\n\n\n0.05\n\n\n32.9601\n\n\n37.8400\n\n\n\n\nhard_drugs_grp\n\n\nCurrent User\n\n\n \n\n\n \n\n\n0.9565\n\n\n1.1577\n\n\n1086\n\n\n0.83\n\n\n0.4088\n\n\n0.05\n\n\n-1.3150\n\n\n3.2281\n\n\n\n\nhard_drugs_grp\n\n\nPrevious User\n\n\n \n\n\n \n\n\n-2.0748\n\n\n1.3039\n\n\n1086\n\n\n-1.59\n\n\n0.1118\n\n\n0.05\n\n\n-4.6332\n\n\n0.4836\n\n\n\n\nhard_drugs_grp\n\n\nNever User\n\n\n \n\n\n \n\n\n0\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n\n\nADH_HIGHVSLOW\n\n\n \n\n\nHigh Adherence\n\n\n \n\n\n2.0593\n\n\n1.0368\n\n\n1086\n\n\n1.99\n\n\n0.0473\n\n\n0.05\n\n\n0.02482\n\n\n4.0937\n\n\n\n\nADH_HIGHVSLOW\n\n\n \n\n\nLow Adherence\n\n\n \n\n\n0\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n\n\nyears\n\n\n \n\n\n \n\n\n \n\n\n-0.5794\n\n\n0.1841\n\n\n1086\n\n\n-3.15\n\n\n0.0017\n\n\n0.05\n\n\n-0.9407\n\n\n-0.2181\n\n\n\n\nyears*hard_drugs_grp\n\n\nCurrent User\n\n\n \n\n\n \n\n\n-1.8024\n\n\n0.5321\n\n\n1086\n\n\n-3.39\n\n\n0.0007\n\n\n0.05\n\n\n-2.8465\n\n\n-0.7582\n\n\n\n\nyears*hard_drugs_grp\n\n\nPrevious User\n\n\n \n\n\n \n\n\n-0.8203\n\n\n0.5990\n\n\n1086\n\n\n-1.37\n\n\n0.1711\n\n\n0.05\n\n\n-1.9956\n\n\n0.3550\n\n\n\n\nyears*hard_drugs_grp\n\n\nNever User\n\n\n \n\n\n \n\n\n0\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n\n\nFRP\n\n\n \n\n\n \n\n\nNo\n\n\n14.3456\n\n\n0.7605\n\n\n1086\n\n\n18.86\n\n\n&lt;.0001\n\n\n0.05\n\n\n12.8534\n\n\n15.8378\n\n\n\n\nFRP\n\n\n \n\n\n \n\n\nYes\n\n\n0\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nType 3 Tests of Fixed Effects\n\n\n\n\nEffect\n\n\nNum DF\n\n\nDen DF\n\n\nF Value\n\n\nPr &gt; F\n\n\n\n\n\n\nhard_drugs_grp\n\n\n2\n\n\n1086\n\n\n1.75\n\n\n0.1736\n\n\n\n\nADH_HIGHVSLOW\n\n\n1\n\n\n1086\n\n\n3.94\n\n\n0.0473\n\n\n\n\nyears\n\n\n1\n\n\n1086\n\n\n31.40\n\n\n&lt;.0001\n\n\n\n\nyears*hard_drugs_grp\n\n\n2\n\n\n1086\n\n\n6.26\n\n\n0.0020\n\n\n\n\nFRP\n\n\n1\n\n\n1086\n\n\n355.83\n\n\n&lt;.0001\n\n\n\n\n\n\n\n\n\n\n\u0014135                                                        The SAS System                    Saturday, December 28, 2024 08:34:00 PM\n\n1253       ods listing close;ods html5 (id=saspy_internal) file=_tomods1 options(bitmap_mode='inline') device=svg style=HTMLBlue;\n1253     ! ods graphics on / outputfmt=png;\n1254       \n1255       * Run with unstructured covariance matrix;\n1256       PROC MIXED DATA = PROJ2LMM.DATA METHOD = REML;\n1257        CLASS hard_drugs_grp (REF = \"Never User\") ADH_HIGHVSLOW FRP;\n1258        MODEL AGG_PHYS = hard_drugs_grp ADH_HIGHVSLOW years hard_drugs_grp*years FRP/ CL;\n1259        RANDOM intercept newid/ SUBJECT = newid TYPE = UN VCORR;\n1260        ODS OUTPUT FitStatistics = Full_Model_Phys;\n1261         ODS SELECT SolutionF Tests3; * Output only these specific tables;\n1262        RUN;\n1263       \n1264       \n1265       ods html5 (id=saspy_internal) close;ods listing;\n1266       \n\u0014136                                                        The SAS System                    Saturday, December 28, 2024 08:34:00 PM\n\n1267       \n\n\n\n\n\n\nRun Model with Both Interaction Terms\n\n* Run with both interaction terms;\nPROC MIXED DATA = PROJ2LMM.DATA METHOD = REML;\n    CLASS hard_drugs_grp (REF = \"Never User\") ADH_HIGHVSLOW FRP;\n    MODEL AGG_PHYS = hard_drugs_grp ADH_HIGHVSLOW years hard_drugs_grp*years ADH_HIGHVSLOW*years FRP/ CL;\n    RANDOM intercept newid/ SUBJECT = newid TYPE = UN VCORR;\n    ODS OUTPUT FitStatistics = Full_Model_Phys2;\n  ODS SELECT SolutionF Tests3; * Output only these specific tables;\n    RUN;\n\nlstlog\n\n\n&lt;!DOCTYPE html&gt;\n\n\n\n\n\nSAS Output\n\n\n\n\n\n\n\nPredicted Mental QOL by Hard Drug Use\n\n\n\n\nThe Mixed Procedure\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolution for Fixed Effects\n\n\n\n\nEffect\n\n\nhard_drugs_grp\n\n\nADH_HIGHVSLOW\n\n\nFRP\n\n\nEstimate\n\n\nStandardError\n\n\nDF\n\n\nt Value\n\n\nPr &gt; |t|\n\n\nAlpha\n\n\nLower\n\n\nUpper\n\n\n\n\n\n\nIntercept\n\n\n \n\n\n \n\n\n \n\n\n37.2176\n\n\n1.3316\n\n\n546\n\n\n27.95\n\n\n&lt;.0001\n\n\n0.05\n\n\n34.6020\n\n\n39.8332\n\n\n\n\nhard_drugs_grp\n\n\nCurrent User\n\n\n \n\n\n \n\n\n1.0279\n\n\n1.1572\n\n\n1085\n\n\n0.89\n\n\n0.3746\n\n\n0.05\n\n\n-1.2427\n\n\n3.2984\n\n\n\n\nhard_drugs_grp\n\n\nPrevious User\n\n\n \n\n\n \n\n\n-2.1340\n\n\n1.3033\n\n\n1085\n\n\n-1.64\n\n\n0.1018\n\n\n0.05\n\n\n-4.6912\n\n\n0.4232\n\n\n\n\nhard_drugs_grp\n\n\nNever User\n\n\n \n\n\n \n\n\n0\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n\n\nADH_HIGHVSLOW\n\n\n \n\n\nHigh Adherence\n\n\n \n\n\n0.06267\n\n\n1.1718\n\n\n1085\n\n\n0.05\n\n\n0.9574\n\n\n0.05\n\n\n-2.2365\n\n\n2.3618\n\n\n\n\nADH_HIGHVSLOW\n\n\n \n\n\nLow Adherence\n\n\n \n\n\n0\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n\n\nyears\n\n\n \n\n\n \n\n\n \n\n\n-2.3895\n\n\n0.5264\n\n\n1085\n\n\n-4.54\n\n\n&lt;.0001\n\n\n0.05\n\n\n-3.4224\n\n\n-1.3565\n\n\n\n\nyears*hard_drugs_grp\n\n\nCurrent User\n\n\n \n\n\n \n\n\n-1.8725\n\n\n0.5293\n\n\n1085\n\n\n-3.54\n\n\n0.0004\n\n\n0.05\n\n\n-2.9110\n\n\n-0.8339\n\n\n\n\nyears*hard_drugs_grp\n\n\nPrevious User\n\n\n \n\n\n \n\n\n-0.7597\n\n\n0.5956\n\n\n1085\n\n\n-1.28\n\n\n0.2024\n\n\n0.05\n\n\n-1.9284\n\n\n0.4090\n\n\n\n\nyears*hard_drugs_grp\n\n\nNever User\n\n\n \n\n\n \n\n\n0\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n\n\nyears*ADH_HIGHVSLOW\n\n\n \n\n\nHigh Adherence\n\n\n \n\n\n2.0131\n\n\n0.5490\n\n\n1085\n\n\n3.67\n\n\n0.0003\n\n\n0.05\n\n\n0.9359\n\n\n3.0904\n\n\n\n\nyears*ADH_HIGHVSLOW\n\n\n \n\n\nLow Adherence\n\n\n \n\n\n0\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n\n\nFRP\n\n\n \n\n\n \n\n\nNo\n\n\n14.3205\n\n\n0.7567\n\n\n1085\n\n\n18.92\n\n\n&lt;.0001\n\n\n0.05\n\n\n12.8357\n\n\n15.8053\n\n\n\n\nFRP\n\n\n \n\n\n \n\n\nYes\n\n\n0\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nType 3 Tests of Fixed Effects\n\n\n\n\nEffect\n\n\nNum DF\n\n\nDen DF\n\n\nF Value\n\n\nPr &gt; F\n\n\n\n\n\n\nhard_drugs_grp\n\n\n2\n\n\n1085\n\n\n1.90\n\n\n0.1507\n\n\n\n\nADH_HIGHVSLOW\n\n\n1\n\n\n1085\n\n\n0.00\n\n\n0.9574\n\n\n\n\nyears\n\n\n1\n\n\n1085\n\n\n44.48\n\n\n&lt;.0001\n\n\n\n\nyears*hard_drugs_grp\n\n\n2\n\n\n1085\n\n\n6.68\n\n\n0.0013\n\n\n\n\nyears*ADH_HIGHVSLOW\n\n\n1\n\n\n1085\n\n\n13.45\n\n\n0.0003\n\n\n\n\nFRP\n\n\n1\n\n\n1085\n\n\n358.14\n\n\n&lt;.0001\n\n\n\n\n\n\n\n\n\n\n\u0014137                                                        The SAS System                    Saturday, December 28, 2024 08:34:00 PM\n\n1270       ods listing close;ods html5 (id=saspy_internal) file=_tomods1 options(bitmap_mode='inline') device=svg style=HTMLBlue;\n1270     ! ods graphics on / outputfmt=png;\n1271       \n1272       * Run with both interaction terms;\n1273       PROC MIXED DATA = PROJ2LMM.DATA METHOD = REML;\n1274        CLASS hard_drugs_grp (REF = \"Never User\") ADH_HIGHVSLOW FRP;\n1275        MODEL AGG_PHYS = hard_drugs_grp ADH_HIGHVSLOW years hard_drugs_grp*years ADH_HIGHVSLOW*years FRP/ CL;\n1276        RANDOM intercept newid/ SUBJECT = newid TYPE = UN VCORR;\n1277        ODS OUTPUT FitStatistics = Full_Model_Phys2;\n1278         ODS SELECT SolutionF Tests3; * Output only these specific tables;\n1279        RUN;\n1280       \n1281       \n1282       ods html5 (id=saspy_internal) close;ods listing;\n1283       \n\u0014138                                                        The SAS System                    Saturday, December 28, 2024 08:34:00 PM\n\n1284       \n\n\n\n\n\n\nCompare BICs\n\n** Perform Model Selection Comparing BIC;\nDATA COMPARE_BIC;\n    SET Full_Model_Phys(in=full) Full_Model_Phys2(in = reduced1);\n    IF full THEN Model = \"One Interaction\";\n    ELSE IF reduced1 THEN MODEL = \"Two Interactions\";\n    \n    IF Descr = \"BIC (Smaller is Better)\";\n    RUN;\n    \n* Print BICs;\nPROC PRINT DATA = Compare_bic;\n    RUN;\n\nlstlog\n\n\n&lt;!DOCTYPE html&gt;\n\n\n\n\n\nSAS Output\n\n\n\n\n\n\n\nPredicted Mental QOL by Hard Drug Use\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObs\n\n\nDescr\n\n\nValue\n\n\nModel\n\n\n\n\n\n\n1\n\n\nBIC (Smaller is Better)\n\n\n11152.6\n\n\nOne Interaction\n\n\n\n\n2\n\n\nBIC (Smaller is Better)\n\n\n11138.6\n\n\nTwo Interaction\n\n\n\n\n\n\n\n\n\n\n\u0014139                                                        The SAS System                    Saturday, December 28, 2024 08:34:00 PM\n\n1287       ods listing close;ods html5 (id=saspy_internal) file=_tomods1 options(bitmap_mode='inline') device=svg style=HTMLBlue;\n1287     ! ods graphics on / outputfmt=png;\n1288       \n1289       ** Perform Model Selection Comparing BIC;\n1290       DATA COMPARE_BIC;\n1291        SET Full_Model_Phys(in=full) Full_Model_Phys2(in = reduced1);\n1292        IF full THEN Model = \"One Interaction\";\n1293        ELSE IF reduced1 THEN MODEL = \"Two Interactions\";\n1294        \n1295        IF Descr = \"BIC (Smaller is Better)\";\n1296        RUN;\n1297        \n1298       * Print BICs;\n1299       PROC PRINT DATA = Compare_bic;\n1300        RUN;\n1301       \n1302       \n1303       ods html5 (id=saspy_internal) close;ods listing;\n1304       \n\u0014140                                                        The SAS System                    Saturday, December 28, 2024 08:34:00 PM\n\n1305       \n\n\n\n\nBIC prefers the final model with both interaction terms.\nTop of Tabset\n\n\n\n\nRun Final Model\n\n* Run with both interaction terms;\nPROC MIXED DATA = PROJ2LMM.DATA METHOD = REML;\n    CLASS hard_drugs_grp (REF = \"Never User\") ADH_HIGHVSLOW FRP;\n    MODEL AGG_PHYS = hard_drugs_grp ADH_HIGHVSLOW years hard_drugs_grp*years ADH_HIGHVSLOW*years FRP/ CL;\n    RANDOM intercept newid/ SUBJECT = newid TYPE = UN VCORR;\n    ODS OUTPUT FitStatistics = Full_Model_Phys2;\n  STORE OUT=Model_AGG_PHYS; *Store the model information for later use;\n    RUN;\n\nlstlog\n\n\n&lt;!DOCTYPE html&gt;\n\n\n\n\n\nSAS Output\n\n\n\n\n\n\n\nPredicted Mental QOL by Hard Drug Use\n\n\n\n\nThe Mixed Procedure\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel Information\n\n\n\n\n\n\nData Set\n\n\nPROJ2LMM.DATA\n\n\n\n\nDependent Variable\n\n\nAGG_PHYS\n\n\n\n\nCovariance Structure\n\n\nUnstructured\n\n\n\n\nSubject Effect\n\n\nnewid\n\n\n\n\nEstimation Method\n\n\nREML\n\n\n\n\nResidual Variance Method\n\n\nProfile\n\n\n\n\nFixed Effects SE Method\n\n\nModel-Based\n\n\n\n\nDegrees of Freedom Method\n\n\nContainment\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nClass Level Information\n\n\n\n\nClass\n\n\nLevels\n\n\nValues\n\n\n\n\n\n\nhard_drugs_grp\n\n\n3\n\n\nCurrent User Previous User Never User\n\n\n\n\nADH_HIGHVSLOW\n\n\n2\n\n\nHigh Adherence Low Adherence\n\n\n\n\nFRP\n\n\n2\n\n\nNo Yes\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDimensions\n\n\n\n\n\n\nCovariance Parameters\n\n\n4\n\n\n\n\nColumns in X\n\n\n14\n\n\n\n\nColumns in Z per Subject\n\n\n2\n\n\n\n\nSubjects\n\n\n550\n\n\n\n\nMax Obs per Subject\n\n\n3\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNumber of Observations\n\n\n\n\n\n\nNumber of Observations Read\n\n\n1650\n\n\n\n\nNumber of Observations Used\n\n\n1640\n\n\n\n\nNumber of Observations Not Used\n\n\n10\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIteration History\n\n\n\n\nIteration\n\n\nEvaluations\n\n\n-2 Res Log Like\n\n\nCriterion\n\n\n\n\n\n\n0\n\n\n1\n\n\n11580.09175522\n\n\n \n\n\n\n\n1\n\n\n2\n\n\n11116.01477552\n\n\n0.00058034\n\n\n\n\n2\n\n\n1\n\n\n11113.41457411\n\n\n0.00001864\n\n\n\n\n3\n\n\n1\n\n\n11113.33718019\n\n\n0.00000002\n\n\n\n\n4\n\n\n1\n\n\n11113.33708101\n\n\n0.00000000\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nConvergence criteria met.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEstimated V Correlation Matrix for Subject 1\n\n\n\n\nRow\n\n\nCol1\n\n\nCol2\n\n\nCol3\n\n\n\n\n\n\n1\n\n\n1.0000\n\n\n0.6090\n\n\n0.6090\n\n\n\n\n2\n\n\n0.6090\n\n\n1.0000\n\n\n0.6090\n\n\n\n\n3\n\n\n0.6090\n\n\n0.6090\n\n\n1.0000\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCovariance Parameter Estimates\n\n\n\n\nCov Parm\n\n\nSubject\n\n\nEstimate\n\n\n\n\n\n\nUN(1,1)\n\n\nnewid\n\n\n45.8858\n\n\n\n\nUN(2,1)\n\n\nnewid\n\n\n-0.03439\n\n\n\n\nUN(2,2)\n\n\nnewid\n\n\n0.000168\n\n\n\n\nResidual\n\n\n \n\n\n29.4126\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFit Statistics\n\n\n\n\n\n\n-2 Res Log Likelihood\n\n\n11113.3\n\n\n\n\nAIC (Smaller is Better)\n\n\n11121.3\n\n\n\n\nAICC (Smaller is Better)\n\n\n11121.4\n\n\n\n\nBIC (Smaller is Better)\n\n\n11138.6\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNull Model Likelihood Ratio Test\n\n\n\n\nDF\n\n\nChi-Square\n\n\nPr &gt; ChiSq\n\n\n\n\n\n\n3\n\n\n466.75\n\n\n&lt;.0001\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolution for Fixed Effects\n\n\n\n\nEffect\n\n\nhard_drugs_grp\n\n\nADH_HIGHVSLOW\n\n\nFRP\n\n\nEstimate\n\n\nStandardError\n\n\nDF\n\n\nt Value\n\n\nPr &gt; |t|\n\n\nAlpha\n\n\nLower\n\n\nUpper\n\n\n\n\n\n\nIntercept\n\n\n \n\n\n \n\n\n \n\n\n37.2176\n\n\n1.3316\n\n\n546\n\n\n27.95\n\n\n&lt;.0001\n\n\n0.05\n\n\n34.6020\n\n\n39.8332\n\n\n\n\nhard_drugs_grp\n\n\nCurrent User\n\n\n \n\n\n \n\n\n1.0279\n\n\n1.1572\n\n\n1085\n\n\n0.89\n\n\n0.3746\n\n\n0.05\n\n\n-1.2427\n\n\n3.2984\n\n\n\n\nhard_drugs_grp\n\n\nPrevious User\n\n\n \n\n\n \n\n\n-2.1340\n\n\n1.3033\n\n\n1085\n\n\n-1.64\n\n\n0.1018\n\n\n0.05\n\n\n-4.6912\n\n\n0.4232\n\n\n\n\nhard_drugs_grp\n\n\nNever User\n\n\n \n\n\n \n\n\n0\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n\n\nADH_HIGHVSLOW\n\n\n \n\n\nHigh Adherence\n\n\n \n\n\n0.06267\n\n\n1.1718\n\n\n1085\n\n\n0.05\n\n\n0.9574\n\n\n0.05\n\n\n-2.2365\n\n\n2.3618\n\n\n\n\nADH_HIGHVSLOW\n\n\n \n\n\nLow Adherence\n\n\n \n\n\n0\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n\n\nyears\n\n\n \n\n\n \n\n\n \n\n\n-2.3895\n\n\n0.5264\n\n\n1085\n\n\n-4.54\n\n\n&lt;.0001\n\n\n0.05\n\n\n-3.4224\n\n\n-1.3565\n\n\n\n\nyears*hard_drugs_grp\n\n\nCurrent User\n\n\n \n\n\n \n\n\n-1.8725\n\n\n0.5293\n\n\n1085\n\n\n-3.54\n\n\n0.0004\n\n\n0.05\n\n\n-2.9110\n\n\n-0.8339\n\n\n\n\nyears*hard_drugs_grp\n\n\nPrevious User\n\n\n \n\n\n \n\n\n-0.7597\n\n\n0.5956\n\n\n1085\n\n\n-1.28\n\n\n0.2024\n\n\n0.05\n\n\n-1.9284\n\n\n0.4090\n\n\n\n\nyears*hard_drugs_grp\n\n\nNever User\n\n\n \n\n\n \n\n\n0\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n\n\nyears*ADH_HIGHVSLOW\n\n\n \n\n\nHigh Adherence\n\n\n \n\n\n2.0131\n\n\n0.5490\n\n\n1085\n\n\n3.67\n\n\n0.0003\n\n\n0.05\n\n\n0.9359\n\n\n3.0904\n\n\n\n\nyears*ADH_HIGHVSLOW\n\n\n \n\n\nLow Adherence\n\n\n \n\n\n0\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n\n\nFRP\n\n\n \n\n\n \n\n\nNo\n\n\n14.3205\n\n\n0.7567\n\n\n1085\n\n\n18.92\n\n\n&lt;.0001\n\n\n0.05\n\n\n12.8357\n\n\n15.8053\n\n\n\n\nFRP\n\n\n \n\n\n \n\n\nYes\n\n\n0\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nType 3 Tests of Fixed Effects\n\n\n\n\nEffect\n\n\nNum DF\n\n\nDen DF\n\n\nF Value\n\n\nPr &gt; F\n\n\n\n\n\n\nhard_drugs_grp\n\n\n2\n\n\n1085\n\n\n1.90\n\n\n0.1507\n\n\n\n\nADH_HIGHVSLOW\n\n\n1\n\n\n1085\n\n\n0.00\n\n\n0.9574\n\n\n\n\nyears\n\n\n1\n\n\n1085\n\n\n44.48\n\n\n&lt;.0001\n\n\n\n\nyears*hard_drugs_grp\n\n\n2\n\n\n1085\n\n\n6.68\n\n\n0.0013\n\n\n\n\nyears*ADH_HIGHVSLOW\n\n\n1\n\n\n1085\n\n\n13.45\n\n\n0.0003\n\n\n\n\nFRP\n\n\n1\n\n\n1085\n\n\n358.14\n\n\n&lt;.0001\n\n\n\n\n\n\n\n\n\n\n\u0014141                                                        The SAS System                    Saturday, December 28, 2024 08:34:00 PM\n\n1308       ods listing close;ods html5 (id=saspy_internal) file=_tomods1 options(bitmap_mode='inline') device=svg style=HTMLBlue;\n1308     ! ods graphics on / outputfmt=png;\n1309       \n1310       * Run with both interaction terms;\n1311       PROC MIXED DATA = PROJ2LMM.DATA METHOD = REML;\n1312        CLASS hard_drugs_grp (REF = \"Never User\") ADH_HIGHVSLOW FRP;\n1313        MODEL AGG_PHYS = hard_drugs_grp ADH_HIGHVSLOW years hard_drugs_grp*years ADH_HIGHVSLOW*years FRP/ CL;\n1314        RANDOM intercept newid/ SUBJECT = newid TYPE = UN VCORR;\n1315        ODS OUTPUT FitStatistics = Full_Model_Phys2;\n1316         STORE OUT=Model_AGG_PHYS; *Store the model information for later use;\n1317        RUN;\n1318       \n1319       \n1320       ods html5 (id=saspy_internal) close;ods listing;\n1321       \n\u0014142                                                        The SAS System                    Saturday, December 28, 2024 08:34:00 PM\n\n1322       \n\n\n\n\nAfter controlling for adherence and between subject means (intercepts), there was no difference in average physical QOL between never hard drug users and previous users (p = 0.95, 95% CI: -4.48, 0.36) and current users (p = 0.30, 95% CI: -1.02, 3.27).\nAfter controlling for hard drug use and between subject means, those with low adherence did not differ significantly in average physical QOL compared to those with high adherence (p = 0.93, 95% CI: -2.32, 2.12).\nThere was a significant decrease in physical QOL over time while controlling for subject-specific means, hard drug use, and adherence, with an average decrease of 0.38 points per year (p &lt; 0.05, 95% CI: -0.75, -0.002). However this value was borderline significant and may not be a true relationshi, especially following correction for multiple pairwise comparisons.\nThe slope for physical QOL over time differed significantly based on hard drug use. On average, current hard drug users had a 1.89 point decrease in physical QOL per year compared to never drug users (p = 0.0004, 95% CI: -2.93, -0.85). The slope for physical QOL over time did not differ between never and previous users (p = 0.2116, 95% CI: -1.916, 0.42).\nThe slope for physical QOL over time also differed significantly based on adherence. On average, those with low adherence had a 2.02 point decrease in physical QOL per year compared to those with high adherence (p = 0.0002, 95% CI: -3.10, -0.94).\nAfter controlling for hard drug user, adherence, and their interactions over time, and between subject means, those with a frailty related phenotype had on average a physical QOL score that was 14.01 points lower than those without a frailty related phenotype (p &lt; 0.0001, 95% CI: -15.49, -12.53).\n\n\nChange Reference Level\n\n* Change reference level;\nPROC MIXED DATA = PROJ2LMM.DATA METHOD = REML;\n    CLASS hard_drugs_grp (REF = \"Previous User\") ADH_HIGHVSLOW FRP;\n    MODEL AGG_PHYS = hard_drugs_grp ADH_HIGHVSLOW years hard_drugs_grp*years ADH_HIGHVSLOW*years FRP/ CL;\n    RANDOM intercept newid/ SUBJECT = newid TYPE = UN VCORR;\n    ODS OUTPUT FitStatistics = Full_Model_Phys2;\n  ODS SELECT SolutionF Tests3; * Output only these specific tables;\n    RUN;\n\nlstlog\n\n\n&lt;!DOCTYPE html&gt;\n\n\n\n\n\nSAS Output\n\n\n\n\n\n\n\nPredicted Mental QOL by Hard Drug Use\n\n\n\n\nThe Mixed Procedure\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolution for Fixed Effects\n\n\n\n\nEffect\n\n\nhard_drugs_grp\n\n\nADH_HIGHVSLOW\n\n\nFRP\n\n\nEstimate\n\n\nStandardError\n\n\nDF\n\n\nt Value\n\n\nPr &gt; |t|\n\n\nAlpha\n\n\nLower\n\n\nUpper\n\n\n\n\n\n\nIntercept\n\n\n \n\n\n \n\n\n \n\n\n35.0836\n\n\n1.7410\n\n\n546\n\n\n20.15\n\n\n&lt;.0001\n\n\n0.05\n\n\n31.6636\n\n\n38.5035\n\n\n\n\nhard_drugs_grp\n\n\nCurrent User\n\n\n \n\n\n \n\n\n3.1618\n\n\n1.6558\n\n\n1085\n\n\n1.91\n\n\n0.0565\n\n\n0.05\n\n\n-0.08708\n\n\n6.4108\n\n\n\n\nhard_drugs_grp\n\n\nNever User\n\n\n \n\n\n \n\n\n2.1340\n\n\n1.3033\n\n\n1085\n\n\n1.64\n\n\n0.1018\n\n\n0.05\n\n\n-0.4232\n\n\n4.6912\n\n\n\n\nhard_drugs_grp\n\n\nPrevious User\n\n\n \n\n\n \n\n\n0\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n\n\nADH_HIGHVSLOW\n\n\n \n\n\nHigh Adherence\n\n\n \n\n\n0.06267\n\n\n1.1718\n\n\n1085\n\n\n0.05\n\n\n0.9574\n\n\n0.05\n\n\n-2.2365\n\n\n2.3618\n\n\n\n\nADH_HIGHVSLOW\n\n\n \n\n\nLow Adherence\n\n\n \n\n\n0\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n\n\nyears\n\n\n \n\n\n \n\n\n \n\n\n-3.1492\n\n\n0.7409\n\n\n1085\n\n\n-4.25\n\n\n&lt;.0001\n\n\n0.05\n\n\n-4.6029\n\n\n-1.6954\n\n\n\n\nyears*hard_drugs_grp\n\n\nCurrent User\n\n\n \n\n\n \n\n\n-1.1128\n\n\n0.7556\n\n\n1085\n\n\n-1.47\n\n\n0.1411\n\n\n0.05\n\n\n-2.5955\n\n\n0.3699\n\n\n\n\nyears*hard_drugs_grp\n\n\nNever User\n\n\n \n\n\n \n\n\n0.7597\n\n\n0.5956\n\n\n1085\n\n\n1.28\n\n\n0.2024\n\n\n0.05\n\n\n-0.4090\n\n\n1.9284\n\n\n\n\nyears*hard_drugs_grp\n\n\nPrevious User\n\n\n \n\n\n \n\n\n0\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n\n\nyears*ADH_HIGHVSLOW\n\n\n \n\n\nHigh Adherence\n\n\n \n\n\n2.0131\n\n\n0.5490\n\n\n1085\n\n\n3.67\n\n\n0.0003\n\n\n0.05\n\n\n0.9359\n\n\n3.0904\n\n\n\n\nyears*ADH_HIGHVSLOW\n\n\n \n\n\nLow Adherence\n\n\n \n\n\n0\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n\n\nFRP\n\n\n \n\n\n \n\n\nNo\n\n\n14.3205\n\n\n0.7567\n\n\n1085\n\n\n18.92\n\n\n&lt;.0001\n\n\n0.05\n\n\n12.8357\n\n\n15.8053\n\n\n\n\nFRP\n\n\n \n\n\n \n\n\nYes\n\n\n0\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nType 3 Tests of Fixed Effects\n\n\n\n\nEffect\n\n\nNum DF\n\n\nDen DF\n\n\nF Value\n\n\nPr &gt; F\n\n\n\n\n\n\nhard_drugs_grp\n\n\n2\n\n\n1085\n\n\n1.90\n\n\n0.1507\n\n\n\n\nADH_HIGHVSLOW\n\n\n1\n\n\n1085\n\n\n0.00\n\n\n0.9574\n\n\n\n\nyears\n\n\n1\n\n\n1085\n\n\n44.48\n\n\n&lt;.0001\n\n\n\n\nyears*hard_drugs_grp\n\n\n2\n\n\n1085\n\n\n6.68\n\n\n0.0013\n\n\n\n\nyears*ADH_HIGHVSLOW\n\n\n1\n\n\n1085\n\n\n13.45\n\n\n0.0003\n\n\n\n\nFRP\n\n\n1\n\n\n1085\n\n\n358.14\n\n\n&lt;.0001\n\n\n\n\n\n\n\n\n\n\n\u0014143                                                        The SAS System                    Saturday, December 28, 2024 08:34:00 PM\n\n1325       ods listing close;ods html5 (id=saspy_internal) file=_tomods1 options(bitmap_mode='inline') device=svg style=HTMLBlue;\n1325     ! ods graphics on / outputfmt=png;\n1326       \n1327       * Change reference level;\n1328       PROC MIXED DATA = PROJ2LMM.DATA METHOD = REML;\n1329        CLASS hard_drugs_grp (REF = \"Previous User\") ADH_HIGHVSLOW FRP;\n1330        MODEL AGG_PHYS = hard_drugs_grp ADH_HIGHVSLOW years hard_drugs_grp*years ADH_HIGHVSLOW*years FRP/ CL;\n1331        RANDOM intercept newid/ SUBJECT = newid TYPE = UN VCORR;\n1332        ODS OUTPUT FitStatistics = Full_Model_Phys2;\n1333         ODS SELECT SolutionF Tests3; * Output only these specific tables;\n1334        RUN;\n1335       \n1336       \n1337       ods html5 (id=saspy_internal) close;ods listing;\n1338       \n\u0014144                                                        The SAS System                    Saturday, December 28, 2024 08:34:00 PM\n\n1339       \n\n\n\n\nCurrent hard drug users had on average a physical QOL score that was 3.19 points higher than previous users (p = 0.042, 95% CI: 0.12, 6.26).\nThe slope for physical QOL over time did not differ between current and previous hard drug users (p = 0.13, 95% CI: -2.63, 0.34)\nTop of Tabset\n\n\n\n\nHard Drug Use\nAfter controlling for adherence and between subject means (intercepts), there was no difference in average physical QOL between never hard drug users and previous users (p = 0.95, 95% CI: -4.48, 0.36) and current users (p = 0.30, 95% CI: -1.02, 3.27). Current hard drug users had on average a physical QOL score that was 3.19 points higher than previous users (p = 0.042, 95% CI: 0.12, 6.26).\n\n\nAdherence\nAfter controlling for hard drug use and between subject means, those with low adherence did not differ significantly in average physical QOL compared to those with high adherence (p = 0.93, 95% CI: -2.32, 2.12).\n\n\nPhysical QOL by Hard Drug Use over Time\nThe slope for physical QOL over time differed significantly based on hard drug use. On average, current hard drug users had a 1.89 point decrease in physical QOL per year compared to never drug users (p = 0.0004, 95% CI: -2.93, -0.85). The slope for physical QOL over time did not differ between never and previous users (p = 0.2116, 95% CI: -1.916, 0.42). The slope for physical QOL over time did not differ between current and previous hard drug users (p = 0.13, 95% CI: -2.63, 0.34).\n\n\nPhysical QOL by Adherence over Time\nThe slope for physical QOL over time also differed significantly based on adherence. On average, those with low adherence had a 2.02 point decrease in physical QOL per year compared to those with high adherence (p = 0.0002, 95% CI: -3.10, -0.94).\n\n\nTime\nThere was a significant decrease in physical QOL over time while controlling for subject-specific means, hard drug use, and adherence, with an average decrease of 0.38 points per year (p &lt; 0.05, 95% CI: -0.75, -0.002). However this value was borderline significant and may not be a true relationshi, especially following correction for multiple pairwise comparisons.\n\n\nFrailty Related Phenotype\nAfter controlling for hard drug user, adherence, and their interactions over time, and between subject means, those with a frailty related phenotype had on average a physical QOL score that was 14.01 points lower than those without a frailty related phenotype (p &lt; 0.0001, 95% CI: -15.49, -12.53).\nTop of Tabset\n\n\n\n\nGenerate Predicted Values with PROC PLM\n\n** Visualize;\n* Use PROC PLM to generate the predicted values under the final model;\nPROC PLM RESTORE = Model_AGG_PHYS;\n    SCORE DATA = Proj2LMM.Data OUT = Proj2LMM.Data PREDICTED = Predicted_AGG_PHYS;\n    RUN;\n\n\n\nVisualize predicted Values\n\n* Visualize the predicted values under the final model;\nPROC SGPLOT DATA = Proj2LMM.Data;\n    REG X = Years Y = Predicted_AGG_PHYS / GROUP = hard_drugs_grp LINEATTRS = (THICKNESS=2) NOMARKERS;\n    XAXIS LABEL = \"Years\";\n    YAXIS LABEL = \"Predicted Physical QOL\";\n    TITLE \"Predicted Physical QOL by Hard Drug Use\";\n    RUN;\n\nlstlog\n\n\n&lt;!DOCTYPE html&gt;\n\n\n\n\n\nSAS Output\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u0014147                                                        The SAS System                    Saturday, December 28, 2024 08:34:00 PM\n\n1356       ods listing close;ods html5 (id=saspy_internal) file=_tomods1 options(bitmap_mode='inline') device=svg style=HTMLBlue;\n1356     ! ods graphics on / outputfmt=png;\n1357       \n1358       * Visualize the predicted values under the final model;\n1359       PROC SGPLOT DATA = Proj2LMM.Data;\n1360        REG X = Years Y = Predicted_AGG_PHYS / GROUP = hard_drugs_grp LINEATTRS = (THICKNESS=2) NOMARKERS;\n1361        XAXIS LABEL = \"Years\";\n1362        YAXIS LABEL = \"Predicted Physical QOL\";\n1363        TITLE \"Predicted Physical QOL by Hard Drug Use\";\n1364        RUN;\n1365       \n1366       \n1367       ods html5 (id=saspy_internal) close;ods listing;\n1368       \n\u0014148                                                        The SAS System                    Saturday, December 28, 2024 08:34:00 PM\n\n1369       \n\n\n\n\nTop of Tabset"
  },
  {
    "objectID": "Project_2/Project_2_SAS/Code/Linear_Mixed_Model_SAS.html#conclusion",
    "href": "Project_2/Project_2_SAS/Code/Linear_Mixed_Model_SAS.html#conclusion",
    "title": "Linear Mixed Model (SAS)",
    "section": "Conclusion",
    "text": "Conclusion\nThus, we have evidence that high adherence improves outcomes from HIV, and those who never used hard drugs have improved recovery rate. Additionally, current drug users in particular have worse outcomes in physical and mental QOL. This is important for the investigators to know and answers their main question of whether they need different different approaches based on hard drug use (they do)."
  },
  {
    "objectID": "Project_2/Project_2_R/Code/Project2.html",
    "href": "Project_2/Project_2_R/Code/Project2.html",
    "title": "Advanced Data Analysis - Project 2",
    "section": "",
    "text": "The aim of the current study is to assess how treatment response differs for HIV+ patients 2 years after initiating Highly Active Antiretroviral Therapy (HAART) based on hard drug usage (such as heroin or cocaine). This study is of particular scientific interest because it is unclear whether the use of hard drugs inhibits the immune system in humans; treatment strategies may differ based on these results. The researchers are interested in comparing subjects who never used hard drugs to current hard drug users (those that use hard drugs at year 2) or previous hard drug users (those who used drugs at year 0 or 1). Outcomes of interest are: viral load (HIV copies in a mL of blood), CD4+ T cell count (a measure of immunologic health), and aggregate physical and quality of life scores from the SF-36.\nThe clinical hypothesis is that, if hard drugs inhibit the immune system in humans, subjects who currently or previously used hard drugs will have higher viral load and lower CD4+ T cell counts than those who never used hard drugs. Additionally, the researchers are interested in knowing if potential differences between the drug use groups can be explained by differences in adherence to the treatment regimen. The researchers are agnostic on how quality of life changes after treatment, since side effects of the treatment are significant.\nThe project description provided by the PI is available below:"
  },
  {
    "objectID": "Project_2/Project_2_R/Code/Project2.html#labeling-categorical-variables",
    "href": "Project_2/Project_2_R/Code/Project2.html#labeling-categorical-variables",
    "title": "Advanced Data Analysis - Project 2",
    "section": "Labeling Categorical Variables",
    "text": "Labeling Categorical Variables\nLet’s factor and label our categorical variables so they are appropriately represented (and not doubles, which will yield incorrect results in models)\n\n# Converting all appropriate variables from doubles to categorical variables\n\ndata$HASHV &lt;- factor(data$HASHV,\n                     levels = c(1, 2),\n                     labels = c(\"No\", \"Yes\"))\n\ndata$HASHF &lt;- factor(data$HASHF,\n                     levels = c(0, 1, 2, 3, 4),\n                     labels = c(\"Never\", \"Daily\", \"Weekly\", \"Monthly\", \"Less Often\"))\n\ndata$income &lt;- factor(data$income,\n                      levels = c(1, 2, 3, 4, 5, 6, 7, 9),\n                      labels = c(\"Less than $10,000\", \"$10,000-$19,999\", \"$20,000-$29,999\", \"$30,000-$39,999\", \"$40,000-$49,999\", \"$50,000-$59,999\", \"$60,000 or more\", \"Do not wish to answer\"))\n\ndata$HBP &lt;- factor(data$HBP,\n                   levels = c(1, 2, 3, 4, 9, -1),\n                   labels = c(\"No\", \"Yes\", \"No, based on data trajectory\", \"Yes, based on data trajectory\", \"Insufficient data, may include reported treatment without diagnosis\", \"Improbable Value\"))\n\ndata$DIAB &lt;- factor(data$DIAB,\n                    levels = c(1, 2, 3, 4, 9),\n                    labels = c(\"No\", \"Yes\", \"No, based on data trajectory\", \"Yes, based on data trajectory\", \"Insufficient data\"))\n                      \ndata$LIV34 &lt;- factor(data$LIV34,\n                     levels = c(1, 2, 9),\n                     labels = c(\"No\", \"Yes\", \"Insufficient Data\"))\n\ndata$KID &lt;- factor(data$KID,\n                   levels = c(1, 2, 3, 4, 9),\n                   labels = c(\"No\", \"Yes\", \"No, based on data trajectory\", \"Yes, based on data trajectory\", \"Insufficient data\"))\n\ndata$FRP &lt;- factor(data$FRP,\n                   levels = c(1,2,9),\n                   labels = c(\"No\", \"Yes\", \"Insufficient Data\"))\n\ndata$FP &lt;- factor(data$FP,\n                  levels = c(1,2,9),\n                  labels = c(\"No\", \"Yes\", \"Insufficient Data\"))\n\ndata$DYSLIP &lt;- factor(data$DYSLIP,\n                      levels = c(1, 2, 3, 4, 9),\n                      labels = c(\"No\", \"Yes\", \"No, based on data trajectory\", \"Yes, based on data trajectory\", \"Insufficient data\"))\n\ndata$SMOKE &lt;- factor(data$SMOKE,\n                     levels = c(1, 2, 3),\n                     labels = c(\"Never Smoked\", \"Former Smoker\", \"Current Smoker\"))\n\ndata$DKGRP &lt;- factor(data$DKGRP,\n                     levels = c(0, 1, 2, 3),\n                     labels = c(\"None\", \"1-3 drinks/week\", \"4-13 drinks/week\", \"&gt;13 drinks/week\"))\n\ndata$HEROPIATE &lt;- factor(data$HEROPIATE,\n                         levels = c(1, 2, -9),\n                         labels = c(\"No\", \"Yes\", \"Not Specified\"))\n\ndata$IDU &lt;- factor(data$IDU,\n                   levels = c(1, 2),\n                   labels = c(\"No\", \"Yes\"))\n\ndata$ADH &lt;- factor(data$ADH,\n                   levels = c(1, 2, 3, 4),\n                   labels = c(\"100%\", \"95-99%\", \"75-94%\", \"&lt;75%\"))\n\ndata$RACE &lt;- factor(data$RACE,\n                    levels = c(1, 2, 3, 4, 5, 6, 7),\n                    labels = c(\"White, non-Hispanic\", \"White, Hispanic\", \"Black, non-Hispanic \", \"Black, Hispanic\",  \"American Indian or Alaskan Native\", \"Asian or Pacific Islander\", \"Other Hispanic\"))\n\ndata$EDUCBAS &lt;- factor(data$EDUCBAS,\n                       levels = c(1, 2, 3, 4, 5, 6, 7),\n                       labels = c(\"8th grade or less \", \"9,10, or 11th grade\", \"12th grade\", \"At least one year college but no degree\", \"Four years college or got degree\", \"Some graduate work\", \"Post-graduate degree\"))\n\ndata$hard_drugs &lt;- factor(data$hard_drugs,\n                          levels = c(0, 1),\n                          labels = c(\"No\", \"Yes\"))\n\n# Create labels for variables to make the names of each variable more professional in outputs\nlabel(data$newid) &lt;- \"ID\"\nlabel(data$AGG_MENT) &lt;- \"Aggregate Mental QOL Score\"\nlabel(data$AGG_PHYS) &lt;- \"Aggregate Physical QOL Score\"\nlabel(data$HASHF) &lt;- \"Hash/Marijuana Use Since Last Visit\"\nlabel(data$HASHV) &lt;- \"Frequency of Hash/Marijuana Use\"\nlabel(data$income) &lt;- \"Income\"\nlabel(data$HBP) &lt;- \"High Blood Pressure\"\nlabel(data$DIAB) &lt;- \"Diabetes\"\nlabel(data$LIV34) &lt;- \"Liver Disease Stage 3/4\"\nlabel(data$KID) &lt;-\"Kidney Disease\"\nlabel(data$FRP) &lt;- \"Frailty Related Phenotype\"\nlabel(data$FP) &lt;- \"Fraily Phenotype\"\nlabel(data$BMI) &lt;- \"BMI\"\nlabel(data$TCHOL) &lt;- \"Total Cholesterol\"\nlabel(data$TRIG) &lt;- \"Triglycerides\"\nlabel(data$LDL) &lt;- \"LDL\"\nlabel(data$DYSLIP) &lt; \"Dyslipidemia\"\nlabel(data$SMOKE) &lt; \"Smoking Status\"\nlabel(data$CESD) &lt;- \"CESD Depression Score\"\nlabel(data$SMOKE) &lt; \"Smoking Status\"\nlabel(data$DKGRP) &lt;- \"Drinking Group\"\nlabel(data$HEROPIATE) &lt;- \"Heroin or Opiate Use Since Last Visit\"\nlabel(data$IDU) &lt;- \"Intravenous Drug Usage Since Last Visit\"\nlabel(data$LEU3N) &lt;- \"CD4+ T Cell Count\"\nlabel(data$VLOAD) &lt;- \"Viral Load\"\nlabel(data$ADH) &lt;- \"Adherence to Treatment Regimen\"\nlabel(data$RACE) &lt;- \"Race\"\nlabel(data$EDUCBAS) &lt;- \"Education at Baseline\"\nlabel(data$hivpos) &lt;- \"HIV Serostatus\"\nlabel(data$age) &lt;- \"Age\"\nlabel(data$ART) &lt;- \"Antiretroviral Therapy\"\nlabel(data$years) &lt;- \"Year of Visit\"\nlabel(data$hard_drugs) &lt;- \"Hard Drug Usage\"\n\nLet’s take another look to check that those variables are no longer doubles.\n\n# Examine data\nglimpse(data)\n\nRows: 3,632\nColumns: 33\n$ newid      &lt;dbl&gt; 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4,…\n$ AGG_MENT   &lt;dbl&gt; 44.90710, 58.20754, 59.65136, 56.80657, 46.34190, 48.71791,…\n$ AGG_PHYS   &lt;dbl&gt; 52.52557, 41.29347, 48.54453, 46.73991, 27.92331, 38.03807,…\n$ HASHV      &lt;fct&gt; No, No, No, No, No, Yes, No, Yes, No, Yes, No, No, Yes, No,…\n$ HASHF      &lt;fct&gt; NA, Less Often, Never, Never, Never, Never, Never, Never, N…\n$ income     &lt;fct&gt; \"$30,000-$39,999\", \"$30,000-$39,999\", \"$30,000-$39,999\", \"$…\n$ BMI        &lt;dbl&gt; 24.71756, 26.06801, 27.16421, 25.71786, 26.66936, 25.96576,…\n$ HBP        &lt;fct&gt; \"No\", \"No\", \"No\", \"No\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", …\n$ DIAB       &lt;fct&gt; \"No\", \"No\", \"No\", \"No\", \"Insufficient data\", \"Insufficient …\n$ LIV34      &lt;fct&gt; No, No, No, No, No, No, No, No, No, No, No, No, No, No, Yes…\n$ KID        &lt;fct&gt; \"No\", \"No\", \"No\", \"No\", \"Insufficient data\", \"Insufficient …\n$ FRP        &lt;fct&gt; No, No, No, No, Yes, No, No, No, No, No, No, No, No, No, No…\n$ FP         &lt;fct&gt; No, No, No, No, Insufficient Data, Insufficient Data, No, N…\n$ TCHOL      &lt;dbl&gt; 133, 131, 180, 171, 125, NA, 134, 105, 141, 153, 170, 170, …\n$ TRIG       &lt;dbl&gt; 176, 107, 233, 139, NA, NA, NA, 104, 162, 127, NA, NA, 82, …\n$ LDL        &lt;dbl&gt; 62, 66, 86, 96, NA, NA, NA, 44, 73, 84, NA, NA, 127, NA, NA…\n$ DYSLIP     &lt;fct&gt; \"Yes\", \"No\", \"Yes\", \"No\", \"Yes\", \"Insufficient data\", \"Yes,…\n$ CESD       &lt;dbl&gt; 14, 2, 1, 18, 20, 18, 18, 21, 23, 17, 18, 22, 23, 14, 1, 1,…\n$ SMOKE      &lt;fct&gt; Current Smoker, Current Smoker, Current Smoker, Current Smo…\n$ DKGRP      &lt;fct&gt; None, &gt;13 drinks/week, None, 1-3 drinks/week, None, &gt;13 dri…\n$ HEROPIATE  &lt;fct&gt; No, No, No, No, No, No, No, No, No, No, No, No, No, No, No,…\n$ IDU        &lt;fct&gt; Yes, No, No, No, Yes, No, No, No, No, No, Yes, Yes, Yes, Ye…\n$ LEU3N      &lt;dbl&gt; 104.1659, 262.0061, 345.4010, 292.3271, 257.8278, 459.4562,…\n$ VLOAD      &lt;dbl&gt; 102013.000000, 27.000000, 60.000000, 9.000000, 8121.000000,…\n$ ADH        &lt;fct&gt; NA, 95-99%, 100%, 100%, NA, 100%, 100%, 100%, 100%, 100%, N…\n$ RACE       &lt;fct&gt; \"White, non-Hispanic\", \"White, non-Hispanic\", \"White, non-H…\n$ EDUCBAS    &lt;fct&gt; \"At least one year college but no degree\", \"At least one ye…\n$ hivpos     &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ age        &lt;dbl&gt; 52, 53, 54, 55, 54, 55, 56, 60, 61, 62, 47, 48, 49, 51, 52,…\n$ ART        &lt;dbl&gt; 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1,…\n$ everART    &lt;dbl&gt; 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1,…\n$ years      &lt;dbl&gt; 0, 1, 2, 3, 0, 1, 2, 6, 7, 8, 0, 1, 2, 4, 5, 6, 7, 8, 0, 1,…\n$ hard_drugs &lt;fct&gt; Yes, No, No, No, Yes, No, No, No, No, No, Yes, Yes, Yes, Ye…\n\n\nLooks good."
  },
  {
    "objectID": "Project_2/Project_2_R/Code/Project2.html#filtering-data-set",
    "href": "Project_2/Project_2_R/Code/Project2.html#filtering-data-set",
    "title": "Advanced Data Analysis - Project 2",
    "section": "Filtering Data Set",
    "text": "Filtering Data Set\nNow let’s take a look at the header to get a good feeling for our data.\n\n# Pretty print data header\npretty_print(head(data))\n\n\n\n\nnewid\nAGG_MENT\nAGG_PHYS\nHASHV\nHASHF\nincome\nBMI\nHBP\nDIAB\nLIV34\nKID\nFRP\nFP\nTCHOL\nTRIG\nLDL\nDYSLIP\nCESD\nSMOKE\nDKGRP\nHEROPIATE\nIDU\nLEU3N\nVLOAD\nADH\nRACE\nEDUCBAS\nhivpos\nage\nART\neverART\nyears\nhard_drugs\n\n\n\n\n1\n44.90710\n52.52557\nNo\nNA\n$30,000-$39,999\n24.71756\nNo\nNo\nNo\nNo\nNo\nNo\n133\n176\n62\nYes\n14\nCurrent Smoker\nNone\nNo\nYes\n104.1659\n102013\nNA\nWhite, non-Hispanic\nAt least one year college but no degree\n1\n52\n0\n0\n0\nYes\n\n\n1\n58.20754\n41.29346\nNo\nLess Often\n$30,000-$39,999\n26.06801\nNo\nNo\nNo\nNo\nNo\nNo\n131\n107\n66\nNo\n2\nCurrent Smoker\n&gt;13 drinks/week\nNo\nNo\n262.0061\n27\n95-99%\nWhite, non-Hispanic\nAt least one year college but no degree\n1\n53\n1\n1\n1\nNo\n\n\n1\n59.65136\n48.54453\nNo\nNever\n$30,000-$39,999\n27.16421\nNo\nNo\nNo\nNo\nNo\nNo\n180\n233\n86\nYes\n1\nCurrent Smoker\nNone\nNo\nNo\n345.4010\n60\n100%\nWhite, non-Hispanic\nAt least one year college but no degree\n1\n54\n1\n1\n2\nNo\n\n\n1\n56.80657\n46.73991\nNo\nNever\n$40,000-$49,999\n25.71786\nNo\nNo\nNo\nNo\nNo\nNo\n171\n139\n96\nNo\n18\nCurrent Smoker\n1-3 drinks/week\nNo\nNo\n292.3271\n9\n100%\nWhite, non-Hispanic\nAt least one year college but no degree\n1\n55\n1\n1\n3\nNo\n\n\n2\n46.34190\n27.92331\nNo\nNever\n$10,000-$19,999\n26.66936\nYes\nInsufficient data\nNo\nInsufficient data\nYes\nInsufficient Data\n125\nNA\nNA\nYes\n20\nCurrent Smoker\nNone\nNo\nYes\n257.8278\n8121\nNA\nBlack, non-Hispanic\n9,10, or 11th grade\n1\n54\n0\n0\n0\nYes\n\n\n2\n48.71791\n38.03807\nYes\nNever\nLess than $10,000\n25.96576\nYes\nInsufficient data\nNo\nInsufficient data\nNo\nInsufficient Data\nNA\nNA\nNA\nInsufficient data\n18\nCurrent Smoker\n&gt;13 drinks/week\nNo\nNo\n459.4562\n21\n100%\nBlack, non-Hispanic\n9,10, or 11th grade\n1\n55\n1\n1\n1\nNo\n\n\n\n\n\n\n\nHmm, we have 8 years worth of data points, but the experimenters are only interested in the first 2 years.\nOut of curiosity, let’s look at how many participants they had each year.\n\n# Visualize patient drop off over 8 years of study\nbarplot(table(data$years))\n\n\n\n\n\n\n\n# Check number of patients in each year\npretty_print(table(data$years))\n\n\n\n\nVar1\nFreq\n\n\n\n\n0\n550\n\n\n1\n550\n\n\n2\n550\n\n\n3\n414\n\n\n4\n381\n\n\n5\n338\n\n\n6\n325\n\n\n7\n272\n\n\n8\n252\n\n\n\n\n\n\n\nThis is interesting, we don’t seem to have as drastic a drop off as I expected. The researchers managed to retain all participants for the first 2 years, and 50% by the end of the 8-year study.\nLet’s filter to only include values from the first 2 years, as this is the timeframe the researchers are interested in.\n\n# Filter long form data set to be include only first 2 years\ndata_2 &lt;- data[data$years &lt;= 2,]\n\n# Check how many visits we have in the filtered data set.\ndim(data_2)\n\n[1] 1650   33\n\n\nWe went from 3632 visits in the 8 year data set to 1650 in the 2 year filtered data set.\n\n# Double check if any patients dropped out within the first 2 years\nany(is.na(data_2$years))\n\n[1] FALSE\n\n\nLuckily, no patients dropped out within the first 2 years of the study!"
  },
  {
    "objectID": "Project_2/Project_2_R/Code/Project2.html#transpose-to-wideform",
    "href": "Project_2/Project_2_R/Code/Project2.html#transpose-to-wideform",
    "title": "Advanced Data Analysis - Project 2",
    "section": "Transpose to Wideform",
    "text": "Transpose to Wideform\nWe can also see that the provided data set is in longform. Let’s convert that to wideform.\n\n# Create new wideform data set for first 2 years of study              \ndata_wide_2 &lt;- pivot_wider(data_2, id_cols = newid, names_from = years, values_from = -c(newid, years))\n\nAnd take a look at the header to check that was done correctly.\n\n# Pretty print header of wideform data\npretty_print(head(data_wide_2))\n\n\n\n\nnewid\nAGG_MENT_0\nAGG_MENT_1\nAGG_MENT_2\nAGG_PHYS_0\nAGG_PHYS_1\nAGG_PHYS_2\nHASHV_0\nHASHV_1\nHASHV_2\nHASHF_0\nHASHF_1\nHASHF_2\nincome_0\nincome_1\nincome_2\nBMI_0\nBMI_1\nBMI_2\nHBP_0\nHBP_1\nHBP_2\nDIAB_0\nDIAB_1\nDIAB_2\nLIV34_0\nLIV34_1\nLIV34_2\nKID_0\nKID_1\nKID_2\nFRP_0\nFRP_1\nFRP_2\nFP_0\nFP_1\nFP_2\nTCHOL_0\nTCHOL_1\nTCHOL_2\nTRIG_0\nTRIG_1\nTRIG_2\nLDL_0\nLDL_1\nLDL_2\nDYSLIP_0\nDYSLIP_1\nDYSLIP_2\nCESD_0\nCESD_1\nCESD_2\nSMOKE_0\nSMOKE_1\nSMOKE_2\nDKGRP_0\nDKGRP_1\nDKGRP_2\nHEROPIATE_0\nHEROPIATE_1\nHEROPIATE_2\nIDU_0\nIDU_1\nIDU_2\nLEU3N_0\nLEU3N_1\nLEU3N_2\nVLOAD_0\nVLOAD_1\nVLOAD_2\nADH_0\nADH_1\nADH_2\nRACE_0\nRACE_1\nRACE_2\nEDUCBAS_0\nEDUCBAS_1\nEDUCBAS_2\nhivpos_0\nhivpos_1\nhivpos_2\nage_0\nage_1\nage_2\nART_0\nART_1\nART_2\neverART_0\neverART_1\neverART_2\nhard_drugs_0\nhard_drugs_1\nhard_drugs_2\n\n\n\n\n1\n44.90710\n58.20754\n59.65136\n52.52557\n41.29346\n48.54453\nNo\nNo\nNo\nNA\nLess Often\nNever\n$30,000-$39,999\n$30,000-$39,999\n$30,000-$39,999\n24.71756\n26.06801\n27.16421\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\n133\n131\n180\n176\n107\n233\n62\n66\n86\nYes\nNo\nYes\n14\n2\n1\nCurrent Smoker\nCurrent Smoker\nCurrent Smoker\nNone\n&gt;13 drinks/week\nNone\nNo\nNo\nNo\nYes\nNo\nNo\n104.1659\n262.0061\n345.4010\n102013.000\n27.00000\n60.00000\nNA\n95-99%\n100%\nWhite, non-Hispanic\nWhite, non-Hispanic\nWhite, non-Hispanic\nAt least one year college but no degree\nAt least one year college but no degree\nAt least one year college but no degree\n1\n1\n1\n52\n53\n54\n0\n1\n1\n0\n1\n1\nYes\nNo\nNo\n\n\n2\n46.34190\n48.71791\n45.41483\n27.92331\n38.03807\n37.32204\nNo\nYes\nNo\nNever\nNever\nNever\n$10,000-$19,999\nLess than $10,000\n$10,000-$19,999\n26.66936\n25.96576\n26.96037\nYes\nYes\nYes\nInsufficient data\nInsufficient data\nInsufficient data\nNo\nNo\nNo\nInsufficient data\nInsufficient data\nInsufficient data\nYes\nNo\nNo\nInsufficient Data\nInsufficient Data\nNo\n125\nNA\n134\nNA\nNA\nNA\nNA\nNA\nNA\nYes\nInsufficient data\nYes, based on data trajectory\n20\n18\n18\nCurrent Smoker\nCurrent Smoker\nCurrent Smoker\nNone\n&gt;13 drinks/week\n4-13 drinks/week\nNo\nNo\nNo\nYes\nNo\nNo\n257.8278\n459.4562\n263.0693\n8121.000\n21.00000\n48.00000\nNA\n100%\n100%\nBlack, non-Hispanic\nBlack, non-Hispanic\nBlack, non-Hispanic\n9,10, or 11th grade\n9,10, or 11th grade\n9,10, or 11th grade\n1\n1\n1\n54\n55\n56\n0\n1\n1\n0\n1\n1\nYes\nNo\nNo\n\n\n3\n40.22337\n44.42011\n41.70079\n60.06970\n62.71705\n58.51450\nNo\nNo\nYes\nLess Often\nLess Often\nLess Often\n$50,000-$59,999\n$50,000-$59,999\n$50,000-$59,999\n28.59085\n28.35320\n28.18510\nInsufficient data, may include reported treatment without diagnosis\nInsufficient data, may include reported treatment without diagnosis\nNo\nInsufficient data\nInsufficient data\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\n170\n170\n180\nNA\nNA\n82\nNA\nNA\n127\nYes\nYes\nYes\n18\n22\n23\nFormer Smoker\nFormer Smoker\nFormer Smoker\nNone\nNone\nNone\nNo\nNo\nNo\nYes\nYes\nYes\n563.1223\n488.9100\n405.1816\n4001.556\n2020.00000\n27.50917\nNA\n100%\n100%\nWhite, non-Hispanic\nWhite, non-Hispanic\nWhite, non-Hispanic\nPost-graduate degree\nPost-graduate degree\nPost-graduate degree\n1\n1\n1\n47\n48\n49\n0\n1\n1\n0\n1\n1\nYes\nYes\nYes\n\n\n4\n42.90638\n31.15971\n52.68223\n50.78850\n44.62883\n51.50533\nYes\nNo\nNo\nLess Often\nWeekly\nNever\nLess than $10,000\nLess than $10,000\nNA\n20.36451\n18.21865\n20.28485\nNo\nNo\nNo\nNo\nNo, based on data trajectory\nNo\nNo\nNo\nNo\nInsufficient data\nInsufficient data\nInsufficient data\nNo\nYes\nNo\nNo\nYes\nNo\n214\n197\n251\n97\nNA\n260\n147\nNA\n152\nYes\nYes, based on data trajectory\nYes\n14\n25\n13\nCurrent Smoker\nCurrent Smoker\nCurrent Smoker\n1-3 drinks/week\n4-13 drinks/week\nNone\nYes\nYes\nNo\nNo\nNo\nNo\n110.4218\n159.6297\n179.6409\n740.000\n26.64732\n27.00000\nNA\n75-94%\n95-99%\nWhite, non-Hispanic\nWhite, non-Hispanic\nWhite, non-Hispanic\nFour years college or got degree\nFour years college or got degree\nFour years college or got degree\n1\n1\n1\n44\n45\n46\n0\n1\n1\n0\n1\n1\nYes\nYes\nNo\n\n\n5\n56.42904\n56.21993\n66.50629\n43.75671\n30.47055\n18.82350\nNo\nNA\nYes\nMonthly\nMonthly\nLess Often\n$50,000-$59,999\n$10,000-$19,999\nDo not wish to answer\n22.26986\n24.97865\n20.80193\nNo\nNo\nNo\nInsufficient data\nInsufficient data\nInsufficient data\nNo\nNo\nInsufficient Data\nInsufficient data\nInsufficient data\nInsufficient data\nNo\nNo\nYes\nInsufficient Data\nInsufficient Data\nYes\n196\n204\nNA\n162\n192\nNA\n135\nNA\nNA\nYes\nInsufficient data\nYes, based on data trajectory\n1\n0\n1\nCurrent Smoker\nCurrent Smoker\nCurrent Smoker\nNone\n1-3 drinks/week\n1-3 drinks/week\nNot Specified\nNA\nNo\nYes\nYes\nYes\n252.6634\n92.6634\n59.6219\n62727.039\n30389.00000\n419.50000\nNA\n100%\n100%\nWhite, non-Hispanic\nWhite, non-Hispanic\nWhite, non-Hispanic\nPost-graduate degree\nPost-graduate degree\nPost-graduate degree\n1\n1\n1\n53\n54\n55\n0\n1\n1\n0\n1\n1\nYes\nYes\nYes\n\n\n6\n59.74437\n53.84956\n50.26010\n56.86261\n57.91396\n55.95668\nNo\nNo\nNo\nNA\nNA\nLess Often\n$30,000-$39,999\n$30,000-$39,999\nNA\n23.22166\n23.75318\n22.41001\nNo\nNo\nNo\nInsufficient data\nInsufficient data\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\n216\n216\n151\nNA\nNA\n125\nNA\nNA\n81\nInsufficient data\nInsufficient data\nNo\n3\n3\n4\nNever Smoked\nNever Smoked\nNever Smoked\n1-3 drinks/week\n1-3 drinks/week\n1-3 drinks/week\nNo\nNo\nNo\nNo\nNo\nYes\n634.1246\n745.6517\n893.4328\n15745.000\n7870.00000\n53.50000\nNA\n95-99%\n95-99%\nWhite, non-Hispanic\nWhite, non-Hispanic\nWhite, non-Hispanic\nSome graduate work\nSome graduate work\nSome graduate work\n1\n1\n1\n36\n37\n38\n0\n1\n1\n0\n1\n1\nNo\nNo\nYes\n\n\n\n\n\n\n\nGood. now we have a long and wide form of the data set for the first two years of the study.\nFinally, let’s just clean that wide data set up a bit to drop repeat measures of variables that are constant over time (race, education at baseline, HIV serostatus, everART)\n\n# Clean up the wide data set a bit by deleting multiple observations across time for constant variables such as race\ndata_wide_2 &lt;- data_wide_2 %&gt;% select(-RACE_1, -RACE_2, -EDUCBAS_1, -EDUCBAS_2, -hivpos_1, - hivpos_2, -everART_1, -everART_2)\n\nNow that our data sets are adequately prepared, we can move on to performing our data checks to ensure fidelity of the data set."
  },
  {
    "objectID": "Project_2/Project_2_R/Code/Project2.html#removing-superfluous-variables",
    "href": "Project_2/Project_2_R/Code/Project2.html#removing-superfluous-variables",
    "title": "Advanced Data Analysis - Project 2",
    "section": "Removing Superfluous Variables",
    "text": "Removing Superfluous Variables\nWe previously determined that LDL, TRIG, DIAB, KID, and DYSLIP had excessive missing values (&gt;40%).\nNow that we have seen that they are not strongly related to the outcome variables, we can be assured that we can safely remove them with no need for imputation.\n\n# Drop variables with excessive missing values from the wideform data set\ndata_2 &lt;- data_2 %&gt;%\n  select(-LDL, -TRIG, -DIAB, -KID, -DYSLIP)\n\nFRP and FP are both precision variables for AGG_PHYS, but highly correlated to each other.\nFP has more missing values (22%) than FRP(~0%), and will thus be dropped.\n\n# Drop FP\ndata_2 &lt;- data_2 %&gt;%\n  select(-FP)\n\nEDUCBASE is highly correlated with income, TCHOL, SMOKE, and RACE.\nincome has 27% missing values and thus will be dropped from further analysis.\nTCHOL has 32% missing values and will thus be dropped.\nSMOKE and RACE will be dropped to avoid issues of multicollinearity, and EDUCBASE used as the covariate of choice.\n\n# Drop income, total cholesterol, smoke, and race\ndata_2 &lt;- data_2 %&gt;%\n  select(-income, -TCHOL, -SMOKE, -RACE)\n\nHard_drugs is highly correlated with HEROPIATE and IDU.\nHard_drugs is our main independent variable of interest and thus we drop HEROPIATE and IDU.\n\n# Drop income, total cholesterol, smoke, and race\ndata_2 &lt;- data_2 %&gt;%\n  select(-HEROPIATE, -IDU)\n\nHBP is lightly correlated with age and BMI. Let’s drop it to avoid multicollinearity.\n\n# Drop high blood pressure.\ndata_2 &lt;- data_2 %&gt;%\n  select(-HBP)"
  },
  {
    "objectID": "Project_2/Project_2_R/Code/Project2.html#correlation-matrix-redux",
    "href": "Project_2/Project_2_R/Code/Project2.html#correlation-matrix-redux",
    "title": "Advanced Data Analysis - Project 2",
    "section": "Correlation Matrix Redux",
    "text": "Correlation Matrix Redux\nLet’s take another look at that correlation matrix now that we have cleaned up our data set to remove variables with excessive missing values and issues of multicollinearity.\n\n# Let's clean our output by making a trimmed dataset excluding extraneous variables\ndata_for_matrix &lt;- select(data_wide_2, AGG_MENT_CHANGE, AGG_PHYS_CHANGE, LEU3N_CHANGE, VLOAD_log_CHANGE, BMI_2, FRP_2, CESD_2, DKGRP_2, ADH_2, ADH_HIGHVSLOW, EDUC_COLLEGE, age_2, hard_drugs_grp)\n\n# We factored our variables at the start. To make a correlation matrix we must reconvert those back to numeric\ndata_for_matrix &lt;- data.frame(lapply(data_for_matrix, function(x) if (is.factor(x)) as.numeric(x) else x))\n\n# Make a correlation matrix with all variables of the trimmed data set\ncorrelation_matrix &lt;- cor(data_for_matrix, use = \"complete.obs\")\n\n# Plot the matrix\ncorrplot(correlation_matrix, method = \"circle\")\n\n\n\n\n\n\n\n\nLooking MUCH better! Here we can see some potentially strong relationships emerge.\n\nCESD as mentioned will be included as a precision variable for AGG_MENT\nFRP as mentioned will be included as a precision variable for AGG_PHYS\nEDUCBASE looks like it will be a predictor for all outcome variables except AGG_MENT\nBMI appears to have a weak correlation with all outcome variables and will likely be included in the final models.\nage also looks weakly correlated to all outcome variables except VLOAD_log\n\nLet’s run some individual regression and assess these relationships more closely.\np-values of &lt; 0.1 will be considered for the final models."
  },
  {
    "objectID": "Project_2/Project_2_R/Code/Project2.html#log-viral-load-change-1",
    "href": "Project_2/Project_2_R/Code/Project2.html#log-viral-load-change-1",
    "title": "Advanced Data Analysis - Project 2",
    "section": "Log Viral Load Change",
    "text": "Log Viral Load Change\n\nFull ModelReduced Model - No InteractionConfoundingVisualizing Main Effects\n\n\nAs determined by interactive variable selection, the variables of interest for a model predicting VLOAD_log_CHANGE are hard_drug_grp, ADH_HIGHVSLOW, and EDUC_COLLEGE.\nThe researchers are interested in if differences in treatment response between the drug use groups can be explained by differences in adherence to the HAART regimen, so we will also include an interaction term between hard drug use group and adherence.\nThus the full model is:\n                                     ADD LATEX MATH SYNTAX HERE.\n                                     \n\nAnalysisModel SelectionInterpretationVisualizing the Interaction\n\n\nLet’s run the regression for the full model.\n\n# Relevel reference group\ndata_wide_2$hard_drugs_grp &lt;- relevel(data_wide_2$hard_drugs_grp, ref = \"Never User\")\n\n# Relevel reference group\ndata_wide_2$ADH_HIGHVSLOW &lt;- relevel(data_wide_2$ADH_HIGHVSLOW, ref = \"Low Adherence\")\n\n# Perform the regression on log viral load change with the full model\nmodel_VLOAD_full1 &lt;- lm(VLOAD_log_CHANGE ~ hard_drugs_grp + ADH_HIGHVSLOW + hard_drugs_grp*ADH_HIGHVSLOW + EDUC_COLLEGE, data = data_wide_2)\nlibrary(sjPlot)\n# Examine summary\nsummary(model_VLOAD_full1)\n\n\nCall:\nlm(formula = VLOAD_log_CHANGE ~ hard_drugs_grp + ADH_HIGHVSLOW + \n    hard_drugs_grp * ADH_HIGHVSLOW + EDUC_COLLEGE, data = data_wide_2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.2508 -1.6120 -0.2134  1.1308  9.2569 \n\nCoefficients:\n                                                        Estimate Std. Error\n(Intercept)                                              -4.9327     0.5241\nhard_drugs_grpPrevious User                               5.8083     1.1857\nhard_drugs_grpCurrent User                                3.4758     1.4138\nADH_HIGHVSLOWHigh Adherence                              -0.7876     0.4898\nEDUC_COLLEGECollege                                      -0.8489     0.2955\nhard_drugs_grpPrevious User:ADH_HIGHVSLOWHigh Adherence  -5.8479     1.2773\nhard_drugs_grpCurrent User:ADH_HIGHVSLOWHigh Adherence   -2.7430     1.4659\n                                                        t value\n(Intercept)                                              -9.412\nhard_drugs_grpPrevious User                               4.898\nhard_drugs_grpCurrent User                                2.458\nADH_HIGHVSLOWHigh Adherence                              -1.608\nEDUC_COLLEGECollege                                      -2.872\nhard_drugs_grpPrevious User:ADH_HIGHVSLOWHigh Adherence  -4.578\nhard_drugs_grpCurrent User:ADH_HIGHVSLOWHigh Adherence   -1.871\n                                                                    Pr(&gt;|t|)\n(Intercept)                                             &lt; 0.0000000000000002\nhard_drugs_grpPrevious User                                       0.00000130\nhard_drugs_grpCurrent User                                           0.01428\nADH_HIGHVSLOWHigh Adherence                                          0.10846\nEDUC_COLLEGECollege                                                  0.00424\nhard_drugs_grpPrevious User:ADH_HIGHVSLOWHigh Adherence           0.00000589\nhard_drugs_grpCurrent User:ADH_HIGHVSLOWHigh Adherence               0.06189\n                                                           \n(Intercept)                                             ***\nhard_drugs_grpPrevious User                             ***\nhard_drugs_grpCurrent User                              *  \nADH_HIGHVSLOWHigh Adherence                                \nEDUC_COLLEGECollege                                     ** \nhard_drugs_grpPrevious User:ADH_HIGHVSLOWHigh Adherence ***\nhard_drugs_grpCurrent User:ADH_HIGHVSLOWHigh Adherence  .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.661 on 513 degrees of freedom\n  (30 observations deleted due to missingness)\nMultiple R-squared:  0.1001,    Adjusted R-squared:  0.08956 \nF-statistic:  9.51 on 6 and 513 DF,  p-value: 0.0000000006388\n\ntab_model(model_VLOAD_full1)\n\n\n\n\n\n\n\n\n\n\n \nLog Viral Load Change\nScore\n\n\nPredictors\nEstimates\nCI\np\n\n\n(Intercept)\n-4.93\n-5.96 – -3.90\n&lt;0.001\n\n\nhard_drugs_grp: Previous\nUser\n5.81\n3.48 – 8.14\n&lt;0.001\n\n\nhard_drugs_grp: Current\nUser\n3.48\n0.70 – 6.25\n0.014\n\n\nADH_HIGHVSLOW: High\nAdherence\n-0.79\n-1.75 – 0.17\n0.108\n\n\nCollege Status: College\n-0.85\n-1.43 – -0.27\n0.004\n\n\nhard_drugs_grpPrevious User:ADH_HIGHVSLOWHigh Adherence\n-5.85\n-8.36 – -3.34\n&lt;0.001\n\n\nhard_drugs_grpCurrent User:ADH_HIGHVSLOWHigh Adherence\n-2.74\n-5.62 – 0.14\n0.062\n\n\nObservations\n520\n\n\nR2 / R2 adjusted\n0.100 / 0.090\n\n\n\n\n\n\n\n\n# Print useful model parameters\nmodel_results(model_VLOAD_full1)\n\n\n\n\n\nEstimate\nStd. Error\nt value\nPr(&gt;|t|)\np_adjusted\n2.5 %\n97.5 %\n\n\n\n\n(Intercept)\n-4.9326626\n0.5240675\n-9.412266\n0.0000000\n0.0000000\n-5.9622450\n-3.9030801\n\n\nhard_drugs_grpPrevious User\n5.8083303\n1.1857389\n4.898490\n0.0000013\n0.0000091\n3.4788288\n8.1378317\n\n\nhard_drugs_grpCurrent User\n3.4757907\n1.4138375\n2.458409\n0.0142848\n0.0999936\n0.6981668\n6.2534145\n\n\nADH_HIGHVSLOWHigh Adherence\n-0.7876174\n0.4898289\n-1.607944\n0.1084627\n0.7592392\n-1.7499349\n0.1747001\n\n\nEDUC_COLLEGECollege\n-0.8489132\n0.2955395\n-2.872418\n0.0042421\n0.0296945\n-1.4295299\n-0.2682965\n\n\nhard_drugs_grpPrevious User:ADH_HIGHVSLOWHigh Adherence\n-5.8479420\n1.2773135\n-4.578314\n0.0000059\n0.0000412\n-8.3573508\n-3.3385331\n\n\nhard_drugs_grpCurrent User:ADH_HIGHVSLOWHigh Adherence\n-2.7429503\n1.4659004\n-1.871171\n0.0618904\n0.4332325\n-5.6228568\n0.1369561\n\n\n\n\n\n\n# Get the VIFs\nvif_values &lt;- vif(model_VLOAD_full1)\n\nthere are higher-order terms (interactions) in this model\nconsider setting type = 'predictor'; see ?vif\n\npretty_print(vif_values)\n\n\n\n\n\nGVIF\nDf\nGVIF^(1/(2*Df))\n\n\n\n\nhard_drugs_grp\n121.062588\n2\n3.317054\n\n\nADH_HIGHVSLOW\n1.307856\n1\n1.143615\n\n\nEDUC_COLLEGE\n1.090681\n1\n1.044357\n\n\nhard_drugs_grp:ADH_HIGHVSLOW\n125.887538\n2\n3.349621\n\n\n\n\n\n\n\nThe overall model is highly significant (p &lt; 0.000001) and we’re getting some interesting significant relationships. It looks like all of our main variables and interaction term are significant. Multicollinearity for this model is not a concern.\nFor low adherence, previous and current drug users have a higher increase in log viral load change over 2 years compared to never hard drug users (p &gt; 0.05).\nFor never drug users, there is no difference in log viral load over 2 years in those with high adherence compared to those with low adherence (p = 0.11).\nLet’s keep going.\nChange the reference category to high adherence, to compare the impact of hard drug use groups for those with high adherence to the treatment regiment.\n\n# Relevel reference group\ndata_wide_2$ADH_HIGHVSLOW &lt;- relevel(data_wide_2$ADH_HIGHVSLOW, ref = \"High Adherence\")\n\n# Perform the regression on log viral load change with the full model\nmodel_VLOAD_full2 &lt;- lm(VLOAD_log_CHANGE ~ hard_drugs_grp + ADH_HIGHVSLOW + hard_drugs_grp*ADH_HIGHVSLOW + EDUC_COLLEGE, data = data_wide_2)\n\n# Examine summary\nsummary(model_VLOAD_full2)\n\n\nCall:\nlm(formula = VLOAD_log_CHANGE ~ hard_drugs_grp + ADH_HIGHVSLOW + \n    hard_drugs_grp * ADH_HIGHVSLOW + EDUC_COLLEGE, data = data_wide_2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.2508 -1.6120 -0.2134  1.1308  9.2569 \n\nCoefficients:\n                                                       Estimate Std. Error\n(Intercept)                                            -5.72028    0.27445\nhard_drugs_grpPrevious User                            -0.03961    0.45829\nhard_drugs_grpCurrent User                              0.73284    0.38167\nADH_HIGHVSLOWLow Adherence                              0.78762    0.48983\nEDUC_COLLEGECollege                                    -0.84891    0.29554\nhard_drugs_grpPrevious User:ADH_HIGHVSLOWLow Adherence  5.84794    1.27731\nhard_drugs_grpCurrent User:ADH_HIGHVSLOWLow Adherence   2.74295    1.46590\n                                                       t value\n(Intercept)                                            -20.843\nhard_drugs_grpPrevious User                             -0.086\nhard_drugs_grpCurrent User                               1.920\nADH_HIGHVSLOWLow Adherence                               1.608\nEDUC_COLLEGECollege                                     -2.872\nhard_drugs_grpPrevious User:ADH_HIGHVSLOWLow Adherence   4.578\nhard_drugs_grpCurrent User:ADH_HIGHVSLOWLow Adherence    1.871\n                                                                   Pr(&gt;|t|)    \n(Intercept)                                            &lt; 0.0000000000000002 ***\nhard_drugs_grpPrevious User                                         0.93116    \nhard_drugs_grpCurrent User                                          0.05540 .  \nADH_HIGHVSLOWLow Adherence                                          0.10846    \nEDUC_COLLEGECollege                                                 0.00424 ** \nhard_drugs_grpPrevious User:ADH_HIGHVSLOWLow Adherence           0.00000589 ***\nhard_drugs_grpCurrent User:ADH_HIGHVSLOWLow Adherence               0.06189 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.661 on 513 degrees of freedom\n  (30 observations deleted due to missingness)\nMultiple R-squared:  0.1001,    Adjusted R-squared:  0.08956 \nF-statistic:  9.51 on 6 and 513 DF,  p-value: 0.0000000006388\n\n\n\n# Print useful model parameters\nmodel_results(model_VLOAD_full2)\n\n\n\n\n\nEstimate\nStd. Error\nt value\nPr(&gt;|t|)\np_adjusted\n2.5 %\n97.5 %\n\n\n\n\n(Intercept)\n-5.7202799\n0.2744497\n-20.8427262\n0.0000000\n0.0000000\n-6.2594636\n-5.1810963\n\n\nhard_drugs_grpPrevious User\n-0.0396117\n0.4582879\n-0.0864341\n0.9311551\n1.0000000\n-0.9399637\n0.8607403\n\n\nhard_drugs_grpCurrent User\n0.7328404\n0.3816725\n1.9200766\n0.0554024\n0.3878167\n-0.0169930\n1.4826737\n\n\nADH_HIGHVSLOWLow Adherence\n0.7876174\n0.4898289\n1.6079437\n0.1084627\n0.7592392\n-0.1747001\n1.7499349\n\n\nEDUC_COLLEGECollege\n-0.8489132\n0.2955395\n-2.8724184\n0.0042421\n0.0296945\n-1.4295299\n-0.2682965\n\n\nhard_drugs_grpPrevious User:ADH_HIGHVSLOWLow Adherence\n5.8479420\n1.2773135\n4.5783138\n0.0000059\n0.0000412\n3.3385331\n8.3573508\n\n\nhard_drugs_grpCurrent User:ADH_HIGHVSLOWLow Adherence\n2.7429503\n1.4659004\n1.8711710\n0.0618904\n0.4332325\n-0.1369561\n5.6228568\n\n\n\n\n\n\n\nAt the high adherence levels, there is no difference between previous and current hard drug users and never hard drug users on log viral load change (p &gt; 0.05).\nChange the reference categories to previous users and low adherence to compare the impact of hard drug use groups for those with low adherence to the treatment regiment.\n\n# Relevel reference group\ndata_wide_2$hard_drugs_grp &lt;- relevel(data_wide_2$hard_drugs_grp, ref = \"Previous User\")\n\n# Relevel reference group\ndata_wide_2$ADH_HIGHVSLOW &lt;- relevel(data_wide_2$ADH_HIGHVSLOW, ref = \"Low Adherence\")\n\n# Perform the regression on log viral load change with the full model\nmodel_VLOAD_full3 &lt;- lm(VLOAD_log_CHANGE ~ hard_drugs_grp + ADH_HIGHVSLOW + hard_drugs_grp*ADH_HIGHVSLOW + EDUC_COLLEGE, data = data_wide_2)\n\n\n# Examine summary\nsummary(model_VLOAD_full3)\n\n\nCall:\nlm(formula = VLOAD_log_CHANGE ~ hard_drugs_grp + ADH_HIGHVSLOW + \n    hard_drugs_grp * ADH_HIGHVSLOW + EDUC_COLLEGE, data = data_wide_2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.2508 -1.6120 -0.2134  1.1308  9.2569 \n\nCoefficients:\n                                                       Estimate Std. Error\n(Intercept)                                              0.8757     1.1260\nhard_drugs_grpNever User                                -5.8083     1.1857\nhard_drugs_grpCurrent User                              -2.3325     1.7242\nADH_HIGHVSLOWHigh Adherence                             -6.6356     1.1785\nEDUC_COLLEGECollege                                     -0.8489     0.2955\nhard_drugs_grpNever User:ADH_HIGHVSLOWHigh Adherence     5.8479     1.2773\nhard_drugs_grpCurrent User:ADH_HIGHVSLOWHigh Adherence   3.1050     1.8278\n                                                       t value     Pr(&gt;|t|)    \n(Intercept)                                              0.778      0.43710    \nhard_drugs_grpNever User                                -4.898 0.0000012967 ***\nhard_drugs_grpCurrent User                              -1.353      0.17672    \nADH_HIGHVSLOWHigh Adherence                             -5.630 0.0000000297 ***\nEDUC_COLLEGECollege                                     -2.872      0.00424 ** \nhard_drugs_grpNever User:ADH_HIGHVSLOWHigh Adherence     4.578 0.0000058885 ***\nhard_drugs_grpCurrent User:ADH_HIGHVSLOWHigh Adherence   1.699      0.08997 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.661 on 513 degrees of freedom\n  (30 observations deleted due to missingness)\nMultiple R-squared:  0.1001,    Adjusted R-squared:  0.08956 \nF-statistic:  9.51 on 6 and 513 DF,  p-value: 0.0000000006388\n\n# Print useful model parameters\nmodel_results(model_VLOAD_full3)\n\n\n\n\n\nEstimate\nStd. Error\nt value\nPr(&gt;|t|)\np_adjusted\n2.5 %\n97.5 %\n\n\n\n\n(Intercept)\n0.8756677\n1.1259682\n0.777702\n0.4371032\n1.0000000\n-1.3364083\n3.0877438\n\n\nhard_drugs_grpNever User\n-5.8083303\n1.1857389\n-4.898490\n0.0000013\n0.0000091\n-8.1378317\n-3.4788288\n\n\nhard_drugs_grpCurrent User\n-2.3325396\n1.7242354\n-1.352796\n0.1767166\n1.0000000\n-5.7199709\n1.0548917\n\n\nADH_HIGHVSLOWHigh Adherence\n-6.6355594\n1.1785472\n-5.630287\n0.0000000\n0.0000002\n-8.9509321\n-4.3201866\n\n\nEDUC_COLLEGECollege\n-0.8489132\n0.2955395\n-2.872418\n0.0042421\n0.0296945\n-1.4295299\n-0.2682965\n\n\nhard_drugs_grpNever User:ADH_HIGHVSLOWHigh Adherence\n5.8479420\n1.2773135\n4.578314\n0.0000059\n0.0000412\n3.3385331\n8.3573508\n\n\nhard_drugs_grpCurrent User:ADH_HIGHVSLOWHigh Adherence\n3.1049916\n1.8277854\n1.698773\n0.0899684\n0.6297785\n-0.4858737\n6.6958570\n\n\n\n\n\n\n\nAt low adherence, there is a significant difference between previous and current drug users, with current drug users having a higher increase in log viral load over 2 years (p &lt; 0.0001).\nFor previous drug users, those with high adherence had less of an increase in log viral load over 2 years compared to those with low adherence (p &lt; 0.0001).\nChange the reference group to previous drug users and high adherence.\n\n# Relevel reference group\ndata_wide_2$hard_drugs_grp &lt;- relevel(data_wide_2$hard_drugs_grp, ref = \"Previous User\")\n\n# Relevel reference group\ndata_wide_2$ADH_HIGHVSLOW &lt;- relevel(data_wide_2$ADH_HIGHVSLOW, ref = \"High Adherence\")\n\n# Perform the regression on log viral load change with the full model\nmodel_VLOAD_full4 &lt;- lm(VLOAD_log_CHANGE ~ hard_drugs_grp + ADH_HIGHVSLOW + hard_drugs_grp*ADH_HIGHVSLOW + EDUC_COLLEGE, data = data_wide_2)\n\n# Examine summary\nsummary(model_VLOAD_full4)\n\n\nCall:\nlm(formula = VLOAD_log_CHANGE ~ hard_drugs_grp + ADH_HIGHVSLOW + \n    hard_drugs_grp * ADH_HIGHVSLOW + EDUC_COLLEGE, data = data_wide_2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.2508 -1.6120 -0.2134  1.1308  9.2569 \n\nCoefficients:\n                                                      Estimate Std. Error\n(Intercept)                                           -5.75989    0.43709\nhard_drugs_grpNever User                               0.03961    0.45829\nhard_drugs_grpCurrent User                             0.77245    0.56988\nADH_HIGHVSLOWLow Adherence                             6.63556    1.17855\nEDUC_COLLEGECollege                                   -0.84891    0.29554\nhard_drugs_grpNever User:ADH_HIGHVSLOWLow Adherence   -5.84794    1.27731\nhard_drugs_grpCurrent User:ADH_HIGHVSLOWLow Adherence -3.10499    1.82779\n                                                      t value\n(Intercept)                                           -13.178\nhard_drugs_grpNever User                                0.086\nhard_drugs_grpCurrent User                              1.355\nADH_HIGHVSLOWLow Adherence                              5.630\nEDUC_COLLEGECollege                                    -2.872\nhard_drugs_grpNever User:ADH_HIGHVSLOWLow Adherence    -4.578\nhard_drugs_grpCurrent User:ADH_HIGHVSLOWLow Adherence  -1.699\n                                                                  Pr(&gt;|t|)    \n(Intercept)                                           &lt; 0.0000000000000002 ***\nhard_drugs_grpNever User                                           0.93116    \nhard_drugs_grpCurrent User                                         0.17587    \nADH_HIGHVSLOWLow Adherence                                    0.0000000297 ***\nEDUC_COLLEGECollege                                                0.00424 ** \nhard_drugs_grpNever User:ADH_HIGHVSLOWLow Adherence           0.0000058885 ***\nhard_drugs_grpCurrent User:ADH_HIGHVSLOWLow Adherence              0.08997 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.661 on 513 degrees of freedom\n  (30 observations deleted due to missingness)\nMultiple R-squared:  0.1001,    Adjusted R-squared:  0.08956 \nF-statistic:  9.51 on 6 and 513 DF,  p-value: 0.0000000006388\n\n\n\n# Print useful model parameters\nmodel_results(model_VLOAD_full4)\n\n\n\n\n\nEstimate\nStd. Error\nt value\nPr(&gt;|t|)\np_adjusted\n2.5 %\n97.5 %\n\n\n\n\n(Intercept)\n-5.7598916\n0.4370859\n-13.1779394\n0.0000000\n0.0000000\n-6.6185902\n-4.9011931\n\n\nhard_drugs_grpNever User\n0.0396117\n0.4582879\n0.0864341\n0.9311551\n1.0000000\n-0.8607403\n0.9399637\n\n\nhard_drugs_grpCurrent User\n0.7724521\n0.5698803\n1.3554637\n0.1758663\n1.0000000\n-0.3471342\n1.8920383\n\n\nADH_HIGHVSLOWLow Adherence\n6.6355594\n1.1785472\n5.6302871\n0.0000000\n0.0000002\n4.3201866\n8.9509321\n\n\nEDUC_COLLEGECollege\n-0.8489132\n0.2955395\n-2.8724184\n0.0042421\n0.0296945\n-1.4295299\n-0.2682965\n\n\nhard_drugs_grpNever User:ADH_HIGHVSLOWLow Adherence\n-5.8479420\n1.2773135\n-4.5783138\n0.0000059\n0.0000412\n-8.3573508\n-3.3385331\n\n\nhard_drugs_grpCurrent User:ADH_HIGHVSLOWLow Adherence\n-3.1049916\n1.8277854\n-1.6987726\n0.0899684\n0.6297785\n-6.6958570\n0.4858737\n\n\n\n\n\n\n\nAt the high adherence level, there is no difference in log viral load change between current and previous hard drug users (p &gt; 0.05).\nChange the reference categories to current users and low adherence to compare the impact of hard drug use groups for those with low adherence to the treatment regiment.\n\n# Relevel reference group\ndata_wide_2$hard_drugs_grp &lt;- relevel(data_wide_2$hard_drugs_grp, ref = \"Current User\")\n\n# Relevel reference group\ndata_wide_2$ADH_HIGHVSLOW &lt;- relevel(data_wide_2$ADH_HIGHVSLOW, ref = \"Low Adherence\")\n\n# Perform the regression on log viral load change with the full model\nmodel_VLOAD_full5 &lt;- lm(VLOAD_log_CHANGE ~ hard_drugs_grp + ADH_HIGHVSLOW + hard_drugs_grp*ADH_HIGHVSLOW + EDUC_COLLEGE, data = data_wide_2)\n\n# Examine summary\nsummary(model_VLOAD_full5)\n\n\nCall:\nlm(formula = VLOAD_log_CHANGE ~ hard_drugs_grp + ADH_HIGHVSLOW + \n    hard_drugs_grp * ADH_HIGHVSLOW + EDUC_COLLEGE, data = data_wide_2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.2508 -1.6120 -0.2134  1.1308  9.2569 \n\nCoefficients:\n                                                        Estimate Std. Error\n(Intercept)                                              -1.4569     1.3389\nhard_drugs_grpPrevious User                               2.3325     1.7242\nhard_drugs_grpNever User                                 -3.4758     1.4138\nADH_HIGHVSLOWHigh Adherence                              -3.5306     1.3823\nEDUC_COLLEGECollege                                      -0.8489     0.2955\nhard_drugs_grpPrevious User:ADH_HIGHVSLOWHigh Adherence  -3.1050     1.8278\nhard_drugs_grpNever User:ADH_HIGHVSLOWHigh Adherence      2.7430     1.4659\n                                                        t value Pr(&gt;|t|)   \n(Intercept)                                              -1.088  0.27704   \nhard_drugs_grpPrevious User                               1.353  0.17672   \nhard_drugs_grpNever User                                 -2.458  0.01428 * \nADH_HIGHVSLOWHigh Adherence                              -2.554  0.01093 * \nEDUC_COLLEGECollege                                      -2.872  0.00424 **\nhard_drugs_grpPrevious User:ADH_HIGHVSLOWHigh Adherence  -1.699  0.08997 . \nhard_drugs_grpNever User:ADH_HIGHVSLOWHigh Adherence      1.871  0.06189 . \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.661 on 513 degrees of freedom\n  (30 observations deleted due to missingness)\nMultiple R-squared:  0.1001,    Adjusted R-squared:  0.08956 \nF-statistic:  9.51 on 6 and 513 DF,  p-value: 0.0000000006388\n\n\nFor current hard drug users, those with high adherence have a lower increase in log viral load over 2 years compared to those with low adherence (p = 0.011).\nTop of Tabset\n\n\nWe can see that all variables, including EDUC_COLLEGE, are significant, and thus there is no need to compare the full model to reduced models; this is our final model for VLOAD_log_CHANGE\nWe can double check this with a partial F-test comparing the full model with the reduced model without the interaction term\n\n# Perform full model with interaction term\nmodel_log_VLOAD_full &lt;- lm(VLOAD_log_CHANGE ~ hard_drugs_grp + ADH_HIGHVSLOW + EDUC_COLLEGE + hard_drugs_grp*ADH_HIGHVSLOW, data = data_wide_2)\n\n# Perform reduced model without interaction term\nmodel_log_VLOAD_red &lt;- lm(VLOAD_log_CHANGE ~ hard_drugs_grp + ADH_HIGHVSLOW + EDUC_COLLEGE, data = data_wide_2)\n\n# Perform F-Test\nanova(model_log_VLOAD_red, model_log_VLOAD_full)\n\nAnalysis of Variance Table\n\nModel 1: VLOAD_log_CHANGE ~ hard_drugs_grp + ADH_HIGHVSLOW + EDUC_COLLEGE\nModel 2: VLOAD_log_CHANGE ~ hard_drugs_grp + ADH_HIGHVSLOW + EDUC_COLLEGE + \n    hard_drugs_grp * ADH_HIGHVSLOW\n  Res.Df    RSS Df Sum of Sq      F     Pr(&gt;F)    \n1    515 3794.7                                   \n2    513 3633.5  2    161.24 11.382 0.00001456 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe p-value is significant (p &lt; 0.0001), indicating that the interaction term adds explanatory power to the model and should be included. Thus we move on with interpreting the interaction.\nTop of Tabset\n\n\n\nOverall Model\nThere were significant differences in change in log viral load based on hard drug usage and adherence to the treatment regiment, while controlling for education at baseline (F~(6, 513)~= 9.51), p &lt; 0.0001).\n\n\nHard Drug Use\nThe relationship between hard drug use and change in log viral load over 2 years depended on adherence to the treatment regiment.\n\nHard Drug Use at Low Adherence\nFor those with low adherence to the treatment regiment, current hard drug users had a change in log viral load that was 3.48 log copies/mL (or change in copies/mL that was 32.32 times) higher than never hard drug users (t = 2.458, p-adjusted = 0.04284, 95% CI: 0.70 to 6.25 log copies/mL, or 2.01 to 519.78 times copies/mL). Additionally, for those with low adherence to the treatment regimen, previous hard drug users had a change in log viral load that was 5.81 log copies/mL (or change in copies/mL that was 333.05 times) greater than never hard drug users (t = 4.90, p-adjusted &lt; 0.0001, 95% CI: 3.48 to 8.14 log copies/mL, or 32.42 to 3421.49 times copies/mL).\nFor those with low adherence to the treatment regiment, the difference between previous and current drug users was not statistically significant (t = -1.35, p-adjusted = 0.177, 95% CI: -5.72 to 1.05 log copies/mL, or 0.0033 to 2.87 times copies/mL).\n\n\nHard Drug use at High Adherence\nFor those with high adherence to the protocol, there was no difference in log viral load between current and never hard drug users (t = 1.920, p-adjusted = 0.388, 95% CI: -0.017 to 1.48 log copies/mL), previous and never hard drug users (t = -0.086, p-adjusted = 1.00, 95% CI: -0.86 to 0.94 log copies/mL), or previous and current hard drug users (t = 1.355, p-adjusted = 1.00, 95% CI: -0.35 to 1.89 log copies/mL).\n\n\n\nAdherence\nThe relationship between adherence to the treatment regiment and change in log viral load over 2 years differed by hard drug use.\n\nAdherence for Current Hard Drug Users\nFor current hard drug users, those with high adherence had a change in log viral load that was 3.53 log copies/mL (or 0.029 times) less than that of those with low adherence (t = -2.554, p-adjusted = 0.0999936, 95% CI: -6.25 to -0.70 log copies/mL, or 0.0019 to 0.50 times). However, this relationship was not statistically significant following Bonferroni correction, likely due to the small number of patients who were hard drug users with low protocol adherence (n = 4). This relationship however does appear to be real based on trends in the data, and we recommend increasing the sample size to discover the true differences.\n\n\nAdherence for Previous Hard Drug Users\nFor previous hard drug users, those with high adherence had a change in log viral load that was 6.64 log copies/mL less (or 0.0013 times) than those with low adherence (t = -5.63, p-adjusted &lt; 0.0001, 95% CI: -8.95 to -4.32 log copies/mL, or 0.00013 to 0.013 times).\n\n\nAdherence for Never Hard Drug Users\nFor never hard drug users, the difference in log viral load between those with high adherence and low adherence was not statistically significant (t = -1.61, p-adusted = 0.759, 95% CI: -1.75 to 0.17 log copies/mL, or 0.17 to 1.19 times).\n\n\n\nAdditional Interaction Comparisons\nThere are additional comparisons we can make in the interaction between hard drug usage and adherence, but they are of little clinical relevance (e.g. current hard drug users with low adherence had a higher viral load compared to never hard drug users with high adherence). Basically, any comparison made between a current or previous hard drug user with low adherence was significant.\n\n\nCollege Education at Baseline\nEducation at baseline is a significant predictor for change in log viral load over 2 years, while controlling for hard drug use and adherence to treatment regiment (t = -2.872, p = 0.00424). On average, those with a college education had an 0.85 log copies/mL (or 2.34 times) greater decrease in log viral load than those without a college education (95% CI:-1.43 to -0.27 log copies/mL).\nTop of Tabset\n\n\n\nThe interaction between hard drug use and adherence group on log viral load can be visualized in the graph below.\n\n# Relevel reference group\ndata_wide_2$hard_drugs_grp &lt;- relevel(data_wide_2$hard_drugs_grp, ref = \"Never User\")\n\n# Relevel reference group\ndata_wide_2$ADH_HIGHVSLOW &lt;- relevel(data_wide_2$ADH_HIGHVSLOW, ref = \"Low Adherence\")\n\n# Perform a regression of the full model predicting log viral load change\nmodel_VLOAD_full &lt;- lm(VLOAD_log_CHANGE ~ hard_drugs_grp + ADH_HIGHVSLOW + hard_drugs_grp * ADH_HIGHVSLOW + EDUC_COLLEGE, data = data_wide_2)\n\nlibrary(interactions)\n\n# Use interactions package to plot interaction\ncat_plot(\n  model_VLOAD_full, \n  pred = hard_drugs_grp, \n  modx = ADH_HIGHVSLOW, \n  geom = \"line\", \n  colors = \"Pastel2\",\n  x.label = \"Hard Drugs Group\",\n  y.label = \"Predicted Log Viral Load Change\"\n)\n\n\n\n\n\n\n\n\nThis very clearly illustrates the relationships we saw in our model.\nWe can see the relationship between adherence and change in log viral load based on hard drug use: for previous and current hard drug users, those with high adherence had a reduced viral load compared to those with low adherence.\nSimilarly, we can see the relationship between hard drug use and change in log viral load based on adherence: For those with low adherence, previous and current drug users had a higher viral load compared to those with high adherence.\nInterestingly, viral load did not seem to differ for never users based on adherence, suggesting that the ART treatment is most important for those with current or previous hard drug use history.\nImportantly however, this plot is not replicated exactly when I recreate it in ggplot2.\n\n# Filter out NA values\ndata_no_na &lt;- data_wide_2 %&gt;% filter(!is.na(ADH_HIGHVSLOW))\n\n# Summarize the data (using mean as an example)\ndata_summary &lt;- data_no_na %&gt;%\n  group_by(hard_drugs_grp, ADH_HIGHVSLOW) %&gt;%\n  summarize(mean_VLOAD_log_CHANGE = mean(VLOAD_log_CHANGE, na.rm = TRUE))\n\n`summarise()` has grouped output by 'hard_drugs_grp'. You can override using\nthe `.groups` argument.\n\n# Create the plot with lines\nggplot(data_no_na, aes(x = hard_drugs_grp, y = VLOAD_log_CHANGE, fill = ADH_HIGHVSLOW)) +\n  geom_boxplot(alpha = 0.5) +\n  scale_fill_brewer(palette = \"Pastel2\") +\n  labs(title = \"Interaction between Hard Drugs and Adherence on Log Viral Load Change\",\n       x = \"Hard Drugs Group\",\n       y = \"Log Viral Load Change\") +\n  guides(fill = guide_legend(title = \"Adherence Group\")) +\n   theme_minimal() \n\nWarning: Removed 18 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\n\n\n\n\nWe can see that the boxplot for low adherence previous users (n = 4), and low adherence hard drug users (n = 6) are either really tiny or really large, respectively. This makes sense, because the sample size for both of those categories is so small.\n\n# Create table of hard drugs group by adherence group\npretty_print(table(data_wide_2$hard_drugs_grp, data_wide_2$ADH_HIGHVSLOW))\n\n\n\n\n\nLow Adherence\nHigh Adherence\n\n\n\n\nNever User\n33\n399\n\n\nCurrent User\n4\n56\n\n\nPrevious User\n6\n40\n\n\n\n\n\n\n\nAnd let’s examine how close those 4 values all are to each other.\n\ndata_wide_2 %&gt;%\n  filter(hard_drugs_grp == \"Current User\" & ADH_HIGHVSLOW == \"Low Adherence\") %&gt;%\n  select(newid, VLOAD_log_CHANGE, hard_drugs_grp, ADH_HIGHVSLOW)\n\n# A tibble: 4 × 4\n  newid VLOAD_log_CHANGE hard_drugs_grp ADH_HIGHVSLOW\n  &lt;dbl&gt;            &lt;dbl&gt; &lt;fct&gt;          &lt;fct&gt;        \n1    11             1.21 Current User   Low Adherence\n2    15            -4.96 Current User   Low Adherence\n3   319             1.21 Current User   Low Adherence\n4   356            -4.99 Current User   Low Adherence\n\n\n\ndata_wide_2 %&gt;%\n  filter(hard_drugs_grp == \"Previous User\" & ADH_HIGHVSLOW == \"Low Adherence\") %&gt;%\n  select(newid, VLOAD_log_CHANGE, hard_drugs_grp, ADH_HIGHVSLOW)\n\n# A tibble: 6 × 4\n  newid VLOAD_log_CHANGE hard_drugs_grp ADH_HIGHVSLOW\n  &lt;dbl&gt;            &lt;dbl&gt; &lt;fct&gt;          &lt;fct&gt;        \n1    20           0.0114 Previous User  Low Adherence\n2    36           0.0330 Previous User  Low Adherence\n3    52           0.0335 Previous User  Low Adherence\n4   425           0.0123 Previous User  Low Adherence\n5   504           0.0349 Previous User  Low Adherence\n6   547           0.0354 Previous User  Low Adherence\n\n\nThe values for current users with low adherence have low variance, explaining the tiny boxplot, and the values for previous users with low adherence have high variance, explaining the larger boxplot.\nSo I guess that’s just a testament to the fact that you can’t rely on packages!\n\n\n\n\n\n\nNote\n\n\n\nThis is a very important limitation that will need to be addressed in the discussion!!\nWhile we see this interaction between hard drug use and adherence on the outcome variables in the data, more data must be collected to get a minimum of 30 patients per group and the analyses re-run to confirm these relationships hold!!\n\n\nTop of Tabset\n\n\n\n\n\n\nAnalysisInterpretationComparisons\n\n\nUnderstanding the limitations of an interaction model with only n=4 or n=6 in some categories, we chose to also run the model without the interaction term to acquire the main effects of hard drug use group and adherence on log viral load change, while controlling for baseline education.\n\n# Relevel reference group\ndata_wide_2$hard_drugs_grp &lt;- relevel(data_wide_2$hard_drugs_grp, ref = \"Never User\")\n\n# Perform a regression on log viral load change without interaction term\nmodel_VLOAD_red1 &lt;- lm(VLOAD_log_CHANGE ~ hard_drugs_grp + ADH_HIGHVSLOW + EDUC_COLLEGE, data = data_wide_2)\n\n# Examine summary\nsummary(model_VLOAD_red1)\n\n\nCall:\nlm(formula = VLOAD_log_CHANGE ~ hard_drugs_grp + ADH_HIGHVSLOW + \n    EDUC_COLLEGE, data = data_wide_2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.7941 -1.7418 -0.2149  1.2043  9.3073 \n\nCoefficients:\n                            Estimate Std. Error t value             Pr(&gt;|t|)\n(Intercept)                  -4.0871     0.4910  -8.324 0.000000000000000771\nhard_drugs_grpCurrent User    0.9170     0.3755   2.442               0.0149\nhard_drugs_grpPrevious User   0.7190     0.4336   1.658               0.0978\nADH_HIGHVSLOWHigh Adherence  -1.8429     0.4377  -4.211 0.000030041916341915\nEDUC_COLLEGECollege          -0.6897     0.2970  -2.322               0.0206\n                               \n(Intercept)                 ***\nhard_drugs_grpCurrent User  *  \nhard_drugs_grpPrevious User .  \nADH_HIGHVSLOWHigh Adherence ***\nEDUC_COLLEGECollege         *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.714 on 515 degrees of freedom\n  (30 observations deleted due to missingness)\nMultiple R-squared:  0.06016,   Adjusted R-squared:  0.05286 \nF-statistic: 8.241 on 4 and 515 DF,  p-value: 0.000001901\n\n\n\n# Print useful model parameters\nmodel_results(model_VLOAD_red1)\n\n\n\n\n\nEstimate\nStd. Error\nt value\nPr(&gt;|t|)\np_adjusted\n2.5 %\n97.5 %\n\n\n\n\n(Intercept)\n-4.0870510\n0.4910084\n-8.323790\n0.0000000\n0.0000000\n-5.0516769\n-3.1224251\n\n\nhard_drugs_grpCurrent User\n0.9169947\n0.3754605\n2.442320\n0.0149284\n0.0746420\n0.1793722\n1.6546172\n\n\nhard_drugs_grpPrevious User\n0.7190166\n0.4335575\n1.658411\n0.0978434\n0.4892169\n-0.1327422\n1.5707754\n\n\nADH_HIGHVSLOWHigh Adherence\n-1.8428916\n0.4376635\n-4.210750\n0.0000300\n0.0001502\n-2.7027170\n-0.9830663\n\n\nEDUC_COLLEGECollege\n-0.6896592\n0.2969844\n-2.322207\n0.0206108\n0.1030539\n-1.2731090\n-0.1062094\n\n\n\n\n\n\n\n\n# Relevel reference group\ndata_wide_2$hard_drugs_grp &lt;- relevel(data_wide_2$hard_drugs_grp, ref = \"Previous User\")\n\n# Perform a regression on log viral load change without interaction term\nmodel_VLOAD_red2 &lt;- lm(VLOAD_log_CHANGE ~ hard_drugs_grp + ADH_HIGHVSLOW + EDUC_COLLEGE, data = data_wide_2)\n\n# Examine summary\nsummary(model_VLOAD_red2)\n\n\nCall:\nlm(formula = VLOAD_log_CHANGE ~ hard_drugs_grp + ADH_HIGHVSLOW + \n    EDUC_COLLEGE, data = data_wide_2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.7941 -1.7418 -0.2149  1.2043  9.3073 \n\nCoefficients:\n                            Estimate Std. Error t value      Pr(&gt;|t|)    \n(Intercept)                  -3.3680     0.5718  -5.890 0.00000000697 ***\nhard_drugs_grpNever User     -0.7190     0.4336  -1.658        0.0978 .  \nhard_drugs_grpCurrent User    0.1980     0.5451   0.363        0.7166    \nADH_HIGHVSLOWHigh Adherence  -1.8429     0.4377  -4.211 0.00003004192 ***\nEDUC_COLLEGECollege          -0.6897     0.2970  -2.322        0.0206 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.714 on 515 degrees of freedom\n  (30 observations deleted due to missingness)\nMultiple R-squared:  0.06016,   Adjusted R-squared:  0.05286 \nF-statistic: 8.241 on 4 and 515 DF,  p-value: 0.000001901\n\n\n\n# Print useful model parameters\nmodel_results(model_VLOAD_red2)\n\n\n\n\n\nEstimate\nStd. Error\nt value\nPr(&gt;|t|)\np_adjusted\n2.5 %\n97.5 %\n\n\n\n\n(Intercept)\n-3.3680344\n0.5718018\n-5.8902135\n0.0000000\n0.0000000\n-4.4913853\n-2.2446835\n\n\nhard_drugs_grpNever User\n-0.7190166\n0.4335575\n-1.6584112\n0.0978434\n0.4892169\n-1.5707754\n0.1327422\n\n\nhard_drugs_grpCurrent User\n0.1979781\n0.5451391\n0.3631698\n0.7166270\n1.0000000\n-0.8729918\n1.2689479\n\n\nADH_HIGHVSLOWHigh Adherence\n-1.8428916\n0.4376635\n-4.2107503\n0.0000300\n0.0001502\n-2.7027170\n-0.9830663\n\n\nEDUC_COLLEGECollege\n-0.6896592\n0.2969844\n-2.3222073\n0.0206108\n0.1030539\n-1.2731090\n-0.1062094\n\n\n\n\n\n\n\nThe overall model is significant (F(~4, 515)~= 8.241, p &lt; 0.0001).\nTop of Tabset\n\n\n\nHard Drug Use\nHard drug use is a significant predictor of log viral load change, while controlling for adherence to the treatment regiment and college education. Specifically, current hard drug users had a change in log viral load that was 0.92 log copies/mL (or 2.50 times) greater than never hard drug users (t = 2.442, p-adjusted = 0.0447, 95% CI: 0.18 to 1.65 log copies/mL, or 1.20 to 5.23 times). There was no difference between previous and current hard drug users in change in log viral load over 2 years (t = 0.363, p-adjusted = 1.00, 95% CI: -0.87 to 1.27 log copies/mL, or 0.42 to 4.81 times).\n\n\nAdherence\nAdherence to the treatment regiment is a significant predictor of change in log viral load, while controlling for hard drug usage and college education (t = -4.21, p &lt; 0.0001). Specifically, those with high adherence to the protocol had an average change in log viral load that was 1.84 log copies/mL (or 0.16 times) less than those with low adherence to the protocol.\n\n\nCollege Education at Baseline\nFinally, college education is a significant predictor of log viral load change, while controlling for hard drug usage and adherence to the treatment regiment (t = -2.32, p = 0.0206). On average, those with a college education had a change in log viral load that was 0.69 log copies/mL (or 1.99 times) less than those without a college education (95% CI: -2.70 to -0.98 log copies/mL, or 0.067 to 0.37 times).\nTop of Tabset\n\n\n\nThe reduced model without the interaction has an adjusted R-squared of 0.0529, compared to the full model with the interaction term (R^2-adjusted = 0.090).\nTherefore, dropping the interaction term causes us to lose explanation of 3.71% of the variance in log viral load change, but is also more statistically sound since we no longer have small sample size issues.\nTop of Tabset\n\n\n\n\n\nUnder the classical definition of a confounder, a variable Z is a confounder if:\n\n\nIt is associated with outcome Y\n\n\nIt is associated with PEV X\n\n\nIt is not on the causal pathway (not a mediator)\n\n\nWe know that ADH_HIGHVSLOW is a predictor of VLOAD_log_CHANGE.\nTo assess if ADH_HIGHVSLOW is associated with hard drug use group, we can run a Fisher’s test\n\n# Create a contingency table\ncontingency_table &lt;- table(data_wide_2$hard_drugs_grp, data_wide_2$ADH_HIGHVSLOW)\n\n# We have to run Fisher's Exact test since we have fewer than 5 observations in some categories\nfisher_test &lt;- fisher.test(contingency_table)\n\nfisher_test\n\n\n    Fisher's Exact Test for Count Data\n\ndata:  contingency_table\np-value = 0.4168\nalternative hypothesis: two.sided\n\n\n\n# Create a contingency table\ncontingency_table &lt;- table(data_wide_2$hard_drugs_grp, data_wide_2$EDUC_COLLEGE)\n\n# We have to run Fisher's Exact test since we have fewer than 5 observations in some categories\nfisher_test &lt;- fisher.test(contingency_table)\n\nThey are not associated.\nHowever, it is not necessary for this associaton to be statistically significant for there to be important confounding present.\nWhen we ran drug_use_grp by itself as a predictor on log viral load, the overall model was significant, but after correcting for multiple pairwise comparisons, none of the between-group comparisons was significant (p &gt; 0.05).\nHowever, after including ADH_HIGHVSLOW in the model, the current vs never drug use comparison is significant, even after performing a Bonferroni correction (p-adjusted = 0.0447). In the model without ADH, this same comparison was p-adjusted = 0.1377.\nThus, ADH_HIGHVSLOW is associated with the outcome variable, and changes the relationship between the PEV and outcome variable in a meaningful way when included in the model (changes it from not significant to significant), meeting the definition of a confounder under the classical definition. However, the beta coefficient of the PEV does not change by &gt; 20%, not satisfying the operational definition of a confounder.\nAdherence to the treatment regiment is therefore a maverick variable!\nTop of Tabset\n\n\nHere we will create some simple plots to help with visualizing the main effects of hard drug use and adherence on log viral load change.\n\n# Create boxplots of log viral load change by hard drug use group\nggplot(data_wide_2, aes(x = hard_drugs_grp, y = VLOAD_log_CHANGE, fill = hard_drugs_grp)) +\n  geom_boxplot(alpha = 0.5) +\n  theme_minimal() +\n  scale_fill_brewer(palette=\"Pastel2\") +\n  labs(title = \"Boxplot of Log Viral Load Change by Hard Drug Use Group\",\n       x = \"Hard Drug Use Group\",\n       y = \"Log Viral Load Change\") +\n  guides(fill = guide_legend(title = \"Hard Drug Use Group\"))\n\nWarning: Removed 18 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\n\n\n\n\nWe can see that current and previous drug users had less of a decrease in log viral load over 2 years compared to never hard drug users.\n\n# Create boxplots of log viral load change by adherence group\nggplot(data_no_na, aes(x = ADH_HIGHVSLOW, y = VLOAD_log_CHANGE, fill = ADH_HIGHVSLOW)) +\n  geom_boxplot(alpha = 0.5) +\n  theme_minimal() +\n  scale_fill_brewer(palette=\"Pastel2\") +\n  labs(title = \"Boxplot of Log Viral Load Change by Adherence at Year 2\",\n       x = \"Adherence at year 2\",\n       y = \"Log Viral Load Change\") +\n  guides(fill = guide_legend(title = \"Adherence Group\"))\n\nWarning: Removed 18 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\n\n\n\n\nAnd here we can see those with high adherence had a higher decrease in log viral load over 2 years compared to those with low adherence.\n\n\n\n:::"
  },
  {
    "objectID": "Project_2/Project_2_R/Code/Project2.html#cd4-t-cell-count-change-1",
    "href": "Project_2/Project_2_R/Code/Project2.html#cd4-t-cell-count-change-1",
    "title": "Advanced Data Analysis - Project 2",
    "section": "CD4+ T Cell Count Change",
    "text": "CD4+ T Cell Count Change\nThe second outcome variable of interest was change in CD4+ T Cell Count over the first 2 years of the study.\n\nFull ModelReduced Model - No Interaction\n\n\n\nModel SelectionAnalysisInterpretationVisualizing the Interaction\n\n\nThe candidate variables for inclusion in our model predicting LEU3N_CHANGE were hard_drugs_grp, ADH_HIGHLOW, BMI_2, FRP_2, and EDUC_COLLEGE.\nThe researchers are interested in if differences in treatment response between the drug use groups can be explained by differences in adherence to the HAART regimen, so we will also include an interaction term between hard drug use group and adherence.\nWe begin by examining the full model with all these variables included.\n\n# Relevel to change to the reference group to never user\ndata_wide_2$hard_drugs_grp&lt;- relevel(data_wide_2$hard_drugs_grp, ref = \"Never User\")\n\n# Relevel to change to the reference group to never user\ndata_wide_2$ADH_HIGHVSLOW &lt;- relevel(data_wide_2$ADH_HIGHVSLOW, ref = \"Low Adherence\")\n\n# Perform regression on CD4+ T Cell count change by hard drugs group, include adherence as confounder, and BMI, frailty related phenotype, and college education as precision variables.\nmodel_LEU3N_full1 &lt;- lm(LEU3N_CHANGE ~ hard_drugs_grp + ADH_HIGHVSLOW + BMI_2 + FRP_2 + EDUC_COLLEGE + CESD_2 + hard_drugs_grp*ADH_HIGHVSLOW, data = data_wide_2)\n\n# Exmamine the summary\nsummary(model_LEU3N_full1)\n\n\nCall:\nlm(formula = LEU3N_CHANGE ~ hard_drugs_grp + ADH_HIGHVSLOW + \n    BMI_2 + FRP_2 + EDUC_COLLEGE + CESD_2 + hard_drugs_grp * \n    ADH_HIGHVSLOW, data = data_wide_2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-665.80 -119.44   -3.02  110.80 1080.47 \n\nCoefficients: (1 not defined because of singularities)\n                                                         Estimate Std. Error\n(Intercept)                                              -46.7866    63.8333\nhard_drugs_grpPrevious User                              -92.1062    31.5556\nhard_drugs_grpCurrent User                              -161.2223   132.3190\nADH_HIGHVSLOWHigh Adherence                               78.9814    35.0133\nBMI_2                                                      6.0075     2.1384\nFRP_2Yes                                                -101.6289    36.3579\nEDUC_COLLEGECollege                                       28.7527    21.5453\nCESD_2                                                    -1.0786     0.8141\nhard_drugs_grpPrevious User:ADH_HIGHVSLOWHigh Adherence        NA         NA\nhard_drugs_grpCurrent User:ADH_HIGHVSLOWHigh Adherence    49.4909   134.8433\n                                                        t value Pr(&gt;|t|)   \n(Intercept)                                              -0.733  0.46397   \nhard_drugs_grpPrevious User                              -2.919  0.00369 **\nhard_drugs_grpCurrent User                               -1.218  0.22370   \nADH_HIGHVSLOWHigh Adherence                               2.256  0.02457 * \nBMI_2                                                     2.809  0.00518 **\nFRP_2Yes                                                 -2.795  0.00541 **\nEDUC_COLLEGECollege                                       1.335  0.18271   \nCESD_2                                                   -1.325  0.18588   \nhard_drugs_grpPrevious User:ADH_HIGHVSLOWHigh Adherence      NA       NA   \nhard_drugs_grpCurrent User:ADH_HIGHVSLOWHigh Adherence    0.367  0.71377   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 180.3 on 448 degrees of freedom\n  (93 observations deleted due to missingness)\nMultiple R-squared:  0.1193,    Adjusted R-squared:  0.1036 \nF-statistic: 7.587 on 8 and 448 DF,  p-value: 0.000000001577\n\n\nWe get an NA for the interaction term. This is likely because we dropped the n=6 patients that were previous hard drug users with low adherence, making the interaction impossible to make. Indeed we can see we only have 7, 453 degrees of freedom.\nBMI is significant, but has 10% missing values. Let’s try running the regression without it so we can examine that interaction term, which is the main research question. (it’s also correlated to college education, so we can kind of capture it with that variable).\n\n# Relevel to change to the reference group to never user\ndata_wide_2$hard_drugs_grp&lt;- relevel(data_wide_2$hard_drugs_grp, ref = \"Current User\")\n\n# Relevel to change to the reference group to never user\ndata_wide_2$ADH_HIGHVSLOW &lt;- relevel(data_wide_2$ADH_HIGHVSLOW, ref = \"Low Adherence\")\n\n# Perform regression on CD4+ T Cell count change by hard drugs group, include adherence as confounder, and BMI, frailty related phenotype, and college education as precision variables.\nmodel_LEU3N_red1 &lt;- lm(LEU3N_CHANGE ~ hard_drugs_grp + ADH_HIGHVSLOW + FRP_2 + EDUC_COLLEGE + CESD_2 + hard_drugs_grp*ADH_HIGHVSLOW, data = data_wide_2)\n\n# Exmamine the summary\nsummary(model_LEU3N_red1)\n\n\nCall:\nlm(formula = LEU3N_CHANGE ~ hard_drugs_grp + ADH_HIGHVSLOW + \n    FRP_2 + EDUC_COLLEGE + CESD_2 + hard_drugs_grp * ADH_HIGHVSLOW, \n    data = data_wide_2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-697.61 -112.73   -1.31  106.13 1073.99 \n\nCoefficients:\n                                                         Estimate Std. Error\n(Intercept)                                              -30.8189    92.4188\nhard_drugs_grpNever User                                 126.1630    96.3440\nhard_drugs_grpPrevious User                              355.9657   119.3430\nADH_HIGHVSLOWHigh Adherence                              104.4788    94.2502\nFRP_2Yes                                                -110.1432    35.5533\nEDUC_COLLEGECollege                                       25.0343    20.1854\nCESD_2                                                    -1.0187     0.7723\nhard_drugs_grpNever User:ADH_HIGHVSLOWHigh Adherence     -17.0631    99.8903\nhard_drugs_grpPrevious User:ADH_HIGHVSLOWHigh Adherence -333.1581   126.6465\n                                                        t value Pr(&gt;|t|)   \n(Intercept)                                              -0.333  0.73892   \nhard_drugs_grpNever User                                  1.310  0.19096   \nhard_drugs_grpPrevious User                               2.983  0.00299 **\nADH_HIGHVSLOWHigh Adherence                               1.109  0.26816   \nFRP_2Yes                                                 -3.098  0.00206 **\nEDUC_COLLEGECollege                                       1.240  0.21547   \nCESD_2                                                   -1.319  0.18776   \nhard_drugs_grpNever User:ADH_HIGHVSLOWHigh Adherence     -0.171  0.86443   \nhard_drugs_grpPrevious User:ADH_HIGHVSLOWHigh Adherence  -2.631  0.00878 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 181.3 on 506 degrees of freedom\n  (35 observations deleted due to missingness)\nMultiple R-squared:  0.09847,   Adjusted R-squared:  0.08422 \nF-statistic: 6.908 on 8 and 506 DF,  p-value: 0.000000012\n\n\n\nBackwards Elimination\nWe will perform model selection using backwards elimination and BIC to select the most parsimonious model.\nA delta BIC of &gt;2 indicates a difference in model performance.\n\n# Perform backward elimination regression selection based on BIC \nols_step_backward_sbc(model_LEU3N_red1, include = c(\"hard_drugs_grp\", \"ADH_HIGHVSLOW\"))\n\n\n                                Stepwise Summary                                \n------------------------------------------------------------------------------\nStep    Variable          AIC         SBC         SBIC        R2       Adj. R2 \n------------------------------------------------------------------------------\n 0      Full Model      6828.740    6871.181    5380.046    0.09847    0.08422 \n 1      EDUC_COLLEGE    6828.303    6866.500    5355.906    0.09573    0.08324 \n 2      CESD_2          6828.406    6862.359    5356.186    0.09203    0.08130 \n------------------------------------------------------------------------------\n\nFinal Model Output \n------------------\n\n                           Model Summary                            \n-------------------------------------------------------------------\nR                         0.303       RMSE                 180.377 \nR-Squared                 0.092       MSE                32535.774 \nAdj. R-Squared            0.081       Coef. Var            114.193 \nPred R-Squared            0.076       AIC                 6828.406 \nMAE                     136.282       SBC                 6862.359 \n-------------------------------------------------------------------\n RMSE: Root Mean Square Error \n MSE: Mean Square Error \n MAE: Mean Absolute Error \n AIC: Akaike Information Criteria \n SBC: Schwarz Bayesian Criteria \n\n                                 ANOVA                                   \n------------------------------------------------------------------------\n                    Sum of                                              \n                   Squares         DF    Mean Square      F        Sig. \n------------------------------------------------------------------------\nRegression     1698306.434          6     283051.072    8.581    0.0000 \nResidual      16755923.716        508      32984.102                    \nTotal         18454230.150        514                                   \n------------------------------------------------------------------------\n\n                                                           Parameter Estimates                                                            \n-----------------------------------------------------------------------------------------------------------------------------------------\n                                                  model        Beta    Std. Error    Std. Beta      t        Sig        lower      upper \n-----------------------------------------------------------------------------------------------------------------------------------------\n                                            (Intercept)     -36.638        90.808                 -0.403    0.687    -215.043    141.767 \n                               hard_drugs_grpNever User     136.867        96.322        0.292     1.421    0.156     -52.373    326.106 \n                            hard_drugs_grpPrevious User     341.997       117.232        0.515     2.917    0.004     111.678    572.317 \n                            ADH_HIGHVSLOWHigh Adherence     116.056        94.071        0.168     1.234    0.218     -68.761    300.873 \n                                               FRP_2Yes    -113.844        35.435       -0.136    -3.213    0.001    -183.462    -44.226 \n   hard_drugs_grpNever User:ADH_HIGHVSLOWHigh Adherence     -25.037        99.815       -0.059    -0.251    0.802    -221.138    171.064 \nhard_drugs_grpPrevious User:ADH_HIGHVSLOWHigh Adherence    -330.080       123.130       -0.467    -2.681    0.008    -571.988    -88.173 \n-----------------------------------------------------------------------------------------------------------------------------------------\n\n\nAs predicted, the most parsimonious model is with CESD_2 and EDUC_COLLEGE removed, and only including FRP_2 as a precision variable.\nTop of Tabset\n\n\n\nLet’s investigate the key relationships in the final model for LEU3N_CHANGE as determined through backwards selection.\nFirst let’s look at the impact of hard drug use on CD4+ T Cell change, at low adherence.\n\n# Relevel to change to the reference group to never user\ndata_wide_2$hard_drugs_grp&lt;- relevel(data_wide_2$hard_drugs_grp, ref = \"Never User\")\n\n# Relevel to change to the reference group to never user\ndata_wide_2$ADH_HIGHVSLOW &lt;- relevel(data_wide_2$ADH_HIGHVSLOW, ref = \"Low Adherence\")\n\n# Perform regression on CD4+ T Cell count change by hard drugs group with covariates as determined by backward elimination\nmodel_LEU3N_final1 &lt;- lm(LEU3N_CHANGE ~ hard_drugs_grp + ADH_HIGHVSLOW + FRP_2 + hard_drugs_grp*ADH_HIGHVSLOW, data = data_wide_2)\n\n# Examine the summary\nsummary(model_LEU3N_final1)\n\n\nCall:\nlm(formula = LEU3N_CHANGE ~ hard_drugs_grp + ADH_HIGHVSLOW + \n    FRP_2 + hard_drugs_grp * ADH_HIGHVSLOW, data = data_wide_2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-700.74 -111.21   -1.67  112.23 1077.98 \n\nCoefficients:\n                                                        Estimate Std. Error\n(Intercept)                                               107.46      31.59\nhard_drugs_grpCurrent User                               -144.10      96.02\nhard_drugs_grpPrevious User                               197.90      80.49\nADH_HIGHVSLOWHigh Adherence                                83.09      32.92\nFRP_2Yes                                                 -113.63      35.38\nhard_drugs_grpCurrent User:ADH_HIGHVSLOWHigh Adherence     32.94      99.51\nhard_drugs_grpPrevious User:ADH_HIGHVSLOWHigh Adherence  -297.13      85.95\n                                                        t value Pr(&gt;|t|)    \n(Intercept)                                               3.402 0.000721 ***\nhard_drugs_grpCurrent User                               -1.501 0.134038    \nhard_drugs_grpPrevious User                               2.459 0.014277 *  \nADH_HIGHVSLOWHigh Adherence                               2.524 0.011895 *  \nFRP_2Yes                                                 -3.212 0.001402 ** \nhard_drugs_grpCurrent User:ADH_HIGHVSLOWHigh Adherence    0.331 0.740764    \nhard_drugs_grpPrevious User:ADH_HIGHVSLOWHigh Adherence  -3.457 0.000592 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 181.3 on 512 degrees of freedom\n  (31 observations deleted due to missingness)\nMultiple R-squared:  0.08984,   Adjusted R-squared:  0.07917 \nF-statistic: 8.423 on 6 and 512 DF,  p-value: 0.000000009913\n\n\n\n# Print useful model parameters\nmodel_results(model_LEU3N_final1)\n\n\n\n\n\nEstimate\nStd. Error\nt value\nPr(&gt;|t|)\np_adjusted\n2.5 %\n97.5 %\n\n\n\n\n(Intercept)\n107.46049\n31.58675\n3.4020752\n0.0007211\n0.0050476\n45.40491\n169.51607\n\n\nhard_drugs_grpCurrent User\n-144.09848\n96.01798\n-1.5007447\n0.1340380\n0.9382660\n-332.73618\n44.53922\n\n\nhard_drugs_grpPrevious User\n197.89868\n80.49146\n2.4586294\n0.0142768\n0.0999374\n39.76450\n356.03287\n\n\nADH_HIGHVSLOWHigh Adherence\n83.09406\n32.91813\n2.5242642\n0.0118953\n0.0832670\n18.42283\n147.76529\n\n\nFRP_2Yes\n-113.62595\n35.37733\n-3.2118295\n0.0014019\n0.0098130\n-183.12853\n-44.12336\n\n\nhard_drugs_grpCurrent User:ADH_HIGHVSLOWHigh Adherence\n32.93895\n99.50721\n0.3310208\n0.7407642\n1.0000000\n-162.55372\n228.43162\n\n\nhard_drugs_grpPrevious User:ADH_HIGHVSLOWHigh Adherence\n-297.12895\n85.95111\n-3.4569531\n0.0005918\n0.0041424\n-465.98920\n-128.26871\n\n\n\n\n\n\n\nThe overall model is highly significant (F(6,512)= 8.42, p &lt; 0.00001).\nAt low adherence, previous hard drug users have a higher CD4+ T Cell count than never hard drug users (p-adjusted = 0.043), but current hard drugs users did not differ from never drug users (p-adjusted = 0.40).\nAdditionally, for never hard drug users, those with high adherence have a higher CD4+ T Cell count than those with low adherence (p = 0.011895)\nNow let’s compare hard drug usage for those with high adherence.\n\n# Relevel to change to the reference group to never user\ndata_wide_2$hard_drugs_grp&lt;- relevel(data_wide_2$hard_drugs_grp, ref = \"Never User\")\n\n# Relevel to change to the reference group to never user\ndata_wide_2$ADH_HIGHVSLOW &lt;- relevel(data_wide_2$ADH_HIGHVSLOW, ref = \"High Adherence\")\n\n# Perform regression on CD4+ T Cell count change by hard drugs group with covariates as determined by backward elimination\nmodel_LEU3N_final2 &lt;- lm(LEU3N_CHANGE ~ hard_drugs_grp + ADH_HIGHVSLOW + FRP_2 + hard_drugs_grp*ADH_HIGHVSLOW, data = data_wide_2)\n\n# Examine the summary\nsummary(model_LEU3N_final2)\n\n\nCall:\nlm(formula = LEU3N_CHANGE ~ hard_drugs_grp + ADH_HIGHVSLOW + \n    FRP_2 + hard_drugs_grp * ADH_HIGHVSLOW, data = data_wide_2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-700.74 -111.21   -1.67  112.23 1077.98 \n\nCoefficients:\n                                                       Estimate Std. Error\n(Intercept)                                              190.56       9.47\nhard_drugs_grpCurrent User                              -111.16      26.04\nhard_drugs_grpPrevious User                              -99.23      30.14\nADH_HIGHVSLOWLow Adherence                               -83.09      32.92\nFRP_2Yes                                                -113.63      35.38\nhard_drugs_grpCurrent User:ADH_HIGHVSLOWLow Adherence    -32.94      99.51\nhard_drugs_grpPrevious User:ADH_HIGHVSLOWLow Adherence   297.13      85.95\n                                                       t value\n(Intercept)                                             20.123\nhard_drugs_grpCurrent User                              -4.269\nhard_drugs_grpPrevious User                             -3.292\nADH_HIGHVSLOWLow Adherence                              -2.524\nFRP_2Yes                                                -3.212\nhard_drugs_grpCurrent User:ADH_HIGHVSLOWLow Adherence   -0.331\nhard_drugs_grpPrevious User:ADH_HIGHVSLOWLow Adherence   3.457\n                                                                   Pr(&gt;|t|)    \n(Intercept)                                            &lt; 0.0000000000000002 ***\nhard_drugs_grpCurrent User                                        0.0000234 ***\nhard_drugs_grpPrevious User                                        0.001064 ** \nADH_HIGHVSLOWLow Adherence                                         0.011895 *  \nFRP_2Yes                                                           0.001402 ** \nhard_drugs_grpCurrent User:ADH_HIGHVSLOWLow Adherence              0.740764    \nhard_drugs_grpPrevious User:ADH_HIGHVSLOWLow Adherence             0.000592 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 181.3 on 512 degrees of freedom\n  (31 observations deleted due to missingness)\nMultiple R-squared:  0.08984,   Adjusted R-squared:  0.07917 \nF-statistic: 8.423 on 6 and 512 DF,  p-value: 0.000000009913\n\n\n\n# Print useful model parameters\nmodel_results(model_LEU3N_final2)\n\n\n\n\n\nEstimate\nStd. Error\nt value\nPr(&gt;|t|)\np_adjusted\n2.5 %\n97.5 %\n\n\n\n\n(Intercept)\n190.55455\n9.469605\n20.1227563\n0.0000000\n0.0000000\n171.9505\n209.15861\n\n\nhard_drugs_grpCurrent User\n-111.15953\n26.036468\n-4.2693782\n0.0000234\n0.0001636\n-162.3110\n-60.00807\n\n\nhard_drugs_grpPrevious User\n-99.23027\n30.144942\n-3.2917718\n0.0010643\n0.0074500\n-158.4533\n-40.00727\n\n\nADH_HIGHVSLOWLow Adherence\n-83.09406\n32.918132\n-2.5242642\n0.0118953\n0.0832670\n-147.7653\n-18.42283\n\n\nFRP_2Yes\n-113.62595\n35.377328\n-3.2118295\n0.0014019\n0.0098130\n-183.1285\n-44.12336\n\n\nhard_drugs_grpCurrent User:ADH_HIGHVSLOWLow Adherence\n-32.93895\n99.507208\n-0.3310208\n0.7407642\n1.0000000\n-228.4316\n162.55372\n\n\nhard_drugs_grpPrevious User:ADH_HIGHVSLOWLow Adherence\n297.12895\n85.951110\n3.4569531\n0.0005918\n0.0041424\n128.2687\n465.98920\n\n\n\n\n\n\n\nFor those with high adherence, previous hard drug users had a lower CD4+ T Cell count (p-adjusted = 0.0032), and current hard drug users had a lower CD4+ T Cell (p-adjusted &lt; 0.0001) count compared to never hard users.\nNow we make the same comparisons with previous hard drug users as the baseline.\n\n# Relevel to change to the reference group to never user\ndata_wide_2$hard_drugs_grp&lt;- relevel(data_wide_2$hard_drugs_grp, ref = \"Previous User\")\n\n# Relevel to change to the reference group to never user\ndata_wide_2$ADH_HIGHVSLOW &lt;- relevel(data_wide_2$ADH_HIGHVSLOW, ref = \"Low Adherence\")\n\n# Perform regression on CD4+ T Cell count change by hard drugs group with covariates as determined by backward elimination\nmodel_LEU3N_final2 &lt;- lm(LEU3N_CHANGE ~ hard_drugs_grp + ADH_HIGHVSLOW + FRP_2 + hard_drugs_grp*ADH_HIGHVSLOW, data = data_wide_2)\n\n# Examine the summary\nsummary(model_LEU3N_final2)\n\n\nCall:\nlm(formula = LEU3N_CHANGE ~ hard_drugs_grp + ADH_HIGHVSLOW + \n    FRP_2 + hard_drugs_grp * ADH_HIGHVSLOW, data = data_wide_2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-700.74 -111.21   -1.67  112.23 1077.98 \n\nCoefficients:\n                                                       Estimate Std. Error\n(Intercept)                                              305.36      74.03\nhard_drugs_grpNever User                                -197.90      80.49\nhard_drugs_grpCurrent User                              -342.00     117.06\nADH_HIGHVSLOWHigh Adherence                             -214.03      79.41\nFRP_2Yes                                                -113.63      35.38\nhard_drugs_grpNever User:ADH_HIGHVSLOWHigh Adherence     297.13      85.95\nhard_drugs_grpCurrent User:ADH_HIGHVSLOWHigh Adherence   330.07     122.95\n                                                       t value  Pr(&gt;|t|)    \n(Intercept)                                              4.125 0.0000433 ***\nhard_drugs_grpNever User                                -2.459  0.014277 *  \nhard_drugs_grpCurrent User                              -2.922  0.003636 ** \nADH_HIGHVSLOWHigh Adherence                             -2.695  0.007266 ** \nFRP_2Yes                                                -3.212  0.001402 ** \nhard_drugs_grpNever User:ADH_HIGHVSLOWHigh Adherence     3.457  0.000592 ***\nhard_drugs_grpCurrent User:ADH_HIGHVSLOWHigh Adherence   2.685  0.007497 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 181.3 on 512 degrees of freedom\n  (31 observations deleted due to missingness)\nMultiple R-squared:  0.08984,   Adjusted R-squared:  0.07917 \nF-statistic: 8.423 on 6 and 512 DF,  p-value: 0.000000009913\n\n\n\n# Print useful model parameters\nmodel_results(model_LEU3N_final2)\n\n\n\n\n\nEstimate\nStd. Error\nt value\nPr(&gt;|t|)\np_adjusted\n2.5 %\n97.5 %\n\n\n\n\n(Intercept)\n305.3592\n74.03481\n4.124535\n0.0000433\n0.0003034\n159.90978\n450.80856\n\n\nhard_drugs_grpNever User\n-197.8987\n80.49146\n-2.458629\n0.0142768\n0.0999374\n-356.03287\n-39.76450\n\n\nhard_drugs_grpCurrent User\n-341.9972\n117.05931\n-2.921572\n0.0036365\n0.0254554\n-571.97284\n-112.02148\n\n\nADH_HIGHVSLOWHigh Adherence\n-214.0349\n79.41319\n-2.695206\n0.0072658\n0.0508607\n-370.05069\n-58.01909\n\n\nFRP_2Yes\n-113.6259\n35.37733\n-3.211829\n0.0014019\n0.0098130\n-183.12853\n-44.12336\n\n\nhard_drugs_grpNever User:ADH_HIGHVSLOWHigh Adherence\n297.1290\n85.95111\n3.456953\n0.0005918\n0.0041424\n128.26871\n465.98920\n\n\nhard_drugs_grpCurrent User:ADH_HIGHVSLOWHigh Adherence\n330.0679\n122.94881\n2.684596\n0.0074971\n0.0524799\n88.52168\n571.61413\n\n\n\n\n\n\n\nAt low adherence, current hard drug users had a CD4+ T Cell count compared to previous hard drug users (p-adjusted = 0.011).\nAdditionally, for previous hard drug users, those with high adherence had a greater decrease in CD4+ T Cell count compared to those with low adherence (p = 0.0073).\nNow let’s change the reference level to compare high adherence previous vs current drug users.\n\n# Relevel to change to the reference group to never user\ndata_wide_2$hard_drugs_grp &lt;- relevel(data_wide_2$hard_drugs_grp, ref = \"Previous User\")\n\n# Relevel to change to the reference group to never user\ndata_wide_2$ADH_HIGHVSLOW &lt;- relevel(data_wide_2$ADH_HIGHVSLOW, ref = \"High Adherence\")\n\n# Perform regression on CD4+ T Cell count change by hard drugs group with covariates as determined by backward elimination\nmodel_LEU3N_final3 &lt;- lm(LEU3N_CHANGE ~ hard_drugs_grp + ADH_HIGHVSLOW + FRP_2 + hard_drugs_grp*ADH_HIGHVSLOW, data = data_wide_2)\n\n# Examine the summary\nsummary(model_LEU3N_final3)\n\n\nCall:\nlm(formula = LEU3N_CHANGE ~ hard_drugs_grp + ADH_HIGHVSLOW + \n    FRP_2 + hard_drugs_grp * ADH_HIGHVSLOW, data = data_wide_2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-700.74 -111.21   -1.67  112.23 1077.98 \n\nCoefficients:\n                                                      Estimate Std. Error\n(Intercept)                                              91.32      28.73\nhard_drugs_grpNever User                                 99.23      30.14\nhard_drugs_grpCurrent User                              -11.93      37.60\nADH_HIGHVSLOWLow Adherence                              214.03      79.41\nFRP_2Yes                                               -113.63      35.38\nhard_drugs_grpNever User:ADH_HIGHVSLOWLow Adherence    -297.13      85.95\nhard_drugs_grpCurrent User:ADH_HIGHVSLOWLow Adherence  -330.07     122.95\n                                                      t value Pr(&gt;|t|)    \n(Intercept)                                             3.179 0.001568 ** \nhard_drugs_grpNever User                                3.292 0.001064 ** \nhard_drugs_grpCurrent User                             -0.317 0.751150    \nADH_HIGHVSLOWLow Adherence                              2.695 0.007266 ** \nFRP_2Yes                                               -3.212 0.001402 ** \nhard_drugs_grpNever User:ADH_HIGHVSLOWLow Adherence    -3.457 0.000592 ***\nhard_drugs_grpCurrent User:ADH_HIGHVSLOWLow Adherence  -2.685 0.007497 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 181.3 on 512 degrees of freedom\n  (31 observations deleted due to missingness)\nMultiple R-squared:  0.08984,   Adjusted R-squared:  0.07917 \nF-statistic: 8.423 on 6 and 512 DF,  p-value: 0.000000009913\n\n\n\n# Print useful model parameters\nmodel_results(model_LEU3N_final3)\n\n\n\n\n\nEstimate\nStd. Error\nt value\nPr(&gt;|t|)\np_adjusted\n2.5 %\n97.5 %\n\n\n\n\n(Intercept)\n91.32428\n28.72807\n3.1789217\n0.0015677\n0.0109736\n34.88488\n147.76367\n\n\nhard_drugs_grpNever User\n99.23027\n30.14494\n3.2917718\n0.0010643\n0.0074500\n40.00727\n158.45327\n\n\nhard_drugs_grpCurrent User\n-11.92926\n37.59689\n-0.3172937\n0.7511501\n1.0000000\n-85.79241\n61.93390\n\n\nADH_HIGHVSLOWLow Adherence\n214.03489\n79.41319\n2.6952058\n0.0072658\n0.0508607\n58.01909\n370.05069\n\n\nFRP_2Yes\n-113.62595\n35.37733\n-3.2118295\n0.0014019\n0.0098130\n-183.12853\n-44.12336\n\n\nhard_drugs_grpNever User:ADH_HIGHVSLOWLow Adherence\n-297.12895\n85.95111\n-3.4569531\n0.0005918\n0.0041424\n-465.98920\n-128.26871\n\n\nhard_drugs_grpCurrent User:ADH_HIGHVSLOWLow Adherence\n-330.06790\n122.94881\n-2.6845962\n0.0074971\n0.0524799\n-571.61413\n-88.52168\n\n\n\n\n\n\n\nFor those with high adherence, previous vs current hard drug users did not differ in CD4+ T Cell Count (p-adjusted= 1.00).\nFinally let’s compare the impact of adherence for hard drug users.\n\n# Relevel to change to the reference group to never user\ndata_wide_2$hard_drugs_grp &lt;- relevel(data_wide_2$hard_drugs_grp, ref = \"Current User\")\n\n# Relevel to change to the reference group to never user\ndata_wide_2$ADH_HIGHVSLOW &lt;- relevel(data_wide_2$ADH_HIGHVSLOW, ref = \"Low Adherence\")\n\n# Perform regression on CD4+ T Cell count change by hard drugs group with covariates as determined by backward elimination\nmodel_LEU3N_final4 &lt;- lm(LEU3N_CHANGE ~ hard_drugs_grp + ADH_HIGHVSLOW + FRP_2 + hard_drugs_grp*ADH_HIGHVSLOW, data = data_wide_2)\n\n# Examine the summary\nsummary(model_LEU3N_final4)\n\n\nCall:\nlm(formula = LEU3N_CHANGE ~ hard_drugs_grp + ADH_HIGHVSLOW + \n    FRP_2 + hard_drugs_grp * ADH_HIGHVSLOW, data = data_wide_2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-700.74 -111.21   -1.67  112.23 1077.98 \n\nCoefficients:\n                                                        Estimate Std. Error\n(Intercept)                                               -36.64      90.67\nhard_drugs_grpPrevious User                               342.00     117.06\nhard_drugs_grpNever User                                  144.10      96.02\nADH_HIGHVSLOWHigh Adherence                               116.03      93.93\nFRP_2Yes                                                 -113.63      35.38\nhard_drugs_grpPrevious User:ADH_HIGHVSLOWHigh Adherence  -330.07     122.95\nhard_drugs_grpNever User:ADH_HIGHVSLOWHigh Adherence      -32.94      99.51\n                                                        t value Pr(&gt;|t|)   \n(Intercept)                                              -0.404  0.68633   \nhard_drugs_grpPrevious User                               2.922  0.00364 **\nhard_drugs_grpNever User                                  1.501  0.13404   \nADH_HIGHVSLOWHigh Adherence                               1.235  0.21729   \nFRP_2Yes                                                 -3.212  0.00140 **\nhard_drugs_grpPrevious User:ADH_HIGHVSLOWHigh Adherence  -2.685  0.00750 **\nhard_drugs_grpNever User:ADH_HIGHVSLOWHigh Adherence     -0.331  0.74076   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 181.3 on 512 degrees of freedom\n  (31 observations deleted due to missingness)\nMultiple R-squared:  0.08984,   Adjusted R-squared:  0.07917 \nF-statistic: 8.423 on 6 and 512 DF,  p-value: 0.000000009913\n\n\n\n# Print useful model parameters\nmodel_results(model_LEU3N_final4)\n\n\n\n\n\nEstimate\nStd. Error\nt value\nPr(&gt;|t|)\np_adjusted\n2.5 %\n97.5 %\n\n\n\n\n(Intercept)\n-36.63799\n90.67376\n-0.4040639\n0.6863344\n1.0000000\n-214.77639\n141.50040\n\n\nhard_drugs_grpPrevious User\n341.99716\n117.05931\n2.9215715\n0.0036365\n0.0254554\n112.02148\n571.97284\n\n\nhard_drugs_grpNever User\n144.09848\n96.01798\n1.5007447\n0.1340380\n0.9382660\n-44.53922\n332.73618\n\n\nADH_HIGHVSLOWHigh Adherence\n116.03301\n93.93276\n1.2352773\n0.2172938\n1.0000000\n-68.50805\n300.57408\n\n\nFRP_2Yes\n-113.62595\n35.37733\n-3.2118295\n0.0014019\n0.0098130\n-183.12853\n-44.12336\n\n\nhard_drugs_grpPrevious User:ADH_HIGHVSLOWHigh Adherence\n-330.06790\n122.94881\n-2.6845962\n0.0074971\n0.0524799\n-571.61413\n-88.52168\n\n\nhard_drugs_grpNever User:ADH_HIGHVSLOWHigh Adherence\n-32.93895\n99.50721\n-0.3310208\n0.7407642\n1.0000000\n-228.43162\n162.55372\n\n\n\n\n\n\n\nFor current hard drug users, those with high adherence did not differ significantly compared to those with low adherence (p = 0.22)\nFRP_2 was a significant predictor for LEU3N_CHANGE, while controlling for hard drug use and adherence to the treatment regiment (t = -3.21, p = 0.0014). On average, those with a Frailty Related Phenotype had a decrease in CD4+ T cells that was 113.63 cells greater than those without a Frailty Related Phenotype (95% CI: 44.12 to 183.13 cells).\nTop of Tabset\n\n\n\nOverall Model\nThere were significant differences in change in CD4+ T Cell count over 2 years based on hard drug usage and adherence to the treatment regiment, while controlling for Frailty Related Phenotype (F(6,512) = 8.42, p &lt; 0.00001).\n\nHard Drug Use\nThe relationship between hard drug use and change in CD4+ T Cell count over 2 years depended on adherence to the treatment regiment.\n\nHard Drug Use at Low Adherence\nFor those with low adherence to the treatment regiment, previous hard drug users had on average a 197.90 cells greater increase in CD4+ T Cell count compared to never hard drug users (t = 2.46, p-adjusted = 0.043, 95%: 39.77 to 356.03). Current hard drug users did not differ from never hard drug users (t = -1.50, p-adjusted = 0.40, 95% CI: -332.74 to 44.54). Additionally, current hard drug users had on average a -342.00 cells greater decrease in CD4+ T Cell count compared to previous hard drug users (t = -2.922, p-adjusted = 0.011, 95% CI: -571.97 to -112.02).\n\n\nHard Drug Use at High Adherence\nFor those with high adherence to the treatment regiment, previous hard drug users had on average a 99.23 cells greater decrease (t = -3.29, p-adjusted = 0.0032, 95% CI: -158.4533 to -40.00727), and current hard drug users had on average a 111.16 cells greater decrease (t = -4.27, p-adjusted &lt; 0.0001, 95% CI: -162.31 to -60.00) compared to never hard drug users. in CD4+ T cell count compared never hard drug users. Current hard drug users did not differ from previous hard drug users on change in CD4+ T Cell count over 2 years (t = -0.32, p-adjusted= 1.00, 95% CI: -85.79 to 61.93).\n\n\n\n\nAdherence\nThe relationship between adherence to the treatment regiment and change in CD4+ T Cell count over 2 years differed by hard drug use.\n\nAdherence for Current Hard Drug Users\nFor current hard drug users, those with high adherence did not differ significantly in average change in CD4+ T Cells over two years compared to those with low adherence (t = 1.24, p-adjusted = 0.22, 95% CI: -68.51 to 300.57).\n\n\n\nAdherence for Previous Hard Drug Users\nFor previous hard drug users, those with high adherence had on average a 214.03 cells greater decrease in CD4+ T Cell count compared to those with low adherence (t = -2.70, p = 0.0073, 95% CI: -370.05 to -58.020).\n\n\nAdherence for Never Hard Drug Users\nFor never drug users, those with high adherence had on average a 83.09 cells greater decrease in CD4+ T Cell count compared to those with low adherence (t = 2.52, p = 0.0119, 95% CI: 18.42283 147.76529).\n\n\nAdditional Interaction Comparisons\nThere are additional comparisons we can make in the interaction between hard drug usage and adherence, but they are of little clinical relevance (e.g. comparing current hard drug users with low adherence to never hard drug users with high adherence). ### Frailty Related Phenotype\nFrailty Related Phenotype was a significant predictor for change in CD4+ T Cell count over 2 years, while controlling for hard drug use and adherence to the treatment regiment (t = -3.21, p = 0.0014). On average, those with a Frailty Related Phenotype had a 113.63 cells greater decrease in CD4+ T Cell count than those without a Frailty Related Phenotype (95% CI: 44.12 to 183.13 cells).\nTop of Tabset\n\n\n\nThe below plot is useful in interpreting the interaction\n\n# Use interactions package to plot interaction\ncat_plot(\n  model_LEU3N_final1, \n  pred = hard_drugs_grp, \n  modx = ADH_HIGHVSLOW, \n  geom = \"line\", \n  colors = \"Pastel2\",\n  x.label = \"Hard Drugs Group\",\n  y.label = \"Predicted CD4+ T Cell Count Change\")\n\n\n\n\n\n\n\n# Filter out NA values\ndata_no_na &lt;- data_wide_2 %&gt;% filter(!is.na(ADH_HIGHVSLOW))\n\n# Create the plot with lines\nggplot(data_no_na, aes(x = hard_drugs_grp, y = LEU3N_CHANGE, fill = ADH_HIGHVSLOW)) +\n  geom_boxplot(alpha = 0.5) +\n  scale_fill_brewer(palette = \"Pastel2\") +\n  labs(title = \"Interaction between Hard Drugs and Adherence on LEU3N Change\",\n       x = \"Hard Drugs Group\",\n       y = \"LEU3N Change\") +\n  guides(fill = guide_legend(title = \"Adherence Group\")) +\n   theme_minimal() \n\nWarning: Removed 18 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\n\n\n\n\nHere we can see the key relationships in our model.\nFor current hard drug users, those with high adherence did not differ from those with low adherence. This may be an artifact of the small number of current hard drug users with low adherence, however (n = 6).\nFor previous hard drug users, those with high adherence had a smaller increase in CD4+ T Cell count compared to those with low adherence. This is a counter-inuitive finding and may be an artifact of the small number of previous hard drug users with low adherence, however (n=4). It may be of interest to the clinicians however, and we therefore report it here.\nFor never hard drug users, those with high adherence had a greater increase in CD4+ T Cell count compared to those with low adherence.\nAt low adherence, previous hard drug users had greater increase in CD4+ T Cell count compared to never hard drug users, and also when compared to current hard drug users. Current hard drugs users did not differ crom never hard drug users in change in CD4+ T Cell count at low adherence.\nAt high adherence, never drug users had a greater increase in CD4+ T cell count compared to previous and current hard drug users. Current hard drug users did not differ from previous hard drug users at high adherence.\nTop of Tabset\n\n\n\n\n\n\nAnalysisInterpretationVisualizing Main Effects\n\n\n\n# Relevel to change to the reference group to never user\ndata_wide_2$hard_drugs_grp&lt;- relevel(data_wide_2$hard_drugs_grp, ref = \"Never User\")\n\n# Relevel to change to the reference group to never user\ndata_wide_2$ADH_HIGHVSLOW &lt;- relevel(data_wide_2$ADH_HIGHVSLOW, ref = \"Low Adherence\")\n\n# Perform regression on CD4+ T Cell count change by hard drugs group with covariates as determined by backward elimination\nmodel_LEU3N_final_noX1 &lt;- lm(LEU3N_CHANGE ~ hard_drugs_grp + ADH_HIGHVSLOW + FRP_2, data = data_wide_2)\n\n# Examine the summary\nsummary(model_LEU3N_final_noX1)\n\n\nCall:\nlm(formula = LEU3N_CHANGE ~ hard_drugs_grp + ADH_HIGHVSLOW + \n    FRP_2, data = data_wide_2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-697.88 -115.61   -3.14  114.98 1080.85 \n\nCoefficients:\n                            Estimate Std. Error t value    Pr(&gt;|t|)    \n(Intercept)                   140.88      28.36   4.967 0.000000927 ***\nhard_drugs_grpCurrent User   -112.84      25.38  -4.446 0.000010724 ***\nhard_drugs_grpPrevious User   -62.31      28.51  -2.185     0.02932 *  \nADH_HIGHVSLOWHigh Adherence    46.81      29.24   1.601     0.11000    \nFRP_2Yes                     -114.33      35.72  -3.201     0.00145 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 183.2 on 514 degrees of freedom\n  (31 observations deleted due to missingness)\nMultiple R-squared:  0.06752,   Adjusted R-squared:  0.06026 \nF-statistic: 9.305 on 4 and 514 DF,  p-value: 0.000000289\n\n\n\n# Examine the summary\nmodel_results(model_LEU3N_final_noX1)\n\n\n\n\n\nEstimate\nStd. Error\nt value\nPr(&gt;|t|)\np_adjusted\n2.5 %\n97.5 %\n\n\n\n\n(Intercept)\n140.8774\n28.36272\n4.966993\n0.0000009\n0.0000046\n85.15631\n196.598539\n\n\nhard_drugs_grpCurrent User\n-112.8387\n25.37954\n-4.446049\n0.0000107\n0.0000536\n-162.69908\n-62.978288\n\n\nhard_drugs_grpPrevious User\n-62.3112\n28.51439\n-2.185255\n0.0293209\n0.1466044\n-118.33027\n-6.292117\n\n\nADH_HIGHVSLOWHigh Adherence\n46.8124\n29.24046\n1.600946\n0.1100033\n0.5500164\n-10.63312\n104.257925\n\n\nFRP_2Yes\n-114.3343\n35.72007\n-3.200841\n0.0014550\n0.0072749\n-184.50955\n-44.158969\n\n\n\n\n\n\n\nChange the reference level to previous users and re-run the model\n\n# Relevel to change to the reference group to never user\ndata_wide_2$hard_drugs_grp&lt;- relevel(data_wide_2$hard_drugs_grp, ref = \"Previous User\")\n\n# Relevel to change to the reference group to never user\ndata_wide_2$ADH_HIGHVSLOW &lt;- relevel(data_wide_2$ADH_HIGHVSLOW, ref = \"Low Adherence\")\n\n# Perform regression on CD4+ T Cell count change by hard drugs group with covariates as determined by backward elimination\nmodel_LEU3N_final_noX2 &lt;- lm(LEU3N_CHANGE ~ hard_drugs_grp + ADH_HIGHVSLOW + FRP_2, data = data_wide_2)\n\n# Examine the summary\nsummary(model_LEU3N_final_noX2)\n\n\nCall:\nlm(formula = LEU3N_CHANGE ~ hard_drugs_grp + ADH_HIGHVSLOW + \n    FRP_2, data = data_wide_2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-697.88 -115.61   -3.14  114.98 1080.85 \n\nCoefficients:\n                            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)                    78.57      37.09   2.118  0.03462 * \nhard_drugs_grpNever User       62.31      28.51   2.185  0.02932 * \nhard_drugs_grpCurrent User    -50.53      36.00  -1.403  0.16110   \nADH_HIGHVSLOWHigh Adherence    46.81      29.24   1.601  0.11000   \nFRP_2Yes                     -114.33      35.72  -3.201  0.00145 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 183.2 on 514 degrees of freedom\n  (31 observations deleted due to missingness)\nMultiple R-squared:  0.06752,   Adjusted R-squared:  0.06026 \nF-statistic: 9.305 on 4 and 514 DF,  p-value: 0.000000289\n\n\n\n# Examine the summary\nmodel_results(model_LEU3N_final_noX2)\n\n\n\n\n\nEstimate\nStd. Error\nt value\nPr(&gt;|t|)\np_adjusted\n2.5 %\n97.5 %\n\n\n\n\n(Intercept)\n78.56623\n37.08694\n2.118434\n0.0346175\n0.1730874\n5.705608\n151.42685\n\n\nhard_drugs_grpNever User\n62.31120\n28.51439\n2.185255\n0.0293209\n0.1466044\n6.292117\n118.33027\n\n\nhard_drugs_grpCurrent User\n-50.52749\n36.00333\n-1.403412\n0.1610979\n0.8054896\n-121.259266\n20.20429\n\n\nADH_HIGHVSLOWHigh Adherence\n46.81240\n29.24046\n1.600946\n0.1100033\n0.5500164\n-10.633119\n104.25793\n\n\nFRP_2Yes\n-114.33426\n35.72007\n-3.200841\n0.0014550\n0.0072749\n-184.509547\n-44.15897\n\n\n\n\n\n\n\n\n\n\nOverall Model\nThe overall model is significant (F~(4, 514)~ = 9.31, p &lt; .0001).\n\n\nHard Drug Use\nHard drug use is a significant predictor of change in CD4+ T Cell count over 2 years.\nCurrent hard drug users had an average change in CD4+ T Cell count that was 112.84 cells lower than never drug users, while controlling for adherence to treatment regiment and Frailty Related Phenotype (p-adjusted &lt; 0.0001, 95% CI: -162.70 to -62.98).\nPrevious hard drug users did not differ from never hard drugs users (p-adjusted = 0.088, 95% CI:-118.33 to -6.29), or current hard drug users (p-adjusted = 0.483, 95% CI: -121.26 to 20.20).\n\n\nAdherence\nAdherence was not a significant predictor for change in CD4+ T Cell count over 2 years, after controlling for hard drug use and Frailty Related Phenotype (t = 1.60, p = 0.110).\n\n\nFrailty Related Phenotype\nFrailty related phenotype is a significant predictor for change in CD4+ T Cell count over 2 years, while controlling for hard drug use and adherence to the treatment regiment (t = -3.20, p = 0.00145). On average, those with a frailty related phenotype had a change in CD4+ T Cell count that was 114.33 cells lower than those without a frailty related phenotype (95% CI: -184.51 to -44.16).\nTop of Tabset\n\n\n\n\nHard Drug Use\nHere we will create some simple plots to help with visualizing the main effects of hard drug use and adherence on change in CD4+ T Cell count.\n\n# Relevel to change the reference cateory to Never User\ndata_wide_2$hard_drugs_grp &lt;- relevel(data_wide_2$hard_drugs_grp, ref = \"Never User\")\n\n# Create boxplots of CD4+ T Cell count change by hard drug use group\nggplot(data_wide_2, aes(x = hard_drugs_grp, y = LEU3N_CHANGE, fill = hard_drugs_grp)) +\n  geom_boxplot(alpha = 0.5) +\n  theme_minimal() +\n  scale_fill_brewer(palette=\"Pastel2\") +\n  labs(title = \"Boxplot of CD4+ T Cell Count Change by Hard Drug Use Group\",\n       x = \"Hard Drug Use Group\",\n       y = \"CD4+ T Cell Count Change\") +\n  guides(fill = guide_legend(title = \"Hard Drug Use Group\"))\n\nWarning: Removed 18 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\n\n\n\n\nCurrent hard drug users had less of an increase in CD4+ T Cells over 2 years compared to never hard drug users. All other between-group comparisons were not significant.\n\n\nAdherence\n\n# Create boxplots of CD4+ T Cell count change by adherence group\nggplot(data_no_na, aes(x = ADH_HIGHVSLOW, y = LEU3N_CHANGE, fill = ADH_HIGHVSLOW)) +\n  geom_boxplot(alpha = 0.5) +\n  theme_minimal() +\n  scale_fill_brewer(palette = \"Pastel2\") +\n  labs(title = \"Boxplot of CD4+ T Cell Count Change by Adherence at Year 2\",\n       x = \"Adherence Level\",\n       y = \"CD4+ T Cell Count Change\") +\n  guides(fill = guide_legend(title = \"Adherence Group\"))\n\nWarning: Removed 18 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\n\n\n\n\nChange in CD4+ T Cell count did not differ based on adherence.\n\n\nFrailty Related Phenotype\n\n# Filter out NA values\ndata_no_na &lt;- data_wide_2 %&gt;% filter(!is.na(FRP_2))\n\n# Create boxplots of CD4+ T Cell count change by frailty related phenotype at year 2\nggplot(data_no_na, aes(x = FRP_2, y = LEU3N_CHANGE, fill = FRP_2)) +\n  geom_boxplot(alpha = 0.5) +\n  theme_minimal() +\n  scale_fill_brewer(palette = \"Pastel2\") +\n  labs(title = \"Boxplot of CD4+ T Cell Count Change by Frailty Related Phenotype\",\n       x = \"Frailty Related Phenotype\",\n       y = \"CD4+ T Cell Count Change\") +\n  guides(fill = guide_legend(title = \"Frailty Related Phenotype\"))\n\nWarning: Removed 18 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\n\n\n\n\nThose with a Frailty Related Phenotype had less of an increase in CD4+ T Cell count compared to those without."
  },
  {
    "objectID": "Project_2/Project_2_R/Code/Project2.html#mental-qol-change",
    "href": "Project_2/Project_2_R/Code/Project2.html#mental-qol-change",
    "title": "Advanced Data Analysis - Project 2",
    "section": "Mental QOL Change",
    "text": "Mental QOL Change\nThe candidate variables for mental QOL change as determined by interactive variable selection are hard_drug_grp, ADH_HIGHLOW, CESD_2.\n\nModel ExplorationModel 1 - Main Research QuestionModel 2 - Depression Interaction TermConclusionBonus - Predicting Depression\n\n\n\nAnalysesDepression as a ConfounderIncluding Depression Interaction Term\n\n\nLet’s begin by running the full model as determined by interactive model selection.\n\n# Relevel to change to the reference group to never user\ndata_wide_2$hard_drugs_grp&lt;- relevel(data_wide_2$hard_drugs_grp, ref = \"Never User\")\n\n# Relevel to change to the reference group to never user\ndata_wide_2$ADH_HIGHVSLOW &lt;- relevel(data_wide_2$ADH_HIGHVSLOW, ref = \"Low Adherence\")\n\n# Perform regression on mental QOL change by hard drugs group, full model\nmodel_MENT_full1 &lt;- lm(AGG_MENT_CHANGE ~ hard_drugs_grp + ADH_HIGHVSLOW +  CESD_2 + hard_drugs_grp*ADH_HIGHVSLOW, data = data_wide_2)\n\n# Examine the summary\nsummary(model_MENT_full1)\n\n\nCall:\nlm(formula = AGG_MENT_CHANGE ~ hard_drugs_grp + ADH_HIGHVSLOW + \n    CESD_2 + hard_drugs_grp * ADH_HIGHVSLOW, data = data_wide_2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-32.537  -6.126  -1.540   5.582  38.662 \n\nCoefficients:\n                                                         Estimate Std. Error\n(Intercept)                                               3.42934    2.16906\nhard_drugs_grpPrevious User                              -6.80279    5.39588\nhard_drugs_grpCurrent User                                9.20460    6.18696\nADH_HIGHVSLOWHigh Adherence                               2.68386    2.14723\nCESD_2                                                   -0.30407    0.04914\nhard_drugs_grpPrevious User:ADH_HIGHVSLOWHigh Adherence  10.70651    5.68836\nhard_drugs_grpCurrent User:ADH_HIGHVSLOWHigh Adherence  -10.79007    6.40420\n                                                        t value      Pr(&gt;|t|)\n(Intercept)                                               1.581        0.1145\nhard_drugs_grpPrevious User                              -1.261        0.2080\nhard_drugs_grpCurrent User                                1.488        0.1374\nADH_HIGHVSLOWHigh Adherence                               1.250        0.2119\nCESD_2                                                   -6.188 0.00000000123\nhard_drugs_grpPrevious User:ADH_HIGHVSLOWHigh Adherence   1.882        0.0604\nhard_drugs_grpCurrent User:ADH_HIGHVSLOWHigh Adherence   -1.685        0.0926\n                                                           \n(Intercept)                                                \nhard_drugs_grpPrevious User                                \nhard_drugs_grpCurrent User                                 \nADH_HIGHVSLOWHigh Adherence                                \nCESD_2                                                  ***\nhard_drugs_grpPrevious User:ADH_HIGHVSLOWHigh Adherence .  \nhard_drugs_grpCurrent User:ADH_HIGHVSLOWHigh Adherence  .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 11.66 on 521 degrees of freedom\n  (22 observations deleted due to missingness)\nMultiple R-squared:  0.1089,    Adjusted R-squared:  0.09867 \nF-statistic: 10.62 on 6 and 521 DF,  p-value: 0.00000000003878\n\n\nCompare hard drug use groups at high adherence\n\n# Relevel to change to the reference group to never user\ndata_wide_2$hard_drugs_grp&lt;- relevel(data_wide_2$hard_drugs_grp, ref = \"Never User\")\n\n# Relevel to change to the reference group to never user\ndata_wide_2$ADH_HIGHVSLOW &lt;- relevel(data_wide_2$ADH_HIGHVSLOW, ref = \"High Adherence\")\n\n# Perform regression on mental QOL change by hard drugs group, full model\nmodel_MENT_full2 &lt;- lm(AGG_MENT_CHANGE ~ hard_drugs_grp + ADH_HIGHVSLOW +  CESD_2 + hard_drugs_grp*ADH_HIGHVSLOW, data = data_wide_2)\n\n# Examine the summary\nsummary(model_MENT_full2)\n\n\nCall:\nlm(formula = AGG_MENT_CHANGE ~ hard_drugs_grp + ADH_HIGHVSLOW + \n    CESD_2 + hard_drugs_grp * ADH_HIGHVSLOW, data = data_wide_2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-32.537  -6.126  -1.540   5.582  38.662 \n\nCoefficients:\n                                                        Estimate Std. Error\n(Intercept)                                              6.11320    0.81275\nhard_drugs_grpPrevious User                              3.90372    1.94422\nhard_drugs_grpCurrent User                              -1.58547    1.68651\nADH_HIGHVSLOWLow Adherence                              -2.68386    2.14723\nCESD_2                                                  -0.30407    0.04914\nhard_drugs_grpPrevious User:ADH_HIGHVSLOWLow Adherence -10.70651    5.68836\nhard_drugs_grpCurrent User:ADH_HIGHVSLOWLow Adherence   10.79007    6.40420\n                                                       t value\n(Intercept)                                              7.522\nhard_drugs_grpPrevious User                              2.008\nhard_drugs_grpCurrent User                              -0.940\nADH_HIGHVSLOWLow Adherence                              -1.250\nCESD_2                                                  -6.188\nhard_drugs_grpPrevious User:ADH_HIGHVSLOWLow Adherence  -1.882\nhard_drugs_grpCurrent User:ADH_HIGHVSLOWLow Adherence    1.685\n                                                                Pr(&gt;|t|)    \n(Intercept)                                            0.000000000000239 ***\nhard_drugs_grpPrevious User                                       0.0452 *  \nhard_drugs_grpCurrent User                                        0.3476    \nADH_HIGHVSLOWLow Adherence                                        0.2119    \nCESD_2                                                 0.000000001234665 ***\nhard_drugs_grpPrevious User:ADH_HIGHVSLOWLow Adherence            0.0604 .  \nhard_drugs_grpCurrent User:ADH_HIGHVSLOWLow Adherence             0.0926 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 11.66 on 521 degrees of freedom\n  (22 observations deleted due to missingness)\nMultiple R-squared:  0.1089,    Adjusted R-squared:  0.09867 \nF-statistic: 10.62 on 6 and 521 DF,  p-value: 0.00000000003878\n\n\nCompare hard drug use groups at low adherence\n\n# Relevel to change to the reference group to never user\ndata_wide_2$hard_drugs_grp&lt;- relevel(data_wide_2$hard_drugs_grp, ref = \"Previous User\")\n\n# Relevel to change to the reference group to never user\ndata_wide_2$ADH_HIGHVSLOW &lt;- relevel(data_wide_2$ADH_HIGHVSLOW, ref = \"Low Adherence\")\n\n# Perform regression on mental QOL change by hard drugs group, full model\nmodel_MENT_full3 &lt;- lm(AGG_MENT_CHANGE ~ hard_drugs_grp + ADH_HIGHVSLOW +  CESD_2 + hard_drugs_grp*ADH_HIGHVSLOW, data = data_wide_2)\n\n# Examine the summary\nsummary(model_MENT_full3)\n\n\nCall:\nlm(formula = AGG_MENT_CHANGE ~ hard_drugs_grp + ADH_HIGHVSLOW + \n    CESD_2 + hard_drugs_grp * ADH_HIGHVSLOW, data = data_wide_2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-32.537  -6.126  -1.540   5.582  38.662 \n\nCoefficients:\n                                                        Estimate Std. Error\n(Intercept)                                             -3.37346    5.22804\nhard_drugs_grpNever User                                 6.80279    5.39588\nhard_drugs_grpCurrent User                              16.00739    7.63391\nADH_HIGHVSLOWHigh Adherence                             13.39037    5.29906\nCESD_2                                                  -0.30407    0.04914\nhard_drugs_grpNever User:ADH_HIGHVSLOWHigh Adherence   -10.70651    5.68836\nhard_drugs_grpCurrent User:ADH_HIGHVSLOWHigh Adherence -21.49658    8.01958\n                                                       t value      Pr(&gt;|t|)\n(Intercept)                                             -0.645       0.51904\nhard_drugs_grpNever User                                 1.261       0.20797\nhard_drugs_grpCurrent User                               2.097       0.03649\nADH_HIGHVSLOWHigh Adherence                              2.527       0.01180\nCESD_2                                                  -6.188 0.00000000123\nhard_drugs_grpNever User:ADH_HIGHVSLOWHigh Adherence    -1.882       0.06037\nhard_drugs_grpCurrent User:ADH_HIGHVSLOWHigh Adherence  -2.681       0.00758\n                                                          \n(Intercept)                                               \nhard_drugs_grpNever User                                  \nhard_drugs_grpCurrent User                             *  \nADH_HIGHVSLOWHigh Adherence                            *  \nCESD_2                                                 ***\nhard_drugs_grpNever User:ADH_HIGHVSLOWHigh Adherence   .  \nhard_drugs_grpCurrent User:ADH_HIGHVSLOWHigh Adherence ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 11.66 on 521 degrees of freedom\n  (22 observations deleted due to missingness)\nMultiple R-squared:  0.1089,    Adjusted R-squared:  0.09867 \nF-statistic: 10.62 on 6 and 521 DF,  p-value: 0.00000000003878\n\n\n\n# Relevel to change to the reference group to never user\ndata_wide_2$hard_drugs_grp&lt;- relevel(data_wide_2$hard_drugs_grp, ref = \"Previous User\")\n\n# Relevel to change to the reference group to never user\ndata_wide_2$ADH_HIGHVSLOW &lt;- relevel(data_wide_2$ADH_HIGHVSLOW, ref = \"High Adherence\")\n\n# Perform regression on mental QOL change by hard drugs group, full model\nmodel_MENT_full4 &lt;- lm(AGG_MENT_CHANGE ~ hard_drugs_grp + ADH_HIGHVSLOW +  CESD_2 + hard_drugs_grp*ADH_HIGHVSLOW, data = data_wide_2)\n\n# Examine the summary\nsummary(model_MENT_full4)\n\n\nCall:\nlm(formula = AGG_MENT_CHANGE ~ hard_drugs_grp + ADH_HIGHVSLOW + \n    CESD_2 + hard_drugs_grp * ADH_HIGHVSLOW, data = data_wide_2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-32.537  -6.126  -1.540   5.582  38.662 \n\nCoefficients:\n                                                       Estimate Std. Error\n(Intercept)                                            10.01691    1.98635\nhard_drugs_grpNever User                               -3.90372    1.94422\nhard_drugs_grpCurrent User                             -5.48919    2.41509\nADH_HIGHVSLOWLow Adherence                            -13.39037    5.29906\nCESD_2                                                 -0.30407    0.04914\nhard_drugs_grpNever User:ADH_HIGHVSLOWLow Adherence    10.70651    5.68836\nhard_drugs_grpCurrent User:ADH_HIGHVSLOWLow Adherence  21.49658    8.01958\n                                                      t value      Pr(&gt;|t|)    \n(Intercept)                                             5.043 0.00000063408 ***\nhard_drugs_grpNever User                               -2.008       0.04517 *  \nhard_drugs_grpCurrent User                             -2.273       0.02344 *  \nADH_HIGHVSLOWLow Adherence                             -2.527       0.01180 *  \nCESD_2                                                 -6.188 0.00000000123 ***\nhard_drugs_grpNever User:ADH_HIGHVSLOWLow Adherence     1.882       0.06037 .  \nhard_drugs_grpCurrent User:ADH_HIGHVSLOWLow Adherence   2.681       0.00758 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 11.66 on 521 degrees of freedom\n  (22 observations deleted due to missingness)\nMultiple R-squared:  0.1089,    Adjusted R-squared:  0.09867 \nF-statistic: 10.62 on 6 and 521 DF,  p-value: 0.00000000003878\n\n\n\n# Relevel to change to the reference group to never user\ndata_wide_2$hard_drugs_grp&lt;- relevel(data_wide_2$hard_drugs_grp, ref = \"Current User\")\n\n# Relevel to change to the reference group to never user\ndata_wide_2$ADH_HIGHVSLOW &lt;- relevel(data_wide_2$ADH_HIGHVSLOW, ref = \"High Adherence\")\n\n# Perform regression on mental QOL change by hard drugs group, full model\nmodel_MENT_full5 &lt;- lm(AGG_MENT_CHANGE ~ hard_drugs_grp + ADH_HIGHVSLOW +  CESD_2 + hard_drugs_grp*ADH_HIGHVSLOW, data = data_wide_2)\n\n# Examine the summary\nsummary(model_MENT_full5)\n\n\nCall:\nlm(formula = AGG_MENT_CHANGE ~ hard_drugs_grp + ADH_HIGHVSLOW + \n    CESD_2 + hard_drugs_grp * ADH_HIGHVSLOW, data = data_wide_2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-32.537  -6.126  -1.540   5.582  38.662 \n\nCoefficients:\n                                                        Estimate Std. Error\n(Intercept)                                              4.52773    1.76048\nhard_drugs_grpPrevious User                              5.48919    2.41509\nhard_drugs_grpNever User                                 1.58547    1.68651\nADH_HIGHVSLOWLow Adherence                               8.10621    6.03476\nCESD_2                                                  -0.30407    0.04914\nhard_drugs_grpPrevious User:ADH_HIGHVSLOWLow Adherence -21.49658    8.01958\nhard_drugs_grpNever User:ADH_HIGHVSLOWLow Adherence    -10.79007    6.40420\n                                                       t value      Pr(&gt;|t|)\n(Intercept)                                              2.572       0.01039\nhard_drugs_grpPrevious User                              2.273       0.02344\nhard_drugs_grpNever User                                 0.940       0.34761\nADH_HIGHVSLOWLow Adherence                               1.343       0.17978\nCESD_2                                                  -6.188 0.00000000123\nhard_drugs_grpPrevious User:ADH_HIGHVSLOWLow Adherence  -2.681       0.00758\nhard_drugs_grpNever User:ADH_HIGHVSLOWLow Adherence     -1.685       0.09262\n                                                          \n(Intercept)                                            *  \nhard_drugs_grpPrevious User                            *  \nhard_drugs_grpNever User                                  \nADH_HIGHVSLOWLow Adherence                                \nCESD_2                                                 ***\nhard_drugs_grpPrevious User:ADH_HIGHVSLOWLow Adherence ** \nhard_drugs_grpNever User:ADH_HIGHVSLOWLow Adherence    .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 11.66 on 521 degrees of freedom\n  (22 observations deleted due to missingness)\nMultiple R-squared:  0.1089,    Adjusted R-squared:  0.09867 \nF-statistic: 10.62 on 6 and 521 DF,  p-value: 0.00000000003878\n\n\n\n# Relevel to change to the reference group to never user\ndata_wide_2$hard_drugs_grp&lt;- relevel(data_wide_2$hard_drugs_grp, ref = \"Current User\")\n\n# Relevel to change to the reference group to never user\ndata_wide_2$ADH_HIGHVSLOW &lt;- relevel(data_wide_2$ADH_HIGHVSLOW, ref = \"Low Adherence\")\n\n# Perform regression on mental QOL change by hard drugs group, full model\nmodel_MENT_full6 &lt;- lm(AGG_MENT_CHANGE ~ hard_drugs_grp + ADH_HIGHVSLOW +  CESD_2 + hard_drugs_grp*ADH_HIGHVSLOW, data = data_wide_2)\n\n# Examine the summary\nsummary(model_MENT_full6)\n\n\nCall:\nlm(formula = AGG_MENT_CHANGE ~ hard_drugs_grp + ADH_HIGHVSLOW + \n    CESD_2 + hard_drugs_grp * ADH_HIGHVSLOW, data = data_wide_2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-32.537  -6.126  -1.540   5.582  38.662 \n\nCoefficients:\n                                                         Estimate Std. Error\n(Intercept)                                              12.63393    5.89652\nhard_drugs_grpPrevious User                             -16.00739    7.63391\nhard_drugs_grpNever User                                 -9.20460    6.18696\nADH_HIGHVSLOWHigh Adherence                              -8.10621    6.03476\nCESD_2                                                   -0.30407    0.04914\nhard_drugs_grpPrevious User:ADH_HIGHVSLOWHigh Adherence  21.49658    8.01958\nhard_drugs_grpNever User:ADH_HIGHVSLOWHigh Adherence     10.79007    6.40420\n                                                        t value      Pr(&gt;|t|)\n(Intercept)                                               2.143       0.03261\nhard_drugs_grpPrevious User                              -2.097       0.03649\nhard_drugs_grpNever User                                 -1.488       0.13742\nADH_HIGHVSLOWHigh Adherence                              -1.343       0.17978\nCESD_2                                                   -6.188 0.00000000123\nhard_drugs_grpPrevious User:ADH_HIGHVSLOWHigh Adherence   2.681       0.00758\nhard_drugs_grpNever User:ADH_HIGHVSLOWHigh Adherence      1.685       0.09262\n                                                           \n(Intercept)                                             *  \nhard_drugs_grpPrevious User                             *  \nhard_drugs_grpNever User                                   \nADH_HIGHVSLOWHigh Adherence                                \nCESD_2                                                  ***\nhard_drugs_grpPrevious User:ADH_HIGHVSLOWHigh Adherence ** \nhard_drugs_grpNever User:ADH_HIGHVSLOWHigh Adherence    .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 11.66 on 521 degrees of freedom\n  (22 observations deleted due to missingness)\nMultiple R-squared:  0.1089,    Adjusted R-squared:  0.09867 \nF-statistic: 10.62 on 6 and 521 DF,  p-value: 0.00000000003878\n\n\nInterestingly, we have lost a lot of the significant relationships we were seeing before with our main predictors of hard drug use and adherence.\nIt seems that the main driver of this model, and best predictor of change in mental QOL score is depression score.\nWithin adherence levels, none of the hard drug use groups differed from each other on QOL (p-adjusted &gt; 0.05).\nThe only significant difference in mental QOL change is for previous drug users. For previous drug users, those with low adherence have a mental QOL change that is on average 13.39 points lower than those with high adherence (p= 0.0118). Depending on how we handle multiple pairwise comparisons, this may not be significant!\nSomething fishy is going on here…\nLet’s examine this in a simplified model with no interactions.\n\n# Relevel to change to the reference group to never user\ndata_wide_2$hard_drugs_grp&lt;- relevel(data_wide_2$hard_drugs_grp, ref = \"Previous User\")\n\n# Relevel to change to the reference group to never user\ndata_wide_2$ADH_HIGHVSLOW &lt;- relevel(data_wide_2$ADH_HIGHVSLOW, ref = \"Low Adherence\")\n\n# Perform regression on mental QOL change by hard drugs group, full model\nmodel_MENT_simple &lt;- lm(AGG_MENT_CHANGE ~ hard_drugs_grp + ADH_HIGHVSLOW +  CESD_2, data = data_wide_2)\n\n# Examine the summary\nsummary(model_MENT_simple)\n\n\nCall:\nlm(formula = AGG_MENT_CHANGE ~ hard_drugs_grp + ADH_HIGHVSLOW + \n    CESD_2, data = data_wide_2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-32.274  -6.421  -1.574   5.343  39.297 \n\nCoefficients:\n                            Estimate Std. Error t value        Pr(&gt;|t|)    \n(Intercept)                  6.17713    2.64623   2.334           0.020 *  \nhard_drugs_grpCurrent User  -3.44081    2.30106  -1.495           0.135    \nhard_drugs_grpNever User    -2.70142    1.85281  -1.458           0.145    \nADH_HIGHVSLOWHigh Adherence  2.94279    1.90898   1.542           0.124    \nCESD_2                      -0.32881    0.04803  -6.845 0.0000000000214 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 11.72 on 523 degrees of freedom\n  (22 observations deleted due to missingness)\nMultiple R-squared:  0.09648,   Adjusted R-squared:  0.08957 \nF-statistic: 13.96 on 4 and 523 DF,  p-value: 0.00000000007883\n\n\nIndeed, we can see that, when including depression in the model, hard drug use and adherence are no longer significant predictors of change in mental QOL!\nThis suggests that depression score is either a mediator, a moderator, or a confounder for the relationship between hard drug use and adherence on mental QOL.\nLet’s explore.\nTop of Tabset\n\n\n\nConfounder is related to PEV\nWe saw earlier that hard drug use was a significant predictor for depression. Specifically:\nHard drug use is a significant predictor of depression score (F~(2, 542)~= 14.31). On average, current hard drug users had depression scores that were 5.23 points higher than never hard drug users (p &lt; 0.0001), and previous hard drug users had depression scores that were 7.29 points higher than those who never used hard drugs (p &lt; 0.0001).\nThus, depression meets criteria 1 of the classical definition for a confounder: It is related to the PEV.\n\n\nConfounder is related to the Outcome Measure\nWe also saw in the interactive variable selection that depression was significantly related change in mental QOL change.\nThus depression meets criteria 2 of the classical definition for a confounder: it is related to the main outcome.\n\n\nConfounder is not a Mediator\nIt is possible that depression is on the causal pathway for mental QOL (hard drug use -&gt; depression -&gt; mental QOL), but it is likewise possible that mental QOL is on the causal pathway for depression (hard drug use -&gt; mental QOL -&gt; depression). Thus we cannot conclude that depression is a mediator.\n\n\nConfounder Meaningfully Changes the Relationship of PEV on Outcome\nWe also saw that including depression in the model changed hard drug use from being significant to not significant. Thus depression score changes the relationship between the PEV and outcome measure in a very meaningful way.\nI therefore feel justified in exploring depression as a confounder, and analyzing whether an interaction term between hard drug use group and depression will add explanatory power to our model.\nTop of Tabset\n\n\n\nLet’s run a new full model, with the included interaction term between hard drug use group and depression. In essence we are running a three way interaction. It will need to be emphasized in the discussion that this was exploratory data analysis.\n\n# Relevel to change to the reference group\ndata_wide_2$hard_drugs_grp&lt;- relevel(data_wide_2$hard_drugs_grp, ref = \"Current User\")\n\n# Relevel to change to the reference group\ndata_wide_2$ADH_HIGHVSLOW &lt;- relevel(data_wide_2$ADH_HIGHVSLOW, ref = \"Low Adherence\")\n\n# Perform regression on mental QOL change by hard drugs group, full model\nmodel_MENT_newfull &lt;- lm(AGG_MENT_CHANGE ~ hard_drugs_grp + ADH_HIGHVSLOW +  CESD_2 + hard_drugs_grp*ADH_HIGHVSLOW + hard_drugs_grp*CESD_2, data = data_wide_2)\n\n# Examine the summary\nsummary(model_MENT_newfull)\n\n\nCall:\nlm(formula = AGG_MENT_CHANGE ~ hard_drugs_grp + ADH_HIGHVSLOW + \n    CESD_2 + hard_drugs_grp * ADH_HIGHVSLOW + hard_drugs_grp * \n    CESD_2, data = data_wide_2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-33.459  -6.016  -1.375   5.512  37.513 \n\nCoefficients:\n                                                        Estimate Std. Error\n(Intercept)                                              9.51665    6.16622\nhard_drugs_grpPrevious User                             30.31096   10.51019\nhard_drugs_grpNever User                                -7.17735    6.52016\nADH_HIGHVSLOWHigh Adherence                             -7.87736    5.82799\nCESD_2                                                  -0.13089    0.14003\nhard_drugs_grpPrevious User:ADH_HIGHVSLOWHigh Adherence -7.15661    8.97026\nhard_drugs_grpNever User:ADH_HIGHVSLOWHigh Adherence    10.75016    6.18586\nhard_drugs_grpPrevious User:CESD_2                      -1.15503    0.21476\nhard_drugs_grpNever User:CESD_2                         -0.09391    0.14973\n                                                        t value    Pr(&gt;|t|)    \n(Intercept)                                               1.543     0.12336    \nhard_drugs_grpPrevious User                               2.884     0.00409 ** \nhard_drugs_grpNever User                                 -1.101     0.27150    \nADH_HIGHVSLOWHigh Adherence                              -1.352     0.17708    \nCESD_2                                                   -0.935     0.35038    \nhard_drugs_grpPrevious User:ADH_HIGHVSLOWHigh Adherence  -0.798     0.42534    \nhard_drugs_grpNever User:ADH_HIGHVSLOWHigh Adherence      1.738     0.08283 .  \nhard_drugs_grpPrevious User:CESD_2                       -5.378 0.000000114 ***\nhard_drugs_grpNever User:CESD_2                          -0.627     0.53083    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 11.26 on 519 degrees of freedom\n  (22 observations deleted due to missingness)\nMultiple R-squared:  0.1729,    Adjusted R-squared:  0.1601 \nF-statistic: 13.56 on 8 and 519 DF,  p-value: &lt; 0.00000000000000022\n\n\nWe get a highly significant p-value for the interaction between hard drug use group and depression (p &lt; 0.0001)\nOur adjusted R-squared also shot up from 0.090 to 0.16. We’re on to something here.\nLet’s run backwards model selection and examine the BIC to see if we need to include the hard drug use * depression interaction score.\n\nBackwards Elimination\nWe are going to force inclusion of our main variables of interest, hard_drugs_grp and ADH_HIGHVSLOW, and the interaction between them, since those were the main questions the researchers posed.\n\n# Relevel to change to the reference group to never user\ndata_wide_2$hard_drugs_grp&lt;- relevel(data_wide_2$hard_drugs_grp, ref = \"Never User\")\n\n# Relevel to change to the reference group to never user\ndata_wide_2$ADH_HIGHVSLOW &lt;- relevel(data_wide_2$ADH_HIGHVSLOW, ref = \"Low Adherence\")\n\n# Perform regression on mental QOL change by hard drugs group, full model\nmodel_MENT_newfull &lt;- lm(AGG_MENT_CHANGE ~ hard_drugs_grp + ADH_HIGHVSLOW +  CESD_2 + hard_drugs_grp*ADH_HIGHVSLOW + hard_drugs_grp*CESD_2, data = data_wide_2)\n\n# Perform backward elimination regression selection based on BIC \nols_step_backward_sbc(model_MENT_newfull, include = c(\"hard_drugs_grp\", \"ADH_HIGHVSLOW\", \"hard_drugs_grp:ADH_HIGHVSLOW\"))\n\n[1] \"No variables have been removed from the model.\"\n\n\nNo variable was removed from the model, indicating that the interaction between hard drug use group and depression adds significant explanatory power and should be included in the model.\nThis puts us in a tricky position, it is clear that there is a relationship between hard drug use and mental quality of life, and that this differs based on both adherence and depression. However we don’t necessarily have the sample size to run a three-way interaction.\n\n\nNext Steps\nThus, we will proceed in two ways from here, and run separate models for:\n\n\nDoes the relationship between hard drug use and mental QOL differ based on adherence (irrespective of depression)?\n\n\nDoes the relationship between hard drug use and mental QOL differ based on depression (irrespective of adherence)?\n\n\nRemoving the confounder of depression in model 1 allows us to answer the main research question for this project. Model 2) will be presented as an exploratory data analysis and allows us to explore how depression is related to hard drug use, and how that is affecting our main model.\nTop of Tabset\n\n\n\n\n\n\nThis is the model predicting AGG_MENT_CHANGE, including only hard_drugs_grp and ADH_HIGHVSLOW, and their interaction term.\nThis addresses the main research question of how mental QOL is impacted by ART, based on hard drug use and adherence to the treatment regimen.\n\nAnalysisInterpretationVisualizing the Interaction\n\n\nLet’s run the regression as specified.\n\n# Relevel to change to the reference group to never user\ndata_wide_2$hard_drugs_grp&lt;- relevel(data_wide_2$hard_drugs_grp, ref = \"Never User\")\n\n# Relevel to change to the reference group to never user\ndata_wide_2$ADH_HIGHVSLOW &lt;- relevel(data_wide_2$ADH_HIGHVSLOW, ref = \"Low Adherence\")\n\n# Perform regression on mental QOL change by hard drugs group, full model\nmodel_MENT_main1 &lt;- lm(AGG_MENT_CHANGE ~ hard_drugs_grp + ADH_HIGHVSLOW+ hard_drugs_grp*ADH_HIGHVSLOW, data = data_wide_2)\n\n# Examine the summary\nsummary(model_MENT_main1)\n\n\nCall:\nlm(formula = AGG_MENT_CHANGE ~ hard_drugs_grp + ADH_HIGHVSLOW + \n    hard_drugs_grp * ADH_HIGHVSLOW, data = data_wide_2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-37.392  -5.850  -0.839   5.089  38.498 \n\nCoefficients:\n                                                        Estimate Std. Error\n(Intercept)                                              -0.8003     2.0950\nhard_drugs_grpCurrent User                                7.9610     6.3718\nhard_drugs_grpPrevious User                             -15.9522     5.3413\nADH_HIGHVSLOWHigh Adherence                               3.4343     2.1812\nhard_drugs_grpCurrent User:ADH_HIGHVSLOWHigh Adherence  -11.1387     6.5996\nhard_drugs_grpPrevious User:ADH_HIGHVSLOWHigh Adherence  18.7589     5.7025\n                                                        t value Pr(&gt;|t|)   \n(Intercept)                                              -0.382  0.70261   \nhard_drugs_grpCurrent User                                1.249  0.21207   \nhard_drugs_grpPrevious User                              -2.987  0.00295 **\nADH_HIGHVSLOWHigh Adherence                               1.574  0.11598   \nhard_drugs_grpCurrent User:ADH_HIGHVSLOWHigh Adherence   -1.688  0.09204 . \nhard_drugs_grpPrevious User:ADH_HIGHVSLOWHigh Adherence   3.290  0.00107 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 12.03 on 526 degrees of freedom\n  (18 observations deleted due to missingness)\nMultiple R-squared:  0.04347,   Adjusted R-squared:  0.03438 \nF-statistic: 4.781 on 5 and 526 DF,  p-value: 0.0002777\n\n\n\n# Print useful model parameters\nmodel_results(model_MENT_main1)\n\n\n\n\n\nEstimate\nStd. Error\nt value\nPr(&gt;|t|)\np_adjusted\n2.5 %\n97.5 %\n\n\n\n\n(Intercept)\n-0.8003171\n2.095019\n-0.3820094\n0.7026087\n1.0000000\n-4.9159498\n3.315316\n\n\nhard_drugs_grpCurrent User\n7.9609869\n6.371753\n1.2494186\n0.2120677\n1.0000000\n-4.5562212\n20.478195\n\n\nhard_drugs_grpPrevious User\n-15.9522320\n5.341272\n-2.9865976\n0.0029525\n0.0177150\n-26.4450776\n-5.459386\n\n\nADH_HIGHVSLOWHigh Adherence\n3.4342726\n2.181205\n1.5744837\n0.1159769\n0.6958614\n-0.8506712\n7.719216\n\n\nhard_drugs_grpCurrent User:ADH_HIGHVSLOWHigh Adherence\n-11.1386718\n6.599562\n-1.6877895\n0.0920444\n0.5522667\n-24.1034081\n1.826064\n\n\nhard_drugs_grpPrevious User:ADH_HIGHVSLOWHigh Adherence\n18.7589336\n5.702522\n3.2895855\n0.0010705\n0.0064233\n7.5564192\n29.961448\n\n\n\n\n\n\n# Get the VIFs\nvif_values &lt;- vif(model_MENT_main1)\n\nthere are higher-order terms (interactions) in this model\nconsider setting type = 'predictor'; see ?vif\n\npretty_print(vif_values)\n\n\n\n\n\nGVIF\nDf\nGVIF^(1/(2*Df))\n\n\n\n\nhard_drugs_grp\n119.991456\n2\n3.309692\n\n\nADH_HIGHVSLOW\n1.298281\n1\n1.139422\n\n\nhard_drugs_grp:ADH_HIGHVSLOW\n121.957959\n2\n3.323170\n\n\n\n\n\n\n\nThe overall model is significant (F(5,526) = 4.78, p = 0.000278, and we are getting significant interactions.\nAt the same time, this is a weaker model than when we included depression. Our R-adjusted is now 0.034, meaning we now only explain 3.4% of the variance in mental QOL (compared to 16% in the full model).\nAt low adherence, the previous group has a greater decrease in mental QOL compared to never users.\nFor never users, there is no difference in mental QOL based on high or low adherence (t = 1.5744837, p-adjusted = 0.116, 95% CI: -0.85 to 7.72).\nChange the reference group to compare drug use group at high adherence\n\n# Relevel to change to the reference group to never user\ndata_wide_2$hard_drugs_grp&lt;- relevel(data_wide_2$hard_drugs_grp, ref = \"Never User\")\n\n# Relevel to change to the reference group to never user\ndata_wide_2$ADH_HIGHVSLOW &lt;- relevel(data_wide_2$ADH_HIGHVSLOW, ref = \"High Adherence\")\n\n# Perform regression on mental QOL change by hard drugs group, full model\nmodel_MENT_main2 &lt;- lm(AGG_MENT_CHANGE ~ hard_drugs_grp + ADH_HIGHVSLOW+ hard_drugs_grp*ADH_HIGHVSLOW, data = data_wide_2)\n\n# Examine the summary\nsummary(model_MENT_main2)\n\n\nCall:\nlm(formula = AGG_MENT_CHANGE ~ hard_drugs_grp + ADH_HIGHVSLOW + \n    hard_drugs_grp * ADH_HIGHVSLOW, data = data_wide_2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-37.392  -5.850  -0.839   5.089  38.498 \n\nCoefficients:\n                                                       Estimate Std. Error\n(Intercept)                                              2.6340     0.6071\nhard_drugs_grpCurrent User                              -3.1777     1.7190\nhard_drugs_grpPrevious User                              2.8067     1.9974\nADH_HIGHVSLOWLow Adherence                              -3.4343     2.1812\nhard_drugs_grpCurrent User:ADH_HIGHVSLOWLow Adherence   11.1387     6.5996\nhard_drugs_grpPrevious User:ADH_HIGHVSLOWLow Adherence -18.7589     5.7025\n                                                       t value  Pr(&gt;|t|)    \n(Intercept)                                              4.339 0.0000172 ***\nhard_drugs_grpCurrent User                              -1.849   0.06508 .  \nhard_drugs_grpPrevious User                              1.405   0.16056    \nADH_HIGHVSLOWLow Adherence                              -1.574   0.11598    \nhard_drugs_grpCurrent User:ADH_HIGHVSLOWLow Adherence    1.688   0.09204 .  \nhard_drugs_grpPrevious User:ADH_HIGHVSLOWLow Adherence  -3.290   0.00107 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 12.03 on 526 degrees of freedom\n  (18 observations deleted due to missingness)\nMultiple R-squared:  0.04347,   Adjusted R-squared:  0.03438 \nF-statistic: 4.781 on 5 and 526 DF,  p-value: 0.0002777\n\n\n\n# Print useful model parameters\nmodel_results(model_MENT_main2)\n\n\n\n\n\nEstimate\nStd. Error\nt value\nPr(&gt;|t|)\np_adjusted\n2.5 %\n97.5 %\n\n\n\n\n(Intercept)\n2.633955\n0.607084\n4.338700\n0.0000172\n0.0001031\n1.441349\n3.8265623\n\n\nhard_drugs_grpCurrent User\n-3.177685\n1.719008\n-1.848557\n0.0650828\n0.3904967\n-6.554649\n0.1992793\n\n\nhard_drugs_grpPrevious User\n2.806702\n1.997389\n1.405185\n0.1605566\n0.9633394\n-1.117138\n6.7305412\n\n\nADH_HIGHVSLOWLow Adherence\n-3.434273\n2.181205\n-1.574484\n0.1159769\n0.6958614\n-7.719216\n0.8506712\n\n\nhard_drugs_grpCurrent User:ADH_HIGHVSLOWLow Adherence\n11.138672\n6.599562\n1.687790\n0.0920444\n0.5522667\n-1.826064\n24.1034081\n\n\nhard_drugs_grpPrevious User:ADH_HIGHVSLOWLow Adherence\n-18.758934\n5.702522\n-3.289585\n0.0010705\n0.0064233\n-29.961448\n-7.5564192\n\n\n\n\n\n\n\nAt high adherence, there is no difference in mental QOL between previous and never hard drug users (t = 1.41, p-adjusted = 0.48, 95% CI: -1.12 to 6.73), or between current and never hard drug users (t = -1.85, p-adjusted = 0.195, 95% CI: -6.55 to 0.20).\nChange reference level to compare previous hard drug users at low adherence.\n\n# Relevel to change to the reference group to never user\ndata_wide_2$hard_drugs_grp&lt;- relevel(data_wide_2$hard_drugs_grp, ref = \"Previous User\")\n\n# Relevel to change to the reference group to never user\ndata_wide_2$ADH_HIGHVSLOW &lt;- relevel(data_wide_2$ADH_HIGHVSLOW, ref = \"Low Adherence\")\n\n# Perform regression on mental QOL change by hard drugs group, full model\nmodel_MENT_main3 &lt;- lm(AGG_MENT_CHANGE ~ hard_drugs_grp + ADH_HIGHVSLOW+ hard_drugs_grp*ADH_HIGHVSLOW, data = data_wide_2)\n\n# Examine the summary\nsummary(model_MENT_main3)\n\n\nCall:\nlm(formula = AGG_MENT_CHANGE ~ hard_drugs_grp + ADH_HIGHVSLOW + \n    hard_drugs_grp * ADH_HIGHVSLOW, data = data_wide_2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-37.392  -5.850  -0.839   5.089  38.498 \n\nCoefficients:\n                                                       Estimate Std. Error\n(Intercept)                                             -16.753      4.913\nhard_drugs_grpNever User                                 15.952      5.341\nhard_drugs_grpCurrent User                               23.913      7.769\nADH_HIGHVSLOWHigh Adherence                              22.193      5.269\nhard_drugs_grpNever User:ADH_HIGHVSLOWHigh Adherence    -18.759      5.703\nhard_drugs_grpCurrent User:ADH_HIGHVSLOWHigh Adherence  -29.898      8.158\n                                                       t value  Pr(&gt;|t|)    \n(Intercept)                                             -3.410  0.000700 ***\nhard_drugs_grpNever User                                 2.987  0.002953 ** \nhard_drugs_grpCurrent User                               3.078  0.002191 ** \nADH_HIGHVSLOWHigh Adherence                              4.212 0.0000298 ***\nhard_drugs_grpNever User:ADH_HIGHVSLOWHigh Adherence    -3.290  0.001071 ** \nhard_drugs_grpCurrent User:ADH_HIGHVSLOWHigh Adherence  -3.665  0.000273 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 12.03 on 526 degrees of freedom\n  (18 observations deleted due to missingness)\nMultiple R-squared:  0.04347,   Adjusted R-squared:  0.03438 \nF-statistic: 4.781 on 5 and 526 DF,  p-value: 0.0002777\n\n\n\n# Print useful model parameters\nmodel_results(model_MENT_main3)\n\n\n\n\n\nEstimate\nStd. Error\nt value\nPr(&gt;|t|)\np_adjusted\n2.5 %\n97.5 %\n\n\n\n\n(Intercept)\n-16.75255\n4.913256\n-3.409663\n0.0007004\n0.0042022\n-26.404563\n-7.100535\n\n\nhard_drugs_grpNever User\n15.95223\n5.341272\n2.986598\n0.0029525\n0.0177150\n5.459386\n26.445078\n\n\nhard_drugs_grpCurrent User\n23.91322\n7.768540\n3.078213\n0.0021911\n0.0131465\n8.652045\n39.174393\n\n\nADH_HIGHVSLOWHigh Adherence\n22.19321\n5.268880\n4.212130\n0.0000298\n0.0001786\n11.842574\n32.543838\n\n\nhard_drugs_grpNever User:ADH_HIGHVSLOWHigh Adherence\n-18.75893\n5.702522\n-3.289585\n0.0010705\n0.0064233\n-29.961448\n-7.556419\n\n\nhard_drugs_grpCurrent User:ADH_HIGHVSLOWHigh Adherence\n-29.89761\n8.158288\n-3.664691\n0.0002728\n0.0016365\n-45.924434\n-13.870777\n\n\n\n\n\n\n\nHere we can see that all our p-values are now very low (all have at least 2 decimal places). This gives me confidence that this is a meaningful and strong model. All the most important comparisons are with previous drug users at low adherence!\nAt low adherence, current hard drug users have a higher mental QOL (p-adjusted = 0.00657), and never hard drug users have a higher mental QOL (p-adjusted = 0.00886) compared to previous hard drug users.\nAdditionally, for previous hard drug users, those with high adherence had a higher increase in mental QOL compared to those with low adherence (p &lt; 0.0001).\nChange reference category to compare high adherence previous hard drug users.\n\n# Relevel to change to the reference group to never user\ndata_wide_2$hard_drugs_grp &lt;- relevel(data_wide_2$hard_drugs_grp, ref = \"Previous User\")\n\n# Relevel to change to the reference group to never user\ndata_wide_2$ADH_HIGHVSLOW &lt;- relevel(data_wide_2$ADH_HIGHVSLOW, ref = \"High Adherence\")\n\n# Perform regression on mental QOL change by hard drugs group, full model\nmodel_MENT_main4 &lt;- lm(AGG_MENT_CHANGE ~ hard_drugs_grp + ADH_HIGHVSLOW + hard_drugs_grp*ADH_HIGHVSLOW, data = data_wide_2)\n\n# Examine the summary\nsummary(model_MENT_main4)\n\n\nCall:\nlm(formula = AGG_MENT_CHANGE ~ hard_drugs_grp + ADH_HIGHVSLOW + \n    hard_drugs_grp * ADH_HIGHVSLOW, data = data_wide_2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-37.392  -5.850  -0.839   5.089  38.498 \n\nCoefficients:\n                                                      Estimate Std. Error\n(Intercept)                                              5.441      1.903\nhard_drugs_grpNever User                                -2.807      1.997\nhard_drugs_grpCurrent User                              -5.984      2.491\nADH_HIGHVSLOWLow Adherence                             -22.193      5.269\nhard_drugs_grpNever User:ADH_HIGHVSLOWLow Adherence     18.759      5.703\nhard_drugs_grpCurrent User:ADH_HIGHVSLOWLow Adherence   29.898      8.158\n                                                      t value  Pr(&gt;|t|)    \n(Intercept)                                             2.859  0.004416 ** \nhard_drugs_grpNever User                               -1.405  0.160557    \nhard_drugs_grpCurrent User                             -2.402  0.016654 *  \nADH_HIGHVSLOWLow Adherence                             -4.212 0.0000298 ***\nhard_drugs_grpNever User:ADH_HIGHVSLOWLow Adherence     3.290  0.001071 ** \nhard_drugs_grpCurrent User:ADH_HIGHVSLOWLow Adherence   3.665  0.000273 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 12.03 on 526 degrees of freedom\n  (18 observations deleted due to missingness)\nMultiple R-squared:  0.04347,   Adjusted R-squared:  0.03438 \nF-statistic: 4.781 on 5 and 526 DF,  p-value: 0.0002777\n\n\n\n# Print useful model parameters\nmodel_results(model_MENT_main4)\n\n\n\n\n\nEstimate\nStd. Error\nt value\nPr(&gt;|t|)\np_adjusted\n2.5 %\n97.5 %\n\n\n\n\n(Intercept)\n5.440657\n1.902896\n2.859146\n0.0044164\n0.0264985\n1.702448\n9.178866\n\n\nhard_drugs_grpNever User\n-2.806702\n1.997389\n-1.405185\n0.1605566\n0.9633394\n-6.730541\n1.117138\n\n\nhard_drugs_grpCurrent User\n-5.984387\n2.491476\n-2.401945\n0.0166539\n0.0999236\n-10.878851\n-1.089922\n\n\nADH_HIGHVSLOWLow Adherence\n-22.193206\n5.268880\n-4.212130\n0.0000298\n0.0001786\n-32.543838\n-11.842574\n\n\nhard_drugs_grpNever User:ADH_HIGHVSLOWLow Adherence\n18.758934\n5.702522\n3.289585\n0.0010705\n0.0064233\n7.556419\n29.961448\n\n\nhard_drugs_grpCurrent User:ADH_HIGHVSLOWLow Adherence\n29.897605\n8.158288\n3.664691\n0.0002728\n0.0016365\n13.870777\n45.924434\n\n\n\n\n\n\n\nAt high adherence, there is a borderline significant difference between previous and current drug users (p-adjusted = 0.050). Considering the small sample size in both categories, this could become a non significant difference with a larger N.\nChange the reference level to compare current hard drug users\n\n# Relevel to change to the reference group to never user\ndata_wide_2$hard_drugs_grp &lt;- relevel(data_wide_2$hard_drugs_grp, ref = \"Current User\")\n\n# Relevel to change to the reference group to never user\ndata_wide_2$ADH_HIGHVSLOW &lt;- relevel(data_wide_2$ADH_HIGHVSLOW, ref = \"Low Adherence\")\n\n# Perform regression on mental QOL change by hard drugs group, full model\nmodel_MENT_main5 &lt;- lm(AGG_MENT_CHANGE ~ hard_drugs_grp + ADH_HIGHVSLOW + hard_drugs_grp*ADH_HIGHVSLOW, data = data_wide_2)\n\n# Examine the summary\nsummary(model_MENT_main5)\n\n\nCall:\nlm(formula = AGG_MENT_CHANGE ~ hard_drugs_grp + ADH_HIGHVSLOW + \n    hard_drugs_grp * ADH_HIGHVSLOW, data = data_wide_2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-37.392  -5.850  -0.839   5.089  38.498 \n\nCoefficients:\n                                                        Estimate Std. Error\n(Intercept)                                                7.161      6.017\nhard_drugs_grpPrevious User                              -23.913      7.769\nhard_drugs_grpNever User                                  -7.961      6.372\nADH_HIGHVSLOWHigh Adherence                               -7.704      6.229\nhard_drugs_grpPrevious User:ADH_HIGHVSLOWHigh Adherence   29.898      8.158\nhard_drugs_grpNever User:ADH_HIGHVSLOWHigh Adherence      11.139      6.600\n                                                        t value Pr(&gt;|t|)    \n(Intercept)                                               1.190 0.234592    \nhard_drugs_grpPrevious User                              -3.078 0.002191 ** \nhard_drugs_grpNever User                                 -1.249 0.212068    \nADH_HIGHVSLOWHigh Adherence                              -1.237 0.216668    \nhard_drugs_grpPrevious User:ADH_HIGHVSLOWHigh Adherence   3.665 0.000273 ***\nhard_drugs_grpNever User:ADH_HIGHVSLOWHigh Adherence      1.688 0.092044 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 12.03 on 526 degrees of freedom\n  (18 observations deleted due to missingness)\nMultiple R-squared:  0.04347,   Adjusted R-squared:  0.03438 \nF-statistic: 4.781 on 5 and 526 DF,  p-value: 0.0002777\n\n\n\n# Print useful model parameters\nmodel_results(model_MENT_main5)\n\n\n\n\n\nEstimate\nStd. Error\nt value\nPr(&gt;|t|)\np_adjusted\n2.5 %\n97.5 %\n\n\n\n\n(Intercept)\n7.160670\n6.017485\n1.189977\n0.2345922\n1.0000000\n-4.660585\n18.981925\n\n\nhard_drugs_grpPrevious User\n-23.913219\n7.768540\n-3.078213\n0.0021911\n0.0131465\n-39.174393\n-8.652045\n\n\nhard_drugs_grpNever User\n-7.960987\n6.371753\n-1.249419\n0.2120677\n1.0000000\n-20.478195\n4.556221\n\n\nADH_HIGHVSLOWHigh Adherence\n-7.704399\n6.228689\n-1.236922\n0.2166682\n1.0000000\n-19.940561\n4.531762\n\n\nhard_drugs_grpPrevious User:ADH_HIGHVSLOWHigh Adherence\n29.897605\n8.158288\n3.664691\n0.0002728\n0.0016365\n13.870777\n45.924434\n\n\nhard_drugs_grpNever User:ADH_HIGHVSLOWHigh Adherence\n11.138672\n6.599562\n1.687790\n0.0920444\n0.5522667\n-1.826064\n24.103408\n\n\n\n\n\n\n\nFor current hard drug users, there was no difference in mental QOL based on adherence (t = -1.24, p = 0.227, 95% CI: -19.94 to 4.53).\nTop of Tabset\n\n\n\nOverall model\nThere were significant differences in change mental QOL over 2 years based on hard drug usage and adherence to the treatment regiment (F(5,526) = 4.78, p = 0.000278).\n\n\nHard Drug Use\nThe relationship between hard drug use and change in mental QOL over 2 years depended on adherence to the treatment regiment.\n\nHard Drug Use at Low Adherence\nFor those with low adherence to the treatment regiment, previous hard drug users had a 15.95 point decrease in mental QOL compared to never hard drug users (t = -2.99, p-adjusted = 0.00886, 95% CI: -26.46 to -5.50), and a 23.91 point decrease compared to current hard drug users (t = 3.078, p-adjusted = 0.00657, 95% CI: 8.65 to 39.17). There was no difference in mental QOL between current and never hard drug users at low adherence (t = 1.25, p-adjusted = 0.636, 95% CI: -4.56 to 20.48).\n\n\nHard Drug use at High Adherence\nFor those with low adherence to the treatment regiment, there was no difference in mental QOL between previous and never hard drug users (t = 1.41, p-adjusted = 0.48, 95% CI: -1.12 to 6.73), or between current and never hard drug users (t = -1.85, p-adjusted = 0.195, 95% CI: -6.55 to 0.20). The difference in mental QOL for current and previous users was borderline significant (t= -2.40, p-adjusted = 0.050, 95% CI: -10.88 to -1.09), and with a larger sample size may become non-significant.\n\n\n\nAdherence\nThe relationship between adherence to the treatment regiment and change in mental QOL over 2 years differed by hard drug use.\n\nAdherence for Current Hard Drug Users\nFor current hard drug users,there was no difference in mental QOL change over 2 years based on adherence (t = -1.24, p = 0.227, 95% CI: -19.94 to 4.53).\n\n\nAdherence for Previous Hard Drug Users\nFor previous hard drug users, there was a statistically significant difference in change in mental QOL over 2 years based on adherence (t = 4.21, p-adjusted = 0.000536). On average, previous hard drug users with high adherence had a 22.19 point increase in mental QOL over 2 years compared to those with low adherence (95% CI: 11.84 to 32.54).\n\n\nAdherence for Never Hard Drug Users\nFor never hard drug users, there was no difference in change in mental QOL over 2 years based on high or low adherence (t = 1.5744837, p-adjusted = 0.116, 95% CI: -0.85 to 7.72).\nTop of Tabset\n\n\n\n\nThe below plot is useful in interpreting the interaction\n\n# Relevel to change to the reference group to never user\ndata_wide_2$hard_drugs_grp&lt;- relevel(data_wide_2$hard_drugs_grp, ref = \"Never User\")\n\n# Relevel to change to the reference group to never user\ndata_wide_2$ADH_HIGHVSLOW &lt;- relevel(data_wide_2$ADH_HIGHVSLOW, ref = \"Low Adherence\")\n\n# Use interactions package to plot interaction\ncat_plot(\n  model_MENT_main1, \n  pred = hard_drugs_grp, \n  modx = ADH_HIGHVSLOW, \n  geom = \"line\", \n  colors = \"Pastel2\",\n  x.label = \"Hard Drugs Group\",\n  y.label = \"Predicted Mental QOL Change\")\n\n\n\n\n\n\n\n\nHere we see the main interaction finding: for previous users, those with low adherence had a decrease in mental QOL over 2 years compared to those with high adherence who had an slight increase.\nAdditionally, for those with low adherence, previous hard drug users had a decrease in mental QOL over 2 years compared to current hard drug users and never hard drug users.\nWe interpret this to mean that previous hard drug users (those in either their first or second year of sobriety) are struggling to deal with withdrawals, addiction, or the adverse effects of the treatment. While current users are in the midst of using hard drugs as a coping mechanism, and thus are protected from the negative impact of the treatment on mental QOL.\nThus, previous hard drug users are a vulnerable population, and high adherence to the treatment regiment might buffer them against the adverse effects of the HAART treatment.\nTop of Tabset\n\n\n\n\n\nThis is the model predicting AGG_MENT_CHANGE, including only hard_drugs_grp and CESD_2, and their interaction term.\nThis addresses the confounding of depression on the relationship between hard drug use and change in mental QOL.\n\nAnalysisInterpretationVisualizing the Interaction\n\n\nCompare slopes for never users.\n\n# Relevel to change to the reference group to never user\ndata_wide_2$hard_drugs_grp&lt;- relevel(data_wide_2$hard_drugs_grp, ref = \"Never User\")\n\n# Perform regression on mental QOL change by hard drugs group, full model\nmodel_MENT_sub1 &lt;- lm(AGG_MENT_CHANGE ~ hard_drugs_grp + CESD_2 + hard_drugs_grp*CESD_2, data = data_wide_2)\n\n# Examine the summary\nsummary(model_MENT_sub1)\n\n\nCall:\nlm(formula = AGG_MENT_CHANGE ~ hard_drugs_grp + CESD_2 + hard_drugs_grp * \n    CESD_2, data = data_wide_2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-34.141  -6.071  -1.545   5.758  37.779 \n\nCoefficients:\n                                   Estimate Std. Error t value       Pr(&gt;|t|)\n(Intercept)                         5.10534    0.81668   6.251 0.000000000836\nhard_drugs_grpCurrent User         -3.04165    2.90324  -1.048          0.295\nhard_drugs_grpPrevious User        16.97744    2.91062   5.833 0.000000009463\nCESD_2                             -0.23480    0.05313  -4.419 0.000011998960\nhard_drugs_grpCurrent User:CESD_2   0.10992    0.15093   0.728          0.467\nhard_drugs_grpPrevious User:CESD_2 -0.80296    0.13010  -6.172 0.000000001341\n                                      \n(Intercept)                        ***\nhard_drugs_grpCurrent User            \nhard_drugs_grpPrevious User        ***\nCESD_2                             ***\nhard_drugs_grpCurrent User:CESD_2     \nhard_drugs_grpPrevious User:CESD_2 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 11.36 on 532 degrees of freedom\n  (12 observations deleted due to missingness)\nMultiple R-squared:  0.1571,    Adjusted R-squared:  0.1492 \nF-statistic: 19.84 on 5 and 532 DF,  p-value: &lt; 0.00000000000000022\n\n# Get the VIFs\nvif_values &lt;- vif(model_MENT_sub1)\n\nthere are higher-order terms (interactions) in this model\nconsider setting type = 'predictor'; see ?vif\n\npretty_print(vif_values)\n\n\n\n\n\nGVIF\nDf\nGVIF^(1/(2*Df))\n\n\n\n\nhard_drugs_grp\n9.440857\n2\n1.752883\n\n\nCESD_2\n1.416421\n1\n1.190135\n\n\nhard_drugs_grp:CESD_2\n11.642359\n2\n1.847184\n\n\n\n\n\n\n\n\n# Print useful model parameters\nmodel_results(model_MENT_sub1)\n\n\n\n\n\nEstimate\nStd. Error\nt value\nPr(&gt;|t|)\np_adjusted\n2.5 %\n97.5 %\n\n\n\n\n(Intercept)\n5.1053388\n0.8166797\n6.2513354\n0.0000000\n0.0000000\n3.5010261\n6.7096514\n\n\nhard_drugs_grpCurrent User\n-3.0416518\n2.9032412\n-1.0476745\n0.2952643\n1.0000000\n-8.7448750\n2.6615715\n\n\nhard_drugs_grpPrevious User\n16.9774420\n2.9106247\n5.8329203\n0.0000000\n0.0000001\n11.2597144\n22.6951696\n\n\nCESD_2\n-0.2347979\n0.0531285\n-4.4194338\n0.0000120\n0.0000720\n-0.3391653\n-0.1304305\n\n\nhard_drugs_grpCurrent User:CESD_2\n0.1099198\n0.1509341\n0.7282635\n0.4667728\n1.0000000\n-0.1865801\n0.4064197\n\n\nhard_drugs_grpPrevious User:CESD_2\n-0.8029584\n0.1301049\n-6.1716247\n0.0000000\n0.0000000\n-1.0585407\n-0.5473761\n\n\n\n\n\n\n\nThe overall model is highly significant (F~(5, 532)~ = 19.84, p &lt; 0.0001, adjusted R-squared = 0.149).\nFor never hard drug users, there was a significant relationship between depression and mental QOL (t = -4.42, p &lt; 0.0001). Specifically, for never hard drug users, a 1 point increase in depression score was associated with 0.23 point decrease in mental QOL (95% CI: -0.34 to -0.13).\nCurrent hard drug users did not differ significantly from never hard drug users while controlling for the other variables in the model (t = -1.05, p-adjusted = 0.89, 95% CI: -8.74 to 2.66). Previous hard drug users had on average a 16.98 higher increase in mental QOL compared to never hard drug users (at low depression scores?) (t = 2.91062, p-adjusted &lt; 0.0001.\nThe slope for depression on mental QOL did not differ between current and never hard drug users (t = 0.73, p-adjusted = 1.00, 95% CI: -0.19 to 0.41). However, the slope for depression on mental QOL did differ between previous and never hard drug users (t = -6.17, p-adjusted &lt; 0.0001). For each point increase in depression, the mental QOL score for previous hard drug users decreases an additional 0.80 points compared to never hard drug users (95% CI: -1.06 to -0.55).\nPrevious hard drug users have an increase of approximately 16.98 units in mental QOL, holding all other variables constant. This difference is statistically significant, suggesting that prior hard drug use is associated with a considerably higher outcome measure in this model. (It’s significant because we’re not account for the interaction with this estimate.)\nCompare slopes for previous users.\n\n# Relevel to change to the reference group to never user\ndata_wide_2$hard_drugs_grp&lt;- relevel(data_wide_2$hard_drugs_grp, ref = \"Previous User\")\n\n# Perform regression on mental QOL change by hard drugs group, full model\nmodel_MENT_sub2 &lt;- lm(AGG_MENT_CHANGE ~ hard_drugs_grp + CESD_2 + hard_drugs_grp*CESD_2, data = data_wide_2)\n\n# Examine the summary\nsummary(model_MENT_sub2)\n\n\nCall:\nlm(formula = AGG_MENT_CHANGE ~ hard_drugs_grp + CESD_2 + hard_drugs_grp * \n    CESD_2, data = data_wide_2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-34.141  -6.071  -1.545   5.758  37.779 \n\nCoefficients:\n                                  Estimate Std. Error t value\n(Intercept)                        22.0828     2.7937   7.904\nhard_drugs_grpNever User          -16.9774     2.9106  -5.833\nhard_drugs_grpCurrent User        -20.0191     3.9455  -5.074\nCESD_2                             -1.0378     0.1188  -8.738\nhard_drugs_grpNever User:CESD_2     0.8030     0.1301   6.172\nhard_drugs_grpCurrent User:CESD_2   0.9129     0.1846   4.946\n                                              Pr(&gt;|t|)    \n(Intercept)                         0.0000000000000156 ***\nhard_drugs_grpNever User            0.0000000094634100 ***\nhard_drugs_grpCurrent User          0.0000005392622284 ***\nCESD_2                            &lt; 0.0000000000000002 ***\nhard_drugs_grpNever User:CESD_2     0.0000000013409353 ***\nhard_drugs_grpCurrent User:CESD_2   0.0000010165357517 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 11.36 on 532 degrees of freedom\n  (12 observations deleted due to missingness)\nMultiple R-squared:  0.1571,    Adjusted R-squared:  0.1492 \nF-statistic: 19.84 on 5 and 532 DF,  p-value: &lt; 0.00000000000000022\n\n\n\n# Print useful model parameters\nmodel_results(model_MENT_sub2)\n\n\n\n\n\nEstimate\nStd. Error\nt value\nPr(&gt;|t|)\np_adjusted\n2.5 %\n97.5 %\n\n\n\n\n(Intercept)\n22.0827808\n2.7937019\n7.904487\n0.0000000\n0.0000000\n16.5947401\n27.5708214\n\n\nhard_drugs_grpNever User\n-16.9774420\n2.9106247\n-5.832920\n0.0000000\n0.0000001\n-22.6951696\n-11.2597144\n\n\nhard_drugs_grpCurrent User\n-20.0190937\n3.9454549\n-5.073963\n0.0000005\n0.0000032\n-27.7696761\n-12.2685114\n\n\nCESD_2\n-1.0377563\n0.1187629\n-8.738048\n0.0000000\n0.0000000\n-1.2710581\n-0.8044544\n\n\nhard_drugs_grpNever User:CESD_2\n0.8029584\n0.1301049\n6.171625\n0.0000000\n0.0000000\n0.5473761\n1.0585407\n\n\nhard_drugs_grpCurrent User:CESD_2\n0.9128781\n0.1845619\n4.946190\n0.0000010\n0.0000061\n0.5503186\n1.2754377\n\n\n\n\n\n\n\nFor previous hard drug users, there was a significant relationship between depression and mental QOL (t = -8.74, p &lt; 0.0001). Specifically, for previous hard drug users, a 1 point increase in depression score was associated with a 1.04 point decrease in mental QOL (95% CI: -1.27 to -0.80). That is nearly 5 times more of a decrease compared to never users!\nCompare slopes for current users.\n\n# Relevel to change to the reference group to never user\ndata_wide_2$hard_drugs_grp&lt;- relevel(data_wide_2$hard_drugs_grp, ref = \"Current User\")\n\n# Perform regression on mental QOL change by hard drugs group, full model\nmodel_MENT_sub3 &lt;- lm(AGG_MENT_CHANGE ~ hard_drugs_grp + CESD_2 + hard_drugs_grp*CESD_2, data = data_wide_2)\n\n# Examine the summary\nsummary(model_MENT_sub3)\n\n\nCall:\nlm(formula = AGG_MENT_CHANGE ~ hard_drugs_grp + CESD_2 + hard_drugs_grp * \n    CESD_2, data = data_wide_2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-34.141  -6.071  -1.545   5.758  37.779 \n\nCoefficients:\n                                   Estimate Std. Error t value    Pr(&gt;|t|)    \n(Intercept)                          2.0637     2.7860   0.741       0.459    \nhard_drugs_grpPrevious User         20.0191     3.9455   5.074 0.000000539 ***\nhard_drugs_grpNever User             3.0417     2.9032   1.048       0.295    \nCESD_2                              -0.1249     0.1413  -0.884       0.377    \nhard_drugs_grpPrevious User:CESD_2  -0.9129     0.1846  -4.946 0.000001017 ***\nhard_drugs_grpNever User:CESD_2     -0.1099     0.1509  -0.728       0.467    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 11.36 on 532 degrees of freedom\n  (12 observations deleted due to missingness)\nMultiple R-squared:  0.1571,    Adjusted R-squared:  0.1492 \nF-statistic: 19.84 on 5 and 532 DF,  p-value: &lt; 0.00000000000000022\n\n\n\n# Print useful model parameters\nmodel_results(model_MENT_sub3)\n\n\n\n\n\nEstimate\nStd. Error\nt value\nPr(&gt;|t|)\np_adjusted\n2.5 %\n97.5 %\n\n\n\n\n(Intercept)\n2.0636870\n2.7860086\n0.7407325\n0.4591825\n1.0000000\n-3.4092406\n7.5366146\n\n\nhard_drugs_grpPrevious User\n20.0190937\n3.9454549\n5.0739634\n0.0000005\n0.0000032\n12.2685114\n27.7696761\n\n\nhard_drugs_grpNever User\n3.0416518\n2.9032412\n1.0476745\n0.2952643\n1.0000000\n-2.6615715\n8.7448750\n\n\nCESD_2\n-0.1248781\n0.1412744\n-0.8839402\n0.3771278\n1.0000000\n-0.4024022\n0.1526460\n\n\nhard_drugs_grpPrevious User:CESD_2\n-0.9128781\n0.1845619\n-4.9461896\n0.0000010\n0.0000061\n-1.2754377\n-0.5503186\n\n\nhard_drugs_grpNever User:CESD_2\n-0.1099198\n0.1509341\n-0.7282635\n0.4667728\n1.0000000\n-0.4064197\n0.1865801\n\n\n\n\n\n\n\nFor current users, the relationship between depression and mental QOL is not significant (t = 0.88, p = 0.377, 95% CI: -0.40 to 0.15).\nThe slope for depression on mental QOL depended on hard drug use (t = -4.946, p-adjusted &lt; 0.0001). For each point increase in depression, the mental QOL score for previous hard drug users decreased by an additional 0.91 points compared to current hard drug users (95%: -1.28 to -0.55).\nTop of Tabset\n\n\n\nOverall Model\nThe overall model is highly significant (F~(5, 532)~ = 19.84, p &lt; 0.0001, adjusted R-squared = 0.149).\nThe relationship between depression and mental QOL depended on hard drug use.\n\n\nDepression Slopes within Hard Drug Use Group\n\nDepression for Current Hard Drug Users\nFor current hard drug users, the relationship between depression and mental QOL is not significant (t = 0.88, p = 0.377, 95% CI: -0.40 to 0.15).\n\n\nDepression for Previous Hard Drug Users\nFor previous hard drug users, there was a significant relationship between depression and mental QOL (t = -8.74, p &lt; 0.0001). Specifically, a 1 point increase in depression score was associated with a 1.04 point decrease in mental QOL for previous hard drug users (95% CI: -1.27 to -0.80). That is nearly 5 times more of a decrease compared to never users!\n\n\nDepression for Never Hard Drug Users\nFor never hard drug users, there was a significant relationship between depression and mental QOL (t = -4.42, p &lt; 0.0001). Specifically, a 1 point increase in depression score was associated with 0.23 point decrease in mental QOL for never hard drug users (95% CI: -0.34 to -0.13).\n\n\n\nDepression Slopes between Hard Drug use Group\nThe slopes for the relationship between depression and QOL differed based on hard drug use.\nFor each point increase in depression, the mental QOL score for previous hard drug users decreased an additional 0.80 points compared to never hard drug users (t = -6.17, p-adjusted &lt; 0.0001, 95% CI: -1.06 to -0.55).\nAdditionally, for each point increase in depression, the mental QOL score for previous hard drug users decreased by an additional 0.91 points compared to current hard drug users (t = -4.946, p-adjusted &lt; 0.0001, 95%: -1.28 to -0.55).\nThe slope for depression on mental QOL did not differ between current and never hard drug users (t = 0.73, p-adjusted = 1.00, 95% CI: -0.19 to 0.41).\nTop of Tabset\n\n\n\nThe plot below is helpful in interpreting the interaction between hard drug use and depression on mental QOL.\n\n# Create a scatterplot of mental QOLchange by depression score, colored by hard drug use group.\nggplot(data_wide_2, aes(x = CESD_2, y = AGG_MENT_CHANGE, color = hard_drugs_grp)) + \n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  theme_minimal() +\n  labs(title = \"Mental QOL Change by Depression at 2 years, Colored by Hard Drug Use Group\",\n       y = \"Mental QOL Change\",\n       x = \"Depression Score\") + \n  scale_fill_brewer(palette = \"Pastel2\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 12 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 12 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nHere we can see the key relationships found in the model.\nThere is a negative relationship between depression and mental QOL scores at each level of drug usage. However, this relationship is much stronger for previous hard drug users, where every 1 point increase in depression scores is associated with an almost 5x greater decrease in mental QOL score over 2 years (~1 point for previous users compared to ~0.2 points for current and never user).\nAnd most importantly the slopes differed significantly by hard drug use group, with previous hard drug users having an additional 0.8 or 0.9 greater decrease in mental QOL score compared to never and current hard drug users, respectively.\nThus, we used this relationship as further evidence that previous hard drug users are a vulnerable group. They are susceptible to losses in mental QOL if they have low adherence to the treatment regiment, and depression has a greater impact on the mental QOL of previous hard drug users compared to current and never hard drug users.\nTop of Tabset\n\n\n\n\n\nWe found a dual interaction where the impact of hard drug use on mental QOL depended on both adherence to the treatment regimen and depression.\nFor adherence, those with previous hard drug use had a decrease in mental QOL if and only if they had low adherence to the treatment regiment.\nFor depression, previous hard drug users had a larger decrease in mental QOL compared to never and current hard drug users.\nThus, we used both of these relationships as strong evidence that previous hard drug users are a vulnerable population. They are susceptible to losses in mental QOL if they have low adherence to the treatment regiment, and depression has a greater impact on their mental QOL compared to current and never hard drug users.\nThis finding has great clinical relevance for the treatment of HIV in hard drug users. Specifically, extra efforts should be made to help previous hard drug users in two ways: One, adherence appears to serve as a buffer for previous hard drug users in the relationship between hard drug use and mental QOL. Efforts to increase adherence should therefore be made to help patients receive this ameliorative effect. Two, previous hard drug users do not have the coping mechanism of heroin or opiates available to them to buffer against the impact of adverse effects from ART, and thus extra aid perhaps in the form of increased therapy should be provided to these patients to provide them with alternate coping strategies.\nTop of Tabset\n\n\nJust for completion’s sake, we can run a regression predicting depression score based on the interaction between hard drug use and adherence.\n\n# Relevel to change to the reference group to never user\ndata_wide_2$hard_drugs_grp&lt;- relevel(data_wide_2$hard_drugs_grp, ref = \"Never User\")\n\n# Relevel to change to the reference group to never user\ndata_wide_2$ADH_HIGHVSLOW &lt;- relevel(data_wide_2$ADH_HIGHVSLOW, ref = \"Low Adherence\")\n\n# Perform regression on mental QOL change by hard drugs group, full model\nmodel_MENT_full1 &lt;- lm(CESD_2 ~ hard_drugs_grp + ADH_HIGHVSLOW+ hard_drugs_grp*ADH_HIGHVSLOW, data = data_wide_2)\n\n# Examine the summary\nsummary(model_MENT_full1)\n\n\nCall:\nlm(formula = CESD_2 ~ hard_drugs_grp + ADH_HIGHVSLOW + hard_drugs_grp * \n    ADH_HIGHVSLOW, data = data_wide_2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.679  -8.508  -2.508   6.492  38.492 \n\nCoefficients:\n                                                        Estimate Std. Error\n(Intercept)                                               13.750      1.848\nhard_drugs_grpCurrent User                                 4.250      5.544\nhard_drugs_grpPrevious User                               30.250      4.651\nADH_HIGHVSLOWHigh Adherence                               -2.242      1.921\nhard_drugs_grpCurrent User:ADH_HIGHVSLOWHigh Adherence     0.921      5.742\nhard_drugs_grpPrevious User:ADH_HIGHVSLOWHigh Adherence  -26.708      4.964\n                                                        t value\n(Intercept)                                               7.440\nhard_drugs_grpCurrent User                                0.767\nhard_drugs_grpPrevious User                               6.504\nADH_HIGHVSLOWHigh Adherence                              -1.167\nhard_drugs_grpCurrent User:ADH_HIGHVSLOWHigh Adherence    0.160\nhard_drugs_grpPrevious User:ADH_HIGHVSLOWHigh Adherence  -5.380\n                                                                 Pr(&gt;|t|)    \n(Intercept)                                             0.000000000000411 ***\nhard_drugs_grpCurrent User                                          0.444    \nhard_drugs_grpPrevious User                             0.000000000181686 ***\nADH_HIGHVSLOWHigh Adherence                                         0.244    \nhard_drugs_grpCurrent User:ADH_HIGHVSLOWHigh Adherence              0.873    \nhard_drugs_grpPrevious User:ADH_HIGHVSLOWHigh Adherence 0.000000111914533 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 10.45 on 528 degrees of freedom\n  (16 observations deleted due to missingness)\nMultiple R-squared:  0.1175,    Adjusted R-squared:  0.1091 \nF-statistic: 14.06 on 5 and 528 DF,  p-value: 0.0000000000006409\n\n\n\n# Use interactions package to plot interaction\ncat_plot(model_MENT_full1, pred = hard_drugs_grp, modx = ADH_HIGHVSLOW, geom = \"line\", colors = \"Pastel2\")\n\n\n\n\n\n\n\n\nThe interaction is significant, and we can see that previous hard drug users with low adherence had higher depression scores at year 2, and that those with high adherance were buffered from this effect.\nThe fact that depression and mental QOL both have this relationship, and are similar constructs, lends credence to the likelihood that this is a real relationship, and a strong one at that."
  },
  {
    "objectID": "Project_2/Project_2_R/Code/Project2.html#Phys",
    "href": "Project_2/Project_2_R/Code/Project2.html#Phys",
    "title": "Advanced Data Analysis - Project 2",
    "section": "Physical QOL Score",
    "text": "Physical QOL Score\nThe fourth outcome variable of interest was change physcal QOL over the first 2 years of the study.\n\nFull Model\n\n\n\nModel SelectionAnalysisInterpretationVisualizing the Interaction\n\n\nThe candidate variables for inclusion in our model predicting AGG_PHYS_CHANGE were hard_drugs_grp, ADH_HIGHVSLOW, FRP_2, CESD_2, BMI, and DKGRP_2\nWe will remove BMI based on the missing values issue identified in the mental QOL model.\nLet’s run the full model including all those variables and the interaction between hard drug use group and adherence high vs low.\n\n# Relevel to change to the reference group to never user\ndata_wide_2$hard_drugs_grp&lt;- relevel(data_wide_2$hard_drugs_grp, ref = \"Never User\")\n\n\n# Relevel to change to the reference group to never user\ndata_wide_2$ADH_HIGHVSLOW &lt;- relevel(data_wide_2$ADH_HIGHVSLOW, ref = \"Low Adherence\")\n\n# Perform regression on Physical QOL change\nmodel_PHYS_full1 &lt;- lm(AGG_PHYS_CHANGE ~ hard_drugs_grp + ADH_HIGHVSLOW + FRP_2 + CESD_2 + DKGRP_2 + hard_drugs_grp*ADH_HIGHVSLOW, data = data_wide_2)\n\n# Examine the model\nsummary(model_PHYS_full1)\n\n\nCall:\nlm(formula = AGG_PHYS_CHANGE ~ hard_drugs_grp + ADH_HIGHVSLOW + \n    FRP_2 + CESD_2 + DKGRP_2 + hard_drugs_grp * ADH_HIGHVSLOW, \n    data = data_wide_2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-32.215  -3.939   0.013   3.673  31.134 \n\nCoefficients:\n                                                          Estimate Std. Error\n(Intercept)                                              -4.584386   1.612457\nhard_drugs_grpCurrent User                              -10.266222   4.130948\nhard_drugs_grpPrevious User                             -11.067396   3.633010\nADH_HIGHVSLOWHigh Adherence                               3.209555   1.451052\nFRP_2Yes                                                -12.656122   1.481490\nCESD_2                                                    0.007245   0.033055\nDKGRP_21-3 drinks/week                                    0.651656   0.906606\nDKGRP_24-13 drinks/week                                   2.333646   1.035604\nDKGRP_2&gt;13 drinks/week                                   -0.398119   1.636008\nhard_drugs_grpCurrent User:ADH_HIGHVSLOWHigh Adherence    6.472409   4.284786\nhard_drugs_grpPrevious User:ADH_HIGHVSLOWHigh Adherence  12.329569   3.847471\n                                                        t value\n(Intercept)                                              -2.843\nhard_drugs_grpCurrent User                               -2.485\nhard_drugs_grpPrevious User                              -3.046\nADH_HIGHVSLOWHigh Adherence                               2.212\nFRP_2Yes                                                 -8.543\nCESD_2                                                    0.219\nDKGRP_21-3 drinks/week                                    0.719\nDKGRP_24-13 drinks/week                                   2.253\nDKGRP_2&gt;13 drinks/week                                   -0.243\nhard_drugs_grpCurrent User:ADH_HIGHVSLOWHigh Adherence    1.511\nhard_drugs_grpPrevious User:ADH_HIGHVSLOWHigh Adherence   3.205\n                                                                    Pr(&gt;|t|)\n(Intercept)                                                          0.00465\nhard_drugs_grpCurrent User                                           0.01327\nhard_drugs_grpPrevious User                                          0.00244\nADH_HIGHVSLOWHigh Adherence                                          0.02742\nFRP_2Yes                                                &lt; 0.0000000000000002\nCESD_2                                                               0.82659\nDKGRP_21-3 drinks/week                                               0.47260\nDKGRP_24-13 drinks/week                                              0.02466\nDKGRP_2&gt;13 drinks/week                                               0.80783\nhard_drugs_grpCurrent User:ADH_HIGHVSLOWHigh Adherence               0.13152\nhard_drugs_grpPrevious User:ADH_HIGHVSLOWHigh Adherence              0.00144\n                                                           \n(Intercept)                                             ** \nhard_drugs_grpCurrent User                              *  \nhard_drugs_grpPrevious User                             ** \nADH_HIGHVSLOWHigh Adherence                             *  \nFRP_2Yes                                                ***\nCESD_2                                                     \nDKGRP_21-3 drinks/week                                     \nDKGRP_24-13 drinks/week                                 *  \nDKGRP_2&gt;13 drinks/week                                     \nhard_drugs_grpCurrent User:ADH_HIGHVSLOWHigh Adherence     \nhard_drugs_grpPrevious User:ADH_HIGHVSLOWHigh Adherence ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.757 on 510 degrees of freedom\n  (29 observations deleted due to missingness)\nMultiple R-squared:  0.2022,    Adjusted R-squared:  0.1865 \nF-statistic: 12.92 on 10 and 510 DF,  p-value: &lt; 0.00000000000000022\n\n\n\n# Get the VIFs\nvif_values &lt;- vif(model_PHYS_full1)\n\nthere are higher-order terms (interactions) in this model\nconsider setting type = 'predictor'; see ?vif\n\npretty_print(vif_values)\n\n\n\n\n\nGVIF\nDf\nGVIF^(1/(2*Df))\n\n\n\n\nhard_drugs_grp\n125.173912\n2\n3.344864\n\n\nADH_HIGHVSLOW\n1.321733\n1\n1.149667\n\n\nFRP_2\n1.031224\n1\n1.015492\n\n\nCESD_2\n1.157182\n1\n1.075724\n\n\nDKGRP_2\n1.052687\n3\n1.008594\n\n\nhard_drugs_grp:ADH_HIGHVSLOW\n124.834338\n2\n3.342593\n\n\n\n\n\n\n\nThe overall model is highly significant (F(10,510) = 12.92, p &lt; 0.0001, Adjusted R-squared = 0.187).\nWe have some nice significant covariates, but it looks like FRP_2 is the main driver for this model.\nWe might have another double interaction situation like we did for the mental QOL model. Let’s create an interaction term between hard_drugs_grp and FRP_2 and run it through backwards elimination to select our final model based on BIC.\n\nBackwards Elimination\nWe will perform model selection using backwards elimination and BIC to select the most parsimonious model.\nA delta BIC of &gt;2 indicates a difference in model performance.\n\n# Relevel to change to the reference group to never user\ndata_wide_2$hard_drugs_grp&lt;- relevel(data_wide_2$hard_drugs_grp, ref = \"Never User\")\n\n# Relevel to change to the reference group to never user\ndata_wide_2$ADH_HIGHVSLOW &lt;- relevel(data_wide_2$ADH_HIGHVSLOW, ref = \"Low Adherence\")\n\n# Perform regression on physical QOL change\nmodel_PHYS_full1 &lt;- lm(AGG_PHYS_CHANGE ~ hard_drugs_grp + ADH_HIGHVSLOW + FRP_2 + CESD_2 + DKGRP_2 + hard_drugs_grp*ADH_HIGHVSLOW + hard_drugs_grp*FRP_2, data = data_wide_2)\n\n# Perform backward elimination regression selection based on BIC \nols_step_backward_sbc(model_PHYS_full1, include = c(\"hard_drugs_grp\", \"ADH_HIGHVSLOW\", \"hard_drugs_grp:ADH_HIGHVSLOW\"))\n\n\n                                    Stepwise Summary                                    \n--------------------------------------------------------------------------------------\nStep    Variable                  AIC         SBC         SBIC        R2       Adj. R2 \n--------------------------------------------------------------------------------------\n 0      Full Model              3622.820    3682.401    2164.416    0.21322    0.19463 \n 1      DKGRP_2                 3623.753    3670.567    2159.814    0.20268    0.18864 \n 2      CESD_2                  3621.873    3664.430    2147.887    0.20250    0.19004 \n 3      hard_drugs_grp:FRP_2    3625.025    3659.071    2143.951    0.19147    0.18204 \n--------------------------------------------------------------------------------------\n\nFinal Model Output \n------------------\n\n                         Model Summary                           \n----------------------------------------------------------------\nR                       0.438       RMSE                  7.726 \nR-Squared               0.191       MSE                  59.694 \nAdj. R-Squared          0.182       Coef. Var          -408.677 \nPred R-Squared          0.174       AIC                3625.025 \nMAE                     5.577       SBC                3659.071 \n----------------------------------------------------------------\n RMSE: Root Mean Square Error \n MSE: Mean Square Error \n MAE: Mean Absolute Error \n AIC: Akaike Information Criteria \n SBC: Schwarz Bayesian Criteria \n\n                                ANOVA                                  \n----------------------------------------------------------------------\n                 Sum of                                               \n                Squares         DF    Mean Square      F         Sig. \n----------------------------------------------------------------------\nRegression     7365.193          6       1227.532    20.287    0.0000 \nResidual      31100.658        514         60.507                     \nTotal         38465.851        520                                    \n----------------------------------------------------------------------\n\n                                                         Parameter Estimates                                                           \n--------------------------------------------------------------------------------------------------------------------------------------\n                                                  model       Beta    Std. Error    Std. Beta      t        Sig       lower     upper \n--------------------------------------------------------------------------------------------------------------------------------------\n                                            (Intercept)     -3.611         1.398                 -2.583    0.010     -6.357    -0.864 \n                             hard_drugs_grpCurrent User     -9.617         4.133       -0.352    -2.327    0.020    -17.736    -1.497 \n                            hard_drugs_grpPrevious User    -10.860         3.470       -0.351    -3.130    0.002    -17.676    -4.043 \n                            ADH_HIGHVSLOWHigh Adherence      3.202         1.452        0.100     2.205    0.028      0.349     6.055 \n                                               FRP_2Yes    -12.737         1.468       -0.345    -8.674    0.000    -15.622    -9.852 \n hard_drugs_grpCurrent User:ADH_HIGHVSLOWHigh Adherence      5.711         4.286        0.203     1.332    0.183     -2.710    14.132 \nhard_drugs_grpPrevious User:ADH_HIGHVSLOWHigh Adherence     12.158         3.713        0.368     3.274    0.001      4.863    19.453 \n--------------------------------------------------------------------------------------------------------------------------------------\n\n\nThe final model selected for AGG_PHYS_CHANGE includes the PEVs of hard_drugs_grp and ADH_HIGHVSLOW and their interaction term, and FRP_2 as a precision variable. Luckily we only have to interpret one interaction for this model!\n\n# Relevel to change to the reference group to never user\ndata_wide_2$hard_drugs_grp&lt;- relevel(data_wide_2$hard_drugs_grp, ref = \"Never User\")\n\n\n# Relevel to change to the reference group to never user\ndata_wide_2$ADH_HIGHVSLOW &lt;- relevel(data_wide_2$ADH_HIGHVSLOW, ref = \"Low Adherence\")\n\n# Perform regression on Physical QOL change\nmodel_PHYS_full1 &lt;- lm(AGG_PHYS_CHANGE ~ hard_drugs_grp + ADH_HIGHVSLOW + FRP_2 + CESD_2 + DKGRP_2 + hard_drugs_grp*ADH_HIGHVSLOW, data = data_wide_2)\n\n# Examine the model\nsummary(model_PHYS_full1)\n\n\nCall:\nlm(formula = AGG_PHYS_CHANGE ~ hard_drugs_grp + ADH_HIGHVSLOW + \n    FRP_2 + CESD_2 + DKGRP_2 + hard_drugs_grp * ADH_HIGHVSLOW, \n    data = data_wide_2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-32.215  -3.939   0.013   3.673  31.134 \n\nCoefficients:\n                                                          Estimate Std. Error\n(Intercept)                                              -4.584386   1.612457\nhard_drugs_grpCurrent User                              -10.266222   4.130948\nhard_drugs_grpPrevious User                             -11.067396   3.633010\nADH_HIGHVSLOWHigh Adherence                               3.209555   1.451052\nFRP_2Yes                                                -12.656122   1.481490\nCESD_2                                                    0.007245   0.033055\nDKGRP_21-3 drinks/week                                    0.651656   0.906606\nDKGRP_24-13 drinks/week                                   2.333646   1.035604\nDKGRP_2&gt;13 drinks/week                                   -0.398119   1.636008\nhard_drugs_grpCurrent User:ADH_HIGHVSLOWHigh Adherence    6.472409   4.284786\nhard_drugs_grpPrevious User:ADH_HIGHVSLOWHigh Adherence  12.329569   3.847471\n                                                        t value\n(Intercept)                                              -2.843\nhard_drugs_grpCurrent User                               -2.485\nhard_drugs_grpPrevious User                              -3.046\nADH_HIGHVSLOWHigh Adherence                               2.212\nFRP_2Yes                                                 -8.543\nCESD_2                                                    0.219\nDKGRP_21-3 drinks/week                                    0.719\nDKGRP_24-13 drinks/week                                   2.253\nDKGRP_2&gt;13 drinks/week                                   -0.243\nhard_drugs_grpCurrent User:ADH_HIGHVSLOWHigh Adherence    1.511\nhard_drugs_grpPrevious User:ADH_HIGHVSLOWHigh Adherence   3.205\n                                                                    Pr(&gt;|t|)\n(Intercept)                                                          0.00465\nhard_drugs_grpCurrent User                                           0.01327\nhard_drugs_grpPrevious User                                          0.00244\nADH_HIGHVSLOWHigh Adherence                                          0.02742\nFRP_2Yes                                                &lt; 0.0000000000000002\nCESD_2                                                               0.82659\nDKGRP_21-3 drinks/week                                               0.47260\nDKGRP_24-13 drinks/week                                              0.02466\nDKGRP_2&gt;13 drinks/week                                               0.80783\nhard_drugs_grpCurrent User:ADH_HIGHVSLOWHigh Adherence               0.13152\nhard_drugs_grpPrevious User:ADH_HIGHVSLOWHigh Adherence              0.00144\n                                                           \n(Intercept)                                             ** \nhard_drugs_grpCurrent User                              *  \nhard_drugs_grpPrevious User                             ** \nADH_HIGHVSLOWHigh Adherence                             *  \nFRP_2Yes                                                ***\nCESD_2                                                     \nDKGRP_21-3 drinks/week                                     \nDKGRP_24-13 drinks/week                                 *  \nDKGRP_2&gt;13 drinks/week                                     \nhard_drugs_grpCurrent User:ADH_HIGHVSLOWHigh Adherence     \nhard_drugs_grpPrevious User:ADH_HIGHVSLOWHigh Adherence ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.757 on 510 degrees of freedom\n  (29 observations deleted due to missingness)\nMultiple R-squared:  0.2022,    Adjusted R-squared:  0.1865 \nF-statistic: 12.92 on 10 and 510 DF,  p-value: &lt; 0.00000000000000022\n\n\n\n# Relevel to change to the reference group to never user\ndata_wide_2$ADH_HIGHVSLOW &lt;- relevel(data_wide_2$ADH_HIGHVSLOW, ref = \"Low Adherence\")\n\n# Perform regression on Physical QOL change by hard drugs group, full model\nmodel_PHYS_full &lt;- lm(AGG_PHYS_CHANGE ~ hard_drugs_grp + ADH_HIGHVSLOW + hard_drugs_grp*ADH_HIGHVSLOW, data = data_wide_2)\n\n# Exmamine the summary\nsummary(model_PHYS_full)\n\n\nCall:\nlm(formula = AGG_PHYS_CHANGE ~ hard_drugs_grp + ADH_HIGHVSLOW + \n    hard_drugs_grp * ADH_HIGHVSLOW, data = data_wide_2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-36.596  -3.832   0.574   4.155  33.175 \n\nCoefficients:\n                                                        Estimate Std. Error\n(Intercept)                                               -3.964      1.443\nhard_drugs_grpCurrent User                                -9.263      4.389\nhard_drugs_grpPrevious User                              -10.506      3.679\nADH_HIGHVSLOWHigh Adherence                                2.890      1.503\nhard_drugs_grpCurrent User:ADH_HIGHVSLOWHigh Adherence     4.642      4.546\nhard_drugs_grpPrevious User:ADH_HIGHVSLOWHigh Adherence   11.967      3.928\n                                                        t value Pr(&gt;|t|)   \n(Intercept)                                              -2.747  0.00622 **\nhard_drugs_grpCurrent User                               -2.110  0.03529 * \nhard_drugs_grpPrevious User                              -2.856  0.00447 **\nADH_HIGHVSLOWHigh Adherence                               1.923  0.05498 . \nhard_drugs_grpCurrent User:ADH_HIGHVSLOWHigh Adherence    1.021  0.30771   \nhard_drugs_grpPrevious User:ADH_HIGHVSLOWHigh Adherence   3.046  0.00243 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.29 on 526 degrees of freedom\n  (18 observations deleted due to missingness)\nMultiple R-squared:  0.07391,   Adjusted R-squared:  0.06511 \nF-statistic: 8.396 on 5 and 526 DF,  p-value: 0.0000001181\n\n# Use interactions package to plot interaction\ncat_plot(model_PHYS_full, pred = hard_drugs_grp, modx = ADH_HIGHVSLOW, geom = \"line\", colors = \"Pastel2\")\n\n\n\n\n\n\n\n\nTop of Tabset\n\n\n\nLet’s investigate the key relationships in the final model for AGG_PHYS_CHANGE as determined through backwards selection.\nFirst let’s look at the impact of hard drug use on physical QOL at low adherence.\n\n# Relevel to change to the reference group to never user\ndata_wide_2$hard_drugs_grp&lt;- relevel(data_wide_2$hard_drugs_grp, ref = \"Never User\")\n\n# Relevel to change to the reference group to never user\ndata_wide_2$ADH_HIGHVSLOW &lt;- relevel(data_wide_2$ADH_HIGHVSLOW, ref = \"Low Adherence\")\n\n# Perform regression on phys QOL\nmodel_PHYS_final1 &lt;- lm(AGG_PHYS_CHANGE ~ hard_drugs_grp + ADH_HIGHVSLOW + FRP_2 + hard_drugs_grp*ADH_HIGHVSLOW, data = data_wide_2)\n\n# Examine the summary\nsummary(model_PHYS_final1)\n\n\nCall:\nlm(formula = AGG_PHYS_CHANGE ~ hard_drugs_grp + ADH_HIGHVSLOW + \n    FRP_2 + hard_drugs_grp * ADH_HIGHVSLOW, data = data_wide_2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-33.074  -4.227  -0.026   3.800  32.493 \n\nCoefficients:\n                                                        Estimate Std. Error\n(Intercept)                                               -3.578      1.351\nhard_drugs_grpCurrent User                                -9.650      4.106\nhard_drugs_grpPrevious User                              -10.893      3.442\nADH_HIGHVSLOWHigh Adherence                                3.185      1.406\nFRP_2Yes                                                 -12.756      1.463\nhard_drugs_grpCurrent User:ADH_HIGHVSLOWHigh Adherence     5.713      4.255\nhard_drugs_grpPrevious User:ADH_HIGHVSLOWHigh Adherence   12.309      3.675\n                                                        t value\n(Intercept)                                              -2.648\nhard_drugs_grpCurrent User                               -2.350\nhard_drugs_grpPrevious User                              -3.164\nADH_HIGHVSLOWHigh Adherence                               2.265\nFRP_2Yes                                                 -8.719\nhard_drugs_grpCurrent User:ADH_HIGHVSLOWHigh Adherence    1.343\nhard_drugs_grpPrevious User:ADH_HIGHVSLOWHigh Adherence   3.349\n                                                                    Pr(&gt;|t|)\n(Intercept)                                                         0.008328\nhard_drugs_grpCurrent User                                          0.019144\nhard_drugs_grpPrevious User                                         0.001644\nADH_HIGHVSLOWHigh Adherence                                         0.023912\nFRP_2Yes                                                &lt; 0.0000000000000002\nhard_drugs_grpCurrent User:ADH_HIGHVSLOWHigh Adherence              0.179911\nhard_drugs_grpPrevious User:ADH_HIGHVSLOWHigh Adherence             0.000868\n                                                           \n(Intercept)                                             ** \nhard_drugs_grpCurrent User                              *  \nhard_drugs_grpPrevious User                             ** \nADH_HIGHVSLOWHigh Adherence                             *  \nFRP_2Yes                                                ***\nhard_drugs_grpCurrent User:ADH_HIGHVSLOWHigh Adherence     \nhard_drugs_grpPrevious User:ADH_HIGHVSLOWHigh Adherence ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.756 on 525 degrees of freedom\n  (18 observations deleted due to missingness)\nMultiple R-squared:  0.1911,    Adjusted R-squared:  0.1818 \nF-statistic: 20.67 on 6 and 525 DF,  p-value: &lt; 0.00000000000000022\n\n\n\n# Print useful model parameters\nmodel_results(model_PHYS_final1)\n\n\n\n\n\nEstimate\nStd. Error\nt value\nPr(&gt;|t|)\np_adjusted\n2.5 %\n97.5 %\n\n\n\n\n(Intercept)\n-3.577634\n1.350818\n-2.648494\n0.0083285\n0.0582993\n-6.2313069\n-0.9239609\n\n\nhard_drugs_grpCurrent User\n-9.649910\n4.106380\n-2.349980\n0.0191437\n0.1340061\n-17.7168637\n-1.5829561\n\n\nhard_drugs_grpPrevious User\n-10.892966\n3.442355\n-3.164394\n0.0016441\n0.0115089\n-17.6554473\n-4.1304844\n\n\nADH_HIGHVSLOWHigh Adherence\n3.184855\n1.406039\n2.265126\n0.0239116\n0.1673814\n0.4227021\n5.9470082\n\n\nFRP_2Yes\n-12.755668\n1.462953\n-8.719122\n0.0000000\n0.0000000\n-15.6296286\n-9.8817067\n\n\nhard_drugs_grpCurrent User:ADH_HIGHVSLOWHigh Adherence\n5.713316\n4.254723\n1.342817\n0.1799114\n1.0000000\n-2.6450571\n14.0716887\n\n\nhard_drugs_grpPrevious User:ADH_HIGHVSLOWHigh Adherence\n12.309256\n3.675079\n3.349385\n0.0008681\n0.0060767\n5.0895895\n19.5289225\n\n\n\n\n\n\n\nThe overall model is highly significant (F~(6, 525)~ = 20.67, p &lt; 0.0001, adjusted R-squared = 0.1818).\nAt low adherence, previous hard drug users had on average a 10.89 point decrease in physical QOL compared to never hard drug users (t = -3.16, p-adjusted = 0.00493, 95% CI: -17.66 to -4.13). Current hard drug users did not differ significantly from never hard drug users at low adherence (t = -2.35, p-adjusted =0.0574, 95% CI: -17.72 to -1.58), although with a larger sample size that could become significant.\nFor never hard drug users, those with high adherence had a 3.19 point increase in physical QOL compared to those with low adherence (t = 2.27, p = 0.0239).\nCompare hard drug use groups at high adherence\n\n# Relevel to change to the reference group to never user\ndata_wide_2$hard_drugs_grp&lt;- relevel(data_wide_2$hard_drugs_grp, ref = \"Never User\")\n\n# Relevel to change to the reference group to never user\ndata_wide_2$ADH_HIGHVSLOW &lt;- relevel(data_wide_2$ADH_HIGHVSLOW, ref = \"High Adherence\")\n\n# Perform regression on phys QOL\nmodel_PHYS_final2 &lt;- lm(AGG_PHYS_CHANGE ~ hard_drugs_grp + ADH_HIGHVSLOW + FRP_2 + hard_drugs_grp*ADH_HIGHVSLOW, data = data_wide_2)\n\n# Examine the summary\nsummary(model_PHYS_final2)\n\n\nCall:\nlm(formula = AGG_PHYS_CHANGE ~ hard_drugs_grp + ADH_HIGHVSLOW + \n    FRP_2 + hard_drugs_grp * ADH_HIGHVSLOW, data = data_wide_2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-33.074  -4.227  -0.026   3.800  32.493 \n\nCoefficients:\n                                                       Estimate Std. Error\n(Intercept)                                             -0.3928     0.3990\nhard_drugs_grpCurrent User                              -3.9366     1.1106\nhard_drugs_grpPrevious User                              1.4163     1.2872\nADH_HIGHVSLOWLow Adherence                              -3.1849     1.4060\nFRP_2Yes                                               -12.7557     1.4630\nhard_drugs_grpCurrent User:ADH_HIGHVSLOWLow Adherence   -5.7133     4.2547\nhard_drugs_grpPrevious User:ADH_HIGHVSLOWLow Adherence -12.3093     3.6751\n                                                       t value\n(Intercept)                                             -0.985\nhard_drugs_grpCurrent User                              -3.545\nhard_drugs_grpPrevious User                              1.100\nADH_HIGHVSLOWLow Adherence                              -2.265\nFRP_2Yes                                                -8.719\nhard_drugs_grpCurrent User:ADH_HIGHVSLOWLow Adherence   -1.343\nhard_drugs_grpPrevious User:ADH_HIGHVSLOWLow Adherence  -3.349\n                                                                   Pr(&gt;|t|)    \n(Intercept)                                                        0.325315    \nhard_drugs_grpCurrent User                                         0.000428 ***\nhard_drugs_grpPrevious User                                        0.271705    \nADH_HIGHVSLOWLow Adherence                                         0.023912 *  \nFRP_2Yes                                               &lt; 0.0000000000000002 ***\nhard_drugs_grpCurrent User:ADH_HIGHVSLOWLow Adherence              0.179911    \nhard_drugs_grpPrevious User:ADH_HIGHVSLOWLow Adherence             0.000868 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.756 on 525 degrees of freedom\n  (18 observations deleted due to missingness)\nMultiple R-squared:  0.1911,    Adjusted R-squared:  0.1818 \nF-statistic: 20.67 on 6 and 525 DF,  p-value: &lt; 0.00000000000000022\n\n\n\n# Print useful model parameters\nmodel_results(model_PHYS_final2)\n\n\n\n\n\nEstimate\nStd. Error\nt value\nPr(&gt;|t|)\np_adjusted\n2.5 %\n97.5 %\n\n\n\n\n(Intercept)\n-0.3927788\n0.3989561\n-0.9845163\n0.3253152\n1.0000000\n-1.176525\n0.3909676\n\n\nhard_drugs_grpCurrent User\n-3.9365941\n1.1105611\n-3.5446893\n0.0004282\n0.0029973\n-6.118283\n-1.7549047\n\n\nhard_drugs_grpPrevious User\n1.4162901\n1.2871848\n1.1003005\n0.2717054\n1.0000000\n-1.112375\n3.9449554\n\n\nADH_HIGHVSLOWLow Adherence\n-3.1848551\n1.4060388\n-2.2651261\n0.0239116\n0.1673814\n-5.947008\n-0.4227021\n\n\nFRP_2Yes\n-12.7556677\n1.4629531\n-8.7191225\n0.0000000\n0.0000000\n-15.629629\n-9.8817067\n\n\nhard_drugs_grpCurrent User:ADH_HIGHVSLOWLow Adherence\n-5.7133158\n4.2547230\n-1.3428173\n0.1799114\n1.0000000\n-14.071689\n2.6450571\n\n\nhard_drugs_grpPrevious User:ADH_HIGHVSLOWLow Adherence\n-12.3092560\n3.6750790\n-3.3493854\n0.0008681\n0.0060767\n-19.528923\n-5.0895895\n\n\n\n\n\n\n\nAt high adherence, current hard drug users had on average a 3.94 point decrease in physical QOL compared to never hard drug users (t = -3.545, p-adjusted = 0.00128, 95% CI: -6.12 to -1.75). Additionally, previous hard drug users did not differ significantly from never hard drug users at high adherence (t = 1.100, p-adjusted = 0.815, 95% CI: -1.11 to 3.94).\nChange reference level to compare previous users\n\n# Relevel to change to the reference group to never user\ndata_wide_2$hard_drugs_grp&lt;- relevel(data_wide_2$hard_drugs_grp, ref = \"Previous User\")\n\n# Relevel to change to the reference group to never user\ndata_wide_2$ADH_HIGHVSLOW &lt;- relevel(data_wide_2$ADH_HIGHVSLOW, ref = \"Low Adherence\")\n\n# Perform regression on phys QOL\nmodel_PHYS_final3 &lt;- lm(AGG_PHYS_CHANGE ~ hard_drugs_grp + ADH_HIGHVSLOW + FRP_2 + hard_drugs_grp*ADH_HIGHVSLOW, data = data_wide_2)\n\n# Examine the summary\nsummary(model_PHYS_final3)\n\n\nCall:\nlm(formula = AGG_PHYS_CHANGE ~ hard_drugs_grp + ADH_HIGHVSLOW + \n    FRP_2 + hard_drugs_grp * ADH_HIGHVSLOW, data = data_wide_2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-33.074  -4.227  -0.026   3.800  32.493 \n\nCoefficients:\n                                                       Estimate Std. Error\n(Intercept)                                             -14.471      3.166\nhard_drugs_grpNever User                                 10.893      3.442\nhard_drugs_grpCurrent User                                1.243      5.006\nADH_HIGHVSLOWHigh Adherence                              15.494      3.396\nFRP_2Yes                                                -12.756      1.463\nhard_drugs_grpNever User:ADH_HIGHVSLOWHigh Adherence    -12.309      3.675\nhard_drugs_grpCurrent User:ADH_HIGHVSLOWHigh Adherence   -6.596      5.258\n                                                       t value\n(Intercept)                                             -4.570\nhard_drugs_grpNever User                                 3.164\nhard_drugs_grpCurrent User                               0.248\nADH_HIGHVSLOWHigh Adherence                              4.562\nFRP_2Yes                                                -8.719\nhard_drugs_grpNever User:ADH_HIGHVSLOWHigh Adherence    -3.349\nhard_drugs_grpCurrent User:ADH_HIGHVSLOWHigh Adherence  -1.254\n                                                                   Pr(&gt;|t|)    \n(Intercept)                                                      0.00000608 ***\nhard_drugs_grpNever User                                           0.001644 ** \nhard_drugs_grpCurrent User                                         0.804000    \nADH_HIGHVSLOWHigh Adherence                                      0.00000631 ***\nFRP_2Yes                                               &lt; 0.0000000000000002 ***\nhard_drugs_grpNever User:ADH_HIGHVSLOWHigh Adherence               0.000868 ***\nhard_drugs_grpCurrent User:ADH_HIGHVSLOWHigh Adherence             0.210242    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.756 on 525 degrees of freedom\n  (18 observations deleted due to missingness)\nMultiple R-squared:  0.1911,    Adjusted R-squared:  0.1818 \nF-statistic: 20.67 on 6 and 525 DF,  p-value: &lt; 0.00000000000000022\n\n\n\n# Print useful model parameters\nmodel_results(model_PHYS_final3)\n\n\n\n\n\nEstimate\nStd. Error\nt value\nPr(&gt;|t|)\np_adjusted\n2.5 %\n97.5 %\n\n\n\n\n(Intercept)\n-14.470600\n3.166243\n-4.5702741\n0.0000061\n0.0000426\n-20.690662\n-8.250537\n\n\nhard_drugs_grpNever User\n10.892966\n3.442355\n3.1643937\n0.0016441\n0.0115089\n4.130484\n17.655447\n\n\nhard_drugs_grpCurrent User\n1.243056\n5.006270\n0.2482998\n0.8039995\n1.0000000\n-8.591726\n11.077838\n\n\nADH_HIGHVSLOWHigh Adherence\n15.494111\n3.396206\n4.5621830\n0.0000063\n0.0000442\n8.822290\n22.165933\n\n\nFRP_2Yes\n-12.755668\n1.462953\n-8.7191225\n0.0000000\n0.0000000\n-15.629629\n-9.881707\n\n\nhard_drugs_grpNever User:ADH_HIGHVSLOWHigh Adherence\n-12.309256\n3.675079\n-3.3493854\n0.0008681\n0.0060767\n-19.528923\n-5.089589\n\n\nhard_drugs_grpCurrent User:ADH_HIGHVSLOWHigh Adherence\n-6.595940\n5.258100\n-1.2544342\n0.2102424\n1.0000000\n-16.925439\n3.733559\n\n\n\n\n\n\n\nAt low adherence, previous hard drug users did not differ significantly on change in physical QOL compared to current hard drug users (t = 0.25, p-adjusted = 1.00, 95% CI: -8.59 to 11.08).\nFor previous hard drug users, those with high adherence to the treatment regiment had on average a 15.49 point increase in physical QOL compared to those with low adherence (t = 4.56, p-adjusted &lt; 0.0001, 95% CI: 8.82 to 22.17).\nChange the reference level to current hard drug users.\n\n# Relevel to change to the reference group to never user\ndata_wide_2$hard_drugs_grp&lt;- relevel(data_wide_2$hard_drugs_grp, ref = \"Current User\")\n\n# Relevel to change to the reference group to never user\ndata_wide_2$ADH_HIGHVSLOW &lt;- relevel(data_wide_2$ADH_HIGHVSLOW, ref = \"Low Adherence\")\n\n# Perform regression on phys QOL\nmodel_PHYS_final4 &lt;- lm(AGG_PHYS_CHANGE ~ hard_drugs_grp + ADH_HIGHVSLOW + FRP_2 + hard_drugs_grp*ADH_HIGHVSLOW, data = data_wide_2)\n\n# Examine the summary\nsummary(model_PHYS_final4)\n\n\nCall:\nlm(formula = AGG_PHYS_CHANGE ~ hard_drugs_grp + ADH_HIGHVSLOW + \n    FRP_2 + hard_drugs_grp * ADH_HIGHVSLOW, data = data_wide_2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-33.074  -4.227  -0.026   3.800  32.493 \n\nCoefficients:\n                                                        Estimate Std. Error\n(Intercept)                                              -13.228      3.878\nhard_drugs_grpPrevious User                               -1.243      5.006\nhard_drugs_grpNever User                                   9.650      4.106\nADH_HIGHVSLOWHigh Adherence                                8.898      4.017\nFRP_2Yes                                                 -12.756      1.463\nhard_drugs_grpPrevious User:ADH_HIGHVSLOWHigh Adherence    6.596      5.258\nhard_drugs_grpNever User:ADH_HIGHVSLOWHigh Adherence      -5.713      4.255\n                                                        t value\n(Intercept)                                              -3.411\nhard_drugs_grpPrevious User                              -0.248\nhard_drugs_grpNever User                                  2.350\nADH_HIGHVSLOWHigh Adherence                               2.215\nFRP_2Yes                                                 -8.719\nhard_drugs_grpPrevious User:ADH_HIGHVSLOWHigh Adherence   1.254\nhard_drugs_grpNever User:ADH_HIGHVSLOWHigh Adherence     -1.343\n                                                                    Pr(&gt;|t|)\n(Intercept)                                                         0.000697\nhard_drugs_grpPrevious User                                         0.804000\nhard_drugs_grpNever User                                            0.019144\nADH_HIGHVSLOWHigh Adherence                                         0.027180\nFRP_2Yes                                                &lt; 0.0000000000000002\nhard_drugs_grpPrevious User:ADH_HIGHVSLOWHigh Adherence             0.210242\nhard_drugs_grpNever User:ADH_HIGHVSLOWHigh Adherence                0.179911\n                                                           \n(Intercept)                                             ***\nhard_drugs_grpPrevious User                                \nhard_drugs_grpNever User                                *  \nADH_HIGHVSLOWHigh Adherence                             *  \nFRP_2Yes                                                ***\nhard_drugs_grpPrevious User:ADH_HIGHVSLOWHigh Adherence    \nhard_drugs_grpNever User:ADH_HIGHVSLOWHigh Adherence       \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.756 on 525 degrees of freedom\n  (18 observations deleted due to missingness)\nMultiple R-squared:  0.1911,    Adjusted R-squared:  0.1818 \nF-statistic: 20.67 on 6 and 525 DF,  p-value: &lt; 0.00000000000000022\n\n\n\n# Print useful model parameters\nmodel_results(model_PHYS_final4)\n\n\n\n\n\nEstimate\nStd. Error\nt value\nPr(&gt;|t|)\np_adjusted\n2.5 %\n97.5 %\n\n\n\n\n(Intercept)\n-13.227544\n3.877840\n-3.4110595\n0.0006970\n0.0048787\n-20.845533\n-5.609554\n\n\nhard_drugs_grpPrevious User\n-1.243056\n5.006270\n-0.2482998\n0.8039995\n1.0000000\n-11.077838\n8.591726\n\n\nhard_drugs_grpNever User\n9.649910\n4.106380\n2.3499799\n0.0191437\n0.1340061\n1.582956\n17.716864\n\n\nADH_HIGHVSLOWHigh Adherence\n8.898171\n4.017005\n2.2151255\n0.0271799\n0.1902593\n1.006793\n16.789549\n\n\nFRP_2Yes\n-12.755668\n1.462953\n-8.7191225\n0.0000000\n0.0000000\n-15.629629\n-9.881707\n\n\nhard_drugs_grpPrevious User:ADH_HIGHVSLOWHigh Adherence\n6.595940\n5.258100\n1.2544342\n0.2102424\n1.0000000\n-3.733559\n16.925439\n\n\nhard_drugs_grpNever User:ADH_HIGHVSLOWHigh Adherence\n-5.713316\n4.254723\n-1.3428173\n0.1799114\n1.0000000\n-14.071689\n2.645057\n\n\n\n\n\n\n\nFor current hard drug users, those with high adherence had on average an 8.90 point increase in physical QOL compared to those with low adherence (t = 2.22, p = 0.027, 95% CI: 1.00 to 16.79).\nChange reference level to high adherence to compare last group.\n\n# Relevel to change to the reference group to never user\ndata_wide_2$hard_drugs_grp&lt;- relevel(data_wide_2$hard_drugs_grp, ref = \"Current User\")\n\n# Relevel to change to the reference group to never user\ndata_wide_2$ADH_HIGHVSLOW &lt;- relevel(data_wide_2$ADH_HIGHVSLOW, ref = \"High Adherence\")\n\n# Perform regression on phys QOL\nmodel_PHYS_final5 &lt;- lm(AGG_PHYS_CHANGE ~ hard_drugs_grp + ADH_HIGHVSLOW + FRP_2 + hard_drugs_grp*ADH_HIGHVSLOW, data = data_wide_2)\n\n# Examine the summary\nsummary(model_PHYS_final5)\n\n\nCall:\nlm(formula = AGG_PHYS_CHANGE ~ hard_drugs_grp + ADH_HIGHVSLOW + \n    FRP_2 + hard_drugs_grp * ADH_HIGHVSLOW, data = data_wide_2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-33.074  -4.227  -0.026   3.800  32.493 \n\nCoefficients:\n                                                       Estimate Std. Error\n(Intercept)                                              -4.329      1.048\nhard_drugs_grpPrevious User                               5.353      1.608\nhard_drugs_grpNever User                                  3.937      1.111\nADH_HIGHVSLOWLow Adherence                               -8.898      4.017\nFRP_2Yes                                                -12.756      1.463\nhard_drugs_grpPrevious User:ADH_HIGHVSLOWLow Adherence   -6.596      5.258\nhard_drugs_grpNever User:ADH_HIGHVSLOWLow Adherence       5.713      4.255\n                                                       t value\n(Intercept)                                             -4.130\nhard_drugs_grpPrevious User                              3.329\nhard_drugs_grpNever User                                 3.545\nADH_HIGHVSLOWLow Adherence                              -2.215\nFRP_2Yes                                                -8.719\nhard_drugs_grpPrevious User:ADH_HIGHVSLOWLow Adherence  -1.254\nhard_drugs_grpNever User:ADH_HIGHVSLOWLow Adherence      1.343\n                                                                   Pr(&gt;|t|)    \n(Intercept)                                                       0.0000421 ***\nhard_drugs_grpPrevious User                                        0.000931 ***\nhard_drugs_grpNever User                                           0.000428 ***\nADH_HIGHVSLOWLow Adherence                                         0.027180 *  \nFRP_2Yes                                               &lt; 0.0000000000000002 ***\nhard_drugs_grpPrevious User:ADH_HIGHVSLOWLow Adherence             0.210242    \nhard_drugs_grpNever User:ADH_HIGHVSLOWLow Adherence                0.179911    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.756 on 525 degrees of freedom\n  (18 observations deleted due to missingness)\nMultiple R-squared:  0.1911,    Adjusted R-squared:  0.1818 \nF-statistic: 20.67 on 6 and 525 DF,  p-value: &lt; 0.00000000000000022\n\n\n\n# Print useful model parameters\nmodel_results(model_PHYS_final5)\n\n\n\n\n\nEstimate\nStd. Error\nt value\nPr(&gt;|t|)\np_adjusted\n2.5 %\n97.5 %\n\n\n\n\n(Intercept)\n-4.329373\n1.048183\n-4.130362\n0.0000421\n0.0002950\n-6.388520\n-2.270226\n\n\nhard_drugs_grpPrevious User\n5.352884\n1.607753\n3.329419\n0.0009314\n0.0065198\n2.194464\n8.511304\n\n\nhard_drugs_grpNever User\n3.936594\n1.110561\n3.544689\n0.0004282\n0.0029973\n1.754905\n6.118283\n\n\nADH_HIGHVSLOWLow Adherence\n-8.898171\n4.017005\n-2.215126\n0.0271799\n0.1902593\n-16.789549\n-1.006793\n\n\nFRP_2Yes\n-12.755668\n1.462953\n-8.719122\n0.0000000\n0.0000000\n-15.629629\n-9.881707\n\n\nhard_drugs_grpPrevious User:ADH_HIGHVSLOWLow Adherence\n-6.595940\n5.258100\n-1.254434\n0.2102424\n1.0000000\n-16.925439\n3.733559\n\n\nhard_drugs_grpNever User:ADH_HIGHVSLOWLow Adherence\n5.713316\n4.254723\n1.342817\n0.1799114\n1.0000000\n-2.645057\n14.071689\n\n\n\n\n\n\n\nAt high adherence, current hard drug users had on average a change in physical QOL that was 5.35 points less than previous hard drug users (t = 3.33, p-adjusted = 0.00279, 95% CI: 2.19 to 8.51).\nTop of Tabset\n\n\n\nOverall Model\nThere were significant differences in change in physical QOL over 2 years based on hard drug usage and adherence to the treatment regiment, while controlling for Frailty Related Phenotype (F~(6, 525)~ = 20.67, p &lt; 0.0001, adjusted R-squared = 0.1818).\n\nHard Drug Use\nThe relationship between hard drug use and change in physical QOL over 2 years depended on adherence to the treatment regiment.\n\n\nHard Drug Use at Low Adherence\nAt low adherence, previous hard drug users had on average a 10.89 point decrease in physical QOL compared to never hard drug users (t = -3.16, p-adjusted = 0.00493, 95% CI: -17.66 to -4.13), but did not differ significantly on change in physical QOL compared to current hard drug users (t = 0.25, p-adjusted = 1.00, 95% CI: -8.59 to 11.08). . Current hard drug users did not differ significantly from never hard drug users at low adherence (t = -2.35, p-adjusted =0.0574, 95% CI: -17.72 to -1.58), although with a larger sample size that could become significant.\n\n\nHard Drug Use at High Adherence\nAt high adherence, current hard drug users had on average a 3.94 point decrease in physical QOL compared to never hard drug users (t = -3.545, p-adjusted = 0.00128, 95% CI: -6.12 to -1.75), and 5.35 points decrease compared to previous hard drug users (t = 3.33, p-adjusted = 0.00279, 95% CI: 2.19 to 8.51). Additionally, previous hard drug users did not differ significantly from never hard drug users at high adherence (t = 1.100, p-adjusted = 0.815, 95% CI: -1.11 to 3.94).\n\n\n\nAdherence\nThe relationship between adherence and change in physical QOL over 2 years depended on hard drug use.\n\nAdherence for Current Hard Drug Users\nFor current hard drug users, those with high adherence had on average an 8.90 point increase in physical QOL compared to those with low adherence (t = 2.22, p = 0.027, 95% CI: 1.00 to 16.79).\n\n\nAdherence for Previous Hard Drug Users\nFor previous hard drug users, those with high adherence to the treatment regiment had on average a 15.49 point increase in physical QOL compared to those with low adherence (t = 4.56, p-adjusted &lt; 0.0001, 95% CI: 8.82 to 22.17).\n\n\nAdherence for Never Hard Drug Users\nFor never hard drug users, those with high adherence had a 3.19 point increase in physical QOL compared to those with low adherence (t = 2.27, p = 0.0239).\n\n\n\nFrailty Related Phenotype\nFrailty related phenotype was a significant predictor of change in physical QOL over 2 years (t = -8.72, p &lt; 0.0001). On average, those with a frailty related phenotype had a 12.76 decrease in physical QOL compared to those without a frailty related phenotype (95% CI: -15.63 to -9.88).\nTop of Tabset\n\n\n\nThe below plot is useful in interpreting the interaction.\n\n# Use interactions package to plot interaction\ncat_plot(\n  model_PHYS_final1, \n  pred = hard_drugs_grp, \n  modx = ADH_HIGHVSLOW, \n  geom = \"line\", \n  colors = \"Pastel2\",\n  x.label = \"Hard Drugs Group\",\n  y.label = \"Predicted Physical QOL Change\")\n\n\n\n\n\n\n\n\nFor those with low adherence, previous users had a greater decrease in physical QOL compared to never users. The difference between current and never hard drug users at low adherence is borderline significant, and may be so with a larger sample size.\nFor those with high adherence, the current hard drug use group had a greater decrease in physical QOL compared to both never and previous hard drug users.\nAnd most importantly, for previous hard drug users, those with low adherence had a decrease in physical QOL when compared to those with high adherence.\nWe take this as further evidence that previous hard drug users are at a vulnerable population. High adherence serves as a buffer and could protect patients that have quit heroin or opiates within the past 2 years.\nAdditionally, we have evidence that even at high adherence, current hard drug users have worse overall physical QOL."
  },
  {
    "objectID": "Project_2/Project_2_R/Code/Project2.html#log-viral-load-1",
    "href": "Project_2/Project_2_R/Code/Project2.html#log-viral-load-1",
    "title": "Advanced Data Analysis - Project 2",
    "section": "Log Viral Load",
    "text": "Log Viral Load\n\nLinearityIndependenceNormalityEqual Variances (Homoscedasticity)Residuals Centered Around ZeroSummary\n\n\nSince the mains IVs are categorical, we do not need to evaluate linearity.\nTop of Tabset\n\n\nIndependence can be assessed in part by the study design and how the data was collected. Based on the information provided by the PI, I will assume subjects are independent from each other.\nAdditionally, we can examine a scatter plot of the model’s residuals against any time point variable (such as ID).\n\ndata_VLOAD &lt;- data_wide_2 %&gt;%\n  dplyr::select(newid, VLOAD_log_CHANGE, hard_drugs_grp, ADH_HIGHVSLOW, EDUC_COLLEGE) %&gt;%\n  filter(\n    !is.na(newid) & \n    !is.na(VLOAD_log_CHANGE) &\n    !is.na(hard_drugs_grp) & \n    !is.na(ADH_HIGHVSLOW) & \n    !is.na(EDUC_COLLEGE) \n  )\n\n# Create a scatterplot of jackknife residuals vs ID to assess independence \nggplot(data_VLOAD, aes(x = newid, y = jackknife_residuals_VLOAD)) + \n  geom_point() +\n  geom_hline(yintercept = 0) +\n  labs(title = \"Scatterplot of Jackknife Residuals vs ID for Log Viral Load\",\n       x = \"ID\",\n       y = \"Jackknife Residuals\")\n\n\n\n\n\n\n\n\nWe may have some non-independence as seen by the further spread above 0.\nTop of Tabset\n\n\nHere we will asses that, for any fixed value of X, Y has a normal distribution. We will do this using Q-Q plots and histograms of the residuals.\n\n# Make the Q-Q plots using the jackknife residuals\nqqnorm(jackknife_residuals_VLOAD, main = \"Q-Q plots of Jackknife Residuals for Log Viral Load\")\nqqline(jackknife_residuals_VLOAD, col = \"black\")\n\n\n\n\n\n\n\n# Create histogram of jackknife residuals\nhist(jackknife_residuals_VLOAD, main = \"Histogram of Jackknife Residuals\",\n     breaks = 24,\n     xlab = \"Jackknife Residuals\",\n     col = \"lightblue\",\n     border = \"black\")\n\n\n\n\n\n\n\n\nWe can also include the Shapiro-Wilk test of normality, which provides a p-value and allows us to numerically establish that the assumption of normality is met. If the p-value is &lt; 0.05 you conclude that the assumption of normality is not met.\n\nshapiro.test(jackknife_residuals_VLOAD)\n\n\n    Shapiro-Wilk normality test\n\ndata:  jackknife_residuals_VLOAD\nW = 0.97025, p-value = 0.000000008927\n\n\nWe have some non-normality as seen by the higher end of the Q-Q Plot.\nLooking at the histograms, the pattern is slightly bimodal, and may have some outliers at the high range.\nWe also fail Shapiro-Wilk’s test, indicating non-normality.\nTop of Tabset\n\n\nTo assess homoscedasticity we examine the residual scatterplots by treatment group. The warning sign to look for here is if the variance differs greatly across groups.\n\n# Make the residual scatterplot using the jackknife residuals\nggplot(data_VLOAD, aes(x = hard_drugs_grp, y = jackknife_residuals_VLOAD)) +\n  geom_boxplot() + \n  labs(title = \"Jackknife Residuals vs Hard Drug Use for Log Viral Load\",\n       x = \"Hard Drug Use Group\",\n       y = \"JackKnife Residuals\")\n\n\n\n\n\n\n\n\n\n# Make the residual scatterplot using the jackknife residuals\nggplot(data_VLOAD, aes(x = ADH_HIGHVSLOW, y = jackknife_residuals_VLOAD)) +\n  geom_boxplot() + \n  labs(title = \"Jackknife Residuals vs Adherence for Log Viral Load\",\n       x = \"Adherence\",\n       y = \"JackKnife Residuals\")\n\n\n\n\n\n\n\n\nIt appears that our hard drug use groups have unequal variances.\nAdditionally, while it is not recommended to perform a statistical test to assess for equality of variances (because formal tests of equality of variance are not very powerful), we can still do this using Bartlett’s test. The null hypothesis of the Bartlett test is that the variances are equal. Thus failing to reject the null (p &gt; 0.05) indicates that the data are consistent with the equal variance assumption.\n\n# Perform Bartlett's test to check for homogeneity of variances\nbartlett.test(VLOAD_log_CHANGE ~ hard_drugs_grp, data = data_VLOAD)\n\n\n    Bartlett test of homogeneity of variances\n\ndata:  VLOAD_log_CHANGE by hard_drugs_grp\nBartlett's K-squared = 23.563, df = 2, p-value = 0.000007644\n\n\nWe also do not meet the assumption for equality of variances (p &lt; 0.05).\nThis is likely because our patients were not randomly assigned into these groups!\nTop of Tabset\n\n\nTo start, we can simply check that the mean of our residuals is close to 0.\n\n# Generate the mean of the jackknife residuals. Should be close to 0.\nmean(jackknife_residuals_VLOAD)\n\n[1] 0.0004681325\n\n# Generate vectors containing the fitted values vs jackknife residuals so we can plot them \nfitted_values_VLOAD &lt;- fitted(model_VLOAD_full1)\n\nggplot(data_VLOAD, aes(x = fitted_values_VLOAD, y = jackknife_residuals_VLOAD)) + \n  geom_point() +\n  geom_smooth(method = \"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nWe definitely have outliers present.\nThe mean is close to 0.\nA more sophisticated approach is to plot the fitted values vs jackknife residuals. We can then compare the trend between groups to see if they are random. The fitted line should gravitate around 0 with no obvious trends.\nTop of Tabset\n\n\nTop of Tabset"
  },
  {
    "objectID": "Project_2/Project_2_R/Code/Project2.html#cd4-t-cell-count-3",
    "href": "Project_2/Project_2_R/Code/Project2.html#cd4-t-cell-count-3",
    "title": "Advanced Data Analysis - Project 2",
    "section": "CD4+ T Cell Count",
    "text": "CD4+ T Cell Count\n\nLinearityIndependenceNormalityEqual Variances (Homoscedasticity)Residuals Centered Around ZeroSummary\n\n\nWe do not need to assess linearity since our main PEVs are categorical.\nTop of Tabset\n\n\nIndependence can be assessed in part by the study design and how the data was collected. Based on the information provided by the PI, I will assume subjects are independent from each other.\nAdditionally, we can examine a scatter plot of the model’s residuals against any time point variable (such as ID).\n\n# Filter to create same data set as in the final analysis\ndata_LEU3N &lt;- data_wide_2 %&gt;%\n  dplyr::select(newid, LEU3N_CHANGE, hard_drugs_grp, ADH_HIGHVSLOW, FRP_2) %&gt;%\n  filter(\n    !is.na(newid) & \n    !is.na(LEU3N_CHANGE) &\n    !is.na(hard_drugs_grp) & \n    !is.na(ADH_HIGHVSLOW) & \n    !is.na(FRP_2) \n  )\n\n# Create a scatterplot of jackknife residuals vs ID to assess independence \nggplot(data_LEU3N, aes(x = newid, y = jackknife_residuals_LEU3N)) + \n  geom_point() +\n  geom_hline(yintercept = 0) +\n  labs(title = \"Scatterplot of Jackknife Residuals vs ID for CD4+ T Cells\",\n       x = \"ID\",\n       y = \"Jackknife Residuals\")\n\n\n\n\n\n\n\n\nLooks like we have 4 outlier points beyound 3 residuals from 0, but otherwise looking good.\nTop of Tabset\n\n\nHere we will asses that, for any fixed value of X, Y has a normal distribution. We will do this using Q-Q plots and histograms of the residuals.\n\n# Make the Q-Q plots using the jackknife residuals\nqqnorm(jackknife_residuals_LEU3N, main = \"Q-Q plots of Jackknife Residuals for Log Viral Load\")\nqqline(jackknife_residuals_LEU3N, col = \"black\")\n\n\n\n\n\n\n\n# Create histogram of jackknife residuals\nhist(jackknife_residuals_LEU3N, main = \"Histogram of Jackknife Residuals\",\n     breaks = 24,\n     xlab = \"Jackknife Residuals\",\n     col = \"lightblue\",\n     border = \"black\")\n\n\n\n\n\n\n\n\nWe look very normal here, sans some obvious outliers.\n\nshapiro.test(jackknife_residuals_LEU3N)\n\n\n    Shapiro-Wilk normality test\n\ndata:  jackknife_residuals_LEU3N\nW = 0.97052, p-value = 0.00000001042\n\n\nWe have non-normality, but that is due to outliers.\nTop of Tabset\n\n\nTo assess homoscedasticity we examine the residual scatterplots by treatment group. The warning sign to look for here is if the variance differs greatly across groups.\n\n# Make the residual scatterplot using the jackknife residuals\nggplot(data_LEU3N, aes(x = hard_drugs_grp, y = jackknife_residuals_LEU3N)) +\n  geom_boxplot() + \n  labs(title = \"Jackknife Residuals vs Hard Drug Use for CD4+ T Cell Count Change\",\n       x = \"Hard Drug Use Group\",\n       y = \"JackKnife Residuals\")\n\n\n\n\n\n\n\n# Make the residual scatterplot using the jackknife residuals\nggplot(data_LEU3N, aes(x = ADH_HIGHVSLOW, y = jackknife_residuals_LEU3N)) +\n  geom_boxplot() + \n  labs(title = \"Jackknife Residuals vs Adherence for CD4+ T Cell Count Change\",\n       x = \"Hard Drug Use Group\",\n       y = \"JackKnife Residuals\")\n\n\n\n\n\n\n\n\n\n# Perform Bartlett's test to check for homogeneity of variances\nbartlett.test(LEU3N_CHANGE ~ hard_drugs_grp, data = data_LEU3N)\n\n\n    Bartlett test of homogeneity of variances\n\ndata:  LEU3N_CHANGE by hard_drugs_grp\nBartlett's K-squared = 5.8811, df = 2, p-value = 0.05284\n\n\nWe can actually conclude that we have equality of variances! This will look even better once we get rid of outliers.\nTop of Tabset\n\n\nto start, we can simply check that the mean of our residuals is close to 0.\n\n# Generate the mean of the jackknife residuals. Should be close to 0.\nmean(jackknife_residuals_LEU3N)\n\n[1] 0.0004433429\n\n# Generate vectors containing the fitted values vs jackknife residuals so we can plot them \nfitted_values_LEU3N &lt;- fitted(model_LEU3N_final1)\n\nggplot(data_LEU3N, aes(x = fitted_values_LEU3N, y = jackknife_residuals_LEU3N)) + \n  geom_point() +\n  geom_smooth(method = \"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nThe mean is close to 0. The residuals look centered around 0 excepting the outliers.\nTop of Tabset\n\n\nTop of Tabset"
  },
  {
    "objectID": "Project_2/Project_2_R/Code/Project2.html#mental-quality-of-life",
    "href": "Project_2/Project_2_R/Code/Project2.html#mental-quality-of-life",
    "title": "Advanced Data Analysis - Project 2",
    "section": "Mental Quality of Life",
    "text": "Mental Quality of Life\n\nLinearityIndependenceNormalityEqual Variances (Homoscedasticity)Residuals Centered Around ZeroSummary\n\n\nWe do not need to assess linearity since our main PEVs are categorical.\nTop of Tabset\n\n\nIndependence can be assessed in part by the study design and how the data was collected. Based on the information provided by the PI, I will assume subjects are independent from each other.\nAdditionally, we can examine a scatter plot of the model’s residuals against any time point variable (such as ID).\n\n# Filter to create same data set as in the final analysis\ndata_MENT_main &lt;- data_wide_2 %&gt;%\n  dplyr::select(newid, AGG_MENT_CHANGE, hard_drugs_grp, ADH_HIGHVSLOW) %&gt;%\n  filter(\n    !is.na(newid) & \n    !is.na(AGG_MENT_CHANGE) &\n    !is.na(hard_drugs_grp) & \n    !is.na(ADH_HIGHVSLOW) \n  )\n\n# Create a scatterplot of jackknife residuals vs ID to assess independence \nggplot(data_MENT_main, aes(x = newid, y = jackknife_residuals_MENT_main)) + \n  geom_point() +\n  geom_hline(yintercept = 0) +\n  labs(title = \"Scatterplot of Jackknife Residuals vs ID for Mental QOL Change\",\n       x = \"ID\",\n       y = \"Jackknife Residuals\")\n\n\n\n\n\n\n\n\nNo clear pattern, we meet the assumption of independence.\nTop of Tabset\n\n\nHere we will asses that, for any fixed value of X, Y has a normal distribution. We will do this using Q-Q plots and histograms of the residuals.\n\n# Make the Q-Q plots using the jackknife residuals\nqqnorm(jackknife_residuals_MENT_main, main = \"Q-Q plots of Jackknife Residuals for Mental QOL Change\")\nqqline(jackknife_residuals_MENT_main, col = \"black\")\n\n\n\n\n\n\n\n# Create histogram of jackknife residuals\nhist(jackknife_residuals_MENT_main, main = \"Histogram of Jackknife Residuals\",\n     breaks = 24,\n     xlab = \"Jackknife Residuals\",\n     col = \"lightblue\",\n     border = \"black\")\n\n\n\n\n\n\n\n\n\nshapiro.test(jackknife_residuals_MENT_main)\n\n\n    Shapiro-Wilk normality test\n\ndata:  jackknife_residuals_MENT_main\nW = 0.96193, p-value = 0.0000000001675\n\n\nWe have non-normality. It does not look like outliers are driving this.\nTop of Tabset\n\n\nTo assess homoscedasticity we examine the residual scatterplots by treatment group. The warning sign to look for here is if the variance differs greatly across groups.\n\n# Make the residual scatterplot using the jackknife residuals\nggplot(data_MENT_main, aes(x = hard_drugs_grp, y = jackknife_residuals_MENT_main)) +\n  geom_boxplot() + \n  labs(title = \"Jackknife Residuals vs Hard Drug Use for Mental QOL Change\",\n       x = \"Hard Drug Use Group\",\n       y = \"JackKnife Residuals\")\n\n\n\n\n\n\n\n\n\n# Make the residual scatterplot using the jackknife residuals\nggplot(data_MENT_main, aes(x = ADH_HIGHVSLOW, y = jackknife_residuals_MENT_main)) +\n  geom_boxplot() + \n  labs(title = \"Jackknife Residuals vs Adherence for Mental QOL Change\",\n       x = \"Hard Drug Use Group\",\n       y = \"JackKnife Residuals\")\n\n\n\n\n\n\n\n\n\n# Perform Bartlett's test to check for homogeneity of variances\nbartlett.test(AGG_MENT_CHANGE ~ hard_drugs_grp, data = data_MENT_main)\n\n\n    Bartlett test of homogeneity of variances\n\ndata:  AGG_MENT_CHANGE by hard_drugs_grp\nBartlett's K-squared = 19.522, df = 2, p-value = 0.00005765\n\n\nThe test is significant, and we conclude we do not have homogeneity of variances.\nTop of Tabset\n\n\n\n# Generate the mean of the jackknife residuals. Should be close to 0.\nmean(jackknife_residuals_MENT_main)\n\n[1] 0.0001715436\n\n# Generate vectors containing the fitted values vs jackknife residuals so we can plot them \nfitted_values_MENT_main &lt;- fitted(model_MENT_main1)\n\nggplot(data_MENT_main, aes(x = fitted_values_MENT_main, y = jackknife_residuals_MENT_main)) + \n  geom_point() +\n  geom_smooth(method = \"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nOur residuals look centered around 0, sans a few outlier points.\nTop of Tabset\n\n\nWe have non-normality and non homogeneity of variances.\nTop of Tabset"
  },
  {
    "objectID": "Project_2/Project_2_R/Code/Project2.html#physical-quality-of-life",
    "href": "Project_2/Project_2_R/Code/Project2.html#physical-quality-of-life",
    "title": "Advanced Data Analysis - Project 2",
    "section": "Physical Quality of Life",
    "text": "Physical Quality of Life\n\nLinearityIndependenceNormalityEqual Variances (Homoscedasticity)Residuals Centered Around ZeroSummaryResiduals Centered Around ZeroSummary\n\n\nWe do not need to assess linearity since our main PEVs are categorical.\nTop of Tabset\n\n\nIndependence can be assessed in part by the study design and how the data was collected. Based on the information provided by the PI, I will assume subjects are independent from each other.\nAdditionally, we can examine a scatter plot of the model’s residuals against any time point variable (such as ID).\n\n# Filter to create same data set as in the final analysis\ndata_PHYS &lt;- data_wide_2 %&gt;%\n  dplyr::select(newid, AGG_PHYS_CHANGE, hard_drugs_grp, ADH_HIGHVSLOW, FRP_2) %&gt;%\n  filter(\n    !is.na(newid) & \n    !is.na(AGG_PHYS_CHANGE) &\n    !is.na(hard_drugs_grp) & \n    !is.na(ADH_HIGHVSLOW) &\n    !is.na(FRP_2)\n  )\n\n# Create a scatterplot of jackknife residuals vs ID to assess independence \nggplot(data_PHYS, aes(x = newid, y = jackknife_residuals_PHYS)) + \n  geom_point() +\n  geom_hline(yintercept = 0) +\n  labs(title = \"Scatterplot of Jackknife Residuals vs ID for Physical QOL Change\",\n       x = \"ID\",\n       y = \"Jackknife Residuals\")\n\n\n\n\n\n\n\n\nIndependence looks pretty good.\nTop of Tabset\n\n\nHere we will asses that, for any fixed value of X, Y has a normal distribution. We will do this using Q-Q plots and histograms of the residuals.\n\n# Make the Q-Q plots using the jackknife residuals\nqqnorm(jackknife_residuals_PHYS, main = \"Q-Q plots of Jackknife Residuals for Mental QOL Change\")\nqqline(jackknife_residuals_PHYS, col = \"black\")\n\n\n\n\n\n\n\n# Create histogram of jackknife residuals\nhist(jackknife_residuals_PHYS, main = \"Histogram of Jackknife Residuals\",\n     breaks = 24,\n     xlab = \"Jackknife Residuals\",\n     col = \"lightblue\",\n     border = \"black\")\n\n\n\n\n\n\n\nshapiro.test(jackknife_residuals_PHYS)\n\n\n    Shapiro-Wilk normality test\n\ndata:  jackknife_residuals_PHYS\nW = 0.9682, p-value = 0.000000002488\n\n\nPhysical QOL looks mostly normal. There may be 1 outier at the upper and lower bound.\nShapiro Wilk’s does say we fail normality however.\nTop of Tabset\n\n\nTo assess homoscedasticity we examine the residual scatterplots by treatment group. The warning sign to look for here is if the variance differs greatly across groups.\n\n# Make the residual scatterplot using the jackknife residuals\nggplot(data_PHYS, aes(x = hard_drugs_grp, y = jackknife_residuals_PHYS)) +\n  geom_boxplot() + \n  labs(title = \"Jackknife Residuals vs Hard Drug Use for Physical QOL Change\",\n       x = \"Hard Drug Use Group\",\n       y = \"JackKnife Residuals\")\n\n\n\n\n\n\n\n# Perform Bartlett's test to check for homogeneity of variances\nbartlett.test(AGG_PHYS_CHANGE ~ hard_drugs_grp, data = data_PHYS)\n\n\n    Bartlett test of homogeneity of variances\n\ndata:  AGG_PHYS_CHANGE by hard_drugs_grp\nBartlett's K-squared = 4.2398, df = 2, p-value = 0.12\n\n\nWe do have homogeneity of variances for physical QOL.\nTop of Tabset\n\n\n\n# Generate the mean of the jackknife residuals. Should be close to 0.\nmean(jackknife_residuals_PHYS)\n\n[1] 0.0002469251\n\n# Generate vectors containing the fitted values vs jackknife residuals so we can plot them \nfitted_values_PHYS &lt;- fitted(model_PHYS_final1)\n\nggplot(data_PHYS, aes(x = fitted_values_PHYS, y = jackknife_residuals_PHYS)) + \n  geom_point() +\n  geom_smooth(method = \"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nTop of Tabset\n\n\nThe assumptions for CD4+ T cell count are almost met. Likely once removing those 4 or so outliers they would be perfect.\nTop of Tabset\nTo assess homoscedasticity we examine the residual scatterplots by treatment group. The warning sign to look for here is if the variance differs greatly across groups.\n\n# Make the residual scatterplot using the jackknife residuals\nggplot(data_PHYS, aes(x = hard_drugs_grp, y = jackknife_residuals_PHYS)) +\n  geom_boxplot() + \n  labs(title = \"Jackknife Residuals vs Hard Drug Use for Physical QOL Change\",\n       x = \"Hard Drug Use Group\",\n       y = \"JackKnife Residuals\")\n\n\n\n\n\n\n\n\n\n# Make the residual scatterplot using the jackknife residuals\nggplot(data_PHYS, aes(x = ADH_HIGHVSLOW, y = jackknife_residuals_PHYS)) +\n  geom_boxplot() + \n  labs(title = \"Jackknife Residuals vs Adherence for Physical QOL Change\",\n       x = \"Hard Drug Use Group\",\n       y = \"JackKnife Residuals\")\n\n\n\n\n\n\n\n\n\n# Perform Bartlett's test to check for homogeneity of variances\nbartlett.test(AGG_PHYS_CHANGE ~ hard_drugs_grp, data = data_PHYS)\n\n\n    Bartlett test of homogeneity of variances\n\ndata:  AGG_PHYS_CHANGE by hard_drugs_grp\nBartlett's K-squared = 4.2398, df = 2, p-value = 0.12\n\n\nWe do have homogeneity of variances.\n\n\n\n# Generate the mean of the jackknife residuals. Should be close to 0.\nmean(jackknife_residuals_PHYS)\n\n[1] 0.0002469251\n\n# Generate vectors containing the fitted values vs jackknife residuals so we can plot them \nfitted_values_PHYS &lt;- fitted(model_PHYS_final1)\n\nggplot(data_PHYS, aes(x = fitted_values_PHYS, y = jackknife_residuals_PHYS)) + \n  geom_point() +\n  geom_smooth(method = \"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nThe residuals appear centered around zero, for the most part.\n\n\nWe do not meet the assumption of normality for physical QOL."
  },
  {
    "objectID": "Project_2/Project_2_R/Code/Project2.html#plotting-outcome-variables-across-full-8-years",
    "href": "Project_2/Project_2_R/Code/Project2.html#plotting-outcome-variables-across-full-8-years",
    "title": "Advanced Data Analysis - Project 2",
    "section": "Plotting Outcome Variables Across Full 8 Years",
    "text": "Plotting Outcome Variables Across Full 8 Years\nThe below plots were used as justification and evidence for separating adherence from four levels to two levels, as this seemed to be whether the most meaninfgul and true split was.\n\nMental QOL\n\n# Get rid of that value of 1 for adherence at baseline for patient 426\ndata$ADH[data$years == 0] &lt;- NA\n\n# Get means for each year\nsummary_data &lt;- data %&gt;%\n  group_by(ADH, years) %&gt;%\n  summarize(Average_QOL = mean(AGG_MENT, na.rm = TRUE))\n\n`summarise()` has grouped output by 'ADH'. You can override using the `.groups`\nargument.\n\n# Create plot\np &lt;- ggplot(summary_data, aes(x = years, y = Average_QOL, color = ADH)) +\n  geom_line(size = 1) +   # Line plot\n  geom_point(size = 2) +  # Add points for clarity\n  labs(title = \"Average Mental QOL Score by Adherence Group Over 8 Years\",\n       x = \"Year\",\n       y = \"Average Mental QOL Score\",\n       color = \"Adherence Group\") +\n  theme_minimal() +       # Clean theme\n  scale_color_brewer(palette = \"Pastel2\")  # Nice color palette for groups\n\nggplotly(p)\n\n\n\n\n\nSuccess! This is very interesting. It appears that the 100% adherence group has the highest average mental QOL score across the 8 year study, followed by the 95-99% group, followed by the &lt;75% group and followed by (with some variation) the &lt;75% and 75-94% groups.\n\n\nPhysical QOL\nLet’s do the same thing but for AGG_PHYS.\n\n# Get means for each year\nsummary_data &lt;- data %&gt;%\n  group_by(ADH, years) %&gt;%\n  summarize(Average_QOL = mean(AGG_PHYS, na.rm = TRUE))\n\n`summarise()` has grouped output by 'ADH'. You can override using the `.groups`\nargument.\n\n# Create plot\nggplot(summary_data, aes(x = years, y = Average_QOL, color = ADH)) +\n  geom_line(size = 1) +   # Line plot\n  geom_point(size = 2) +  # Add points for clarity\n  labs(title = \"Average Physical QOL Score by Adherence Group Over 8 Years\",\n       x = \"Year\",\n       y = \"Average Physical QOL Score\",\n       color = \"Adherence Group\") +\n  theme_minimal() +       # Clean theme\n  scale_color_brewer(palette = \"Pastel2\")  # Nice color palette for groups\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_line()`).\n\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nWe see a similiar relationship here, though there’s more variation. For some reason that &lt;75% group likes to spike in their QOL at year 7. Interesting.\n\n\nLog Viral Load\n\n# Get means for each year\nsummary_data &lt;- data %&gt;%\n  group_by(ADH, years) %&gt;%\n  summarize(Average_LEU3N = mean(LEU3N, na.rm = TRUE))\n\n`summarise()` has grouped output by 'ADH'. You can override using the `.groups`\nargument.\n\n# Create plot\np &lt;- ggplot(summary_data, aes(x = years, y = Average_LEU3N, color = ADH)) +\n  geom_line(size = 1) +   # Line plot\n  geom_point(size = 2) +  # Add points for clarity\n  labs(title = \"Average LEU3N  by Adherence Group Over 8 Years\",\n       x = \"Year\",\n       y = \"Average LEU3N\",\n       color = \"Adherence Group\") +\n  theme_minimal() +       # Clean theme\n  scale_color_brewer(palette = \"Pastel2\")  # Nice color palette for groups\n\nggplotly(p)\n\n\n\n\n\n\n\nCD4+ T Cell Count\n\n# Create log vload variable for original 8 year data set.\ndata$VLOAD_log &lt;- log(data$VLOAD)\n\n# Get means for each year\nsummary_data &lt;- data %&gt;%\n  group_by(ADH, years) %&gt;%\n  summarize(Average_VLOAD_log = mean(VLOAD_log, na.rm = TRUE))\n\n`summarise()` has grouped output by 'ADH'. You can override using the `.groups`\nargument.\n\n# Create plot\np &lt;- ggplot(summary_data, aes(x = years, y = Average_VLOAD_log, color = ADH)) +\n  geom_line(size = 1) +   # Line plot\n  geom_point(size = 2) +  # Add points for clarity\n  labs(title = \"Average VLOAD log by Adherence Group Over 8 Years\",\n       x = \"Year\",\n       y = \"Average VLOAD log\",\n       color = \"Adherence Group\") +\n  theme_minimal() +       # Clean theme\n  scale_color_brewer(palette = \"Pastel2\")  # Nice color palette for groups\n\nggplotly(p)\n\n\n\n\n\nThat’s very interesting. The groups with the highest vload log had the least adherence. There is an inverse relationship here.\nThis is strong evidence that the treatment is efficacious and the ADH should be considered as a confounder.\nOhhhhh, that’s super cool. So there is an inverse relationship between LEU3N and VLOAD_log, such that spikes in one should be dips in the other. We can see that in this data set! It appears that for the adherence group of &lt;75%, they had viral load spikes at years 5 and 7, which correspond with leukocyte dips at years 5 and 7! They weren’t adhering to the protocol, which resulted in them getting infected (?) more and having more of the virus present compared to the other groups.\n\n\nComparing High Vs Low Adherence Groups\n\ndata$ADH_HIGHLOW &lt;- ifelse(data$ADH == \"&lt;75%\" | data$ADH == \"75-94%\", 0, 1)\n\ndata$ADH_HIGHLOW &lt;- factor(data$ADH_HIGHLOW,\n                           levels = c(0,1),\n                           labels = c(\"Low Adherence\", \"High Adherence\"))\n\n#### tRY WITH HIGH LOW ADH\n# Create log vload variable for original 8 year data set.\ndata$VLOAD_log &lt;- log(data$VLOAD)\n\n# Get means for each year\nsummary_data &lt;- data %&gt;%\n  group_by(ADH_HIGHLOW, years) %&gt;%\n  summarize(Average_VLOAD_log = mean(VLOAD_log, na.rm = TRUE))\n\n`summarise()` has grouped output by 'ADH_HIGHLOW'. You can override using the\n`.groups` argument.\n\n# Create plot\np &lt;- ggplot(summary_data, aes(x = years, y = Average_VLOAD_log, color = ADH_HIGHLOW)) +\n  geom_line(size = 1) +   # Line plot\n  geom_point(size = 2) +  # Add points for clarity\n  labs(title = \"Average VLOAD log by Adherence Group Over 8 Years\",\n       x = \"Year\",\n       y = \"Average VLOAD log\",\n       color = \"Adherence Group\") +\n  theme_minimal() +       # Clean theme\n  scale_color_brewer(palette = \"Pastel2\")  # Nice color palette for groups\n\nggplotly(p)\n\n\n\n\n\nHigh adherence patients have lower viral load then low adherence\nSame for leu3n\n\n#### tRY WITH HIGH LOW ADH\n\n# Get means for each year\nsummary_data &lt;- data %&gt;%\n  group_by(ADH_HIGHLOW, years) %&gt;%\n  summarize(Average_LEU3N = mean(LEU3N, na.rm = TRUE))\n\n`summarise()` has grouped output by 'ADH_HIGHLOW'. You can override using the\n`.groups` argument.\n\n# Create plot\np &lt;- ggplot(summary_data, aes(x = years, y = Average_LEU3N, color = ADH_HIGHLOW)) +\n  geom_line(size = 1) +   # Line plot\n  geom_point(size = 2) +  # Add points for clarity\n  labs(title = \"Average LEU3N by Adherence Group Over 8 Years\",\n       x = \"Year\",\n       y = \"Average LEU3N\",\n       color = \"Adherence Group\") +\n  theme_minimal() +       # Clean theme\n  scale_color_brewer(palette = \"Pastel2\")  # Nice color palette for groups\n\nggplotly(p)\n\n\n\n\n\nHigh adherence users have higher CD4+ T Cells, for the most part.\n\n\nOther literature\nhttps://pmc.ncbi.nlm.nih.gov/articles/PMC9234842/\nThis one shows that poor adherence is related to anxiety and depression, leading credence to why I want to examine the impact of hard drugs and adherence while controlling for depression."
  },
  {
    "objectID": "Project_1_Regression/Project_1_Regression_R/Project_1_Regression.html",
    "href": "Project_1_Regression/Project_1_Regression_R/Project_1_Regression.html",
    "title": "Advanced Data Analysis - Project 1",
    "section": "",
    "text": "The aim of the current study is to assess whether a new gel treatment for gum disease results in lower whole-mouth average pocket depth and attachment loss after one year. Average pocket depth and attachment loss were taken at baseline, and participants were assigned into one of five groups (1 = placebo, 2 = no treatment, 3 = low concentration, 4 = medium concentration, 5 = high concentration) and instructed to apply the gel to their gums twice a day. After 1-year, average pocket depth and attachment loss were recorded again. Data was received as a .csv file containing treatment level, average pocket depth and attachment loss at baseline and at one-year, demographic information, gender, age, number of sites measured, and smoking status. The clinical hypothesis is that average pocket depth and attachment loss in participants who applied the gel will be lower compared to participants who did not. Gender, age, ethnicity, and smoking status will be investigated as potential covariates.\nThe project description provided by the PI is available below:\n\n\n\nFirst we begin by loading in the necessary packages\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(tidyverse)\nlibrary(knitr)\nlibrary(kableExtra) # This lets me pretty print the tables\nlibrary(naniar) # Used to visualize missing data\nlibrary(glue) # This is used for f-strings in R\nlibrary(purrr) # Used for summary tables\nlibrary(corrplot) # Used to make the correlation matrix\nlibrary(corrtable) # used to make the table for the correlation matrix\nlibrary(xtable) # Used to make the table for the correlation matrix\nlibrary(htmlTable) # Used to make Table 1\nlibrary(boot) # Used to make Table 1\nlibrary(table1) # Used to make Table 1\nlibrary(sjPlot) # Used to generate publication quality tables for regressions\nlibrary(mice) # Used for multiple imputation\nlibrary(car) # Used for outlier diagnostics with jackknife residuals\n\nThen we will import the dataset\n\n# Import dataset\ndata&lt;- read.csv(\"RawData/Project1_data.csv\")\nnames(data)[1] &lt;- \"id\" # Renaming the weird character out of this colname\n\nAnd visualize the dataset\n\n# Create a nicely formatted table, code adapted from ChatGPT\nkable(head(data), format = \"html\") %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\"))\n\n\n\n\nid\ntrtgroup\ngender\nrace\nage\nsmoker\nsites\nattachbase\nattach1year\npdbase\npd1year\n\n\n\n\n101\n4\n2\n5\n44.57221\n1\n162\n2.432099\n2.577640\n3.246914\n3.407407\n\n\n102\n5\n2\n5\n35.57290\n1\n162\n2.543210\nNA\n3.006173\nNA\n\n\n103\n2\n2\n5\n47.94524\n1\n144\n2.881944\n3.076389\n3.118056\n3.125000\n\n\n104\n3\n2\n5\n55.17864\n1\n138\n4.956522\n5.304348\n5.217391\n4.891304\n\n\n105\n1\n2\n2\n43.79740\n1\n168\n1.773810\n1.452381\n3.363095\n2.898810\n\n\n106\n4\n2\n5\n42.14921\n1\n168\n2.369048\n1.922619\n3.910714\n3.083333\n\n\n\n\n\n\n# Acquire number of variables and observations of the data set\ndim(data)\n\n[1] 130  11\n\n\nOur data set is comprised of 130 observations, with 11 variables\n\n\nThe variables in the provided data set are as follows:\n\nid: Patient ID\ntrtgroup: Treatment group (1 = placebo, 2 = no treatment control, 3 = low concentration, 4 = medium concentration, 5 = high concentration gel)\ngender: Gender (1 = male, 2 = female)\nrace: Race, (1 = Native American, 2 = African American, 3 = Not used, 4 = Asian, 5 = White)\nage: Age in years\nsmoker: Smoking status (0 = No, 1 = Yes)\nsites: Number of sites gum measurements were averaged from\nattachbase: Whole-mouth average attachment loss taken at base timepoint\nattach1year: Whole-mouth average attachment loss after 1 year\npdbase: Whole-mouth average pocket loss at base timepoint\npd1year: Whole-mouth average pocket loss after 1 year\n\n\n\n\n\n\n\nNote\n\n\n\nPocket depth and attachment loss are both measurements of how far the gums have pulled away from the teeth, hence smaller values are better"
  },
  {
    "objectID": "Project_1_Regression/Project_1_Regression_R/Project_1_Regression.html#data-preparation",
    "href": "Project_1_Regression/Project_1_Regression_R/Project_1_Regression.html#data-preparation",
    "title": "Advanced Data Analysis - Project 1",
    "section": "",
    "text": "First we begin by loading in the necessary packages\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(tidyverse)\nlibrary(knitr)\nlibrary(kableExtra) # This lets me pretty print the tables\nlibrary(naniar) # Used to visualize missing data\nlibrary(glue) # This is used for f-strings in R\nlibrary(purrr) # Used for summary tables\nlibrary(corrplot) # Used to make the correlation matrix\nlibrary(corrtable) # used to make the table for the correlation matrix\nlibrary(xtable) # Used to make the table for the correlation matrix\nlibrary(htmlTable) # Used to make Table 1\nlibrary(boot) # Used to make Table 1\nlibrary(table1) # Used to make Table 1\nlibrary(sjPlot) # Used to generate publication quality tables for regressions\nlibrary(mice) # Used for multiple imputation\nlibrary(car) # Used for outlier diagnostics with jackknife residuals\n\nThen we will import the dataset\n\n# Import dataset\ndata&lt;- read.csv(\"RawData/Project1_data.csv\")\nnames(data)[1] &lt;- \"id\" # Renaming the weird character out of this colname\n\nAnd visualize the dataset\n\n# Create a nicely formatted table, code adapted from ChatGPT\nkable(head(data), format = \"html\") %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\"))\n\n\n\n\nid\ntrtgroup\ngender\nrace\nage\nsmoker\nsites\nattachbase\nattach1year\npdbase\npd1year\n\n\n\n\n101\n4\n2\n5\n44.57221\n1\n162\n2.432099\n2.577640\n3.246914\n3.407407\n\n\n102\n5\n2\n5\n35.57290\n1\n162\n2.543210\nNA\n3.006173\nNA\n\n\n103\n2\n2\n5\n47.94524\n1\n144\n2.881944\n3.076389\n3.118056\n3.125000\n\n\n104\n3\n2\n5\n55.17864\n1\n138\n4.956522\n5.304348\n5.217391\n4.891304\n\n\n105\n1\n2\n2\n43.79740\n1\n168\n1.773810\n1.452381\n3.363095\n2.898810\n\n\n106\n4\n2\n5\n42.14921\n1\n168\n2.369048\n1.922619\n3.910714\n3.083333\n\n\n\n\n\n\n# Acquire number of variables and observations of the data set\ndim(data)\n\n[1] 130  11\n\n\nOur data set is comprised of 130 observations, with 11 variables\n\n\nThe variables in the provided data set are as follows:\n\nid: Patient ID\ntrtgroup: Treatment group (1 = placebo, 2 = no treatment control, 3 = low concentration, 4 = medium concentration, 5 = high concentration gel)\ngender: Gender (1 = male, 2 = female)\nrace: Race, (1 = Native American, 2 = African American, 3 = Not used, 4 = Asian, 5 = White)\nage: Age in years\nsmoker: Smoking status (0 = No, 1 = Yes)\nsites: Number of sites gum measurements were averaged from\nattachbase: Whole-mouth average attachment loss taken at base timepoint\nattach1year: Whole-mouth average attachment loss after 1 year\npdbase: Whole-mouth average pocket loss at base timepoint\npd1year: Whole-mouth average pocket loss after 1 year\n\n\n\n\n\n\n\nNote\n\n\n\nPocket depth and attachment loss are both measurements of how far the gums have pulled away from the teeth, hence smaller values are better"
  },
  {
    "objectID": "Project_1_Regression/Project_1_Regression_R/Project_1_Regression.html#Descriptives",
    "href": "Project_1_Regression/Project_1_Regression_R/Project_1_Regression.html#Descriptives",
    "title": "Advanced Data Analysis - Project 1",
    "section": "Descriptive Statistics",
    "text": "Descriptive Statistics\nHere we will acquire the descriptive statistics of our data set and create Table 1. Code modified from cran.r-project.org\n\n# Duplicate the dataset so we are not modifying the original\ndata2 &lt;- data\n\n# Factor the basic variables that we're interested in\ndata2$trtgroup &lt;- factor(data2$trtgroup,\n                                levels = c(1,2,3,4,5),\n                                labels = c(\"Placebo\", \"Control\", \"Low\", \"Medium\", \"High\"))\n\ndata2$gender &lt;- factor(data2$gender,\n                              levels = c(1,2),\n                              labels = c(\"Male\", \"Female\"))\n\ndata2$race &lt;- factor(data2$race,\n                             levels = c(1,2,4,5),\n                             labels = c(\"Native American\", \"African American\", \"White\", \"Asian\"))\n                             \ndata2$smoker &lt;- factor(data2$smoker,\n                               levels = c(0,1),\n                               labels = c(\"Non-Smoker\", \"Smoker\"))\n\n# Create labels to make the names of each variable more professional\nlabel(data2$gender) &lt;- \"Gender\"\nlabel(data2$race) &lt;- \"Race\"\nlabel(data2$age) &lt;- \"Age (Years)\"\nlabel(data2$smoker) &lt;- \"Smoking Status\"\nlabel(data2$sites) &lt;- \"Sites\"\nlabel(data2$attachbase) &lt;- \"Attachment Loss at Baseline\"\nlabel(data2$attach1year) &lt;- \"Attachment Loss at 1 Year\"\nlabel(data2$pdbase) &lt;- \"Pocket Depth at Baseline\"\nlabel(data2$pd1year) &lt;- \"Pocket Depth at 1 Year\"\nlabel(data2$attachchange) &lt;- \"Attachment Loss Change\"\nlabel(data2$pdchange) &lt;- \"Pocket Depth Change\"\n\n\n# Create table 1\ntable1 &lt;- table1(~ gender + race + age + smoker + sites + attachbase + attach1year + pdbase + pd1year + attachchange + pdchange| trtgroup,  data = data2, caption = \"Descriptive Statistics\", overall = c(left=\"Total\"))\ntable1\n\n\n\nDescriptive Statistics\n\n\n\n\n\n\n\n\n\n\n\n\nTotal\n(N=130)\nPlacebo\n(N=26)\nControl\n(N=26)\nLow\n(N=26)\nMedium\n(N=26)\nHigh\n(N=26)\n\n\n\n\nGender\n\n\n\n\n\n\n\n\nMale\n54 (41.5%)\n11 (42.3%)\n10 (38.5%)\n11 (42.3%)\n11 (42.3%)\n11 (42.3%)\n\n\nFemale\n76 (58.5%)\n15 (57.7%)\n16 (61.5%)\n15 (57.7%)\n15 (57.7%)\n15 (57.7%)\n\n\nRace\n\n\n\n\n\n\n\n\nNative American\n4 (3.1%)\n0 (0%)\n1 (3.8%)\n1 (3.8%)\n0 (0%)\n2 (7.7%)\n\n\nAfrican American\n9 (6.9%)\n2 (7.7%)\n1 (3.8%)\n5 (19.2%)\n0 (0%)\n1 (3.8%)\n\n\nWhite\n3 (2.3%)\n1 (3.8%)\n1 (3.8%)\n0 (0%)\n1 (3.8%)\n0 (0%)\n\n\nAsian\n114 (87.7%)\n23 (88.5%)\n23 (88.5%)\n20 (76.9%)\n25 (96.2%)\n23 (88.5%)\n\n\nAge (Years)\n\n\n\n\n\n\n\n\nMean (SD)\n49.9 (10.0)\n47.1 (8.61)\n50.7 (9.90)\n51.9 (10.8)\n49.0 (9.49)\n50.8 (11.2)\n\n\nMedian [Min, Max]\n48.6 [28.6, 74.5]\n44.7 [30.4, 67.1]\n49.2 [36.1, 73.3]\n51.5 [36.9, 71.9]\n48.1 [28.6, 70.9]\n49.9 [34.1, 74.5]\n\n\nMissing\n1 (0.8%)\n1 (3.8%)\n0 (0%)\n0 (0%)\n0 (0%)\n0 (0%)\n\n\nSmoking Status\n\n\n\n\n\n\n\n\nNon-Smoker\n81 (62.3%)\n15 (57.7%)\n17 (65.4%)\n18 (69.2%)\n14 (53.8%)\n17 (65.4%)\n\n\nSmoker\n48 (36.9%)\n11 (42.3%)\n9 (34.6%)\n8 (30.8%)\n11 (42.3%)\n9 (34.6%)\n\n\nMissing\n1 (0.8%)\n0 (0%)\n0 (0%)\n0 (0%)\n1 (3.8%)\n0 (0%)\n\n\nSites\n\n\n\n\n\n\n\n\nMean (SD)\n158 (11.3)\n160 (10.1)\n154 (10.9)\n161 (8.54)\n155 (15.7)\n157 (9.65)\n\n\nMedian [Min, Max]\n162 [114, 168]\n162 [138, 168]\n159 [126, 168]\n162 [138, 168]\n162 [114, 168]\n159 [138, 168]\n\n\nAttachment Loss at Baseline\n\n\n\n\n\n\n\n\nMean (SD)\n2.15 (0.797)\n1.79 (0.646)\n2.46 (0.687)\n2.07 (0.987)\n2.17 (0.656)\n2.24 (0.858)\n\n\nMedian [Min, Max]\n2.03 [0.895, 5.09]\n1.71 [0.899, 3.64]\n2.48 [1.22, 4.39]\n1.77 [0.895, 4.96]\n2.12 [1.02, 4.01]\n1.97 [1.26, 5.09]\n\n\nAttachment Loss at 1 Year\n\n\n\n\n\n\n\n\nMean (SD)\n2.10 (0.772)\n1.74 (0.542)\n2.33 (0.551)\n2.08 (1.06)\n2.24 (0.652)\n2.15 (0.915)\n\n\nMedian [Min, Max]\n1.98 [0.865, 5.30]\n1.64 [0.964, 3.10]\n2.23 [1.46, 3.49]\n1.74 [0.865, 5.30]\n2.25 [1.35, 3.83]\n1.71 [1.22, 4.04]\n\n\nMissing\n27 (20.8%)\n3 (11.5%)\n3 (11.5%)\n5 (19.2%)\n6 (23.1%)\n10 (38.5%)\n\n\nPocket Depth at Baseline\n\n\n\n\n\n\n\n\nMean (SD)\n3.14 (0.437)\n3.09 (0.372)\n3.28 (0.473)\n3.17 (0.593)\n3.05 (0.402)\n3.11 (0.273)\n\n\nMedian [Min, Max]\n3.10 [2.26, 5.22]\n3.11 [2.47, 4.08]\n3.11 [2.65, 4.77]\n3.07 [2.26, 5.22]\n3.09 [2.42, 3.91]\n3.14 [2.62, 3.60]\n\n\nPocket Depth at 1 Year\n\n\n\n\n\n\n\n\nMean (SD)\n2.88 (0.488)\n2.75 (0.482)\n2.95 (0.455)\n3.02 (0.578)\n2.84 (0.469)\n2.80 (0.423)\n\n\nMedian [Min, Max]\n2.90 [1.96, 4.89]\n2.70 [1.96, 3.75]\n2.90 [2.24, 4.07]\n2.97 [2.16, 4.89]\n2.90 [2.05, 3.78]\n2.87 [2.04, 3.40]\n\n\nMissing\n27 (20.8%)\n3 (11.5%)\n3 (11.5%)\n5 (19.2%)\n6 (23.1%)\n10 (38.5%)\n\n\nAttachment Loss Change\n\n\n\n\n\n\n\n\nMean (SD)\n-0.0995 (0.276)\n-0.0871 (0.242)\n-0.222 (0.280)\n-0.0178 (0.266)\n-0.00656 (0.231)\n-0.165 (0.326)\n\n\nMedian [Min, Max]\n-0.0679 [-1.05, 0.452]\n-0.0247 [-0.599, 0.452]\n-0.123 [-0.901, 0.194]\n0.0298 [-0.705, 0.348]\n-0.0160 [-0.446, 0.339]\n-0.0579 [-1.05, 0.199]\n\n\nMissing\n27 (20.8%)\n3 (11.5%)\n3 (11.5%)\n5 (19.2%)\n6 (23.1%)\n10 (38.5%)\n\n\nPocket Depth Change\n\n\n\n\n\n\n\n\nMean (SD)\n-0.294 (0.268)\n-0.350 (0.277)\n-0.338 (0.232)\n-0.206 (0.279)\n-0.203 (0.272)\n-0.382 (0.245)\n\n\nMedian [Min, Max]\n-0.284 [-0.858, 0.455]\n-0.383 [-0.858, 0.161]\n-0.367 [-0.759, 0.0145]\n-0.244 [-0.661, 0.455]\n-0.200 [-0.827, 0.175]\n-0.347 [-0.845, 0.0536]\n\n\nMissing\n27 (20.8%)\n3 (11.5%)\n3 (11.5%)\n5 (19.2%)\n6 (23.1%)\n10 (38.5%)"
  },
  {
    "objectID": "Project_1_Regression/Project_1_Regression_R/Project_1_Regression.html#Preliminary",
    "href": "Project_1_Regression/Project_1_Regression_R/Project_1_Regression.html#Preliminary",
    "title": "Advanced Data Analysis - Project 1",
    "section": "Preliminary Evaluation of Assumptions",
    "text": "Preliminary Evaluation of Assumptions\nBefore we begin digging into the data, let’s take a closer look at the relationships between our variables.\nWe will first run a correlation matrix to assess the correlations between our IVs. Then we will explore any high correlations that which could affect our analysis.\nFinally we will make histograms of attachment loss and pocket depth change to gauge whether they will be normally distributed or if we need to run some transformations.\n\nCorrelation MatrixGender and MissingnessNormality of Dependent Variables\n\n\nFirst we will begin by making a correlation matrix to assess whether any of our IVs are related to each other (multicollinearity). This will inform which variables to incorporate into the final model.\n\n# Since we made dummy codes, we will get spurious correlations that will obfuscate the main relationships we are interested in (e.g. between 'medium' and 'trtgroup'. So we will first make a separate dataset excluding the dummy coded variables\ndata_for_matrix &lt;- select(data_missing, -placebo, -control, -low, -medium,  -high, -trt, -trt3groups)\n\n# Make a correlation matrix with all variables of the trimmed data set\ncorrelation_matrix &lt;- cor(data_for_matrix, use = \"complete.obs\")\n\n# Plot the matrix\ncorrplot(correlation_matrix, method = \"circle\")\n\n\n\n\n\n\n\n# Trim the matrix\ncorrelation_matrix[upper.tri(correlation_matrix)] &lt;- NA\n\n# Save the matrix as a LaTex file for paper\ncor_table &lt;- xtable(correlation_matrix, caption = \"Correlation Matrix\", label = \"tab:correlation\")\nprint(cor_table, type = \"latex\", file = \"correlation_matrix999.tex\")\n\nkable(correlation_matrix, format = \"html\", table.attr = \"class='table table-bordered'\")\n\n\n\n\n\nid\ntrtgroup\ngender\nrace\nage\nsmoker\nsites\nattachbase\nattach1year\npdbase\npd1year\nattachchange\npdchange\n\n\n\n\nid\n1.0000000\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\ntrtgroup\n-0.1452850\n1.0000000\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\ngender\n-0.8306185\n0.1428866\n1.0000000\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nrace\n0.1319767\n-0.0314771\n-0.0955531\n1.0000000\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nage\n0.0593032\n0.1100051\n-0.0453862\n0.1787040\n1.0000000\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nsmoker\n-0.2742888\n-0.0495029\n0.0206456\n0.0377211\n-0.2229620\n1.0000000\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nsites\n0.0520821\n-0.0393742\n-0.1291127\n0.0505164\n-0.0274695\n-0.0226381\n1.0000000\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nattachbase\n-0.1641513\n0.1440999\n0.2416242\n0.0313438\n0.1354434\n0.1456234\n-0.4237813\n1.0000000\nNA\nNA\nNA\nNA\nNA\n\n\nattach1year\n-0.1440182\n0.1671499\n0.2021284\n0.0671713\n0.0850851\n0.1987327\n-0.4042743\n0.9450189\n1.0000000\nNA\nNA\nNA\nNA\n\n\npdbase\n-0.2010672\n-0.0224838\n0.2645908\n0.0182289\n-0.0884687\n0.2614919\n-0.1805543\n0.6047132\n0.6093346\n1.0000000\nNA\nNA\nNA\n\n\npd1year\n-0.0685783\n0.0126731\n0.1362804\n0.0606735\n-0.1246106\n0.2447030\n-0.1901940\n0.5601052\n0.6685457\n0.8436653\n1.0000000\nNA\nNA\n\n\nattachchange\n0.0964391\n0.0296043\n-0.1696216\n0.0928969\n-0.1742586\n0.1135740\n0.1578683\n-0.3976360\n-0.0757224\n-0.1342018\n0.1679506\n1.0000000\nNA\n\n\npdchange\n0.2251487\n0.0622762\n-0.2123192\n0.0789059\n-0.0731704\n-0.0091784\n-0.0323859\n-0.0317726\n0.1579534\n-0.2031295\n0.3543035\n0.5400668\n1\n\n\n\n\n\n\n\nVisually, we can see a cluster of high positive correlations between all of attachment loss and pocket depth at baseline and 1 year. This makes sense, as all measurements were taken from the same sites in the gums. This will be explored later (See Exploratory Data Analysis - Dependent Variables)\nJust for sanity (and practice), let’s make a table of our correlation coefficients.\n\n# Convert the matrix to a dataframe for better formatting\ncorrelation_df &lt;- as.data.frame(correlation_matrix)\n\n# Use Kable to pretty print the table\nkable(correlation_df, caption = \"Correlation Matrix\", format = \"html\") %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\"))\n\n\nCorrelation Matrix\n\n\n\nid\ntrtgroup\ngender\nrace\nage\nsmoker\nsites\nattachbase\nattach1year\npdbase\npd1year\nattachchange\npdchange\n\n\n\n\nid\n1.0000000\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\ntrtgroup\n-0.1452850\n1.0000000\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\ngender\n-0.8306185\n0.1428866\n1.0000000\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nrace\n0.1319767\n-0.0314771\n-0.0955531\n1.0000000\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nage\n0.0593032\n0.1100051\n-0.0453862\n0.1787040\n1.0000000\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nsmoker\n-0.2742888\n-0.0495029\n0.0206456\n0.0377211\n-0.2229620\n1.0000000\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nsites\n0.0520821\n-0.0393742\n-0.1291127\n0.0505164\n-0.0274695\n-0.0226381\n1.0000000\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nattachbase\n-0.1641513\n0.1440999\n0.2416242\n0.0313438\n0.1354434\n0.1456234\n-0.4237813\n1.0000000\nNA\nNA\nNA\nNA\nNA\n\n\nattach1year\n-0.1440182\n0.1671499\n0.2021284\n0.0671713\n0.0850851\n0.1987327\n-0.4042743\n0.9450189\n1.0000000\nNA\nNA\nNA\nNA\n\n\npdbase\n-0.2010672\n-0.0224838\n0.2645908\n0.0182289\n-0.0884687\n0.2614919\n-0.1805543\n0.6047132\n0.6093346\n1.0000000\nNA\nNA\nNA\n\n\npd1year\n-0.0685783\n0.0126731\n0.1362804\n0.0606735\n-0.1246106\n0.2447030\n-0.1901940\n0.5601052\n0.6685457\n0.8436653\n1.0000000\nNA\nNA\n\n\nattachchange\n0.0964391\n0.0296043\n-0.1696216\n0.0928969\n-0.1742586\n0.1135740\n0.1578683\n-0.3976360\n-0.0757224\n-0.1342018\n0.1679506\n1.0000000\nNA\n\n\npdchange\n0.2251487\n0.0622762\n-0.2123192\n0.0789059\n-0.0731704\n-0.0091784\n-0.0323859\n-0.0317726\n0.1579534\n-0.2031295\n0.3543035\n0.5400668\n1\n\n\n\n\n\n\n\nOther than that, there is a correlation between gender and ID which seems spurious. Let’s investigate that in the next tab.\nBack to top of tabset\n\n\nThere is a correlation between gender and ID. Let’s make a simple plot to investigate.\n\n# Creating a simple plot of id and gender\nggplot(data_missing, aes(x = factor(gender), y = id)) + \n  geom_point() + \n  labs(title = \"ID by Gender\")\n\n\n\n\n\n\n\n\nInterestingly, it appears that the experimenters assigned ID based on gender. That is, females received ID’s starting at 101, and males received ID’s starting at 201 (for some reason there’s a few females with ID’s &gt; 200).\nIt will be important to double check with the PI’s how they assigned participants to treatment group to ensure it was in fact random.\nLet’s make a contingency table to see what the breakdown between gender and treatment group is.\n\n# First make a contingency table of both variables\ncontingency_table &lt;- table(data_missing$gender, data_missing$trtgroup)\n\n# Set the row and column names\ndimnames(contingency_table) &lt;- list(\n  \"Gender\" = c(\"Male\", \"Female\"),\n  \"Treatment Condition\" = c(\"Placebo\", \"Control\", \"Low\", \"Medium\", \"High\"))\n\n# Convert the table to a dataframe for better formatting (from ChatGPT)\ncontingency_df &lt;- as.data.frame.matrix(contingency_table)\n\n# Pretty print the table using kable\nkable(contingency_df, format = \"html\") %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\"))\n\n\n\n\n\nPlacebo\nControl\nLow\nMedium\nHigh\n\n\n\n\nMale\n10\n7\n9\n7\n3\n\n\nFemale\n13\n16\n12\n13\n13\n\n\n\n\n\n\n\nThat’s not good! It looks like males were less likely to be in the high treatment condition compared to females.\nThis could be because males were more likely to drop out then females. Let’s make a quick table using the original data set before we dropped the missing variables.\n\n# First make a contingency table of both variables\ncontingency_table_clean &lt;- table(data$gender, data$trtgroup)\n\n# Set the row and column names\ndimnames(contingency_table_clean) &lt;- list(\n  \"Gender\" = c(\"Male\", \"Female\"),\n  \"Treatment Condition\" = c(\"Placebo\", \"Control\", \"Low\", \"Medium\", \"High\"))\n\n# Convert the table to a dataframe for better formatting (from ChatGPT)\ncontingency_df_clean &lt;- as.data.frame.matrix(contingency_table_clean)\n\n# Pretty print the table using kable\nkable(contingency_df_clean, format = \"html\") %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\"))\n\n\n\n\n\nPlacebo\nControl\nLow\nMedium\nHigh\n\n\n\n\nMale\n11\n10\n11\n11\n11\n\n\nFemale\n15\n16\n15\n15\n15\n\n\n\n\n\n\n\nIt looks more balanced before I took out participants with missing data.\nChi-square is known to be unsuitable if a cell has &lt; 5 counts, which we have in this case (3 males in high concentration condition). So I will run Fisher’s test to see if that difference is statistically significant.\n\nfisher_test &lt;- fisher.test(contingency_table)\nfisher_test\n\n\n    Fisher's Exact Test for Count Data\n\ndata:  contingency_table\np-value = 0.506\nalternative hypothesis: two.sided\n\n\nThe p-value is not signficant (p = 0.506).\nWe know from visualizing the missing data that there were 27 missing data points for the gum measurement DVs, and these all belong to the same people. Furthermore, it appears that males in the treatment groups were more likely to have missing values than in the placebo (and maybe control) group. Is it possible that the gel was having an adverse effect on these participants? Does the gel have an adverse effect only on males and not females for some reason? Let’s explore.\nFirst, I want to investigate if males were more likely to have missing data points. It’s possible if their gums were hurting they simply rejected or avoided having these measurements taken.\nLet’s repeat this process and make a contingency table of gender and missing variables.\n\n# First let's add a new dummy code for if a participant is missing any data points\ndata$missing &lt;- ifelse(apply(data, 1, function(row) any(is.na(row))), 1, 0)\n\n# First make a contingency table of both variables\ncontingency_table_missing &lt;- table(data$gender, data$missing)\n\n# Set the row and column names\ndimnames(contingency_table_missing) &lt;- list(\n  \"Gender\" = c(\"Male\", \"Female\"),\n  \"Missing\" = c(\"Not Missing\", \"Missing\"))\n\n# Convert the table to a dataframe for better formatting (from ChatGPT)\ncontingency_df_missing &lt;- as.data.frame.matrix(contingency_table_missing)\n\n# Pretty print the table using kable\nkable(contingency_df_missing, format = \"html\") %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\"))\n\n\n\n\n\nNot Missing\nMissing\n\n\n\n\nMale\n35\n19\n\n\nFemale\n66\n10\n\n\n\n\n\n\n\nProportionally, it appears that males may be more likely to have missing variables than females. Let’s run a chi-square to check.\n\nchi_square_test &lt;- chisq.test(contingency_df_missing)\nchi_square_test\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  contingency_df_missing\nX-squared = 7.6127, df = 1, p-value = 0.005796\n\n\nSuccess! Males were more likely to have missing values compared to females (p = 0.005796). This could be a problem (counfound) if something was causing males to avoid having their gums measured compared to females (such as adverse reactions from the gel)\nLet’s do a quick chi square test to check if there is a relationship between missing values and treatment condition.\nWe start off the same way by making a contingency table and running a chi-square test.\n\n# First make a contingency table of both variables\ncontingency_table_missing2 &lt;- table(data$missing, data$trtgroup)\n\n# Set the row and column names\ndimnames(contingency_table_missing2) &lt;- list(\"Missing\" = c(\"Not Missing\", \"Missing\"),\n                                             \"Treatmtent Condition\" = c(\"Placebo\", \"Control\", \"Low\", \"Medium\", \"High\"))\n\n# Convert the table to a dataframe for better formatting (from ChatGPT)\ncontingency_df_missing2 &lt;- as.data.frame.matrix(contingency_table_missing2)\n\n# Pretty print the table using kable\nkable(contingency_df_missing2, format = \"html\") %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\"))\n\n\n\n\n\nPlacebo\nControl\nLow\nMedium\nHigh\n\n\n\n\nNot Missing\n22\n23\n21\n19\n16\n\n\nMissing\n4\n3\n5\n7\n10\n\n\n\n\n\n\n\nIt does appear that there are more missing variables in the high concentration condition. Is it statistically significant?\n\n# Run a Fisher's Exact Test (since we have &lt; 5 observations in cells)\nfisher_test &lt;- fisher.test(contingency_df_missing2)\nfisher_test\n\n\n    Fisher's Exact Test for Count Data\n\ndata:  contingency_df_missing2\np-value = 0.1675\nalternative hypothesis: two.sided\n\n\nNot significant (p = 0.1675). So we can conclude that there is no difference in gender or missing values based on treatment condition (i.e., participants in all treatment conditions were equally likely to be male or female, or have missing values)\nHowever, across the board, males were more likely to have missing values than females. This will be important to note as a caveat during interpretation of the final results.\nBack to top of tabset\n\n\nHere we will simply plot the histograms of attachment loss and pocket depth changes scores, to assess if they appear normally distributed or if we will have to perform a transformation of some kind.\n\n# Plot simple histogram of attachment loss change score\nhist(data_missing$attachchange,\n     main  = \"Histogram of Attachment Loss Change\",\n     xlab = \"Attachment Loss Change\",\n     ylab = \"Frequency\")\n\n\n\n\n\n\n\n# Plot simple histogram of pocket depth change score\nhist(data_missing$pdchange,\n     main  = \"Histogram of Attachment Loss Change\",\n     xlab = \"Pocket Depth Change\",\n     ylab = \"Frequency\")\n\n\n\n\n\n\n\n\nBoth attachment loss and pocket depth change appear to be normally distributed and will not need to be transformed. Attachment loss change is slightly left-tailed but this could be due to outliers, or may just not impact the analysis."
  },
  {
    "objectID": "Project_1_Regression/Project_1_Regression_R/Project_1_Regression.html#Exploratory",
    "href": "Project_1_Regression/Project_1_Regression_R/Project_1_Regression.html#Exploratory",
    "title": "Advanced Data Analysis - Project 1",
    "section": "Exploratory Data Analysis",
    "text": "Exploratory Data Analysis\nHere I will plot the data and perform a number of simple linear regressions to examine relationships between variables in order to determine which covariates to include in the model.\n\nPrimary Explanatory VariableCovariatesDependent VariablesSummary\n\n\n\n5 Treatment Groups3 Treatment Groups\n\n\nLet’s examine if there appears to be a difference in the average attachment loss and pocket depth change according to treatment level\n\nggplot(data_missing, aes(x = factor(trtgroup), y = attachchange)) + \n  geom_boxplot() +\n  labs(title = \"Boxplot of Attachment Loss change by Treatment\")\n\n\n\n\n\n\n\nggplot(data_missing, aes(x = factor(trtgroup), y = pdchange)) + \n  geom_boxplot()  +\n  labs(title = \"Boxplot of Pocket Depth Change by treatment\")\n\n\n\n\n\n\n\n\nAttachment Loss Change\nVisually, there does not appear to be a difference between the treatment group levels (low, medium, high concentration gel) compared to each other. Additionally, there does not seem to be a difference between the treatment groups and the placebo for attachment loss change.\nPocket Depth Change\nInterestingly, the high concentration condition appears to have had a negative effect. Additionally, there seems to be a difference in the treatment groups compared to the no treatment control, but NOT when compared to the placebo. The exception is the low concentration condition compared to the placebo when looking at pocket depth change. Further analysis will reveal whether these differences are statistically significant or not.\nBack to top of tabset\n\n\nThe relationship between low vs medium vs high gel concentration does not look very strong. Furthermore, we technically do not have a large enough sample size in the high concentration condition to include it.\nFor those reasons, let’s make the same comparisons but while combining all treatment levels into one group called treatment.\n\nggplot(data_missing, aes(x = factor(trt3groups), y = attachchange)) + \n  geom_boxplot()\n\n\n\n\n\n\n\nggplot(data_missing, aes(x = factor(trt3groups), y = attachchange)) +\n  geom_boxplot() +\n  labs(title = \"Boxplot of Attachment Loss Change by Treatment\")\n\n\n\n\n\n\n\nggplot(data_missing, aes(x = factor(trt3groups), y = pdchange)) + \n  geom_boxplot() +\n  labs(title = \"Boxplot of Pocket Depth Change by Treatment\")\n\n\n\n\n\n\n\n\nBy eye, it appears as if there is no difference between the placebo and collapsed treatment groups in attachment loss change or pocket depth change. However, both placebo and any treatment conditions appear to have decreased (?) attachment loss and pocket depth. It may be that this study has null results, unless including one of the covariates changes the results. We will see come the analysis section.\nBack to top of tabset\n\n\n\n\n\nNow we will exlore the relationships between our potential covariates and attachment loss and pocket depth change\n\nAgeSitesGenderRaceSmoking Status\n\n\n\n# Plot age vs attachment loss change\nggplot(data_missing, aes(x = age, y = attachchange)) + \n  geom_point() + \n  labs(title = \"Scatterplot of Age vs Attachment Loss Change\")\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n# Plot age vs pocket depth change\nggplot(data_missing, aes(x = age, y = pdchange)) +\n  geom_point() + \n  labs(title = \"Scatterplot of Age vs Pocket Depth Change\")\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nThere does not appear to be any kind of linear relationship between age and attachment loss change or pocket depth change. I will run a model where I include age, but it will likely be removed in the final model.\nBack to top of tabset\n\n\nIs there any relationship between the number of sites measured from and attachment loss change or pocket depth change (e.g. a lower number of sites could lead to less accurate readings)? If so this could be something to include in our data analysis\n\n# Plot sites vs attachment change loss\nggplot(data_missing, aes(x = sites, y = attachchange)) + \n  geom_point() +\n  labs(title = \"Scatterplot of Attachment Loss Change vs Sites\")\n\n\n\n\n\n\n\n# Plot sites vs pocket depth chagne\nggplot(data_missing, aes(x = sites, y = pdchange)) + \n  geom_point() +\n  labs(title = \"Scatterplot of Pocket Depth Change by Sites\")\n\n\n\n\n\n\n\n\nThere does not seem to be a dramatic difference in attachment loss or pocket depth change based on the number of sites measured from. It’s a bit hard to tell with those two low site subjects, but the points are similar enough to the rest of the dataset that I do not think we have to worry about site number in our analysis.\nBack to top of tabset\n\n\nIt is conceivable that there are sex differences in regards to gum health. Let’s examine if there is a difference in attachment loss or pocket depth change based on gender\n\n# Plot gender vs attachment loss change\nggplot(data_missing, aes(x = factor(gender), y = attachchange)) +\n  geom_boxplot() + \n  labs(title = \"Boxplot of Attachment Loss Change by Gender\",\n      x = \"Gender\")\n\n\n\n\n\n\n\n# Plot gender vs pocket depth change\nggplot(data_missing, aes(x = factor(gender), y = pdchange)) +\n  geom_boxplot() + \n  labs(title = \"Boxplot of Pocket Depth Change by Gender\",\n      x = \"Gender\")\n\n\n\n\n\n\n\n\nBy eye, it appears that males may have more pocket depth loss compared with females. This will be a good variable to include as a covariate.\nLet’s try a t-test to see if there is any difference in attachment loss or pocket depth change based on gender\n\n# Running a t-test on attachment loss change by gender\nmale &lt;- data_missing$attachchange[data_missing$gender == 1]\nfemale &lt;- data_missing$attachchange[data_missing$gender == 2]\nt_test_result &lt;- t.test(male, female)\nt_test_result\n\n\n    Welch Two Sample t-test\n\ndata:  male and female\nt = 2.1969, df = 100.8, p-value = 0.03032\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n 0.01003279 0.19682842\nsample estimates:\n  mean of x   mean of y \n-0.03217094 -0.13560154 \n\n\nThere is a significant difference in attachment loss change score based on gender (t = 2.0502, p = 0.04299)\nLet’s repeat for pocket depth change\n\n# Running a t-test on pocket depth change by gender\nmale &lt;- data_missing$pdchange[data_missing$gender == 1]\nfemale &lt;- data_missing$pdchange[data_missing$gender == 2]\nt_test_result &lt;- t.test(male, female)\nt_test_result\n\n\n    Welch Two Sample t-test\n\ndata:  male and female\nt = 2.3534, df = 81.683, p-value = 0.02101\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n 0.01884469 0.22488042\nsample estimates:\n mean of x  mean of y \n-0.2150844 -0.3369470 \n\n\nThere is also a statistically significant difference in pocket depth change based on gender (t = 2.2626, p = 0.02641).\nTherefore I will include gender as a covariate in the analysis!\nBack to top of tabset\n\n\n\n# Plot race vs attachment loss change\nggplot(data_missing, aes(x = factor(race), y = attachchange)) +\n  geom_boxplot() +\n  labs(title = \"Boxplot of Attachment Change by Race\",\n       x = \"Race\")\n\n\n\n\n\n\n\nggplot(data_missing, aes(x = factor(race), y = pdchange)) + \n  geom_boxplot() +\n  labs(title = \"Boxplot of Pocket Depth Change by Race\",\n  x = \"Race\")\n\n\n\n\n\n\n\n\nThere does not seem to be much of a difference in attachment loss or pocket depth based on race. MAYBE African Americans (2) have more pocket depth loss compared to Asians(4), but the sample size was very small for both races (88.1% of the sample identified as White)\nBack to top of tabset\n\n\nWhether the participant is a smoker or not likely has a dramatic effect on gum health. Let’s assess\n\n# Plot smoking status vs attachment loss change\nggplot(data_missing, aes(x = factor(smoker), y = attachchange)) +\n  geom_boxplot() +\n  labs(title = \"Attachment loss by smoking status\",\n       x = \"Smoking Status\")\n\n\n\n\n\n\n\n# Plot smoking status vs pocket depth change\nggplot(data_missing, aes(x = factor(smoker), y = pdchange)) +\n  geom_boxplot() +\n  labs(title = \"Attachment loss by smoking status\",\n       x = \"Smoking Status\")\n\n\n\n\n\n\n\n\nThere does not appear to be any difference in attachment loss or pocket depth change based on smoking status. I will test a model with smoking status included, but it will likely be dropped in the final model.\nBack to top of tabset\n\n\n\n\n\n\nBaseline vs 1 YearAttachment Loss vs Pocket Depth\n\n\nI am curious what the relationship between base line and 1 year measurements are. The study is an RCT so even if baseline measurements affect 1 year measurements, participants should have been randomly assigned to groups so it is essentially controlled for in the study design. It will still be important to assess this relationship as a moderator however. Maybe treatment only worked for those with high attachment loss or pocket depth at the beginning?\nFirst let’s make a simple plot of attachment loss at baseline and at 1 year\n\n# Creating a scatter plot of attachment loss at baseline and 1 year\nggplot(data_missing, aes(x = attachbase, y = attach1year)) + \n  geom_point() +\n  labs(title = \"Scatterplot of Attachment Loss at Baseline and 1 Year\")\n\n\n\n\n\n\n\n\nLet’s check the correlation coefficient.\n\n# Run a correlation test between attachment at base and 1 year\ncorrelation &lt;- cor.test(data_missing$attachbase, data_missing$attach1year)\ncorrelation\n\n\n    Pearson's product-moment correlation\n\ndata:  data_missing$attachbase and data_missing$attach1year\nt = 29.198, df = 101, p-value &lt; 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.9204657 0.9628839\nsample estimates:\n      cor \n0.9455558 \n\n\nThose are highly correlated (R = 0.946, p &lt;.0001)! Let’s run a simple linear regression\n\n# Run a regression with attachment loss at 1 year as the DV and attachment at base as the IV\nmodel &lt;- lm(attach1year ~ attachbase, data = data_missing)\nsummary(model)\n\n\nCall:\nlm(formula = attach1year ~ attachbase, data = data_missing)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.55683 -0.15641 -0.01841  0.15202  0.82062 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.19872    0.06975   2.849  0.00531 ** \nattachbase   0.86452    0.02961  29.198  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2525 on 101 degrees of freedom\nMultiple R-squared:  0.8941,    Adjusted R-squared:  0.893 \nF-statistic: 852.5 on 1 and 101 DF,  p-value: &lt; 2.2e-16\n\n# Plot the model\nggplot(data_missing, aes(x = attachbase, y = attach1year)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", col = \"blue\") +\n  labs(title = \"Simple Linear Regression of Attachment Loss at Baseline and 1 Year\",\n      x = \"Attachment Loss at Baseline\",\n      y = \"Attachment Loss at 1 Year\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nAttachment loss at baseline is a significant predictor of attachment loss at 1 year (t = 29.20, p &lt;.0001). This is important! We should account for attachment loss at baseline by including it as a covariate in our final model!\nLet’s do the same process of pocket depth at baseline and 1 year\n\n# Create a scatterplot of pocket depth at base vs 1 year\nggplot(data_missing, aes(x = pdbase, y = pd1year)) +\n  geom_point() +\n  labs(title = \"Scatterplot of Pocket Depth at Baseline and 1 Year\")\n\n\n\n\n\n\n\n\nSimilar to attachment loss, we see a relationship between pocket depth at baseline and 1 year. Let’s run the correlation.\n\n# Run a correlation between pocket depth at baseline and 1 year\ncorrelation &lt;- cor.test(data_missing$pdbase, data_missing$pd1year)\ncorrelation\n\n\n    Pearson's product-moment correlation\n\ndata:  data_missing$pdbase and data_missing$pd1year\nt = 15.767, df = 101, p-value &lt; 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.7764571 0.8913340\nsample estimates:\n      cor \n0.8432691 \n\n\nWhile not as strong as attachment loss, there is still a strong relationship between pocket depth at baseline and 1 year (R = 0.84, p &lt;.0001). Let’s run the SLR.\n\n# Run an SLR with pocket depth at 1 year as the DV and pocket depth at baseline as the DV\nmodel &lt;- lm(pd1year ~ pdbase, data = data_missing)\nsummary(model)\n\n\nCall:\nlm(formula = pd1year ~ pdbase, data = data_missing)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.55105 -0.17484  0.01996  0.19627  0.64381 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.07504    0.17948   0.418    0.677    \npdbase       0.88346    0.05603  15.767   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2634 on 101 degrees of freedom\nMultiple R-squared:  0.7111,    Adjusted R-squared:  0.7082 \nF-statistic: 248.6 on 1 and 101 DF,  p-value: &lt; 2.2e-16\n\n# Plot the model\nggplot(data_missing, aes(x = pdbase, y = pd1year)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", col = \"blue\") +\n  labs(title = \"Simple Linear Regression of Pocket Depth at Baseline and 1 Year\",\n       x = \"Pocket Depth at Baseline\",\n       y = \"Pocket Depth at 1 year\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nPocket depth at baseline is also a significant predictor of pocket depth at 1 year (t = 15.78, p = &lt;.0001). We should also therefore include pocket depth at baseline as a covariate in our model to control for it!\nBack to top of tabset\n\n\nI am interested in how attachment loss and pocket depth change are related. Since they are both measurements taken from the same sites in the gums, they are likely to be highly correlated. This could have implications on how we perform the analysis and interpret the results.\nFirst, we plot attachment loss change against pocket depth change\n\n# Creating a scatter plot of attachment loss change against pocket depth change \nggplot(data_missing, aes(x = attachchange, y = pdchange)) + \n  geom_point() +\n  labs(title = \"Scatterplot of Attachment Loss Change and Pocket Depth Change\")\n\n\n\n\n\n\n\n\nThat looks like a linear relationship! Let’s run a correlation and an SLR.\n\n# Running the correlation\ncorrelation &lt;- cor.test(data_missing$attachchange, data_missing$pdchange)\ncorrelation\n\n\n    Pearson's product-moment correlation\n\ndata:  data_missing$attachchange and data_missing$pdchange\nt = 6.3717, df = 101, p-value = 5.621e-09\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.3814636 0.6605362\nsample estimates:\n      cor \n0.5354593 \n\n# Running the regression\nmodel &lt;- lm(attachchange ~ pdchange, data = data_missing)\nsummary(model)\n\n\nCall:\nlm(formula = attachchange ~ pdchange, data = data_missing)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.83130 -0.12683  0.00563  0.14913  0.46480 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.06312    0.03441   1.835   0.0695 .  \npdchange     0.55231    0.08668   6.372 5.62e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2343 on 101 degrees of freedom\nMultiple R-squared:  0.2867,    Adjusted R-squared:  0.2797 \nF-statistic:  40.6 on 1 and 101 DF,  p-value: 5.621e-09\n\n# Creating the plot\nggplot(data_missing, aes(x = attachchange, y = pdchange)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", col = \"blue\") +\n  labs(title = \"Simple Linear Regression of Attachment Loss and Pocket Depth Change\",\n       x = \"Pocket Depth Change\",\n       y = \"Attachment Loss Change\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nThe correlation shows that attachment loss and pocket depth change are very correlated (R = 0.54, p &lt; .0001). The simple linear regression shows that pocket depth change significantly predicts attachment loss change (t = 6.37, p &lt;.0001).\nHowever, multicollinearity is only an issue when IVs are correlated with each other. We can still run a multivariate multiple linear regression even though the DVs are correlated. In fact this is often the case, and is one of the justifications for using a multivariate MLR in the first place! Back to top of tabset\n\n\n\n\n\nTreatment Condition\nCollapsing the low, medium, and high concentration gel groups into 1 group does not seem to improve the relationship between treatment and attachment loss or pocket depth change. Only models including all 5 treatment groups will therefore be considered from here on out to keep in alignment with the original study design.\nGender\nGender is related to attachment loss and pocket depth change, and as such will be included in the final model.\nSupressor / Counfound Variables\nA model will be tested with all covariates to assess for possible suppressor / confound variables. But the other potential covariates of race, age, sites, and smoking status were not related to attachment loss or pocket depth change by themselves.\nBaseline vs 1 Year Measurements\nThe baseline measurements of attachment loss and pocket depth were significant predictors of attachment loss and pocket depth at 1 year, respectively. While the RCT nature of the study should ensure that participants were randomly assigned into treatment condition regardless of their baseline measurements, it will still be good practice to include baseline attachment loss and pocket depth into the final model.\nAttachment Loss vs Pocket Depth Change Scores\nAttachment loss and pocket depth change are highly related to each other, but this should not impact the analysis. PI’s will need to be consulted to interpret the clinical significance of findings, and to help fully understand the implications of any possible differences that may arise between attachment loss and pocket depth change in the analysis.\nBack to top of tabset"
  },
  {
    "objectID": "Project_1_Regression/Project_1_Regression_R/Project_1_Regression.html#Model_1",
    "href": "Project_1_Regression/Project_1_Regression_R/Project_1_Regression.html#Model_1",
    "title": "Advanced Data Analysis - Project 1",
    "section": "Running the Model",
    "text": "Running the Model\nFor ease of interpretation I will be performing this analysis as two separate linear regressions, one for each outcome variable of either attachment loss or pocket depth change. I will begin with a simple linear regression only including the PEV and either attachment loss change or pocket depth change.\n\nAttachment Loss ChangePocket Depth ChangePost-Hoc Analysis\n\n\nWe start by constructing a model with attachment loss change as the dependent variable, and treatment group as the independent variable.\n\n# Running a SLR with attachment loss change as the DV, treatment group as the IV with no treatment as the reference group\nmodel_attach1 &lt;- lm(attachchange ~ placebo + low +  medium + high, data = data_missing)\nsummary(model_attach1)\n\n\nCall:\nlm(formula = attachchange ~ placebo + low + medium + high, data = data_missing)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.88283 -0.14813  0.05115  0.17174  0.53945 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.22169    0.05590  -3.966 0.000139 ***\nplacebo      0.13462    0.07906   1.703 0.091771 .  \nlow          0.20388    0.08092   2.520 0.013365 *  \nmedium       0.21514    0.08197   2.625 0.010063 *  \nhigh         0.05690    0.08728   0.652 0.515950    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2681 on 98 degrees of freedom\nMultiple R-squared:  0.09368,   Adjusted R-squared:  0.05669 \nF-statistic: 2.532 on 4 and 98 DF,  p-value: 0.0451\n\n# Apply Bonferroni correction\np_values &lt;- summary(model_attach1)$coefficients[,4] # This line selects the fourth column of the resulting coefficients table from summary(model_attach), which is the p-values\np_adjusted &lt;- p.adjust(p_values, method = \"bonferroni\")\n\n# Compare adjusted p-values to unadjusted p-values\np_comparison &lt;- cbind(p_values, p_adjusted)\np_comparison\n\n                p_values   p_adjusted\n(Intercept) 0.0001392148 0.0006960742\nplacebo     0.0917709125 0.4588545624\nlow         0.0133650749 0.0668253747\nmedium      0.0100625287 0.0503126435\nhigh        0.5159498143 1.0000000000\n\n# Get confidence intervals\nconf_intervals &lt;- confint(model_attach1)\nconf_intervals\n\n                  2.5 %     97.5 %\n(Intercept) -0.33262557 -0.1107577\nplacebo     -0.02226569  0.2915028\nlow          0.04330175  0.3644540\nmedium       0.05247446  0.3777966\nhigh        -0.11629489  0.2300962\n\n\nThe overall model is significant (F(4,98) = 2.53, p = 0.0451). Looking at the individual t-statistics, we see that - following adjustment for multiple pairwise comparisons - the placebo group (p = 0.549), low concentration group (p = 0.0668), and high concentration group (p = 1.000) are not statistically different from the control group. The medium concentration group is approaching significance (p = 0.0503).\nThat was with the no treatment control group as the reference. Let’s see if any of our treatment conditions were different from the placebo group.\n\n# Running a SLR with attachment loss change as the DV, treatment group as the IV with placebo as the reference group\nmodel_attach2 &lt;- lm(attachchange ~ control + low +  medium + high, data = data_missing)\nsummary(model_attach2)\n\n\nCall:\nlm(formula = attachchange ~ control + low + medium + high, data = data_missing)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.88283 -0.14813  0.05115  0.17174  0.53945 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept) -0.08707    0.05590  -1.558   0.1225  \ncontrol     -0.13462    0.07906  -1.703   0.0918 .\nlow          0.06926    0.08092   0.856   0.3941  \nmedium       0.08052    0.08197   0.982   0.3284  \nhigh        -0.07772    0.08728  -0.890   0.3754  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2681 on 98 degrees of freedom\nMultiple R-squared:  0.09368,   Adjusted R-squared:  0.05669 \nF-statistic: 2.532 on 4 and 98 DF,  p-value: 0.0451\n\n# Apply Bonferroni correction\np_values &lt;- summary(model_attach2)$coefficients[,4] # This line selects the fourth column of the resulting coefficients table from summary(model_attach), which is the p-values\np_adjusted &lt;- p.adjust(p_values, method = \"bonferroni\")\n\n# Compare adjusted p-values to unadjusted p-values\np_comparison &lt;- cbind(p_values, p_adjusted)\np_comparison\n\n              p_values p_adjusted\n(Intercept) 0.12254523  0.6127262\ncontrol     0.09177091  0.4588546\nlow         0.39412117  1.0000000\nmedium      0.32836700  1.0000000\nhigh        0.37538435  1.0000000\n\n# Get confidence intervals\nconf_intervals &lt;- confint(model_attach2)\nconf_intervals\n\n                  2.5 %     97.5 %\n(Intercept) -0.19800701 0.02386082\ncontrol     -0.29150281 0.02226569\nlow         -0.09131681 0.22983549\nmedium      -0.08214410 0.24317801\nhigh        -0.25091345 0.09547760\n\n\nAfter applying a bonferroni correction, none of the groups are significantly different from the placebo group (p &gt; .05).\nConclusion\nWhile the overall model was significant, the only groups that were statistically different from the no treatment control were the low and medium concentration gel groups (p &lt; 0.05). However, since this was a placebo-controlled RCT, and none of the treatment groups were significantly different from the placebo group, we can conclude that we fail to reject the null hypothesis that the average attachment loss over 1 year is the same between all groups.\nTables\n\n\nNote: p-values are unadjusted.\nBack to top of tabset\n\n\nNow we will create our model with pocket depth change as the dependent variable and treatment group as the independent variable.\n\n# Running the regression with pocket depth change as the DV, treatment group as the IV with no treatment as the reference group\nmodel_pd &lt;- lm(pdchange ~ placebo + low + medium + high, data = data_missing)\nsummary(model_pd)\n\n\nCall:\nlm(formula = pdchange ~ placebo + low + medium + high, data = data_missing)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.62483 -0.14595 -0.01768  0.16029  0.66130 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.33817    0.05466  -6.187 1.43e-08 ***\nplacebo     -0.01152    0.07730  -0.149   0.8818    \nlow          0.13200    0.07912   1.668   0.0984 .  \nmedium       0.13562    0.08015   1.692   0.0938 .  \nhigh        -0.04413    0.08534  -0.517   0.6063    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2621 on 98 degrees of freedom\nMultiple R-squared:  0.07806,   Adjusted R-squared:  0.04043 \nF-statistic: 2.074 on 4 and 98 DF,  p-value: 0.08994\n\n\nThe overall model is not statistically significant (F(4,98)= 2.074, p = 0.0899). Based on this model, it appears that the gel treatment has no effect on pocket depth change after 1 year.\nConclusion\nNo groups were significantly different from each other in pocket depth change after 1 year (F(4,98)= 2.074, p = 0.0899), and we fail to reject the null hypothesis that the average pocket depth change over 1 year is the same between all groups.\nTables\n\nNote: p-values are unadjusted\nBack to top of tabset\n\n\nWe did not find a main effect based on treatment group. However I am still interested if including any of the covariates changes the results.\n\n# Run a SLR with attachment loss change as the DV and gender as a covariate\nmodel2_attach &lt;- lm(attachchange ~ placebo + low + medium + high + gender, data = data_missing) \nsummary(model2_attach)\n\n\nCall:\nlm(formula = attachchange ~ placebo + low + medium + high + gender, \n    data = data_missing)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.86656 -0.17533  0.02334  0.16395  0.57717 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept) -0.07460    0.10988  -0.679   0.4988  \nplacebo      0.12330    0.07883   1.564   0.1210  \nlow          0.19310    0.08064   2.395   0.0186 *\nmedium       0.21118    0.08143   2.593   0.0110 *\nhigh         0.06704    0.08690   0.771   0.4423  \ngender      -0.08675    0.05593  -1.551   0.1242  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2662 on 97 degrees of freedom\nMultiple R-squared:  0.1156,    Adjusted R-squared:  0.07003 \nF-statistic: 2.536 on 5 and 97 DF,  p-value: 0.03346\n\n# Apply Bonferroni correction\np_values &lt;- summary(model2_attach)$coefficients[,4] # This line selects the fourth column of the resulting coefficients table from summary(model_attach), which is the p-values\np_adjusted &lt;- p.adjust(p_values, method = \"bonferroni\")\n\n# Compare adjusted p-values to unadjusted p-values\np_comparison &lt;- cbind(p_values, p_adjusted)\np_comparison\n\n              p_values p_adjusted\n(Intercept) 0.49883151 1.00000000\nplacebo     0.12104977 0.72629859\nlow         0.01856222 0.11137333\nmedium      0.01097151 0.06582903\nhigh        0.44234244 1.00000000\ngender      0.12415264 0.74491582\n\n# Get confidence intervals\nconf_intervals &lt;- confint(model2_attach)\nconf_intervals\n\n                  2.5 %     97.5 %\n(Intercept) -0.29268916 0.14349242\nplacebo     -0.03315881 0.27976619\nlow          0.03304940 0.35315426\nmedium       0.04956814 0.37278247\nhigh        -0.10544029 0.23951403\ngender      -0.19775102 0.02425639\n\n\nA model including gender does not seem to help anything. What about with all covariates (just for fun)?\n\n# Run a SLR with attachment loss change as the DV and gender as a covariate\nmodel3_attach &lt;- lm(attachchange ~ placebo + low + medium + high + gender + race + age + smoker + sites + attachbase + pdbase , data = data_missing) \nsummary(model3_attach)\n\n\nCall:\nlm(formula = attachchange ~ placebo + low + medium + high + gender + \n    race + age + smoker + sites + attachbase + pdbase, data = data_missing)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.60415 -0.16360  0.02131  0.16645  0.57481 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.160630   0.480855   0.334 0.739127    \nplacebo      0.017898   0.079113   0.226 0.821541    \nlow          0.151477   0.077598   1.952 0.054074 .  \nmedium       0.172196   0.079066   2.178 0.032060 *  \nhigh         0.045948   0.081336   0.565 0.573557    \ngender      -0.050360   0.054953  -0.916 0.361924    \nrace         0.032325   0.026871   1.203 0.232181    \nage         -0.002472   0.002709  -0.912 0.364067    \nsmoker       0.069384   0.055052   1.260 0.210844    \nsites       -0.001501   0.002539  -0.591 0.555885    \nattachbase  -0.165154   0.043233  -3.820 0.000246 ***\npdbase       0.094444   0.072226   1.308 0.194370    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2456 on 89 degrees of freedom\n  (2 observations deleted due to missingness)\nMultiple R-squared:  0.2908,    Adjusted R-squared:  0.2032 \nF-statistic: 3.318 on 11 and 89 DF,  p-value: 0.0007435\n\n# Apply Bonferroni correction\np_values &lt;- summary(model3_attach)$coefficients[,4] # This line selects the fourth column of the resulting coefficients table from summary(model_attach), which is the p-values\np_adjusted &lt;- p.adjust(p_values, method = \"bonferroni\")\n\n# Compare adjusted p-values to unadjusted p-values\np_comparison &lt;- cbind(p_values, p_adjusted)\np_comparison\n\n               p_values  p_adjusted\n(Intercept) 0.739127379 1.000000000\nplacebo     0.821540779 1.000000000\nlow         0.054074263 0.648891153\nmedium      0.032060232 0.384722790\nhigh        0.573556835 1.000000000\ngender      0.361924208 1.000000000\nrace        0.232180658 1.000000000\nage         0.364067210 1.000000000\nsmoker      0.210844428 1.000000000\nsites       0.555885079 1.000000000\nattachbase  0.000246493 0.002957916\npdbase      0.194369803 1.000000000\n\n# Get confidence intervals\nconf_intervals &lt;- confint(model3_attach)\nconf_intervals\n\n                   2.5 %       97.5 %\n(Intercept) -0.794818756  1.116078333\nplacebo     -0.139298371  0.175094025\nlow         -0.002709329  0.305662938\nmedium       0.015093048  0.329298305\nhigh        -0.115665991  0.207561289\ngender      -0.159551369  0.058830891\nrace        -0.021067104  0.085716311\nage         -0.007855313  0.002911679\nsmoker      -0.040003508  0.178771331\nsites       -0.006546011  0.003543891\nattachbase  -0.251056725 -0.079251471\npdbase      -0.049067286  0.237955593\n\n\nNope. Interestingly the best predictor of attachment loss at 1 year is attachment loss at baseline. The results for pocket depth are likely the same.\nI also want to assess if controlling for baseline attachment loss or pocket depth change affects things, since those were strong predictors of each measurement at 1 year (still need to add those SLRs).\n\nmodel4_attach &lt;- lm(attachchange ~ placebo + low + medium + high + attachbase, data = data_missing)\nsummary(model4_attach)\n\n\nCall:\nlm(formula = attachchange ~ placebo + low + medium + high + attachbase, \n    data = data_missing)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.52469 -0.16400  0.02084  0.15231  0.73422 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.10707    0.09304   1.151   0.2527    \nplacebo      0.04202    0.07616   0.552   0.5824    \nlow          0.14605    0.07592   1.924   0.0573 .  \nmedium       0.17586    0.07622   2.307   0.0232 *  \nhigh         0.02665    0.08087   0.330   0.7425    \nattachbase  -0.12902    0.03039  -4.246 4.99e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2475 on 97 degrees of freedom\nMultiple R-squared:  0.2357,    Adjusted R-squared:  0.1963 \nF-statistic: 5.984 on 5 and 97 DF,  p-value: 7.236e-05\n\n# Apply Bonferroni correction\np_values &lt;- summary(model4_attach)$coefficients[,4] # This line selects the fourth column of the resulting coefficients table from summary(model_attach), which is the p-values\np_adjusted &lt;- p.adjust(p_values, method = \"bonferroni\")\n\n# Compare adjusted p-values to unadjusted p-values\np_comparison &lt;- cbind(p_values, p_adjusted)\np_comparison\n\n                p_values   p_adjusted\n(Intercept) 2.526878e-01 1.0000000000\nplacebo     5.824313e-01 1.0000000000\nlow         5.731590e-02 0.3438953943\nmedium      2.317045e-02 0.1390227138\nhigh        7.424826e-01 1.0000000000\nattachbase  4.989769e-05 0.0002993861\n\n\nAfter controlling for treatment group, the only significant predictor of attachment loss change is baseline attachment loss scores (padj = 0.001)\nThese models were just for exploration, fun, and practice. The final models selected are the SLR’s with treatment group as the IV and either attachment loss change or pocket depth change as the DV. Back to top of tabset"
  },
  {
    "objectID": "Project_1_Regression/Project_1_Regression_R/Project_1_Regression.html#Assumptions",
    "href": "Project_1_Regression/Project_1_Regression_R/Project_1_Regression.html#Assumptions",
    "title": "Advanced Data Analysis - Project 1",
    "section": "Evaluating Assumptions",
    "text": "Evaluating Assumptions\nIn order to evaluate the assumptions of our models, we will first gather the residuals of the model predicting attachment loss change score and the model predicting pocket depth change score.\n\n# Calculate the jackknife residuals of model_attach\njackknife_residuals_attach &lt;- rstudent(model_attach1)\n\n# Calculate the jackknife residuals of model_pd\njackknife_residuals_pd &lt;- rstudent(model_pd)\n\nNow that we have our residuals, we can take a closer look at the assumptions.\n\nLinearityIndependenceNormalityEqual Variances (Homoscedasticity)Residuals Centered Around ZeroSummary\n\n\nSince the IV is categorical, we do not need to assess linearity.\nBack to top of tabset\n\n\nIndependence can be assessed in part by the study design and how the data was collected. Based on the information provided by the PI, I will assume subjects are independent from each other (e.g. not siblings).\nAdditionally, we can examine a scatter plot of the model’s residuals against any time point variable (such as ID). Let’s do that.\n\n# Create a scatterplot of jackknife residuals vs ID to assess independence for pocket depth change\nggplot(data_missing, aes(x = id, y = jackknife_residuals_attach)) + \n  geom_point() +\n  geom_hline(yintercept = 0) +\n  labs(title = \"Scatterplot of Jackknife Residuals vs ID for Attachment Loss Change\",\n       x = \"ID\",\n       y = \"Jackknife Residuals\")\n\n\n\n\n\n\n\n# Create a scatterplot of jackknife residuals vs ID to assess independence for pocket depth change\nggplot(data_missing, aes(x = id, y = jackknife_residuals_pd)) + \n  geom_point() +\n  geom_hline(yintercept = 0) +\n  labs(title = \"Scatterplot of Residuals vs ID for Pocket Depth Change\",\n       x = \"ID\",\n       y = \"Jackknife Residuals\")\n\n\n\n\n\n\n\n\nThe pattern appears random, suggesting independence.\nNote: The gap between ID’s 170 and 200 looks odd, but is an artifact from how the experimenters assigned ID, with females starting at 101, and males starting at 201.\nBack to top of tabset\n\n\nHere we will asses that, for any fixed value of X, Y has a normal distribution. We will do this using Q-Q plots and histograms of the residuals.\n\n# Make the Q-Q plots using the jackknife residuals for attachment loss change\nqqnorm(jackknife_residuals_attach, main = \"Q-Q plots of Jackknife Residuals for model_attach\")\nqqline(jackknife_residuals_attach, col = \"black\")\n\n\n\n\n\n\n\n# Create histogram of jackknife residuals for model_attach\nhist(jackknife_residuals_attach, main = \"Histogram of Jackknife Residuals\",\n     xlab = \"Jackknife Residuals\",\n     col = \"lightblue\",\n     border = \"black\")\n\n\n\n\n\n\n\n# Make the Q-Q plots using the jackknife residuals for pocket depth change\nqqnorm(jackknife_residuals_pd, main = \"Q-Q plots of Jackknife Residuals for model_pd\")\nqqline(jackknife_residuals_pd, col = \"black\")\n\n\n\n\n\n\n\n# Create histogram of jackknife residuals for model_pd\nhist(jackknife_residuals_pd, main = \"Histogram of Jackknife Residuals\",\n     xlab = \"Jackknife Residuals\",\n     col = \"lightblue\",\n     border = \"black\")\n\n\n\n\n\n\n\n\nThe histogram for attachment loss change is a little left-tailed. This could be from an outlier. Comparatively, the Q-Q plot and histogram of the residuals for pocket depth change are normally distributed.\nWe can also include the Shapiro-Wilk test of normality, which provides a p-value and allows us to numerically establish that the assumption of normality is met. If the p-value is &lt; 0.05 you conclude that the assumption of normality is not met.\n\nshapiro.test(jackknife_residuals_attach)\n\n\n    Shapiro-Wilk normality test\n\ndata:  jackknife_residuals_attach\nW = 0.9604, p-value = 0.003601\n\nshapiro.test(jackknife_residuals_pd)\n\n\n    Shapiro-Wilk normality test\n\ndata:  jackknife_residuals_pd\nW = 0.99059, p-value = 0.6933\n\n\nThe assumption of normality is violated for the attachment loss change model (p = 0.0036), but not for the pocket depth change model (p = 0.6933). Looking at the histogram of the residuals for attachment loss change, this is likely due to an outlier.\nSummary\nFor attachment loss change, we have a slight violation of normality, which could be due to the presence of an outlier. However, regressions are robust to violations of assumptions and this may not actually be an issue. Outliers in the model will be assessed using jackknife residuals to confirm that these points do not have an excessive amount of influence on the model.\nBased on the Q-Q plots and histograms of the residuals for pocket depth change, we can conclude that we satisfy the assumption of normality.\nBack to top of tabset\n\n\nUsing the scale-location plot will allow us to evaluate the constant variance assumption. This will allow us to see whether the variability of the residuals is roughly constant between each group (Source).\nTo assess homoscedasticity we examine the residual scatterplots by treatment group. The warning sign to look for here is if the variance differs greatly across groups.\n\n# Make the residual scatterplot using the jackknife residuals for model_attach\nggplot(data_missing, aes(x = trtgroup, y = jackknife_residuals_attach)) +\n  geom_point() + \n  labs(title = \"Jackknife Residuals vs Treatment Condition for Attachment Loss Change\",\n       x = \"Treatment Condition\",\n       y = \"JackKnife Residuals\")\n\n\n\n\n\n\n\n# Make the residual scatterplot using the jackknife residuals for model_pd\nggplot(data_missing, aes(x = trtgroup, y = jackknife_residuals_pd)) +\n  geom_point() + \n  labs(title = \"Jackknife Residuals vs Treatment Condition for Pocket Depth Change\",\n       x = \"Treatment Condition\",\n       y = \"JackKnife Residuals\")\n\n\n\n\n\n\n\n\nIt appears that our variances in all groups are equal!\nAdditionally, while it is not recommended to perform a statistical test to assess for equality of variances (because formal tests of equality of variance are not very powerful), we can still do this using Bartlett’s test.\n\nbartlett.test(attachchange ~ trtgroup, data = data_missing)\n\n\n    Bartlett test of homogeneity of variances\n\ndata:  attachchange by trtgroup\nBartlett's K-squared = 2.5462, df = 4, p-value = 0.6364\n\n\nThe null hypothesis of the Bartlett test is that the variances are equal. Thus failing to reject the null (p &gt; 0.05) indicates that the data are consistent with the equal variance assumption.\nSo we meet the assumption of equality of variances, looking good!\nBack to top of tabset\n\n\nTo start, we can simply check that the mean of our residuals is close to 0.\n\n#### For attachment loss change score\n# Generate the mean of the jackknife residuals for attachment loss change. Should be close to 0.\nmean(jackknife_residuals_attach)\n\n[1] -0.004132486\n\n\nWe are looking good for attachment loss change score. What about for pocket depth change?\n\n#### For pocket depth change score\n# Generate the mean of the jackknife residuals for attachment loss change. Should be close to 0.\nmean(jackknife_residuals_pd)\n\n[1] -6.695748e-06\n\n\nA more sophisticated approach is to plot the fitted values vs jackknife residuals. We can then compare the trend between groups to see if they are random. The fitted line should gravitate around 0 with no obvious trends.\n\n# Generate vectors containing the fitted values vs jackknife residuals so we can plot them \nfitted_values_pd &lt;- fitted(model_pd)\n\nggplot(data_missing, aes(x = fitted_values_pd, y = jackknife_residuals_pd)) + \n  geom_point() +\n  geom_smooth(method = \"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n  labs(title = \"Jackknife Residuals vs Fitted Values for Attachment Loss Change\",\n       x = \"Fitted Values\",\n       y = \"Jackknife Residuals\")\n\n$x\n[1] \"Fitted Values\"\n\n$y\n[1] \"Jackknife Residuals\"\n\n$title\n[1] \"Jackknife Residuals vs Fitted Values for Attachment Loss Change\"\n\nattr(,\"class\")\n[1] \"labels\"\n\n\nThe pattern looks random and we can conclude we meet this assumption.\nBack to top of tabset\n\n\nWe meet the assumptions of independence, equal variances, and errors centered around zero required for this analysis. There is a slight violation of normality for the attachment loss change model, but this could be due to an outlier."
  },
  {
    "objectID": "Project_1_Regression/Project_1_Regression_R/Project_1_Regression.html#Multiple_imputation",
    "href": "Project_1_Regression/Project_1_Regression_R/Project_1_Regression.html#Multiple_imputation",
    "title": "Advanced Data Analysis - Project 1",
    "section": "Multiple Imputation",
    "text": "Multiple Imputation\n\nBackgroundCoding ExamplePerforming the Multiple Imputation\n\n\nNote: This information is taken from Biostats II (BIOS 6612) slides, week 13 lecture 20, and from Flexible Imputation of Missing Data 2nd edition.\nMissing Data\nTo review, there are several assumptions for missing data. The first is MCAR (Missing Completely at Random). This is when the probability of having a missing value is the same for everyone, and is rarely ever the case with real data.\nThe second is MAR (missing at random). This is when the probability of having a missing value is the same within groups defined by the observed data. That is, if we know that second variable by which the data is NOT MCAR, in addition to the measurements, we can assume MCAR within that second variable. For instance, if we know the missing data is not equal between the sexes, then we can assume MCAR within groups defined by sex. MAR is often observed in situations such as males missing more data than females, or older participants missing more data than younger ones, etc. MAR is more general and more realistic than MCAR. Modern missing data assumptions typically start from the MAR assumption.\nMNAR (Missing Not At Random) is the final category of missingness, and is when the the probability of having a missing value varies for reasons that are unknown to you. For example, in public opinion research, those with weaker opinions may respond less than those with stronger opinions, and you may not know this ahead of time. MNAR is the most complex and if you have it, you have your work cut out for you. In that situation, you can find more data about the causes for the missingness, or run lots and lots of sensitivity analyses.\nAnd as they say, the best way to handle missing data is not to have it.\nMultiple Imputation\nIn the current experiment, we know that data is not MCAR (males are missing more data than females), and thus cannot simply throw out subjects with missing values. If the data are not MCAR, listwise deletion (deleting any subject with a missing value) can severely bias estimates of means, regression coefficients and correlations.\nMultiple Imputation is a commonly used method to handle missing data when data is not MCAR, but is MAR. It is a process by which you use observed data to “predict” missing data, then use those “imputed” values in further analyses. Multiple imputation builds upon and pools together “single” imputation approaches.\nFor example, there exists very simple ways of imputing data, such as plugging in the average or median values in place of missing values. This is not sophisticated and kind of sketchy.\nThen there exists regression imputation, where for each variable you fit a regression model of it to the observed data, and then use that model to predict missing values, and use those predictions in place of the missing values. However, this injects bias into the estimate for the correlation between X and Y (since the values fall perfectly in line with the hypothesis that X and Y have a non zero correlation. (and are in a perfect line)).\n\nThen there is stochastic regression imputation, which is like regression imputation but adds noise back into the imputations based on the variance of their residuals. This helps account for variance in the results due to missing data.\n\nMultiple imputation works by fitting multiple stochastic imputation models (can be 100’s or 1000’s), analyzing each dataset separately to produce estimates of interest, and finally pooling together these statistics of interest. If done properly, the pooled statistics are unbiased under MAR, and the SE’s will be correct!\n\nMultiple Imputation is “simple, elegant and powerful. It is simple because it fills the holes in the data with plausible values. It is elegant because the uncertainty about the unknown data is coded in the data itself. And it is powerful because it can solve “other” problems that are actually missing data problems in disguise.” - Stef van Buuren\nBack to top of tabset\n\n\nFirst we will begin with an example of multiple imputation using the built in airquality dataset in R.\n\n# Examining data set and missingness\nhead(airquality)\n\n  Ozone Solar.R Wind Temp Month Day\n1    41     190  7.4   67     5   1\n2    36     118  8.0   72     5   2\n3    12     149 12.6   74     5   3\n4    18     313 11.5   62     5   4\n5    NA      NA 14.3   56     5   5\n6    28      NA 14.9   66     5   6\n\ncolSums(is.na(airquality))\n\n  Ozone Solar.R    Wind    Temp   Month     Day \n     37       7       0       0       0       0 \n\nvis_miss(airquality)\n\n\n\n\n\n\n\n\nWe’re missing 37 values in the Ozone column (24% of values!). We will have to address this missing data somehow.\nMean Imputation\nWe could quickly fill in missing data with mean imputation…\n\n# Here's how you do mean imputation in mice\nimp &lt;- mice(airquality, method = \"mean\", m = 1, maxit = 1)\n\n\n iter imp variable\n  1   1  Ozone  Solar.R\n\n\nBut mean imputation will “underestimate the variance, disturb the relations between variables, bias almost any estimate other than the mean and bias the estimate of the mean when data are not MCAR.” You can use mean imputation as a quick fix when there’s a handful of missing values, but it should be avoided in general.\nRegression Imputation\nWe could alternatively do regression imputation, where we fit a model and then use that model to predict the missing values. In regression imputation, you are essentially using the observed data to predict the missing data.\n\n# Performing regression imputation using the mice package\ndata_example &lt;- airquality[, c(\"Ozone\", \"Solar.R\")]\nimp &lt;- mice(data_example, method = \"norm.predict\", seed = 1,\n           m = 1, print = FALSE)\n\n# Plotting missing vs observed values\nxyplot(imp, Ozone ~ Solar.R,\n       main = \"Missing vs Observed values for Ozone ~ Solar.R\",\n       auto.key = list(corner = c(0,1),\n                  points = TRUE, \n                  text = c(\"Observed\", \"Missing\"), col = c(\"steelblue\", \"red3\")))\n\n\n\n\n\n\n\n\nYou predict the values of the missing datapoints with the regression equation resulting from your model. That is, the imputed values correspond to the most likely values under that model. However, the imputed values (red) vary less than the observed values (blue). So each value is the best under the model, but it is very unlikely that the real values would have had this distribution.\nRegression imputation yields unbiased estimates of the means and of the regression weights of the model under MCAR. It also does so under MAR, provided that the factors that influence missinginess are part of the model. However, correlations are biased to be greater/higher.\n“Regression imputation, as well as its modern incarnations in machine learning is probably the most dangerous of all methods described here. We may be led to believe that we’re to do a good job by preserving the relations between the variables. In reality however, regression imputation artificially strengthens the relations in the data. Correlations are biased upwards. Variability is underestimated. Imputations are too good to be true. Regression imputation is a recipe for false positive and spurious relations.”\nStochastic regression imputation\nStochastic regression imputation is a refinement of regression imputation that attempts to address correlation bias by adding noise back into the predictions of the missing values.\n\n# Impute Ozone from Solar.R by stochastic regression imputation using the mice package.\ndata_example &lt;- airquality[, c(\"Ozone\", \"Solar.R\")]\n\n# Perform stochastic regression imputation\nimp &lt;- mice(data_example, method = \"norm.nob\", m = 1, maxit = 1, seed = 1, print = FALSE)\n\n# Plotting missing vs observed values\nxyplot(imp, Ozone ~ Solar.R,\n       main = \"Missing vs Observed values for Ozone ~ Solar.R\",\n       auto.key = list(corner = c(0,1),\n                  points = TRUE, \n                  text = c(\"Observed\", \"Missing\"), col = c(\"steelblue\", \"red3\")))\n\n\n\n\n\n\n\n\n“The method = norm.nob argument requests a plain, non-Bayesian, stochastic regression method. This method first estimates the intercept, slope and residual variance under the linear model, then calculates the predicted value for each missing value, and adds a random draw from the residual to the prediction”.\nThis can create issues though, such as we have one imputed value that is negative! Which might not be plausible (such as in this case, there is no such thing as a negative ozone level).\nA more convenient solution is multiple imputation\nMultiple Imputation\nMultiple imputation creates m &gt; 1 complete data sets. The m results are then pooled into a final point estimate plus standard error. So each data set is identical in observed values, but differs in imputed values.\nWe then estimate the parameters of interest from each dataset. This is done by applying the analytic method you would have used if the dataset was complete in the first place (here, a regression). The results of the model on each dataset will differ because the data is different. These differences are caused by the uncertainty of what value to impute.\nThe last step is that these parameter estimates are then pooled together into a single value, and its variance estimated. The variance is assessed by combining the “within-imputation variance with the extra variance caused by the missing data (between-imputation variance)”. So under the appropriate conditions the pooled estimates are unbiased. MI solves the problem of ‘too small’ SEs of other imputation methods we just covered.\nNow let’s perform the multiple imputation on the airquality data set.\n\n# Perform multiple imputation on the airquality data set\nimp &lt;- mice(airquality, seed = 1, m = 20, print = FALSE) # This line imputes the missing data 20 times\n\n# Fit a linear regression \nmodel_imp &lt;- with(imp, lm(Ozone ~ Wind + Temp + Solar.R)) # You have to run the regression with \"with(imp, lm(etc))\n                  \nsummary(model_imp) # This runs a regression on each imputed dataset (so 20 different regressions)\n\n# A tibble: 80 × 6\n   term        estimate std.error statistic  p.value  nobs\n   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;int&gt;\n 1 (Intercept) -60.7      17.9        -3.40 8.78e- 4   153\n 2 Wind         -2.95      0.514      -5.75 4.96e- 8   153\n 3 Temp          1.53      0.197       7.74 1.42e-12   153\n 4 Solar.R       0.0671    0.0184      3.64 3.70e- 4   153\n 5 (Intercept) -56.8      18.5        -3.07 2.56e- 3   153\n 6 Wind         -3.33      0.532      -6.25 4.04e- 9   153\n 7 Temp          1.56      0.205       7.58 3.49e-12   153\n 8 Solar.R       0.0554    0.0191      2.89 4.37e- 3   153\n 9 (Intercept) -74.0      18.7        -3.95 1.19e- 4   153\n10 Wind         -2.69      0.540      -4.99 1.67e- 6   153\n# ℹ 70 more rows\n\nsummary(pool(model_imp)) # This pools together the parameters of all 20 regressions (statistic is Wald's test) into a single model.\n\n         term     estimate   std.error statistic       df      p.value\n1 (Intercept) -65.87829658 23.09377412 -2.852643 69.97033 5.696702e-03\n2        Wind  -3.01897171  0.66252377 -4.556775 70.51194 2.125022e-05\n3        Temp   1.63483547  0.25107557  6.511328 75.99913 7.203792e-09\n4     Solar.R   0.05861581  0.02267832  2.584662 90.10797 1.135441e-02\n\n\n\n# mids workflow using pipes\nest3 &lt;- airquality %&gt;%\n  mice(seed = 1, print = FALSE, m = 20) %&gt;%\n  with(lm(formula = Ozone ~ Wind + Temp + Solar.R)) %&gt;%\n  pool()\nsummary(est3)\n\n         term     estimate   std.error statistic       df      p.value\n1 (Intercept) -65.87829658 23.09377412 -2.852643 69.97033 5.696702e-03\n2        Wind  -3.01897171  0.66252377 -4.556775 70.51194 2.125022e-05\n3        Temp   1.63483547  0.25107557  6.511328 75.99913 7.203792e-09\n4     Solar.R   0.05861581  0.02267832  2.584662 90.10797 1.135441e-02\n\n\n\n# Run a regression on the imputed airquality data set. \nimp &lt;- mice(airquality, seed = 1, print = FALSE)\n\nfit &lt;- with(imp, lm(Ozone ~ Solar.R))\n\n# Print out the estimates for the first and second data set\ncoef(fit$analyses[[1]])\n\n(Intercept)     Solar.R \n 22.2963201   0.1057437 \n\ncoef(fit$analyses[[2]])\n\n(Intercept)     Solar.R \n 20.7891622   0.1140081 \n\n\nNotice that the paremter estimates differ because of the uncertainty created by the missing data.\nApplying the standard pooling rules is done with\n\nest &lt;- pool(fit)\nsummary(est)\n\n         term   estimate  std.error statistic       df      p.value\n1 (Intercept) 21.5321395 5.67991951  3.790923 136.3544 0.0002245626\n2     Solar.R  0.1091546 0.02846503  3.834693 102.7169 0.0002170873\n\n\nAny R expression produced by expression() can be used on the multiply imputed data…\n\n# Extract residuals and fitted values\n\n# This gets you ALL the residuals for ALL the models/dataset\nimp_residuals &lt;- sapply(model_imp[[4]], residuals)\n\n# Same thing but for the fitted\nimp_fitted &lt;- sapply(model_imp[[4]],fitted)\n\n# Don't know where to go from here. Apparently you can take the average of all of these to get the average residuals, but it's not published anywhere.\n\nWe can compare the results of our multiple imputation model with the listwise deletion model to see how different they came out.\n\nmodel_non_imp &lt;- lm(Ozone ~ Wind + Temp + Solar.R, data = airquality, na.action = na.omit)\nsummary(model_non_imp)\n\n\nCall:\nlm(formula = Ozone ~ Wind + Temp + Solar.R, data = airquality, \n    na.action = na.omit)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-40.485 -14.219  -3.551  10.097  95.619 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -64.34208   23.05472  -2.791  0.00623 ** \nWind         -3.33359    0.65441  -5.094 1.52e-06 ***\nTemp          1.65209    0.25353   6.516 2.42e-09 ***\nSolar.R       0.05982    0.02319   2.580  0.01124 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 21.18 on 107 degrees of freedom\n  (42 observations deleted due to missingness)\nMultiple R-squared:  0.6059,    Adjusted R-squared:  0.5948 \nF-statistic: 54.83 on 3 and 107 DF,  p-value: &lt; 2.2e-16\n\nsummary(pool(model_imp))\n\n         term     estimate   std.error statistic       df      p.value\n1 (Intercept) -65.87829658 23.09377412 -2.852643 69.97033 5.696702e-03\n2        Wind  -3.01897171  0.66252377 -4.556775 70.51194 2.125022e-05\n3        Temp   1.63483547  0.25107557  6.511328 75.99913 7.203792e-09\n4     Solar.R   0.05861581  0.02267832  2.584662 90.10797 1.135441e-02\n\n\nOur regression using MI and the complete case (i.e. listwise deletion) data set are comparable!\nNote: what we are NOT doing in multiple imputation is taking an average of the imputed data and running a model on that as if its a single, complete data set. “Researchers are often tempted to average the multiply imputed data, and analyze the averaged data as if it were complete. This method yields incorrect standard errors, confidence intervals and p-values, and thus should not be used if any form of statistical testing or uncertainty analysis is to be done on the imputed data. The reason is that the procedure ignores the between-imputation variability, and hence shares all the drawbacks of single imputation”.\nNote: It’s recommended to impute then transform, because if you create variables based on other variables, there are relationships between those variables that MI doesn’t account for (it might create combinations of variables that are unrealistic)\nYou can do this as\n\n# Example of impute then transform method (i.e. how to add variables to an imputed dataset)\ndata_ex &lt;- boys[, c(\"age\", \"hgt\", \"wgt\", \"hc\", \"reg\")]\nimp &lt;- mice(data_ex, print = FALSE, seed = 1)\n \n# put the data in long format\nlong &lt;- mice::complete(imp, \"long\", include = TRUE)\n\nlong$new_var &lt;- with(long, 100 * wgt / hgt)\n\nimp.itt &lt;- as.mids(long)\n\nBack to top of tabset\n\n\nNow that we have reviewed the background and gone through coding examples of multiple imputation, we are ready to perform it for the dataset for Project 1.\n\n# Pool the parameters of interest for a regression on the imputed values for attach change control as reference\nmodel_imp &lt;- data %&gt;%\n  mice(seed = 1, print = FALSE, m = 20) %&gt;%\n  with(lm(attachchange ~ placebo + low + medium + high)) %&gt;%\n  pool()\n\nWarning: Number of logged events: 1158\n\n\nNone of the p-values for the pooled regressions on the MI data are significant (p &gt; 0.05).\nHow do those parameter estimates compare to the complete case analysis model?\n\n# Compare coefficients to complete case analysis for attachment loss with control as reference category\nMI &lt;- summary(model_imp)\nMI\n\n         term   estimate  std.error  statistic       df     p.value\n1 (Intercept) -0.1853493 0.06920536 -2.6782512 65.79196 0.009337056\n2     placebo  0.1158204 0.09076258  1.2760813 91.37003 0.205161055\n3         low  0.1608502 0.10027097  1.6041549 59.48514 0.113976747\n4      medium  0.1612700 0.11393844  1.4154130 37.61151 0.165174849\n5        high  0.0930271 0.15435067  0.6026997 18.41109 0.554060413\n\ncomplete_case &lt;- summary(model_attach1)\ncomplete_case$coefficients\n\n               Estimate Std. Error    t value     Pr(&gt;|t|)\n(Intercept) -0.22169165 0.05590110 -3.9657832 0.0001392148\nplacebo      0.13461856 0.07905610  1.7028232 0.0917709125\nlow          0.20387790 0.08091650  2.5196086 0.0133650749\nmedium       0.21513551 0.08196711  2.6246567 0.0100625287\nhigh         0.05690063 0.08727557  0.6519652 0.5159498143\n\n\nThey are drastically different, although still not significant.\nWe can also plot the imputed values for a sample of m dataset (not too many or it’s hard to see!) in order to visually assess that our imputed values are comparabe to the observed values.\n\n# Plot imputed values for m = 5 data set\nimpy &lt;- mice(data, seed = 1, print = FALSE, m = 5)\n\nWarning: Number of logged events: 285\n\nmodel_impy &lt;- with(imp, attachchange ~ trtgroup)\n\n# This is cool, here's how I can plot the imputed values for attach change \nxyplot(impy, attachchange ~ trtgroup,\n       main = \"Example of Imputed Values for m = 5 Dataset\",\n       auto.key = list(space = \"right\",\n        points = TRUE, \n        text = c(\"Observed\", \"Missing\"), col = c(\"blue4\", \"red3\")))\n\n\n\n\n\n\n\n\nWhat about for the same model but with placebo as the reference group?\n\n# Pool the parameters of interest for a regression on the imputed values for attach change placebo as reference\nmodel_imp &lt;- data %&gt;%\n  mice(seed = 1, print = FALSE, m = 20) %&gt;%\n  with(lm(attachchange ~ control + low + medium + high)) %&gt;%\n  pool()\n\nWarning: Number of logged events: 1158\n\n# Compare coefficients for complete case analysis with attachment with placebo as reference category\nsummary(model_imp)\n\n         term    estimate  std.error  statistic       df   p.value\n1 (Intercept) -0.06952890 0.06697153 -1.0381859 75.81298 0.3024834\n2     control -0.11582043 0.09076258 -1.2760813 91.37003 0.2051611\n3         low  0.04502973 0.11027780  0.4083299 41.79227 0.6851161\n4      medium  0.04544952 0.11870540  0.3828766 33.28878 0.7042453\n5        high -0.02279333 0.16350701 -0.1394028 16.62753 0.8908064\n\nsummary(model_attach2)$coefficients\n\n               Estimate Std. Error    t value   Pr(&gt;|t|)\n(Intercept) -0.08707309 0.05590110 -1.5576275 0.12254523\ncontrol     -0.13461856 0.07905610 -1.7028232 0.09177091\nlow          0.06925934 0.08091650  0.8559360 0.39412117\nmedium       0.08051695 0.08196711  0.9823081 0.32836700\nhigh        -0.07771793 0.08727557 -0.8904889 0.37538435\n\n\nThe estimates are more comparable, although still pretty different. Still not significant with both methods.\nAnd finally for pocket depth change.\n\n# Pool the parameters of interest for a regression on the imputed values for pocket depth  change placebo as reference\nmodel_imp &lt;- data %&gt;%\n  mice(seed = 1, print = FALSE, m = 20) %&gt;%\n  with(lm(pdchange ~ placebo + low + medium + high)) %&gt;%\n  pool()\n\nWarning: Number of logged events: 1158\n\n# Compare coefficients with complete case analysis for pocket depth change\nMI &lt;- summary(model_imp)\nMI\n\n         term    estimate  std.error  statistic       df      p.value\n1 (Intercept) -0.33597389 0.06144440 -5.4679330 66.87925 7.296278e-07\n2     placebo -0.00982096 0.08061544 -0.1218248 92.74234 9.033013e-01\n3         low  0.11415425 0.07995226  1.4277802 96.05307 1.565984e-01\n4      medium  0.11260463 0.08900896  1.2650932 60.47912 2.106907e-01\n5        high -0.02704497 0.10936047 -0.2473011 30.38938 8.063384e-01\n\n\nHere the models are a LOT more comparable, although still not significant. It seems we can conclude that missing values impact the attachment loss measurements more than pocket depth change.\nLet’s finish by plotting the imputed values for pocket depth change\n\n# Plot imputed values for m = 5 data set\nimpy &lt;- mice(data, seed = 1, print = FALSE, m = 5)\n\nWarning: Number of logged events: 285\n\nmodel_impy &lt;- with(imp, pdchange ~ trtgroup)\n\n# This is cool, here's how I can plot the imputed values for attach change \nxyplot(impy, pdchange ~ trtgroup,\n       main = \"Example of Imputed Values for m = 5 Dataset\",\n       auto.key = list(space = \"right\",\n        points = TRUE, \n        text = c(\"Observed\", \"Missing\"), col = c(\"blue4\", \"red3\")))\n\n\n\n\n\n\n\n\nLooks pretty good.\nBack to top of tabset"
  },
  {
    "objectID": "Project_1_Regression/Project_1_Regression_R/Project_1_Regression.html#Jackknife",
    "href": "Project_1_Regression/Project_1_Regression_R/Project_1_Regression.html#Jackknife",
    "title": "Advanced Data Analysis - Project 1",
    "section": "Identify Outliers using Jackknife Residuals",
    "text": "Identify Outliers using Jackknife Residuals\nIn this section we will explore how our analysis would have changed if we identified outliers using the jackknife residuals.\n\nBackgroundIdentifying OutliersRerunning the Model with Outliers Removed\n\n\n*Note: This information is from BIOS 6602 Week 6 Lecture 10\nJackknife residuals are a type of residual used in regressions that allows us to detect outliers that have a significant impact on the regression coefficients of the model.\nJackknife residuals are calculated by systematically leaving out one observation (i) at a time from a dataset, fitting the regression model to the remaining dataset, and predicting the left out observation. The residual for each observation is then computed based on that prediction.\nThis process follows three step:\n\nFit the regression model to the data, excluding observation (i)\nUse the model to predict the left-out observation (i)\nCalculate the jackknife residual as the difference between the actual value and that fitted value\n\nJackknife residuals are similar to standardized residuals. Standardized residuals are standardized by the standard error of the regression, and are primarily used for identifying outliers. Jackknife residuals on the other hand allow you to assess the leverage of individuals data points, to see how much they actually influence the model (Source)\n\nJackknife residuals follow exactly a t(n-p-2) distribution, where approximately 5% of residuals are expected to exceed 1.96 in absolute value. Jackknife residuals make suspicious values more obvious compared to other residuals.\nDefinitions vary, but we generally consider a residual to be an outlier if the jackknife residual is +- 3.\nHowever, a potential outlier value may not actually have that dramatic of an impact on the model (which is what we are concerned that outliers will do). That is why we use jackknife residuals to investigate leverage and influence.\n\nExtreme X values can have high leverage.\nExtreme X values can have high influence\nA high-leverage point becomes an influential point if its Y value doesn’t follow the pattern of the rest of the data (i.e. is too low or too high)\n\n\nAn observation is influential if removing it substantially changes the estimate of the coefficients for that model. We use five measurements to assess influence: Jackknife residuals, Leverage, Cook’s Distance (Cook’s D), DFFITS, and DFBETAS.\n\nLeverage: Measures how far a measurement deviates from the mean. You want to examine values greater than 2(p+1)/n\nCook’s D: Measures the influence of an observation on regression predictions. You want to examine observations with d_i &gt; 1.0\nDFFITS: Measures the influence of an observation on regression predictions (related to Cook’s D). You want to examine observations outside the range of +-2(sqrt(p+10n)). If h_i is near zero, then that observation has little effect.\nDDFBETAS: Measure the influence of an observation on *individual* coefficient estimates. You want to examine estimates outside the range of +- 2/sqrt(n). A large DFBETA for variable k indicates that the i-th observation has a sizeable impact on the k-th regression coefficient.\n\nNote: p = the number of variables in the model\n\nBack to top of tabset\n\n\nThis website was used for coding information for this section, alongside the notes from BIOS 6602\nNow that we have covered the background for jackknife residuals, we can apply that knowledge to assess for outliers in our dataset for this project!\n\nAttachment LossPocket Depth\n\n\n\nSet upJackknife ResidualsLeverageCook’s DDFFITSDFBETASAdditional PlotsFinal Look and Conclusion\n\n\nRecall that we computed the jackknife residuals earlier.\n\n# Calculate the jackknife residuals of model_attach\njackknife_residuals_attach &lt;- rstudent(model_attach1)\n\n# Calculate the jackknife residuals of model_pd\njackknife_residuals_pd &lt;- rstudent(model_pd)\n\nLet’s generate a table so we can handily compare: ID, jackknife residual, leverage (diagonal hat), DFFITS, and DFBETAs for each participant.\n\n# Get leverage values (hat values)\nhat_values_attach &lt;- hatvalues(model_attach1)\n\n# Get Cook's D values\ncooks_d_attach &lt;- cooks.distance(model_attach1)\n\n# Get the DFFITS values\ndffits_attach &lt;- dffits(model_attach1)\n\n# Get the DFBETAS\ndfbetas_attach &lt;- dfbetas(model_attach1)\n\n# Make a table with ID and all diagnostic values\ndiagnostics_attach &lt;- data.frame(id = data_missing$id, jackknife_residuals = jackknife_residuals_attach, leverage = hat_values_attach, cooks_D = cooks_d_attach, dffits = dffits_attach, dfbetas = dfbetas_attach)\n\nkable(head(diagnostics_attach), caption = \"Diagnostics for Attachment Loss Change Model\", format = \"html\") %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\"))\n\n\nDiagnostics for Attachment Loss Change Model\n\n\nid\njackknife_residuals\nleverage\ncooks_D\ndffits\ndfbetas..Intercept.\ndfbetas.placebo\ndfbetas.low\ndfbetas.medium\ndfbetas.high\n\n\n\n\n101\n0.5800959\n0.0500000\n0.0035664\n0.1330831\n0.0000000\n0.0000000\n0.0000000\n0.0973313\n0.0000000\n\n\n103\n1.5996714\n0.0434783\n0.0228989\n0.3410511\n0.3410511\n-0.2411595\n-0.2356149\n-0.2325949\n-0.2184475\n\n\n104\n1.4044559\n0.0476190\n0.0195311\n0.3140459\n0.0000000\n0.0000000\n0.2270548\n0.0000000\n0.0000000\n\n\n105\n-0.8928811\n0.0434783\n0.0072626\n-0.1903629\n0.0000000\n-0.1346069\n0.0000000\n0.0000000\n0.0000000\n\n\n106\n-1.6995151\n0.0500000\n0.0298289\n-0.3898955\n0.0000000\n0.0000000\n0.0000000\n-0.2851530\n0.0000000\n\n\n107\n-2.7107885\n0.0476190\n0.0690131\n-0.6061507\n0.0000000\n0.0000000\n-0.4382463\n0.0000000\n0.0000000\n\n\n\n\n\n\n\nBack to top of tabset\n\n\nWe can start by making a plot of the jackknife residuals to assess if there are outliers &lt; -3 or &gt; 3 SDs).\n\n# Plot jackknife outiers and label any values that are &gt; 3 or &lt; -3.\nggplot(data_missing, aes(x = id, y = jackknife_residuals_attach)) + geom_point() + \n  geom_hline(yintercept = c(-3,0,3)) + ylim(-4,4) + \n  labs(y = \"Jackknife Residuals\",\n       title = \"Jackknife Residuals vs ID\") + \n  geom_text(aes(label = ifelse(jackknife_residuals_attach &gt; 3 | jackknife_residuals_attach &lt; -3, id, '')), \n            vjust = -1)\n\n\n\n\n\n\n\n\nParticipant 168 is a potential outlier based on the residual value.\nLet’s look at this participant more closely.\n\n# Examine participant 168 to assess what values could make them an outlier\ndata_missing[data_missing$id == 168,]\n\n    id trtgroup gender race      age smoker sites attachbase attach1year\n58 168        5      2    5 54.20397      0   168   5.089286    4.041667\n     pdbase  pd1year attachchange   pdchange placebo control low medium high\n58 3.410714 2.904762    -1.047619 -0.5059524       0       0   0      0    1\n   trt trt3groups\n58   1          3\n\n# Sort our dataset by descending order of attachment at baseline to see if participant 168 is the highest\n\nkable(head(data_missing[order(-data_missing$attachbase),]), caption = \"Diagnostics for Attachment Loss Change Model\", format = \"html\") %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\"))\n\n\nDiagnostics for Attachment Loss Change Model\n\n\n\nid\ntrtgroup\ngender\nrace\nage\nsmoker\nsites\nattachbase\nattach1year\npdbase\npd1year\nattachchange\npdchange\nplacebo\ncontrol\nlow\nmedium\nhigh\ntrt\ntrt3groups\n\n\n\n\n58\n168\n5\n2\n5\n54.20397\n0\n168\n5.089286\n4.041667\n3.410714\n2.904762\n-1.0476190\n-0.5059524\n0\n0\n0\n0\n1\n1\n3\n\n\n3\n104\n3\n2\n5\n55.17864\n1\n138\n4.956522\n5.304348\n5.217391\n4.891304\n0.3478261\n-0.3260870\n0\n0\n1\n0\n0\n1\n3\n\n\n40\n144\n2\n2\n2\n49.07598\n0\n126\n4.388889\n3.488000\n3.666667\n3.230159\n-0.9008889\n-0.4365079\n0\n1\n0\n0\n0\n0\n2\n\n\n102\n269\n3\n1\n5\n64.90075\n0\n162\n4.080247\n3.685185\n3.370370\n3.265432\n-0.3950617\n-0.1049383\n0\n0\n1\n0\n0\n1\n3\n\n\n18\n121\n4\n2\n5\n54.49692\n1\n138\n4.014493\n3.825000\n3.608696\n3.783333\n-0.1894928\n0.1746377\n0\n0\n0\n1\n0\n1\n3\n\n\n17\n120\n3\n2\n5\n41.83710\n1\n150\n3.886667\n3.920000\n4.200000\n3.880000\n0.0333333\n-0.3200000\n0\n0\n1\n0\n0\n1\n3\n\n\n\n\n\n\n\nWe have confirmed that participant 168 has the largest values for attachment loss at baseline (5.09). If that’s the case however, participant 3 is not far behind them (4.96) (who incidentally has the highest baseline pocket depth measurement), followed by participant 40 a large amount lower (4.39).\nWe can also (apparently) run a test statistic to test if we have an outlier using the ‘car’ package\n\noutlierTest(model_attach1)\n\nNo Studentized residuals with Bonferroni p &lt; 0.05\nLargest |rstudent|:\n    rstudent unadjusted p-value Bonferroni p\n58 -3.602896         0.00049863     0.051359\n\n\nThis is a test of a hypothesis that we do not have an outlier. We reject that hypothesis (p &lt; 0.05) so we have an outlier (I think).\nBack to top of tabset\n\n\nLeverage is a measure of geometric distance of an observation’s predictor point from the center point of the predictor space. In other words, leverage is a measurement of how far that observation deviates from the mean of that variable. High leverage observations have the potential to be very influential, but they are not necessarily influential. It’s possible for a high leverage point to not be influential, but very difficult for a low leverage point to be influential.\nLeverage is calculated as\n&lt;img src=“Media/leverage.png” width=“90%&gt;\nIf X _i is close to Xbar, then h_i is small (i.e. the i-th point has low leverage).\n\n# Caculate cutoff for leverage\ncutoff_leverage &lt;- 2*((4+1)/103)\ncutoff_leverage\n\n[1] 0.09708738\n\n# Make leverage plot in ggplot. Set a horizontal line at the cutoff point, and label ID's for participants that pass that line\nggplot(diagnostics_attach, aes(x = id, y = leverage)) +\n  geom_point() + \n  geom_hline(yintercept = cutoff_leverage) +\n  labs(title = \"Leverage Plot with Cutoff Line\",\n       x = \"ID\",\n       y = \"Hat Values\") + \n  geom_text(aes(label = ifelse(leverage &gt; cutoff_leverage, id, \"\")), vjust = -0.5)\n\n\n\n\n\n\n\n\nNone of our data points pass the cutoff point for leverage/hat values. I think this may be because the IV is categorical which changes the pattern and interpretation. Will check with a professor.\nBack to top of tabset\n\n\nCook’s D combines information about the residuals and leverage into a single value. A higher Cook’s D value signifies that the data point woud greatly change the regression coefficients, and is therefore influential and may impact the model’s accuracy.\nApparently we actually want to look at a cutoff of Cook’s D as 4/(n-p-1). Let’s calculate that and store it.\n\n# Calculate cutoff for Cook's D\ncutoff_d &lt;- 4/((nrow(data_missing) - length(model_attach1$coefficients) - 1))\ncutoff_d\n\n[1] 0.04123711\n\n# Make Cook's D plot in ggplot. Set a horizontal line at the cutoff point, and label ID's for participants that pass that line\nggplot(diagnostics_attach, aes(x = id, y = cooks_D)) +\n  geom_point() + \n  geom_hline(yintercept = cutoff_d) +\n  labs(title = \"Cook's D with Cutoff Line\",\n       x = \"ID\",\n       y = \"Cook's D\") + \n  geom_text(aes(label = ifelse(cooks_D &gt; cutoff_d, id, \"\")), vjust = -0.5)\n\n\n\n\n\n\n\n\nWe see that participants 107, 144, 146 and 168 are past the cutoff for Cook’s D. As when looking at the residuals plot, participant 168 is drastically different compared to the other potential outliers.\nBack to top of tabset\n\n\nNow we will examine DFFITS (Difference in fits). DFFITS is a measure used to identify influential data points in a regression analysis. It quantifes how much the predicted values (Y) of a model change when that particular observation is left out from the analysis. So in this case it is the difference in attachment loss change if we take out observation i from the analysis.\nLet’s make a plot where we label the values that are past the cutoff.\n\n# Calculate the cutoff for DFFITS\ncutoff_dffits &lt;- 2 * sqrt((4+1)/103)\ncutoff_dffits\n\n[1] 0.4406526\n\n# Make DFFITS plot in ggplot. Set a horizontal line at the cutoff points, and label ID's for participants that pass those lines.\nggplot(diagnostics_attach, aes(x = id, y = dffits)) + \n  geom_point() +\n  geom_hline(yintercept = c(cutoff_dffits, -cutoff_dffits)) + \n  geom_text(aes(label = ifelse(dffits &lt; -cutoff_dffits | dffits &gt; cutoff_dffits, id, \"\")), vjust = -0.5) +\n  labs(title = \"DFFITS by ID\",\n       x = \"ID\",\n       y = \"DFFITS\")\n\n\n\n\n\n\n\n\nAgain we are flagging participants 107, 113, 144, 168. The new addition is 113, who is right on the cutoff line. Again participant 168 is the most egregious, and the other potential outliers are a lot closer to the cutoff.\nBack to top of tabset\n\n\nNow we can look at the DFBETAs (Difference in Betas). DFBETAS quantify how much the beta coefficients for each variable in the model change when you exclude observation i from the analysis.\n\n# Calculate the cutoff for DFBETAS\ncutoff_dfbetas &lt;- 2/sqrt(103)\ncutoff_dfbetas\n\n[1] 0.1970659\n\n# Make DFBETAS plot in ggplot. Set a horizontal line at the cutoff points, and label ID's for participants that pass those lines.\n\n# For placebo group\nggplot(diagnostics_attach, aes(x = id, y = dfbetas.placebo)) + \n  geom_point() +\n  geom_hline(yintercept = c(cutoff_dfbetas, -cutoff_dfbetas)) + \n  geom_text(aes(label = ifelse(dfbetas.placebo &lt; -cutoff_dfbetas | dfbetas.placebo &gt; cutoff_dfbetas, id, \"\")), vjust = -0.5) +\n  labs(title = \"DFBETAS by ID for Placebo\",\n       x = \"ID\",\n       y = \"DFBETA\")\n\n\n\n\n\n\n\n# For low dose group\nggplot(diagnostics_attach, aes(x = id, y = dfbetas.low)) + \n  geom_point() +\n  geom_hline(yintercept = c(cutoff_dfbetas, -cutoff_dfbetas)) + \n  geom_text(aes(label = ifelse(dfbetas.low &lt; -cutoff_dfbetas | dfbetas.low &gt; cutoff_dfbetas, id, \"\")), vjust = -0.5) +\n  labs(title = \"DFBETAS by ID for Low Dose\",\n       x = \"ID\",\n       y = \"DFBETA\")\n\n\n\n\n\n\n\n# For medium dose group\nggplot(diagnostics_attach, aes(x = id, y = dfbetas.medium)) + \n  geom_point() +\n  geom_hline(yintercept = c(cutoff_dfbetas, -cutoff_dfbetas)) + \n  geom_text(aes(label = ifelse(dfbetas.medium &lt; -cutoff_dfbetas | dfbetas.medium &gt; cutoff_dfbetas, id, \"\")), vjust = -0.5) +\n  labs(title = \"DFBETAS by ID for Medium Dose\",\n       x = \"ID\",\n       y = \"DFBETA\")\n\n\n\n\n\n\n\n# For high does group\nggplot(diagnostics_attach, aes(x = id, y = dfbetas.high)) + \n  geom_point() +\n  geom_hline(yintercept = c(cutoff_dfbetas, -cutoff_dfbetas)) + \n  geom_text(aes(label = ifelse(dfbetas.high &lt; -cutoff_dfbetas | dfbetas.high &gt; cutoff_dfbetas, id, \"\")), vjust = -0.5) +\n  labs(title = \"DFBETAS by ID for High Dose\",\n       x = \"ID\",\n       y = \"DFBETA\")\n\n\n\n\n\n\n\n\nThat’s a lot of points that are past the cutoff! I think for these plots it might matter less if we’re pass that point. I think the idea here is we get a fine tune look at, for instance, how participant 168 drastically changes the beta for high, compared to how much any other point changes the betas. Other possible values of concern are 144 and 146. All the other data points are still roughly around the cutoff of .20, but 168 is around .7!\nBack to top of tabset\n\n\nThe car package in R has some handy plots that we can use to help us with diagnosing outliers.\nThe first is the influence plot, which essentially combines the jackknife residuals plot with the Cook’s D plot.\n\n# Influence plot\ninfluencePlot(model_attach1, main = \"Influence Plot\",\n              sub = \"Circle size is proportional to Cook's Distance\")\n\n\n\n\n\n\n\n\n      StudRes        Hat       CookD\n6  -2.7107885 0.04761905 0.069013121\n8  -0.5238664 0.06250000 0.003686439\n12  1.4073671 0.06250000 0.026147438\n58 -3.6028957 0.06250000 0.154223694\n\n\nUsing this plot, we can see that obseration 58 (ID 168) has the largest residual value and Cook’s D value by a large margin!\nAdditionally, a single line of code provides us with some useful diagnostic plots.\n\n# infIndexPlot gives us a series of plots that we need to investigate influence points\ninfIndexPlot(model_attach1)\n\n\n\n\n\n\n\n\nThe first plot is the Cook’s D plot, the second plot is the studentized residuals plot, and the leverage/hat values plot, all of which we plotted before. New is the third plot which is the Bonferroni P-value plot.\nBased on these plots we can see pretty readily that participant 168 is a true outlier. That is, they are a point that has high leverage AND influence in this model.\nBack to top of tabset\n\n\nWe saw that participant 168 is the most egregious offender, and participants 107, 144, 146, and MAYBE 113 are flagging past our different cutoff points.\nLet’s take one last look at the table of these participants\n\n# Pretty print diagnostics table of potential outliers\nkable(diagnostics_attach[diagnostics_attach$id %in% c(107,113,144,146,168),], caption = \"Correlation Matrix\", format = \"html\") %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\"))\n\n\nCorrelation Matrix\n\n\n\nid\njackknife_residuals\nleverage\ncooks_D\ndffits\ndfbetas..Intercept.\ndfbetas.placebo\ndfbetas.low\ndfbetas.medium\ndfbetas.high\n\n\n\n\n6\n107\n-2.710789\n0.0476190\n0.0690131\n-0.6061507\n0.0000000\n0.0000000\n-0.4382463\n0.0000000\n0.0000000\n\n\n11\n113\n2.092587\n0.0434783\n0.0384816\n0.4461411\n0.0000000\n0.3154694\n0.0000000\n0.0000000\n0.0000000\n\n\n40\n144\n-2.670168\n0.0434783\n0.0610008\n-0.5692818\n-0.5692818\n0.4025431\n0.3932880\n0.3882470\n0.3646322\n\n\n42\n146\n-2.291912\n0.0434783\n0.0457672\n-0.4886373\n-0.4886373\n0.3455188\n0.3375748\n0.3332479\n0.3129784\n\n\n58\n168\n-3.602896\n0.0625000\n0.1542237\n-0.9302637\n0.0000000\n0.0000000\n0.0000000\n0.0000000\n-0.7143938\n\n\n\n\n\n\n\nAcross the board we can see that participant 168 has worse values for almost everything, in particular the jackknife residual, Cook’s D, and DFFITS values. I can confidently conclude that we should remove participant 168 as an outlier, and feel justified in keeping participants 107, 113, 144, and 146 based on the closeness to the rest of the data points on the previous plots.\n\n\n\n\n\n\nSet UpJackknife ResidualsLeverageCook’s DDFFITSDFBETASAdditional PlotsFinal Look and Conclusion\n\n\nLet’s generate a table so we can handily compare: ID, jackknife residual, leverage (diagonal hat), DFFITS, and DFBETAs for each participant.\n\n# Get leverage values (hat values)\nhat_values_pd &lt;- hatvalues(model_pd)\n\n# Get Cook's D values\ncooks_d_pd &lt;- cooks.distance(model_pd)\n\n# Get the DFFITS values\ndffits_pd &lt;- dffits(model_pd)\n\n# Get the DFBETAS\ndfbetas_pd &lt;- dfbetas(model_pd)\n\n# Make a table with ID and all diagnostic values\ndiagnostics_pd &lt;- data.frame(id = data_missing$id, jackknife_residuals = jackknife_residuals_pd, leverage = hat_values_pd, cooks_D = cooks_d_pd, dffits = dffits_pd, dfbetas = dfbetas_pd)\n\nkable(head(diagnostics_pd), caption = \"Diagnostics for Attachment Loss Change Model\", format = \"html\") %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\"))\n\n\nDiagnostics for Attachment Loss Change Model\n\n\nid\njackknife_residuals\nleverage\ncooks_D\ndffits\ndfbetas..Intercept.\ndfbetas.placebo\ndfbetas.low\ndfbetas.medium\ndfbetas.high\n\n\n\n\n101\n1.4284091\n0.0500000\n0.0212518\n0.3276995\n0.0000000\n0.0000000\n0.0000000\n0.2396655\n0.0000000\n\n\n103\n1.3517762\n0.0434783\n0.0164727\n0.2881997\n0.2881997\n-0.2037879\n-0.1991025\n-0.1965505\n-0.1845955\n\n\n104\n-0.4668576\n0.0476190\n0.0021971\n-0.1043925\n0.0000000\n0.0000000\n-0.0754757\n0.0000000\n0.0000000\n\n\n105\n-0.4451251\n0.0434783\n0.0018161\n-0.0949010\n0.0000000\n-0.0671051\n0.0000000\n0.0000000\n0.0000000\n\n\n106\n-2.5107398\n0.0500000\n0.0629491\n-0.5760032\n0.0000000\n0.0000000\n0.0000000\n-0.4212642\n0.0000000\n\n\n107\n-1.7949702\n0.0476190\n0.0315049\n-0.4013675\n0.0000000\n0.0000000\n-0.2901882\n0.0000000\n0.0000000\n\n\n\n\n\n\n\nBack to top of tabset\n\n\nWe can start by making a plot of the jackknife residuals to assess if there are outliers &lt; -3 or &gt; 3 SDs).\n\n# Plot jackknife outiers and label any values that are &gt; 3 or &lt; -3.\nggplot(data_missing, aes(x = id, y = jackknife_residuals_pd)) + geom_point() + \n  geom_hline(yintercept = c(-3,0,3)) + ylim(-4,4) + \n  labs(y = \"Jackknife Residuals\",\n       title = \"Jackknife Residuals vs ID\") + \n  geom_text(aes(label = ifelse(jackknife_residuals_pd &gt; 3 | jackknife_residuals_pd &lt; -3, id, '')), \n            vjust = -1)\n\n\n\n\n\n\n\n\nWe have no jackknife residuals +- 3. That’s a good sign!\nLet’s sort our dataset and see who has the highest pocket depth at baseline.\n\n# Sort our dataset by descending order of pocket depth at baseline to see who is the highest\nkable(head(data_missing[order(-data_missing$pdbase),]), caption = \"Diagnostics for Attachment Loss Change Model\", format = \"html\") %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\"))\n\n\nDiagnostics for Attachment Loss Change Model\n\n\n\nid\ntrtgroup\ngender\nrace\nage\nsmoker\nsites\nattachbase\nattach1year\npdbase\npd1year\nattachchange\npdchange\nplacebo\ncontrol\nlow\nmedium\nhigh\ntrt\ntrt3groups\n\n\n\n\n3\n104\n3\n2\n5\n55.17864\n1\n138\n4.956522\n5.304348\n5.217391\n4.891304\n0.3478261\n-0.3260870\n0\n0\n1\n0\n0\n1\n3\n\n\n20\n124\n2\n2\n5\n41.01574\n1\n162\n2.901235\n2.808642\n4.771605\n4.067901\n-0.0925926\n-0.7037037\n0\n1\n0\n0\n0\n0\n2\n\n\n17\n120\n3\n2\n5\n41.83710\n1\n150\n3.886667\n3.920000\n4.200000\n3.880000\n0.0333333\n-0.3200000\n0\n0\n1\n0\n0\n1\n3\n\n\n77\n234\n1\n2\n5\n51.12115\n0\n144\n2.756944\n2.527778\n4.083333\n3.631944\n-0.2291667\n-0.4513889\n1\n0\n0\n0\n0\n0\n1\n\n\n5\n106\n4\n2\n5\n42.14921\n1\n168\n2.369048\n1.922619\n3.910714\n3.083333\n-0.4464286\n-0.8273810\n0\n0\n0\n1\n0\n1\n3\n\n\n6\n107\n3\n2\n5\n37.15811\n1\n156\n3.544872\n2.839744\n3.897436\n3.237179\n-0.7051282\n-0.6602564\n0\n0\n1\n0\n0\n1\n3\n\n\n\n\n\n\n\nParticipant 104 has the highest baseline pocket depth. Interestingly it is not 168, who was our outlier for attachment loss.\nLet’s run that test we did earlier to see if there’s an outlier in the pocket depth model.\n\noutlierTest(model_pd)\n\nNo Studentized residuals with Bonferroni p &lt; 0.05\nLargest |rstudent|:\n   rstudent unadjusted p-value Bonferroni p\n82 2.664155          0.0090393      0.93105\n\n\nThis is a test of a hypothesis that we do not have an outlier. We fail to reject the null and thus have more evidence that we do not have outliers for the pocket depth model.\nBack to top of tabset\n\n\nLet’s plot leverage for our model on pocket depth change.\n\n# Caculate cutoff for leverage\ncutoff_leverage &lt;- 2*((4+1)/103)\ncutoff_leverage\n\n[1] 0.09708738\n\n# Make leverage plot in ggplot. Set a horizontal line at the cutoff point, and label ID's for participants that pass that line\nggplot(diagnostics_pd, aes(x = id, y = leverage)) +\n  geom_point() + \n  geom_hline(yintercept = cutoff_leverage) +\n  labs(title = \"Leverage Plot with Cutoff Line\",\n       x = \"ID\",\n       y = \"Hat Values\") + \n  geom_text(aes(label = ifelse(leverage &gt; cutoff_leverage, id, \"\")), vjust = -0.5)\n\n\n\n\n\n\n\n\nAgain we are good on leverage.\nBack to top of tabset\n\n\nLet’s make our plot for Cook’s D for pocket depth change.\n\n# Calculate cutoff for Cook's D\ncutoff_d &lt;- 4/((nrow(data_missing) - length(model_attach1$coefficients) - 1))\ncutoff_d\n\n[1] 0.04123711\n\n# Make Cook's D plot in ggplot. Set a horizontal line at the cutoff point, and label ID's for participants that pass that line\nggplot(diagnostics_pd, aes(x = id, y = cooks_D)) +\n  geom_point() + \n  geom_hline(yintercept = cutoff_d) +\n  labs(title = \"Cook's D with Cutoff Line\",\n       x = \"ID\",\n       y = \"Cook's D\") + \n  geom_text(aes(label = ifelse(cooks_D &gt; cutoff_d, id, \"\")), vjust = -0.5)\n\n\n\n\n\n\n\n\nWe see that participants 106, 118, and 239 have Cook’s D values past the cutoff. Interestingly participant 104 who had the highest baseline pocket depth did not flag here.\nBack to top of tabset\n\n\nLet’s make the DFFITS plot for pocket depth change.\n\n# Calculate the cutoff for DFFITS\ncutoff_dffits &lt;- 2 * sqrt((4+1)/103)\ncutoff_dffits\n\n[1] 0.4406526\n\n# Make DFFITS plot in ggplot. Set a horizontal line at the cutoff points, and label ID's for participants that pass those lines.\nggplot(diagnostics_pd, aes(x = id, y = dffits)) + \n  geom_point() +\n  geom_hline(yintercept = c(cutoff_dffits, -cutoff_dffits)) + \n  geom_text(aes(label = ifelse(dffits &lt; -cutoff_dffits | dffits &gt; cutoff_dffits, id, \"\")), vjust = -0.5) +\n  labs(title = \"DFFITS by ID\",\n       x = \"ID\",\n       y = \"DFFITS\")\n\n\n\n\n\n\n\n\nParticipants 106, 118, and 239 are flagging here again. In addition we have 137 and 233, which are near enough to the cutoff that we can ignore them.\nBack to top of tabset\n\n\nNow we can look at the DFBETAS plot for pocket depth change.\n\n# Calculate the cutoff for DFBETAS\ncutoff_dfbetas &lt;- 2/sqrt(103)\ncutoff_dfbetas\n\n[1] 0.1970659\n\n# Make DFBETAS plot in ggplot. Set a horizontal line at the cutoff points, and label ID's for participants that pass those lines.\n\n# For placebo group\nggplot(diagnostics_pd, aes(x = id, y = dfbetas.placebo)) + \n  geom_point() +\n  geom_hline(yintercept = c(cutoff_dfbetas, -cutoff_dfbetas)) + \n  geom_text(aes(label = ifelse(dfbetas.placebo &lt; -cutoff_dfbetas | dfbetas.placebo &gt; cutoff_dfbetas, id, \"\")), vjust = -0.5) +\n  labs(title = \"DFBETAS by ID for Placebo\",\n       x = \"ID\",\n       y = \"DFBETA\")\n\n\n\n\n\n\n\n# For low dose group\nggplot(diagnostics_pd, aes(x = id, y = dfbetas.low)) + \n  geom_point() +\n  geom_hline(yintercept = c(cutoff_dfbetas, -cutoff_dfbetas)) + \n  geom_text(aes(label = ifelse(dfbetas.low &lt; -cutoff_dfbetas | dfbetas.low &gt; cutoff_dfbetas, id, \"\")), vjust = -0.5) +\n  labs(title = \"DFBETAS by ID for Low Dose\",\n       x = \"ID\",\n       y = \"DFBETA\")\n\n\n\n\n\n\n\n# For medium dose group\nggplot(diagnostics_pd, aes(x = id, y = dfbetas.medium)) + \n  geom_point() +\n  geom_hline(yintercept = c(cutoff_dfbetas, -cutoff_dfbetas)) + \n  geom_text(aes(label = ifelse(dfbetas.medium &lt; -cutoff_dfbetas | dfbetas.medium &gt; cutoff_dfbetas, id, \"\")), vjust = -0.5) +\n  labs(title = \"DFBETAS by ID for Medium Dose\",\n       x = \"ID\",\n       y = \"DFBETA\")\n\n\n\n\n\n\n\n# For high does group\nggplot(diagnostics_pd, aes(x = id, y = dfbetas.high)) + \n  geom_point() +\n  geom_hline(yintercept = c(cutoff_dfbetas, -cutoff_dfbetas)) + \n  geom_text(aes(label = ifelse(dfbetas.high &lt; -cutoff_dfbetas | dfbetas.high &gt; cutoff_dfbetas, id, \"\")), vjust = -0.5) +\n  labs(title = \"DFBETAS by ID for High Dose\",\n       x = \"ID\",\n       y = \"DFBETA\")\n\n\n\n\n\n\n\n\nAgain we have a lot of points that are past the cutoff here. None as far off as 168 in the attachment loss model, 239 is the only one that is concerning.\nBack to top of tabset\n\n\nThe car package in R has some handy plots that we can use to help us with diagnosing outliers.\nThe first is the influence plot, which essentially combines the jackknife residuals plot with the Cook’s D plot.\n\n# Influence plot\ninfluencePlot(model_pd, main = \"Influence Plot\",\n              sub = \"Circle size is proportional to Cook's Distance\")\n\n\n\n\n\n\n\n\n      StudRes        Hat       CookD\n5  -2.5107398 0.05000000 0.062949101\n8  -0.8158788 0.06250000 0.008905825\n12  0.4184503 0.06250000 0.002354494\n82  2.6641548 0.04761905 0.066819586\n\n\nHere we can see that none of our residuals are +- 3, but we do have some concerns with large Cook’s distance, particularly with observation 82 (ID = 239).\nAdditionally, a single line of code provides us with some useful diagnostic plots.\n\n# infIndexPlot gives us a series of plots that we need to investigate influence points\ninfIndexPlot(model_pd)\n\n\n\n\n\n\n\n\nBased on these plots, it is arguable that participant 239 is an outlier for pocket depth. Though they do not have an extreme jackknife residual, the large Cook’s distance suggests that this data point has substantial infuence on the model’s parameters.\nHowever, a closer look reveals that the influence and leverage values for the pocket depth model are all comparable to the potential outliers in the attachmnet loss model we chose to keep (Cook’s D ~0.06). Specifically, while it appears that participant 239 here has a large Cook’s D (0.068) compared to the rest of the values, it is nowhere near as high as participant 168 was in the attachment loss model (0.15)!\nBack to top of tabset\n\n\nParticipants 106 and 239 were the only potential concerns here. However, as noted, their Cook’s D is comparable to the values we kept in the attachment loss model, and are &lt; 1/2 of what the Cook’s D was for participant 168 who we plan to remove as an outlier in the attachment loss model!\n\n# Pretty print diagnostics table of potential outliers\nkable(diagnostics_pd[diagnostics_attach$id %in% c(106,239),], caption = \"Correlation Matrix\", format = \"html\") %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\"))\n\n\nCorrelation Matrix\n\n\n\nid\njackknife_residuals\nleverage\ncooks_D\ndffits\ndfbetas..Intercept.\ndfbetas.placebo\ndfbetas.low\ndfbetas.medium\ndfbetas.high\n\n\n\n\n5\n106\n-2.510740\n0.050000\n0.0629491\n-0.5760032\n0\n0\n0.0000000\n-0.4212642\n0\n\n\n82\n239\n2.664155\n0.047619\n0.0668196\n0.5957231\n0\n0\n0.4307071\n0.0000000\n0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Remove participant 168 from the dataset\ndata_missing_outlier &lt;- data_missing[data_missing$id != 168,]\n\n# Run model 1 attachment loss\nmodel_attach1_outlier &lt;- lm(attachchange ~ placebo + low + medium + high, data = data_missing_outlier)\nsummary(model_attach1_outlier)\n\n\nCall:\nlm(formula = attachchange ~ placebo + low + medium + high, data = data_missing_outlier)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.68731 -0.16279  0.04936  0.16823  0.53945 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.22169    0.05277  -4.201  5.9e-05 ***\nplacebo      0.13462    0.07463   1.804  0.07435 .  \nlow          0.20388    0.07638   2.669  0.00891 ** \nmedium       0.21514    0.07737   2.780  0.00652 ** \nhigh         0.11576    0.08399   1.378  0.17130    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2531 on 97 degrees of freedom\nMultiple R-squared:  0.09493,   Adjusted R-squared:  0.05761 \nF-statistic: 2.543 on 4 and 97 DF,  p-value: 0.04442\n\n# Run model 2 attachmnent loss\nmodel_attach2_outlier &lt;- lm(attachchange ~ control + low + medium + high, data = data_missing_outlier)\nsummary(model_attach2_outlier)\n\n\nCall:\nlm(formula = attachchange ~ control + low + medium + high, data = data_missing_outlier)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.68731 -0.16279  0.04936  0.16823  0.53945 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept) -0.08707    0.05277  -1.650   0.1022  \ncontrol     -0.13462    0.07463  -1.804   0.0743 .\nlow          0.06926    0.07638   0.907   0.3668  \nmedium       0.08052    0.07737   1.041   0.3006  \nhigh        -0.01886    0.08399  -0.225   0.8228  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2531 on 97 degrees of freedom\nMultiple R-squared:  0.09493,   Adjusted R-squared:  0.05761 \nF-statistic: 2.543 on 4 and 97 DF,  p-value: 0.04442\n\n# Run the pocket depth model\nmodel_pd_outlier &lt;- lm(pdchange ~ control + low + medium + high, data = data_missing_outlier)\nsummary(model_pd_outlier)\n\n\nCall:\nlm(formula = pdchange ~ control + low + medium + high, data = data_missing_outlier)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-0.6248 -0.1513 -0.0129  0.1616  0.6613 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.34969    0.05488  -6.372 6.26e-09 ***\ncontrol      0.01152    0.07761   0.148   0.8823    \nlow          0.14352    0.07943   1.807   0.0739 .  \nmedium       0.14714    0.08046   1.829   0.0705 .  \nhigh        -0.02437    0.08734  -0.279   0.7809    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2632 on 97 degrees of freedom\nMultiple R-squared:  0.07456,   Adjusted R-squared:  0.0364 \nF-statistic: 1.954 on 4 and 97 DF,  p-value: 0.1077\n\n\nThere is no difference in our models after removing outlier 168."
  },
  {
    "objectID": "Data_Wrangling/Treadmill_Data_Cleaning/Code/Automated_Treadmill_Data_Cleaning.html",
    "href": "Data_Wrangling/Treadmill_Data_Cleaning/Code/Automated_Treadmill_Data_Cleaning.html",
    "title": "Automated Treadmill Data Cleaning",
    "section": "",
    "text": "This was the final project for my BIOS 6644 Data Wrangling course.\nIn this project, we create a program to automate the data cleaning of vital mouse treadmill data needed to validate Genetically Encoded Voltage Indicators for visualizing neuronal activity, exponentially saving experimenter time on manually cleaning data files and greatly expediting the data analysis process.\nThe result is a program that takes a few seconds to perform what would otherwise take 1-2 hours per recording session to clean by hand!"
  },
  {
    "objectID": "Data_Wrangling/Treadmill_Data_Cleaning/Code/Automated_Treadmill_Data_Cleaning.html#the-motivator",
    "href": "Data_Wrangling/Treadmill_Data_Cleaning/Code/Automated_Treadmill_Data_Cleaning.html#the-motivator",
    "title": "Automated Treadmill Data Cleaning",
    "section": "The Motivator",
    "text": "The Motivator\nAt this point, we had successfully expressed and captured neuronal activity in a single VIP+ neuron using a GEVI, which can be seen here.\n\n\n\nImage and Plots made by Forest Speed\n\n\nHowever, we now needed to validate these signals against a control. That is, we needed to prove that we were capturing real voltage signals.\nOne approach to this problem was to perform voltage imaging with a GEVI in the motor cortex and correlate that activity to a physiological indicator of movement - mice walking on a treadmill. The idea is simple: If we were capturing true voltage signals, we would see spikes every time the mice walked.\nThus, we created a paradigm where we could image mice while they walked on a treadmill. (This experimental set-up would also allow for further experiments down the road of particular interest to my lab, such as documenting the relationships between inhibitory interneurons during locomotion).\nThis experimental set up can be seen here."
  },
  {
    "objectID": "Data_Wrangling/Treadmill_Data_Cleaning/Code/Automated_Treadmill_Data_Cleaning.html#the-problem",
    "href": "Data_Wrangling/Treadmill_Data_Cleaning/Code/Automated_Treadmill_Data_Cleaning.html#the-problem",
    "title": "Automated Treadmill Data Cleaning",
    "section": "The Problem",
    "text": "The Problem\nWe had successfully created a paradigm where we could perform voltage imaging on freely moving mice. However, the data we were receiving would have to be cleaned before we could do anything with it!\nSpecifically the recording on the treadmill was beginning before the recording on the camera. This resulted in treadmill files that were capturing data when the camera was turned off at both at the beginning and end of each treadmill file.\nThis can be visualized here.\n\n\n\nThus, the treadmill data would have to be cleaned in order for the treadmill and camera recordings to be synchronized with each other."
  },
  {
    "objectID": "Data_Wrangling/Treadmill_Data_Cleaning/Code/Automated_Treadmill_Data_Cleaning.html#outline",
    "href": "Data_Wrangling/Treadmill_Data_Cleaning/Code/Automated_Treadmill_Data_Cleaning.html#outline",
    "title": "Automated Treadmill Data Cleaning",
    "section": "Outline",
    "text": "Outline\nAs can be seen in the previous image, the recording software for the camera was detecting the timepoint that the camera was turned on as a change from 0 to ~3 Volts. This is the timestamp that we will use to sychronize the treadmill data with the camera recordings.\nTo do so, we need all of the treadmill recordings to have this timepoint where the camera switches from 0 to 3 Volts as timestamp 0.00 seconds.\nThis will also have to be done en masse, to clean as many treadmill files as needed simultaneously.\n\nPseudo Code\nThus, the pseudo code for the program is laid out as follows:\n\nIterate through all the treadmill.xlsx’s in a specified folder.\nIn each treadmill.xlsx:\n\nDelete every second row (since these are empty rows)\nCheck the second column of the treadmill.xlsx (which detects when the Camera was turned on)\nOnly consider rows where the camera was turned on (i.e. the “Camera On/Off” value &gt; 3)\nConsider the first row as time stamp 0.\nFrom that point on, all remaining time stamps will be shifted by x seconds, where x is the timestamp where the camera was turned on.\nSave the .xlsx in a new folder with the exact same file name but appended with the suffix “_cleaned.xlsx”.\n\n\nThis process can be visualized below."
  },
  {
    "objectID": "Data_Wrangling/Treadmill_Data_Cleaning/Code/Automated_Treadmill_Data_Cleaning.html#converting-treadmill-position-from-volts-to-degrees",
    "href": "Data_Wrangling/Treadmill_Data_Cleaning/Code/Automated_Treadmill_Data_Cleaning.html#converting-treadmill-position-from-volts-to-degrees",
    "title": "Automated Treadmill Data Cleaning",
    "section": "Converting Treadmill Position from Volts to Degrees",
    "text": "Converting Treadmill Position from Volts to Degrees\nIt might easier to deal with/conceptualize the treadmill position as degrees of a circle instead of volts in the future. That way we can calculate distance over time later using the radius of the circle.\nFor reference, the magnetic encoder that the treadmill is attached to captures position in volts, which can be converted directly into degrees of a circle.\n\n\n\n\n\nLet’s make that conversion now instead of later.\n\n# The highest value for Treadmill Position (in Volts) gets designated as 360 degrees, or the end of the circle\ncircle_360 = treadmill[\"Treadmill Position (V)\"].max() # Set 360 as the max value for Treadmill Position (V) \none_volt = 360 / circle_360 # One volt is therefore (360 / the max value for Treadmill Position (V)) degrees (e.g., if the max was 5 volts, then 360 / 5 = 72 degrees, or 1 volt = 72 degrees)\n\n# Create a new column that is the Volts * how many degrees one volt is equal to\ntreadmill[\"Treadmill Position (degree)\"] = treadmill[\"Treadmill Position (V)\"] * one_volt\ntreadmill\n\n\n\n\n\n\n\n\nTreadmill Position (V)\nCamera On/Off\nTime (sec)\nTreadmill Position (degree)\n\n\n\n\n145\n2.474988\n3.324904\n0.000000\n174.575180\n\n\n147\n2.461392\n3.327091\n0.012003\n173.616208\n\n\n149\n2.472487\n3.317091\n0.024006\n174.398817\n\n\n151\n2.479988\n3.322404\n0.036008\n174.927905\n\n\n153\n2.476550\n3.325373\n0.048010\n174.685407\n\n\n...\n...\n...\n...\n...\n\n\n5071\n1.836623\n3.321466\n29.997960\n129.547662\n\n\n5073\n1.834592\n3.323810\n30.009963\n129.404368\n\n\n5075\n1.848500\n3.335060\n30.021966\n130.385382\n\n\n5077\n1.839905\n3.326779\n30.033968\n129.779137\n\n\n5079\n1.823340\n3.320216\n30.045971\n128.610738\n\n\n\n\n2468 rows × 4 columns\n\n\n\nLooks good. This treadmill file is now fully cleaned."
  },
  {
    "objectID": "Data_Wrangling/Treadmill_Data_Cleaning/Code/Automated_Treadmill_Data_Cleaning.html#saving-the-cleaned-data",
    "href": "Data_Wrangling/Treadmill_Data_Cleaning/Code/Automated_Treadmill_Data_Cleaning.html#saving-the-cleaned-data",
    "title": "Automated Treadmill Data Cleaning",
    "section": "Saving the Cleaned Data",
    "text": "Saving the Cleaned Data\nWe will want to save each treadmill file after it is cleaned.\nFor thorough record keeping, let’s create a subfolder within the folder that the treadmill.xlsx files are found within and store the cleaned files there.\n\n# Name the new folder \"Cleaned Treadmill Files\", and place it within the folder that the uncleaned treadmill files will all be kept in\nnew_folder = r\"../Treadmill Files Uncleaned/Cleaned Treadmill Files\"\n\n# Make the new folder\nos.makedirs(new_folder, exist_ok=True)\n\n# Save the cleaned file to this new folder as an excel file\noutput_file = os.path.join(new_folder, \"Cleaned 1.xlsx\")\ntreadmill.to_excel(output_file, index=False)"
  },
  {
    "objectID": "Data_Wrangling/Treadmill_Data_Cleaning/Code/Automated_Treadmill_Data_Cleaning.html#create-a-function",
    "href": "Data_Wrangling/Treadmill_Data_Cleaning/Code/Automated_Treadmill_Data_Cleaning.html#create-a-function",
    "title": "Automated Treadmill Data Cleaning",
    "section": "Create a Function",
    "text": "Create a Function\nWe just wrote code to import, clean, and save a single treadmill.xlsx file.\nFor the final product, we need to make a program that walks through every uncleaned treadmill file in a folder and cleans it.\nTo that end, we need to convert the file-cleaning-and-saving steps into a single function, that we can then apply to multiple files en masse.\n\n# Making a function to clean a single treadmill.xlsx file\ndef treadmill_cleaner(file_path, save_location):\n    '''\n    This function cleans a single treadmill.xlsx file\n    And saves it in the save_location folder\n    '''\n    \n    #Import the File\n    col_names = [\"Treadmill Position (V)\", \"Camera On/Off\", \"Time (sec)\"] # Define column names\n    treadmill = pd.read_excel(file_path, names = col_names) # Import treadmill data as a pd df, set column names to col_names\n\n    # Clean the File\n    treadmill = treadmill.dropna() # Drop NaN values\n    treadmill = treadmill[treadmill[\"Camera On/Off\"] &gt; 3] # Slice the treadmill df so it only includes values where the camera was turned on (i.e. value &gt; 3)\n    time_start = treadmill[\"Time (sec)\"].iloc[0] # Set the first Time (sec) value to 0\n    treadmill[\"Time (sec)\"] = treadmill[\"Time (sec)\"] - time_start # Delete all Time (sec) values by the starting time\n    circle_360 = treadmill[\"Treadmill Position (V)\"].max() # Set 360 as the max value for Treadmill Position (V) \n    one_volt = 360 / circle_360 # One volt is therefore (360 / the max value for Treadmill Position (V)) degrees (e.g., if the max was 5 volts, then 360 / 5 = 72 degrees, or 1 volt = 72 degrees)\n    treadmill[\"Treadmill Position (degree)\"] = treadmill[\"Treadmill Position (V)\"] * one_volt\n    \n    # Save the Cleaned File\n    output_file = os.path.join(save_location, filename) # Generates the name of the output file (save location + file_name)\n    base_name, extension = os.path.splitext(output_file) # Separates the file name to be \"file_name\" and \".xlsx\" (modified from ChatGPT)\n    cleaned_path = f\"{base_name}_cleaned{extension}\" # Creates a new file name that is \"file_name_cleaned.xlsx\" (modified from ChatGPT)\n    treadmill.to_excel(cleaned_path, index=False) # Saves the file and appends \"_cleaned\" to the end of it"
  },
  {
    "objectID": "Data_Wrangling/Treadmill_Data_Cleaning/Code/Automated_Treadmill_Data_Cleaning.html#clean-a-single-treadmill-file",
    "href": "Data_Wrangling/Treadmill_Data_Cleaning/Code/Automated_Treadmill_Data_Cleaning.html#clean-a-single-treadmill-file",
    "title": "Automated Treadmill Data Cleaning",
    "section": "Clean a Single Treadmill File",
    "text": "Clean a Single Treadmill File\nIf we want to run the program to clean a single file, we can run the code below.\n\n# Change file paths as needed below\nfilename = \"121523_RC1_Pace_1.xlsx\" # Change this to what you want the last part of the file name to be!\nfile_path = r\"../Treadmill Files Uncleaned/121523_RC1_Pace_1.xlsx\" # Change this to the file path of the treadmill file you want to clean\nsave_location = r\"../Treadmill Files Uncleaned/Cleaned Treadmill Files\" #Change this to the name of the folder you want to save in\nos.makedirs(save_location, exist_ok=True)  # Makes the save_location as a new folder, skips if it already exists\n\n# Run the program to clean a single treadmill file\ntreadmill_cleaner(file_path, save_location)"
  },
  {
    "objectID": "Data_Wrangling/Treadmill_Data_Cleaning/Code/Automated_Treadmill_Data_Cleaning.html#clean-an-entire-folder-containing-raw-treadmill-files",
    "href": "Data_Wrangling/Treadmill_Data_Cleaning/Code/Automated_Treadmill_Data_Cleaning.html#clean-an-entire-folder-containing-raw-treadmill-files",
    "title": "Automated Treadmill Data Cleaning",
    "section": "Clean an Entire Folder Containing Raw Treadmill Files",
    "text": "Clean an Entire Folder Containing Raw Treadmill Files\nIf we want to clean a folder containing multiple treadmill files, we can run the chunk below.\n\n# This program walks through every file in a designated folder and applies the above treadmill_cleaner() function to each file, saving the cleaned file in the same subfolder\n\n#__Enter the file path to the folder containing treadmill files here!!!!!__\ndirectory = r\"../Treadmill Files Uncleaned\" \n\n##### Create the save location as a new folder (if needed)\n# Names the save location as the folder containing treadmill files \n# + \"\\Cleaned Treadmill Files\"\nsave_location = f\"{directory}\" + \"\\Cleaned Treadmill Files\" \n# Makes the save_location as a new folder, skips if it already exists\nos.makedirs(save_location, exist_ok=True)  \nprint(\"Save location is:\", save_location)\n\n##### Walk through every file in the directory and apply the treadmill_cleaner() function\ntry: # Made this a try statement so it does not apply to subfolders\n    for filename in os.listdir(directory):\n        print(\"File name is:\", filename)\n        filepath = os.path.join(directory, filename)\n        print(\"File path is:\", filepath)\n        treadmill_cleaner(filepath, save_location)\nexcept PermissionError:  # Skips subfolders (and anything else really, will throw an error if you have the .xlsx file open!)\n    print(\"File is a folder -  skipping\")\n\n&lt;&gt;:9: SyntaxWarning:\n\ninvalid escape sequence '\\C'\n\n&lt;&gt;:9: SyntaxWarning:\n\ninvalid escape sequence '\\C'\n\nC:\\Users\\sviea\\AppData\\Local\\Temp\\ipykernel_30796\\4019197621.py:9: SyntaxWarning:\n\ninvalid escape sequence '\\C'\n\n\n\nSave location is: ../Treadmill Files Uncleaned\\Cleaned Treadmill Files\nFile name is: 121523_RC1_Pace_1.xlsx\nFile path is: ../Treadmill Files Uncleaned\\121523_RC1_Pace_1.xlsx\nFile name is: 121523_RC1_Pace_2.xlsx\nFile path is: ../Treadmill Files Uncleaned\\121523_RC1_Pace_2.xlsx\nFile name is: 121523_RC1_Pace_3.xlsx\nFile path is: ../Treadmill Files Uncleaned\\121523_RC1_Pace_3.xlsx\nFile name is: 121523_RC1_Pace_4.xlsx\nFile path is: ../Treadmill Files Uncleaned\\121523_RC1_Pace_4.xlsx\nFile name is: 121523_RC1_Pace_5.xlsx\nFile path is: ../Treadmill Files Uncleaned\\121523_RC1_Pace_5.xlsx\nFile name is: 121523_RC1_Pace_6.xlsx\nFile path is: ../Treadmill Files Uncleaned\\121523_RC1_Pace_6.xlsx\nFile name is: Cleaned Treadmill Files\nFile path is: ../Treadmill Files Uncleaned\\Cleaned Treadmill Files\nFile is a folder -  skipping\n\n\nWe just cleaned 6 files in a matter of seconds. That would have taken the better part of an hour to do by hand!"
  },
  {
    "objectID": "Data_Wrangling/Treadmill_Data_Cleaning/Code/Automated_Treadmill_Data_Cleaning.html#checking-the-output",
    "href": "Data_Wrangling/Treadmill_Data_Cleaning/Code/Automated_Treadmill_Data_Cleaning.html#checking-the-output",
    "title": "Automated Treadmill Data Cleaning",
    "section": "Checking the Output",
    "text": "Checking the Output\nFor good measure, let’s check the output of the first and last file we cleaned.\n\n# Check first file\nfile_1 = pd.read_excel(r\"../Treadmill Files Uncleaned/Cleaned Treadmill Files/121523_RC1_Pace_1_cleaned.xlsx\")\nfile_1\n\n\n\n\n\n\n\n\nTreadmill Position (V)\nCamera On/Off\nTime (sec)\nTreadmill Position (degree)\n\n\n\n\n0\n2.474988\n3.324904\n0.000000\n174.575180\n\n\n1\n2.461392\n3.327091\n0.012003\n173.616208\n\n\n2\n2.472487\n3.317091\n0.024006\n174.398817\n\n\n3\n2.479988\n3.322404\n0.036008\n174.927905\n\n\n4\n2.476550\n3.325373\n0.048010\n174.685407\n\n\n...\n...\n...\n...\n...\n\n\n2463\n1.836623\n3.321466\n29.997960\n129.547662\n\n\n2464\n1.834592\n3.323810\n30.009963\n129.404368\n\n\n2465\n1.848500\n3.335060\n30.021966\n130.385382\n\n\n2466\n1.839905\n3.326779\n30.033968\n129.779137\n\n\n2467\n1.823340\n3.320216\n30.045971\n128.610738\n\n\n\n\n2468 rows × 4 columns\n\n\n\n\n# Check last file\nfile_6 = pd.read_excel(r\"../Treadmill Files Uncleaned/Cleaned Treadmill Files/121523_RC1_Pace_6_cleaned.xlsx\")\nfile_6\n\n\n\n\n\n\n\n\nTreadmill Position (V)\nCamera On/Off\nTime (sec)\nTreadmill Position (degree)\n\n\n\n\n0\n1.552993\n3.318966\n0.000000\n109.813950\n\n\n1\n1.547993\n3.324123\n0.012003\n109.460349\n\n\n2\n1.550337\n3.318966\n0.024005\n109.626099\n\n\n3\n1.543148\n3.312560\n0.036008\n109.117798\n\n\n4\n1.544867\n3.318498\n0.048011\n109.239348\n\n\n...\n...\n...\n...\n...\n\n\n4966\n1.637379\n3.326779\n60.020822\n115.780965\n\n\n4967\n1.615189\n3.315998\n60.032824\n114.211861\n\n\n4968\n1.624409\n3.316935\n60.044827\n114.863812\n\n\n4969\n1.625190\n3.327091\n60.056829\n114.919062\n\n\n4970\n1.643161\n3.327248\n60.068832\n116.189816\n\n\n\n\n4971 rows × 4 columns\n\n\n\nLooks great!"
  },
  {
    "objectID": "Data_Management_(SAS)/Project_5/Project_5_Report_and_Analysis.html",
    "href": "Data_Management_(SAS)/Project_5/Project_5_Report_and_Analysis.html",
    "title": "Project 5 - Analysis and Report",
    "section": "",
    "text": "For this project, we implement the Data Analysis Plan (DAP) we created for project 4, and analyze the PI’s research question and our own question of interest. Importantly, we write SAS code to create a reproducible pdf report directly from SAS."
  },
  {
    "objectID": "Data_Management_(SAS)/Project_5/Project_5_Report_and_Analysis.html#Table_1A",
    "href": "Data_Management_(SAS)/Project_5/Project_5_Report_and_Analysis.html#Table_1A",
    "title": "Project 5 - Analysis and Report",
    "section": "Table 1A: Continuous Variables",
    "text": "Table 1A: Continuous Variables\n\nCreate Data SetClean Data SetTranspose Data SetFinal Table\n\n\n\nCreate Data Set of Means and Standard Deviations\n\nTITLE \"Table 1A. Means and Standard Deviation\";\n*Step 1- Use ODS TRACE to identify name of Table;\n*Step 2- Use ODS OUTPUT to create a dataset;\n/*  ods trace on;*/\nODS OUTPUT Table = Table1_pre;\nPROC TABULATE DATA = IMPT.BRFSS_FINAL MISSING;\n    VAR BMI SmokeTime Age;\n    TABLE BMI SmokeTime Age, mean*f=comma9.2 std*f=comma9.2/nocellmerge;\n    RUN;\n/*  ods trace off;*/\n\n\n\nExamine Table\n\n* Step 3- Examine table;\nPROC PRINT DATA = Table1_pre;\n    RUN;\n\nlstlog\n\n\n&lt;!DOCTYPE html&gt;\n\n\n\n\n\nSAS Output\n\n\n\n\n\n\n\nTable 1A. Means and Standard Deviation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObs\n\n\nTYPE\n\n\nPAGE\n\n\nTABLE\n\n\nBMI_Mean\n\n\nBMI_Std\n\n\nSmokeTime_Mean\n\n\nSmokeTime_Std\n\n\nAge_Mean\n\n\nAge_Std\n\n\n\n\n\n\n1\n\n\n0\n\n\n1\n\n\n1\n\n\n29.1836\n\n\n6.66191\n\n\n6.03782\n\n\n11.8816\n\n\n1.98128\n\n\n0.81638\n\n\n\n\n\n\n\n\n\n\n\u001417                                                         The SAS System                    Saturday, December 28, 2024 08:22:00 PM\n\n190        ods listing close;ods html5 (id=saspy_internal) file=_tomods1 options(bitmap_mode='inline') device=svg style=HTMLBlue;\n190      ! ods graphics on / outputfmt=png;\n191        \n192        * Step 3- Examine table;\n193        PROC PRINT DATA = Table1_pre;\n194         RUN;\n195        \n196        \n197        ods html5 (id=saspy_internal) close;ods listing;\n198        \n\u001418                                                         The SAS System                    Saturday, December 28, 2024 08:22:00 PM\n\n199        \n\n\n\n\nTop of Tabset\n\n\n\n\nFormat Data Set to be More Visually Appealling\n\n* Step 4-  Clean the dataset with the output to look pretty;\nDATA Table1_pre2;\n    SET Table1_pre;\n        BMI = strip(put(BMI_mean,8.2))||\" (\"||strip(put(BMI_std, 8.2))||\")\";\n        SmokeTime = strip(put(SmokeTime_mean,8.2))||\" (\"||strip(put(SmokeTime_std, 8.2))||\")\";\n        Age = strip(put(Age_mean,8.2))||\" (\"||strip(put(Age_std, 8.2))||\")\";\n        RUN;\n\n\n\nExamine Table\n\n* Step 5- Examine table;\nPROC PRINT DATA = table1_pre2;\n    VAR BMI SmokeTime Age;\n    RUN;\n\nlstlog\n\n\n&lt;!DOCTYPE html&gt;\n\n\n\n\n\nSAS Output\n\n\n\n\n\n\n\nTable 1A. Means and Standard Deviation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObs\n\n\nBMI\n\n\nSmokeTime\n\n\nAge\n\n\n\n\n\n\n1\n\n\n29.18 (6.66)\n\n\n6.04 (11.88)\n\n\n1.98 (0.82)\n\n\n\n\n\n\n\n\n\n\n\u001421                                                         The SAS System                    Saturday, December 28, 2024 08:22:00 PM\n\n218        ods listing close;ods html5 (id=saspy_internal) file=_tomods1 options(bitmap_mode='inline') device=svg style=HTMLBlue;\n218      ! ods graphics on / outputfmt=png;\n219        \n220        * Step 5- Examine table;\n221        PROC PRINT DATA = table1_pre2;\n222         VAR BMI SmokeTime Age;\n223         RUN;\n224        \n225        \n226        ods html5 (id=saspy_internal) close;ods listing;\n227        \n\u001422                                                         The SAS System                    Saturday, December 28, 2024 08:22:00 PM\n\n228        \n\n\n\n\nTop of Tabset\n\n\n\n\nTranspose from Long Form to Wide Form\n\n*Step 5. Tranpose the data;\nPROC TRANSPOSE DATA = Table1_Pre2 OUT = Table1_Clean(RENAME = (_NAME_ = Variable Col1 = MeanSD));\n    VAR BMI SmokeTime Age;\n    RUN;\n\n\n\nExamine Table\n\nPROC PRINT DATA = Table1_Clean LABEL NOOBS;\n    RUN;\n\nlstlog\n\n\n&lt;!DOCTYPE html&gt;\n\n\n\n\n\nSAS Output\n\n\n\n\n\n\n\nTable 1A. Means and Standard Deviation\n\n\n\n\n\n\n\n\n\n\n\n\n\nNAME OF FORMER VARIABLE\n\n\nMeanSD\n\n\n\n\n\n\nBMI\n\n\n29.18 (6.66)\n\n\n\n\nSmokeTime\n\n\n6.04 (11.88)\n\n\n\n\nAge\n\n\n1.98 (0.82)\n\n\n\n\n\n\n\n\n\n\n\u001425                                                         The SAS System                    Saturday, December 28, 2024 08:22:00 PM\n\n244        ods listing close;ods html5 (id=saspy_internal) file=_tomods1 options(bitmap_mode='inline') device=svg style=HTMLBlue;\n244      ! ods graphics on / outputfmt=png;\n245        \n246        PROC PRINT DATA = Table1_Clean LABEL NOOBS;\n247         RUN;\n248        \n249        \n250        ods html5 (id=saspy_internal) close;ods listing;\n251        \n\u001426                                                         The SAS System                    Saturday, December 28, 2024 08:22:00 PM\n\n252        \n\n\n\n\nTop of Tabset\n\n\n\n\nReapply Labels\n\n* Reapply Labels;\nDATA Table1_Clean;\n    SET Table1_Clean;\n    LABEL Variable = \"Variable\" MeanSd = \"Mean(SD)\";\n    FORMAT Variable $Varnames.;\n    RUN;\n\n\n\nExamine Final Table\n\n* Examine Final clean table 1a;\nPROC PRINT DATA = Table1_Clean LABEL NOOBS;\n    RUN;\nTITLE;\n\nlstlog\n\n\n&lt;!DOCTYPE html&gt;\n\n\n\n\n\nSAS Output\n\n\n\n\n\n\n\nTable 1A. Means and Standard Deviation\n\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\n\n\nMean(SD)\n\n\n\n\n\n\nBMI\n\n\n29.18 (6.66)\n\n\n\n\nYears Smoked\n\n\n6.04 (11.88)\n\n\n\n\nAge\n\n\n1.98 (0.82)\n\n\n\n\n\n\n\n\n\n\n\u001429                                                         The SAS System                    Saturday, December 28, 2024 08:22:00 PM\n\n270        ods listing close;ods html5 (id=saspy_internal) file=_tomods1 options(bitmap_mode='inline') device=svg style=HTMLBlue;\n270      ! ods graphics on / outputfmt=png;\n271        \n272        * Examine Final clean table 1a;\n273        PROC PRINT DATA = Table1_Clean LABEL NOOBS;\n274         RUN;\n275        TITLE;\n276        \n277        \n278        ods html5 (id=saspy_internal) close;ods listing;\n279        \n\u001430                                                         The SAS System                    Saturday, December 28, 2024 08:22:00 PM\n\n280        \n\n\n\n\nTop of Tabset"
  },
  {
    "objectID": "Data_Management_(SAS)/Project_5/Project_5_Report_and_Analysis.html#Table_1B",
    "href": "Data_Management_(SAS)/Project_5/Project_5_Report_and_Analysis.html#Table_1B",
    "title": "Project 5 - Analysis and Report",
    "section": "Table 1B: Categorical Variables",
    "text": "Table 1B: Categorical Variables\n\nCreate Data SetClean Data SetFinal Table\n\n\n\nCreate Data Set of Frequencies and Proportions\n\n*Step 1- Use ODS TRACE to identify name of Table;\n*Step 2- Use OUT to create a dataset;\n*ODS TRACE ON;\nTITLE \"Table 1B. Frequency and Proportion\";\nPROC FREQ DATA = IMPT.BRFSS_FINAL;\n    TABLES Marijuana / OUT = Marijuana_Freq;\n    TABLES Cancer / OUT = Cancer_Freq;\n    TABLES Alcohol / OUT = Alcohol_Freq;\n    RUN;\n*ODS TRACE OFF;\n\nlstlog\n\n\n&lt;!DOCTYPE html&gt;\n\n\n\n\n\nSAS Output\n\n\n\n\n\n\n\nTable 1B. Frequency and Proportion\n\n\n\n\nThe FREQ Procedure\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMarijuana use in past 30 days\n\n\n\n\nMarijuana\n\n\nFrequency\n\n\nPercent\n\n\nCumulativeFrequency\n\n\nCumulativePercent\n\n\n\n\n\n\nFrequency Missing = 25898\n\n\n\n\n\n\nYes\n\n\n3220\n\n\n14.35\n\n\n3220\n\n\n14.35\n\n\n\n\nNo\n\n\n19214\n\n\n85.65\n\n\n22434\n\n\n100.00\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEver had Cancer\n\n\n\n\nCancer\n\n\nFrequency\n\n\nPercent\n\n\nCumulativeFrequency\n\n\nCumulativePercent\n\n\n\n\n\n\nFrequency Missing = 52\n\n\n\n\n\n\nYes\n\n\n4234\n\n\n8.77\n\n\n4234\n\n\n8.77\n\n\n\n\nNo\n\n\n44046\n\n\n91.23\n\n\n48280\n\n\n100.00\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nConsumed alcohol in past 30 days\n\n\n\n\nAlcohol\n\n\nFrequency\n\n\nPercent\n\n\nCumulativeFrequency\n\n\nCumulativePercent\n\n\n\n\n\n\nFrequency Missing = 6528\n\n\n\n\n\n\nYes\n\n\n25303\n\n\n60.53\n\n\n25303\n\n\n60.53\n\n\n\n\nNo\n\n\n16501\n\n\n39.47\n\n\n41804\n\n\n100.00\n\n\n\n\n\n\n\n\n\n\n\n\u001431                                                         The SAS System                    Saturday, December 28, 2024 08:22:00 PM\n\n283        ods listing close;ods html5 (id=saspy_internal) file=_tomods1 options(bitmap_mode='inline') device=svg style=HTMLBlue;\n283      ! ods graphics on / outputfmt=png;\n284        \n285        *Step 1- Use ODS TRACE to identify name of Table;\n286        *Step 2- Use OUT to create a dataset;\n287        *ODS TRACE ON;\n288        TITLE \"Table 1B. Frequency and Proportion\";\n289        PROC FREQ DATA = IMPT.BRFSS_FINAL;\n290         TABLES Marijuana / OUT = Marijuana_Freq;\n291         TABLES Cancer / OUT = Cancer_Freq;\n292         TABLES Alcohol / OUT = Alcohol_Freq;\n293         RUN;\n294        *ODS TRACE OFF;\n295        \n296        \n297        ods html5 (id=saspy_internal) close;ods listing;\n298        \n\u001432                                                         The SAS System                    Saturday, December 28, 2024 08:22:00 PM\n\n299        \n\n\n\n\nTop of Tabset\n\n\n\n\nHandle Missing Values and Examine Data Sets\n\nMarijuana Usage\n\n* Step 3- Examine data set;\n* Handle Missing;\nDATA Marijuana_Freq;\n    SET Marijuana_Freq;\n    IF Marijuana = . THEN DELETE;\n    ELSE IF Marijuana = 1 THEN Marijuana_Status = \"Marijuana Usage: Yes\";\n    ELSE IF Marijuana = 2 THEN Marijuana_Status = \"Marijuana Usage: No\";\n    DROP Marijuana;\n    RENAME Marijuana_Status = Marijuana;\n    RUN;\n    \nPROC PRINT DATA = Marijuana_Freq;\n    RUN;\n\nlstlog\n\n\n&lt;!DOCTYPE html&gt;\n\n\n\n\n\nSAS Output\n\n\n\n\n\n\n\nTable 1B. Frequency and Proportion\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObs\n\n\nCOUNT\n\n\nPERCENT\n\n\nMarijuana\n\n\n\n\n\n\n1\n\n\n3220\n\n\n14.3532\n\n\nMarijuana Usage: Yes\n\n\n\n\n2\n\n\n19214\n\n\n85.6468\n\n\nMarijuana Usage: No\n\n\n\n\n\n\n\n\n\n\n\u001433                                                         The SAS System                    Saturday, December 28, 2024 08:22:00 PM\n\n302        ods listing close;ods html5 (id=saspy_internal) file=_tomods1 options(bitmap_mode='inline') device=svg style=HTMLBlue;\n302      ! ods graphics on / outputfmt=png;\n303        \n304        * Step 3- Examine data set;\n305        * Handle Missing;\n306        DATA Marijuana_Freq;\n307         SET Marijuana_Freq;\n308         IF Marijuana = . THEN DELETE;\n309         ELSE IF Marijuana = 1 THEN Marijuana_Status = \"Marijuana Usage: Yes\";\n310         ELSE IF Marijuana = 2 THEN Marijuana_Status = \"Marijuana Usage: No\";\n311         DROP Marijuana;\n312         RENAME Marijuana_Status = Marijuana;\n313         RUN;\n314         \n315        PROC PRINT DATA = Marijuana_Freq;\n316         RUN;\n317        \n318        \n319        ods html5 (id=saspy_internal) close;ods listing;\n320        \n\u001434                                                         The SAS System                    Saturday, December 28, 2024 08:22:00 PM\n\n321        \n\n\n\n\n\n\nCancer\n\n* Handle Missing;\nDATA Cancer_Freq;\n    SET Cancer_Freq;\n    IF Cancer = . THEN DELETE;\n    ELSE IF Cancer = 1 THEN Cancer_Status = \"Had Cancer: Yes\";\n    ELSE IF Cancer = 2 THEN Cancer_Status = \"Had Cancer: No\";\n    DROP Cancer;\n    RENAME Cancer_Status = Cancer;\n    RUN;\n\nPROC PRINT DATA = Cancer_Freq;\n    RUN;\n\nlstlog\n\n\n&lt;!DOCTYPE html&gt;\n\n\n\n\n\nSAS Output\n\n\n\n\n\n\n\nTable 1B. Frequency and Proportion\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObs\n\n\nCOUNT\n\n\nPERCENT\n\n\nCancer\n\n\n\n\n\n\n1\n\n\n4234\n\n\n8.7697\n\n\nHad Cancer: Yes\n\n\n\n\n2\n\n\n44046\n\n\n91.2303\n\n\nHad Cancer: No\n\n\n\n\n\n\n\n\n\n\n\u001435                                                         The SAS System                    Saturday, December 28, 2024 08:22:00 PM\n\n324        ods listing close;ods html5 (id=saspy_internal) file=_tomods1 options(bitmap_mode='inline') device=svg style=HTMLBlue;\n324      ! ods graphics on / outputfmt=png;\n325        \n326        * Handle Missing;\n327        DATA Cancer_Freq;\n328         SET Cancer_Freq;\n329         IF Cancer = . THEN DELETE;\n330         ELSE IF Cancer = 1 THEN Cancer_Status = \"Had Cancer: Yes\";\n331         ELSE IF Cancer = 2 THEN Cancer_Status = \"Had Cancer: No\";\n332         DROP Cancer;\n333         RENAME Cancer_Status = Cancer;\n334         RUN;\n335        \n336        PROC PRINT DATA = Cancer_Freq;\n337         RUN;\n338        \n339        \n340        ods html5 (id=saspy_internal) close;ods listing;\n341        \n\u001436                                                         The SAS System                    Saturday, December 28, 2024 08:22:00 PM\n\n342        \n\n\n\n\n\n\nAlcohol Usage\n\n* Handle Missing;\nDATA Alcohol_Freq;\n    SET Alcohol_Freq;\n    IF Alcohol = . THEN DELETE;\n    ELSE IF Alcohol = 1 THEN Alcohol_Status = \"Drink Alcohol: Yes\";\n    ELSE IF Alcohol = 2 THEN Alcohol_Status = \"Drink Alcohol: No\";\n    DROP Alcohol;\n    RENAME Alcohol_Status = Alcohol;\n    RUN;\n    \nPROC PRINT DATA = Alcohol_Freq;\n    RUN;\n\nlstlog\n\n\n&lt;!DOCTYPE html&gt;\n\n\n\n\n\nSAS Output\n\n\n\n\n\n\n\nTable 1B. Frequency and Proportion\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObs\n\n\nCOUNT\n\n\nPERCENT\n\n\nAlcohol\n\n\n\n\n\n\n1\n\n\n25303\n\n\n60.5277\n\n\nDrink Alcohol: Yes\n\n\n\n\n2\n\n\n16501\n\n\n39.4723\n\n\nDrink Alcohol: No\n\n\n\n\n\n\n\n\n\n\n\u001437                                                         The SAS System                    Saturday, December 28, 2024 08:22:00 PM\n\n345        ods listing close;ods html5 (id=saspy_internal) file=_tomods1 options(bitmap_mode='inline') device=svg style=HTMLBlue;\n345      ! ods graphics on / outputfmt=png;\n346        \n347        * Handle Missing;\n348        DATA Alcohol_Freq;\n349         SET Alcohol_Freq;\n350         IF Alcohol = . THEN DELETE;\n351         ELSE IF Alcohol = 1 THEN Alcohol_Status = \"Drink Alcohol: Yes\";\n352         ELSE IF Alcohol = 2 THEN Alcohol_Status = \"Drink Alcohol: No\";\n353         DROP Alcohol;\n354         RENAME Alcohol_Status = Alcohol;\n355         RUN;\n356         \n357        PROC PRINT DATA = Alcohol_Freq;\n358         RUN;\n359        \n360        \n361        ods html5 (id=saspy_internal) close;ods listing;\n362        \n\u001438                                                         The SAS System                    Saturday, December 28, 2024 08:22:00 PM\n\n363        \n\n\n\n\n\n\n\n\n\nConcatenate and Examine Final Data Set\n\n*Step 4- Concatanate the data;\nDATA Table1b_Combine;\n    SET Marijuana_Freq(Rename = (Marijuana = Variable))\n        Cancer_Freq(Rename = (Cancer = Variable))\n        Alcohol_Freq(Rename = (Alcohol = Variable));\n    N_PERCENT = strip(put(count,8.)||\" (\"||strip(put(round(percent,0.2),8.))||\"%)\");\n    LABEL N_PERCENT = \"N(%)\" Variable = \"Variable\";\n    DROP COUNT PERCENT;\n    RUN;\n\nPROC PRINT DATA = Table1b_Combine LABEL NOOBS;\n    RUN;\nTITLE;\n\nlstlog\n\n\n&lt;!DOCTYPE html&gt;\n\n\n\n\n\nSAS Output\n\n\n\n\n\n\n\nTable 1B. Frequency and Proportion\n\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\n\n\nN(%)\n\n\n\n\n\n\nMarijuana Usage: Yes\n\n\n3220 (14%)\n\n\n\n\nMarijuana Usage: No\n\n\n19214 (86%)\n\n\n\n\nHad Cancer: Yes\n\n\n4234 (9%)\n\n\n\n\nHad Cancer: No\n\n\n44046 (91%)\n\n\n\n\nDrink Alcohol: Yes\n\n\n25303 (61%)\n\n\n\n\nDrink Alcohol: No\n\n\n16501 (39%)\n\n\n\n\n\n\n\n\n\n\n\u001439                                                         The SAS System                    Saturday, December 28, 2024 08:22:00 PM\n\n366        ods listing close;ods html5 (id=saspy_internal) file=_tomods1 options(bitmap_mode='inline') device=svg style=HTMLBlue;\n366      ! ods graphics on / outputfmt=png;\n367        \n368        *Step 4- Concatanate the data;\n369        DATA Table1b_Combine;\n370         SET Marijuana_Freq(Rename = (Marijuana = Variable))\n371             Cancer_Freq(Rename = (Cancer = Variable))\n372             Alcohol_Freq(Rename = (Alcohol = Variable));\n373         N_PERCENT = strip(put(count,8.)||\" (\"||strip(put(round(percent,0.2),8.))||\"%)\");\n374         LABEL N_PERCENT = \"N(%)\" Variable = \"Variable\";\n375         DROP COUNT PERCENT;\n376         RUN;\n377        \n378        PROC PRINT DATA = Table1b_Combine LABEL NOOBS;\n379         RUN;\n380        TITLE;\n381        \n382        \n383        ods html5 (id=saspy_internal) close;ods listing;\n384        \n\u001440                                                         The SAS System                    Saturday, December 28, 2024 08:22:00 PM\n\n385"
  },
  {
    "objectID": "Data_Management_(SAS)/Project_5/Project_5_Report_and_Analysis.html#T-Test",
    "href": "Data_Management_(SAS)/Project_5/Project_5_Report_and_Analysis.html#T-Test",
    "title": "Project 5 - Analysis and Report",
    "section": "Table 2: T-Test",
    "text": "Table 2: T-Test\n\nMeans and Standard DeviationsPerform T-TestFinal Table\n\n\n\nAcquire Means and Standard Deviations for each State\n\n*Step 1- Use ODS TRACE to identify name of Table;\n*Step 2- Use ODS OUTPUT to create a dataset;\n/*  ods trace on;*/\nTITLE \"Table 2. T-Test of BMI By State\";\nODS OUTPUT TABLE = Table2_pre;\n    PROC TABULATE DATA = IMPT.BRFSS_FINAL;\n    WHERE State NE \"9\";\n    CLASS State;\n    VAR BMI;\n    TABLE BMI,(State)*(mean*f=comma9.2 std*f=comma9.2)/nocellmerge;\n    RUN;\n/*  ods trace off;*/\n\nlstlog\n\n\n\n\n\n\u001441                                                         The SAS System                    Saturday, December 28, 2024 08:22:00 PM\n\n388        ods listing close;ods html5 (id=saspy_internal) file=_tomods1 options(bitmap_mode='inline') device=svg style=HTMLBlue;\n388      ! ods graphics on / outputfmt=png;\n389        \n390        *Step 1- Use ODS TRACE to identify name of Table;\n391        *Step 2- Use ODS OUTPUT to create a dataset;\n392        /*   ods trace on;*/\n393        TITLE \"Table 2. T-Test of BMI By State\";\n394        ODS OUTPUT TABLE = Table2_pre;\n395         \n395      !  PROC TABULATE DATA = IMPT.BRFSS_FINAL;\n396         WHERE State NE \"9\";\nERROR: Variable State is not on file IMPT.BRFSS_FINAL.\n397         CLASS State;\nERROR: Variable STATE not found.\n398         VAR BMI;\n399         TABLE BMI,(State)*(mean*f=comma9.2 std*f=comma9.2)/nocellmerge;\n400         RUN;\nWARNING: Output 'TABLE' was not created.  Make sure that the output object name, label, or path is spelled correctly.  Also, verify \n         that the appropriate procedure options are used to produce the requested output object.  For example, verify that the \n         NOPRINT option is not used.\n401        /*   ods trace off;*/\n402        \n403        \n404        ods html5 (id=saspy_internal) close;ods listing;\n405        \n\u001442                                                         The SAS System                    Saturday, December 28, 2024 08:22:00 PM\n\n406        \n\n\n\n\n\n\nExamine Table\n\n* Step 3- Examine table;\nPROC PRINT DATA = Table2_Pre;\n    RUN;\n\nlstlog\n\n\n\n\n\n\u001443                                                         The SAS System                    Saturday, December 28, 2024 08:22:00 PM\n\n409        ods listing close;ods html5 (id=saspy_internal) file=_tomods1 options(bitmap_mode='inline') device=svg style=HTMLBlue;\n409      ! ods graphics on / outputfmt=png;\n410        \n411        * Step 3- Examine table;\n412        PROC PRINT DATA = Table2_Pre;\nERROR: File WORK.TABLE2_PRE.DATA does not exist.\n413         RUN;\n414        \n415        \n416        ods html5 (id=saspy_internal) close;ods listing;\n417        \n\u001444                                                         The SAS System                    Saturday, December 28, 2024 08:22:00 PM\n\n418        \n\n\n\n\n\n\nManipulate Table for Desired Appearance\n\n* Step 4- Manipulate SAS Table for desired appearance;\nDATA table2_pre;\n    SET table2_pre;\n\n    /* Assuming BMI_Mean and BMI_Std are the correct variable names for means and standard deviations */\n    ARRAY clean(1) $12 BMI_clean; /* Create a new character variable for the formatted result */\n    ARRAY means(1) BMI_Mean;\n    ARRAY sds(1) BMI_Std;\n\n    DO i = 1 TO dim(clean);\n        clean(i) = STRIP(PUT(means(i), 9.2)) || \" (\" || STRIP(PUT(sds(i), 9.2)) || \")\";\n    END;\n\n    KEEP State BMI_clean; /* Keep the state and new formatted BMI variable */\n    RENAME BMI_clean = BMI; /* Rename the formatted variable to BMI */\nRUN;\n\n* Step 5- Check table;\nPROC PRINT DATA = Table2_pre;\n    RUN;\n\nlstlog\n\n\n\n\n\n\u001445                                                         The SAS System                    Saturday, December 28, 2024 08:22:00 PM\n\n421        ods listing close;ods html5 (id=saspy_internal) file=_tomods1 options(bitmap_mode='inline') device=svg style=HTMLBlue;\n421      ! ods graphics on / outputfmt=png;\n422        \n423        * Step 4- Manipulate SAS Table for desired appearance;\n424        DATA table2_pre;\n425            SET table2_pre;\nERROR: File WORK.TABLE2_PRE.DATA does not exist.\n426        \n427            /* Assuming BMI_Mean and BMI_Std are the correct variable names for means and standard deviations */\n428            ARRAY clean(1) $12 BMI_clean; /* Create a new character variable for the formatted result */\n429            ARRAY means(1) BMI_Mean;\n430            ARRAY sds(1) BMI_Std;\n431        \n432            DO i = 1 TO dim(clean);\n433                clean(i) = STRIP(PUT(means(i), 9.2)) || \" (\" || STRIP(PUT(sds(i), 9.2)) || \")\";\n434            END;\n435        \n436            KEEP State BMI_clean; /* Keep the state and new formatted BMI variable */\n437            RENAME BMI_clean = BMI; /* Rename the formatted variable to BMI */\n438        RUN;\nWARNING: The variable State in the DROP, KEEP, or RENAME list has never been referenced.\nWARNING: The data set WORK.TABLE2_PRE may be incomplete.  When this step was stopped there were 0 observations and 1 variables.\n439        \n440        * Step 5- Check table;\n441        PROC PRINT DATA = Table2_pre;\n442         RUN;\n443        \n444        \n445        ods html5 (id=saspy_internal) close;ods listing;\n446        \n\u001446                                                         The SAS System                    Saturday, December 28, 2024 08:22:00 PM\n\n447        \n\n\n\n\n\n\nTranspose Table\n\n* Step 6- Transpose;\nPROC TRANSPOSE DATA = Table2_pre OUT = Table2_Clean;\n    VAR BMI;\n    ID State;\n    RUN;\n\n\n\nExamine Transposed Table\n\n* Step 7- Check table;\nPROC PRINT DATA = Table2_Clean;\n    RUN;\n\nlstlog\n\n\n\n\n\n\u001449                                                         The SAS System                    Saturday, December 28, 2024 08:22:00 PM\n\n464        ods listing close;ods html5 (id=saspy_internal) file=_tomods1 options(bitmap_mode='inline') device=svg style=HTMLBlue;\n464      ! ods graphics on / outputfmt=png;\n465        \n466        * Step 7- Check table;\n467        PROC PRINT DATA = Table2_Clean;\n468         RUN;\n469        \n470        \n471        ods html5 (id=saspy_internal) close;ods listing;\n472        \n\u001450                                                         The SAS System                    Saturday, December 28, 2024 08:22:00 PM\n\n473        \n\n\n\n\nTop of Tabset\n\n\n\n\nPerform the T-Test and Save Results Table\n\n*Step 8- Use ODS TRACE to identify name of Table;\n*Step 9- Use ODS OUTPUT to create a dataset;\n/*  ods trace on;*/\nODS OUTPUT ttests=ttest_results;\n    PROC TTEST DATA = IMPT.BRFSS_FINAL;\n    WHERE State NE \"9\";\n    CLASS State;\n    VAR BMI;\n    RUN;\n/*  ods trace off;*/\n\nlstlog\n\n\n\n\n\n\u001451                                                         The SAS System                    Saturday, December 28, 2024 08:22:00 PM\n\n476        ods listing close;ods html5 (id=saspy_internal) file=_tomods1 options(bitmap_mode='inline') device=svg style=HTMLBlue;\n476      ! ods graphics on / outputfmt=png;\n477        \n478        *Step 8- Use ODS TRACE to identify name of Table;\n479        *Step 9- Use ODS OUTPUT to create a dataset;\n480        /*   ods trace on;*/\n481        ODS OUTPUT ttests=ttest_results;\n482         \n482      !  PROC TTEST DATA = IMPT.BRFSS_FINAL;\n483         WHERE State NE \"9\";\nERROR: Variable State is not on file IMPT.BRFSS_FINAL.\n484         CLASS State;\nERROR: Variable STATE not found.\n485         VAR BMI;\n486         RUN;\nWARNING: Output 'ttests' was not created.  Make sure that the output object name, label, or path is spelled correctly.  Also, \n         verify that the appropriate procedure options are used to produce the requested output object.  For example, verify that \n         the NOPRINT option is not used.\n487        /*   ods trace off;*/\n488        \n489        \n490        ods html5 (id=saspy_internal) close;ods listing;\n491        \n\u001452                                                         The SAS System                    Saturday, December 28, 2024 08:22:00 PM\n\n492        \n\n\n\n\n\n\nExamine T-Test Results Table\n\n*Step 10 Look at the SAS table;\nPROC PRINT DATA = ttest_results;\n    RUN;\n\nlstlog\n\n\n\n\n\n\u001453                                                         The SAS System                    Saturday, December 28, 2024 08:22:00 PM\n\n495        ods listing close;ods html5 (id=saspy_internal) file=_tomods1 options(bitmap_mode='inline') device=svg style=HTMLBlue;\n495      ! ods graphics on / outputfmt=png;\n496        \n497        *Step 10 Look at the SAS table;\n498        PROC PRINT DATA = ttest_results;\nERROR: File WORK.TTEST_RESULTS.DATA does not exist.\n499         RUN;\n500        \n501        \n502        ods html5 (id=saspy_internal) close;ods listing;\n503        \n\u001454                                                         The SAS System                    Saturday, December 28, 2024 08:22:00 PM\n\n504        \n\n\n\n\nTop of Tabset\n\n\n\n\nChange Number of Decimals\n\n* Change number of decimals;\nDATA ttest_results;\n    SET ttest_results;\n    FORMAT Probt 8.3;\n    RUN;\n\n\n\nMerge both Tables\n\n*Step 11 Merge the tables (descriptives and p-value) together;\nPROC SQL;\n    CREATE TABLE Final_Table2 AS\n    SELECT \n        a._name_ as Variable label = \"Variable\",\n        a.California LABEL = \"California\",\n        a.Colorado LABEL = \"Colorado\",\n        b.Probt label = \"p-Value\"\n    FROM Table2_Clean a     \n    LEFT JOIN ttest_Results(WHERE=(VARIANCES = \"Unequal\")) b\n    ON a._name_ = b.variable;\n    QUIT;\n\n\n\nExamine Final Table\n\n* Examine Final Table;\nPROC PRINT DATA = Final_table2 NOOBS LABEL;\n    RUN;\nTITLE;\n\nlstlog\n\n\n\n\n\n\u001459                                                         The SAS System                    Saturday, December 28, 2024 08:22:00 PM\n\n542        ods listing close;ods html5 (id=saspy_internal) file=_tomods1 options(bitmap_mode='inline') device=svg style=HTMLBlue;\n542      ! ods graphics on / outputfmt=png;\n543        \n544        * Examine Final Table;\n545        PROC PRINT DATA = Final_table2 NOOBS LABEL;\nERROR: File WORK.FINAL_TABLE2.DATA does not exist.\n546         RUN;\n547        TITLE;\n548        \n549        \n550        ods html5 (id=saspy_internal) close;ods listing;\n551        \n\u001460                                                         The SAS System                    Saturday, December 28, 2024 08:22:00 PM\n\n552"
  },
  {
    "objectID": "Data_Management_(SAS)/Project_5/Project_5_Report_and_Analysis.html#MLR",
    "href": "Data_Management_(SAS)/Project_5/Project_5_Report_and_Analysis.html#MLR",
    "title": "Project 5 - Analysis and Report",
    "section": "Table 3: Multiple Logistic Regression",
    "text": "Table 3: Multiple Logistic Regression\n\nPerform RegressionClean TablesFinal Table\n\n\n\nPerform Multiple Logistic Regression\n\n\nSave Parameter Estimates and Odds Ratio Tables\n\nTITLE \"Table 3. Multiple Logistic Regression Predicting Ever Having Cancer\";\n/* ODS TRACE ON; */\nODS OUTPUT ParameterEstimates = Parameter_estimates OddsRatios = Odds_Ratio;\nPROC LOGISTIC DATA = Impt.BRFSS_Final;\n    MODEL Cancer = marijuana SmokeNum alcohol age;\n    RUN;\n/* ODS TRACE OFF; */\n\nlstlog\n\n\n&lt;!DOCTYPE html&gt;\n\n\n\n\n\nSAS Output\n\n\n\n\n\n\n\nTable 3. Multiple Logistic Regression Predicting Ever Having Cancer\n\n\n\n\nThe LOGISTIC Procedure\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel Information\n\n\n\n\n\n\nData Set\n\n\nIMPT.BRFSS_FINAL\n\n\n \n\n\n\n\nResponse Variable\n\n\nCancer\n\n\nEver had Cancer\n\n\n\n\nNumber of Response Levels\n\n\n2\n\n\n \n\n\n\n\nModel\n\n\nbinary logit\n\n\n \n\n\n\n\nOptimization Technique\n\n\nFisher's scoring\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNumber of Observations Read\n\n\n48332\n\n\n\n\nNumber of Observations Used\n\n\n8125\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nResponse Profile\n\n\n\n\nOrderedValue\n\n\nCancer\n\n\nTotalFrequency\n\n\n\n\n\n\n1\n\n\nNo\n\n\n7044\n\n\n\n\n2\n\n\nYes\n\n\n1081\n\n\n\n\n\n\nProbability modeled is Cancer='No'.\n\n\n\n\n\nNote:40207 observations were deleted due to missing values for the response or explanatory variables.\n\n\n\n\n\n\n\n\n\n\n\n\nModel Convergence Status\n\n\n\n\n\n\nConvergence criterion (GCONV=1E-8) satisfied.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel Fit Statistics\n\n\n\n\nCriterion\n\n\nIntercept Only\n\n\nIntercept and Covariates\n\n\n\n\n\n\nAIC\n\n\n6374.219\n\n\n5769.319\n\n\n\n\nSC\n\n\n6381.222\n\n\n5804.333\n\n\n\n\n-2 Log L\n\n\n6372.219\n\n\n5759.319\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTesting Global Null Hypothesis: BETA=0\n\n\n\n\nTest\n\n\nChi-Square\n\n\nDF\n\n\nPr &gt; ChiSq\n\n\n\n\n\n\nLikelihood Ratio\n\n\n612.9002\n\n\n4\n\n\n&lt;.0001\n\n\n\n\nScore\n\n\n542.5563\n\n\n4\n\n\n&lt;.0001\n\n\n\n\nWald\n\n\n462.4615\n\n\n4\n\n\n&lt;.0001\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnalysis of Maximum Likelihood Estimates\n\n\n\n\nParameter\n\n\nDF\n\n\nEstimate\n\n\nStandardError\n\n\nWaldChi-Square\n\n\nPr &gt; ChiSq\n\n\n\n\n\n\nIntercept\n\n\n1\n\n\n4.7476\n\n\n0.2306\n\n\n423.7121\n\n\n&lt;.0001\n\n\n\n\nMarijuana\n\n\n1\n\n\n-0.1448\n\n\n0.0884\n\n\n2.6854\n\n\n0.1013\n\n\n\n\nSmokeNum\n\n\n1\n\n\n-0.0681\n\n\n0.0487\n\n\n1.9563\n\n\n0.1619\n\n\n\n\nAlcohol\n\n\n1\n\n\n0.1975\n\n\n0.0703\n\n\n7.8968\n\n\n0.0050\n\n\n\n\nAge\n\n\n1\n\n\n-1.1786\n\n\n0.0562\n\n\n440.1670\n\n\n&lt;.0001\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOdds Ratio Estimates\n\n\n\n\nEffect\n\n\nPoint Estimate\n\n\n95% WaldConfidence Limits\n\n\n\n\n\n\nMarijuana\n\n\n0.865\n\n\n0.728\n\n\n1.029\n\n\n\n\nSmokeNum\n\n\n0.934\n\n\n0.849\n\n\n1.028\n\n\n\n\nAlcohol\n\n\n1.218\n\n\n1.062\n\n\n1.398\n\n\n\n\nAge\n\n\n0.308\n\n\n0.276\n\n\n0.344\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAssociation of Predicted Probabilities and Observed Responses\n\n\n\n\n\n\nPercent Concordant\n\n\n70.8\n\n\nSomers' D\n\n\n0.436\n\n\n\n\nPercent Discordant\n\n\n27.2\n\n\nGamma\n\n\n0.445\n\n\n\n\nPercent Tied\n\n\n2.0\n\n\nTau-a\n\n\n0.100\n\n\n\n\nPairs\n\n\n7614564\n\n\nc\n\n\n0.718\n\n\n\n\n\n\n\n\n\n\n\u001461                                                         The SAS System                    Saturday, December 28, 2024 08:22:00 PM\n\n555        ods listing close;ods html5 (id=saspy_internal) file=_tomods1 options(bitmap_mode='inline') device=svg style=HTMLBlue;\n555      ! ods graphics on / outputfmt=png;\n556        \n557        TITLE \"Table 3. Multiple Logistic Regression Predicting Ever Having Cancer\";\n558        /* ODS TRACE ON; */\n559        ODS OUTPUT ParameterEstimates = Parameter_estimates OddsRatios = Odds_Ratio;\n560        PROC LOGISTIC DATA = Impt.BRFSS_Final;\n561         MODEL Cancer = marijuana SmokeNum alcohol age;\n562         RUN;\n563        /* ODS TRACE OFF; */\n564        \n565        \n566        ods html5 (id=saspy_internal) close;ods listing;\n567        \n\u001462                                                         The SAS System                    Saturday, December 28, 2024 08:22:00 PM\n\n568        \n\n\n\n\n\n\nExamine Parameter Estimates Table\n\n* Examine table;\nPROC PRINT DATA = Parameter_estimates;\n    RUN;\n\nlstlog\n\n\n&lt;!DOCTYPE html&gt;\n\n\n\n\n\nSAS Output\n\n\n\n\n\n\n\nTable 3. Multiple Logistic Regression Predicting Ever Having Cancer\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObs\n\n\nVariable\n\n\nDF\n\n\nEstimate\n\n\nStdErr\n\n\nWaldChiSq\n\n\nProbChiSq\n\n\nESTTYPE\n\n\n\n\n\n\n1\n\n\nIntercept\n\n\n1\n\n\n4.7476\n\n\n0.2306\n\n\n423.7121\n\n\n&lt;.0001\n\n\nMLE\n\n\n\n\n2\n\n\nMarijuana\n\n\n1\n\n\n-0.1448\n\n\n0.0884\n\n\n2.6854\n\n\n0.1013\n\n\nMLE\n\n\n\n\n3\n\n\nSmokeNum\n\n\n1\n\n\n-0.0681\n\n\n0.0487\n\n\n1.9563\n\n\n0.1619\n\n\nMLE\n\n\n\n\n4\n\n\nAlcohol\n\n\n1\n\n\n0.1975\n\n\n0.0703\n\n\n7.8968\n\n\n0.0050\n\n\nMLE\n\n\n\n\n5\n\n\nAge\n\n\n1\n\n\n-1.1786\n\n\n0.0562\n\n\n440.1670\n\n\n&lt;.0001\n\n\nMLE\n\n\n\n\n\n\n\n\n\n\n\u001463                                                         The SAS System                    Saturday, December 28, 2024 08:22:00 PM\n\n571        ods listing close;ods html5 (id=saspy_internal) file=_tomods1 options(bitmap_mode='inline') device=svg style=HTMLBlue;\n571      ! ods graphics on / outputfmt=png;\n572        \n573        * Examine table;\n574        PROC PRINT DATA = Parameter_estimates;\n575         RUN;\n576        \n577        \n578        ods html5 (id=saspy_internal) close;ods listing;\n579        \n\u001464                                                         The SAS System                    Saturday, December 28, 2024 08:22:00 PM\n\n580        \n\n\n\n\n\n\nExamine Odds Ratio Table\n\n* Examine table;\nPROC PRINT DATA = Odds_Ratio;\n    RUN;\n\nlstlog\n\n\n&lt;!DOCTYPE html&gt;\n\n\n\n\n\nSAS Output\n\n\n\n\n\n\n\nTable 3. Multiple Logistic Regression Predicting Ever Having Cancer\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObs\n\n\nEffect\n\n\nOddsRatioEst\n\n\nLowerCL\n\n\nUpperCL\n\n\n\n\n\n\n1\n\n\nMarijuana\n\n\n0.865\n\n\n0.728\n\n\n1.029\n\n\n\n\n2\n\n\nSmokeNum\n\n\n0.934\n\n\n0.849\n\n\n1.028\n\n\n\n\n3\n\n\nAlcohol\n\n\n1.218\n\n\n1.062\n\n\n1.398\n\n\n\n\n4\n\n\nAge\n\n\n0.308\n\n\n0.276\n\n\n0.344\n\n\n\n\n\n\n\n\n\n\n\u001465                                                         The SAS System                    Saturday, December 28, 2024 08:22:00 PM\n\n583        ods listing close;ods html5 (id=saspy_internal) file=_tomods1 options(bitmap_mode='inline') device=svg style=HTMLBlue;\n583      ! ods graphics on / outputfmt=png;\n584        \n585        * Examine table;\n586        PROC PRINT DATA = Odds_Ratio;\n587         RUN;\n588        \n589        \n590        ods html5 (id=saspy_internal) close;ods listing;\n591        \n\u001466                                                         The SAS System                    Saturday, December 28, 2024 08:22:00 PM\n\n592        \n\n\n\n\nTop of Tabset\n\n\n\n\nClean Parameter Estimates Table\n\n* Trim data set;\nDATA parameter_estimates LABEL;\n    SET Parameter_estimates;\n    if _N_ &gt; 1; /* This condition excludes the first row */\n    DROP _ESTTYPE_;\n    FORMAT Estimate StdErr WaldChiSq 8.2;\n    FORMAT ProbChiSq 8.3; \n    RENAME Estimate = Beta StdErr = SE ProbChiSq = p_Value;\n    RUN;\n    \n* Double Check;\nPROC PRINT DATA = parameter_estimates;\n    RUN;\n\nlstlog\n\n\n&lt;!DOCTYPE html&gt;\n\n\n\n\n\nSAS Output\n\n\n\n\n\n\n\nTable 3. Multiple Logistic Regression Predicting Ever Having Cancer\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObs\n\n\nVariable\n\n\nDF\n\n\nBeta\n\n\nSE\n\n\nWaldChiSq\n\n\np_Value\n\n\n\n\n\n\n1\n\n\nMarijuana\n\n\n1\n\n\n-0.14\n\n\n0.09\n\n\n2.69\n\n\n0.101\n\n\n\n\n2\n\n\nSmokeNum\n\n\n1\n\n\n-0.07\n\n\n0.05\n\n\n1.96\n\n\n0.162\n\n\n\n\n3\n\n\nAlcohol\n\n\n1\n\n\n0.20\n\n\n0.07\n\n\n7.90\n\n\n0.005\n\n\n\n\n4\n\n\nAge\n\n\n1\n\n\n-1.18\n\n\n0.06\n\n\n440.17\n\n\n0.000\n\n\n\n\n\n\n\n\n\n\n\u001467                                                         The SAS System                    Saturday, December 28, 2024 08:22:00 PM\n\n595        ods listing close;ods html5 (id=saspy_internal) file=_tomods1 options(bitmap_mode='inline') device=svg style=HTMLBlue;\n595      ! ods graphics on / outputfmt=png;\n596        \n597        * Trim data set;\n598        DATA parameter_estimates LABEL;\n599         SET Parameter_estimates;\n600         if _N_ &gt; 1; /* This condition excludes the first row */\n601         DROP _ESTTYPE_;\n602         FORMAT Estimate StdErr WaldChiSq 8.2;\n603         FORMAT ProbChiSq 8.3;\n604         RENAME Estimate = Beta StdErr = SE ProbChiSq = p_Value;\n605         RUN;\n606         \n607        * Double Check;\n608        PROC PRINT DATA = parameter_estimates;\n609         RUN;\n610        \n611        \n612        ods html5 (id=saspy_internal) close;ods listing;\n613        \n\u001468                                                         The SAS System                    Saturday, December 28, 2024 08:22:00 PM\n\n614        \n\n\n\n\n\n\nClean Odds Ratio Table\n\n* Clean up OR table;\nDATA Odds_Ratio;\n    SET Odds_Ratio;\n    FORMAT OddsRatioEst LowerCL UpperCL 8.2;\n    RUN;\n    \nPROC PRINT DATA = Odds_Ratio;\n    RUN;\n\nlstlog\n\n\n&lt;!DOCTYPE html&gt;\n\n\n\n\n\nSAS Output\n\n\n\n\n\n\n\nTable 3. Multiple Logistic Regression Predicting Ever Having Cancer\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObs\n\n\nEffect\n\n\nOddsRatioEst\n\n\nLowerCL\n\n\nUpperCL\n\n\n\n\n\n\n1\n\n\nMarijuana\n\n\n0.87\n\n\n0.73\n\n\n1.03\n\n\n\n\n2\n\n\nSmokeNum\n\n\n0.93\n\n\n0.85\n\n\n1.03\n\n\n\n\n3\n\n\nAlcohol\n\n\n1.22\n\n\n1.06\n\n\n1.40\n\n\n\n\n4\n\n\nAge\n\n\n0.31\n\n\n0.28\n\n\n0.34\n\n\n\n\n\n\n\n\n\n\n\u001469                                                         The SAS System                    Saturday, December 28, 2024 08:22:00 PM\n\n617        ods listing close;ods html5 (id=saspy_internal) file=_tomods1 options(bitmap_mode='inline') device=svg style=HTMLBlue;\n617      ! ods graphics on / outputfmt=png;\n618        \n619        * Clean up OR table;\n620        DATA Odds_Ratio;\n621         SET Odds_Ratio;\n622         FORMAT OddsRatioEst LowerCL UpperCL 8.2;\n623         RUN;\n624         \n625        PROC PRINT DATA = Odds_Ratio;\n626         RUN;\n627        \n628        \n629        ods html5 (id=saspy_internal) close;ods listing;\n630        \n\u001470                                                         The SAS System                    Saturday, December 28, 2024 08:22:00 PM\n\n631        \n\n\n\n\n\n\nJoin Tables with SQL\n\n* Join Tables;\nPROC SQL;\n    CREATE TABLE Final_Table3 AS\n    SELECT a.Effect as Variable label = \"Variable\",\n           a.OddsRatioEst LABEL = \"OR\",\n           a.LowerCL LABEL = \"Lower 95% CI\",\n           a.UpperCL LABEL = \"Upper 95% CI\",\n           b.p_Value LABEL = \"p-Value\"\n    FROM Odds_ratio a \n    LEFT JOIN Parameter_estimates b\n    ON a.Effect = b.Variable;\n    QUIT;\n    \nDATA Final_Table3;\n    LENGTH Variable $32;\n    SET Final_Table3;\n    IF Variable = \"Alcohol\" THEN Variable = \"Alcohol Used\";\n    ELSE IF Variable = \"Marijuana\" THEN Variable = \"Marijuana Used\";\n    ELSE IF Variable = \"SmokeNum\" THEN Variable = \"Number of Cigarette Packs Smoked\";\n    RUN;\n\nTop of Tabset\n\n\n\n\nExamine Final Table\n\n* Check final table3;\nPROC PRINT DATA = Final_Table3 LABEL NOOBS;\n    RUN;\nTITLE;\n\nlstlog\n\n\n&lt;!DOCTYPE html&gt;\n\n\n\n\n\nSAS Output\n\n\n\n\n\n\n\nTable 3. Multiple Logistic Regression Predicting Ever Having Cancer\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\n\n\nOR\n\n\nLower 95% CI\n\n\nUpper 95% CI\n\n\np-Value\n\n\n\n\n\n\nAge\n\n\n0.31\n\n\n0.28\n\n\n0.34\n\n\n0.000\n\n\n\n\nAlcohol Used\n\n\n1.22\n\n\n1.06\n\n\n1.40\n\n\n0.005\n\n\n\n\nMarijuana Used\n\n\n0.87\n\n\n0.73\n\n\n1.03\n\n\n0.101\n\n\n\n\nNumber of Cigarette Packs Smoked\n\n\n0.93\n\n\n0.85\n\n\n1.03\n\n\n0.162\n\n\n\n\n\n\n\n\n\n\n\u001473                                                         The SAS System                    Saturday, December 28, 2024 08:22:00 PM\n\n663        ods listing close;ods html5 (id=saspy_internal) file=_tomods1 options(bitmap_mode='inline') device=svg style=HTMLBlue;\n663      ! ods graphics on / outputfmt=png;\n664        \n665        * Check final table3;\n666        PROC PRINT DATA = Final_Table3 LABEL NOOBS;\n667         RUN;\n668        TITLE;\n669        \n670        \n671        ods html5 (id=saspy_internal) close;ods listing;\n672        \n\u001474                                                         The SAS System                    Saturday, December 28, 2024 08:22:00 PM\n\n673        \n\n\n\n\nTop of Tabset"
  },
  {
    "objectID": "Data_Management_(SAS)/Project_5/Project_5_Report_and_Analysis.html#figure-1-boxplots",
    "href": "Data_Management_(SAS)/Project_5/Project_5_Report_and_Analysis.html#figure-1-boxplots",
    "title": "Project 5 - Analysis and Report",
    "section": "Figure 1: Boxplots",
    "text": "Figure 1: Boxplots\n\nCreate Boxplot of BMI by State\n\nTITLE \"Boxplot of BMI by State\";\nPROC SGPLOT DATA = IMPT.BRFSS_FINAL;\n    WHERE State = \"6\" OR State = \"8\";\n    VBOX BMI/CATEGORY=state NOOUTLIERS  fillattrs=(color=green) lineattrs=(color=black) whiskerattrs=(color=black) medianattrs=(color=black) meanattrs=(color=black);\n    YAXIS MIN = 0 MAX = 60; \n    RUN;\nTITLE;\n\nlstlog\n\n\n\n\n\n\u001475                                                         The SAS System                    Saturday, December 28, 2024 08:22:00 PM\n\n676        ods listing close;ods html5 (id=saspy_internal) file=_tomods1 options(bitmap_mode='inline') device=svg style=HTMLBlue;\n676      ! ods graphics on / outputfmt=png;\n677        \n678        TITLE \"Boxplot of BMI by State\";\n679        PROC SGPLOT DATA = IMPT.BRFSS_FINAL;\n680         WHERE State = \"6\" OR State = \"8\";\nERROR: Variable State is not on file IMPT.BRFSS_FINAL.\n681         VBOX BMI/CATEGORY=state NOOUTLIERS  fillattrs=(color=green) lineattrs=(color=black) whiskerattrs=(color=black)\n681      ! medianattrs=(color=black) meanattrs=(color=black);\nERROR: Variable STATE not found.\n682         YAXIS MIN = 0 MAX = 60; \n683         RUN;\n684        TITLE;\n685        \n686        \n687        ods html5 (id=saspy_internal) close;ods listing;\n688        \n\u001476                                                         The SAS System                    Saturday, December 28, 2024 08:22:00 PM\n\n689"
  },
  {
    "objectID": "Data_Management_(SAS)/Project_5/Project_5_Report_and_Analysis.html#figure-2-barplots",
    "href": "Data_Management_(SAS)/Project_5/Project_5_Report_and_Analysis.html#figure-2-barplots",
    "title": "Project 5 - Analysis and Report",
    "section": "Figure 2: Barplots",
    "text": "Figure 2: Barplots\n\nCreate Barplot of Cancer Occurence by Marijuana Usage\n\nTITLE \"Barplot of Cancer by Marijuana Usage\";\nPROC SGPLOT DATA = IMPT.BRFSS_FINAL;\n    VBAR Cancer / GROUP = Marijuana\n                GROUPDISPLAY=STACK;\n    RUN;\nTITLE;\n\n\nlstlog\n\n\n&lt;!DOCTYPE html&gt;\n\n\n\n\n\nSAS Output\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001477                                                         The SAS System                    Saturday, December 28, 2024 08:22:00 PM\n\n692        ods listing close;ods html5 (id=saspy_internal) file=_tomods1 options(bitmap_mode='inline') device=svg style=HTMLBlue;\n692      ! ods graphics on / outputfmt=png;\n693        \n694        TITLE \"Barplot of Cancer by Marijuana Usage\";\n695        PROC SGPLOT DATA = IMPT.BRFSS_FINAL;\n696         VBAR Cancer / GROUP = Marijuana\n697                     GROUPDISPLAY=STACK;\n698         RUN;\n699        TITLE;\n700        \n701        \n702        \n703        ods html5 (id=saspy_internal) close;ods listing;\n704        \n\u001478                                                         The SAS System                    Saturday, December 28, 2024 08:22:00 PM\n\n705"
  },
  {
    "objectID": "Data_Management_(SAS)/Project_5/Project_5_Report_and_Analysis.html#research-question-1",
    "href": "Data_Management_(SAS)/Project_5/Project_5_Report_and_Analysis.html#research-question-1",
    "title": "Project 5 - Analysis and Report",
    "section": "Research Question 1",
    "text": "Research Question 1\nThere is a statistically significant difference in BMI between Colorado and California (p = 0.012). On average, California (BMI = 28.56, SE = 6.56) has a BMI that is 0.24 units larger than Colorado (BMI = 28.32, SE = 6.13). While statistically significant, this difference is not clinically significant."
  },
  {
    "objectID": "Data_Management_(SAS)/Project_5/Project_5_Report_and_Analysis.html#research-question-2",
    "href": "Data_Management_(SAS)/Project_5/Project_5_Report_and_Analysis.html#research-question-2",
    "title": "Project 5 - Analysis and Report",
    "section": "Research Question 2",
    "text": "Research Question 2\nMarijuana usage is not a significant predictor of ever having cancer, while controlling for the number of cigarettes smoked per day, alcohol consumption, and age (p = 0.1013). On average, those who use marijuana had 0.87 times the odds of ever having cancer compared to those who did not use marijuana (95% CI: 0.73, 1.029)."
  },
  {
    "objectID": "Data_Management_(SAS)/Project_1/Project_1_Importing.html",
    "href": "Data_Management_(SAS)/Project_1/Project_1_Importing.html",
    "title": "Project 1 - Import Data Into SAS",
    "section": "",
    "text": "This is the first part of the main project for BIOS 6680: Data Management Using SAS, in which we plan and carry out data management activities including data cleaning/quality assurance, data documentation, analytic data set creation, and a final analysis. A key goal of this project is to demonstrate reproducible research and reporting.\nIn this part, we read in all the datasets consististing of multiple filetypes and delimiters, and create a BRFSSImpt import library"
  },
  {
    "objectID": "Data_Management_(SAS)/Project_1/Project_1_Importing.html#race-and-ethnicity",
    "href": "Data_Management_(SAS)/Project_1/Project_1_Importing.html#race-and-ethnicity",
    "title": "Project 1 - Import Data Into SAS",
    "section": "Race and Ethnicity",
    "text": "Race and Ethnicity\nRead in the race and ethnicity data set.\n\nImport Statement\nThis is a .xlsx file so we simply use PROC IMPORT with the DBMS as XLSX.\n\n* Import race and ethnicity data;\nPROC IMPORT \n        DATAFILE =  \"&CourseRoot/BRFSS/Data/1_Source/race_eth.xlsx\"\n        OUT     =   Impt.race_eth\n        DBMS    =   XLSX\n        REPLACE ;\n    RUN;\n\n\n\nPrint Statement\nLet’s examine that data set to be sure of how it looks.\n\n* Print race and ethnicity file;\nTITLE \"Race and Ethnicity\";\nPROC PRINT DATA = Impt.race_eth (OBS = 6);\n    RUN;\n\nlstlog\n\n\n&lt;!DOCTYPE html&gt;\n\n\n\n\n\nSAS Output\n\n\n\n\n\n\n\nRace and Ethnicity\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObs\n\n\nSEQNO\n\n\n_PRACE2\n\n\n_HISPANC\n\n\n\n\n\n\n1\n\n\n2022000001\n\n\n1\n\n\n2\n\n\n\n\n2\n\n\n2022000002\n\n\n1\n\n\n2\n\n\n\n\n3\n\n\n2022000003\n\n\n1\n\n\n2\n\n\n\n\n4\n\n\n2022000004\n\n\n1\n\n\n2\n\n\n\n\n5\n\n\n2022000005\n\n\n1\n\n\n2\n\n\n\n\n6\n\n\n2022000006\n\n\n1\n\n\n2\n\n\n\n\n\n\n\n\n\n\n\u001411                                                         The SAS System                    Saturday, December 28, 2024 08:17:00 PM\n\n84         ods listing close;ods html5 (id=saspy_internal) file=_tomods1 options(bitmap_mode='inline') device=svg style=HTMLBlue;\n84       ! ods graphics on / outputfmt=png;\n85         \n86         * Print race and ethnicity file;\n87         TITLE \"Race and Ethnicity\";\n88         PROC PRINT DATA = Impt.race_eth (OBS = 6);\n89          RUN;\n90         \n91         \n92         ods html5 (id=saspy_internal) close;ods listing;\n93         \n\u001412                                                         The SAS System                    Saturday, December 28, 2024 08:17:00 PM\n\n94         \n\n\n\n\nLooks good."
  },
  {
    "objectID": "Data_Management_(SAS)/Project_1/Project_1_Importing.html#general-demographics",
    "href": "Data_Management_(SAS)/Project_1/Project_1_Importing.html#general-demographics",
    "title": "Project 1 - Import Data Into SAS",
    "section": "General Demographics",
    "text": "General Demographics\nRead in the general demographics data set.\n\nImport Statement\nThis is a .csv file with the first row as the column names, thus we use MISSOVER DSD FIRSTOBS = 2 to denote a comma as the delimiter, and to start reading at the second row.\n\n* Import general demographics data;\nDATA Impt.gen_dem;\n    INFILE \"&CourseRoot/BRFSS/Data/1_Source/gen_dem.csv\" MISSOVER DSD FIRSTOBS = 2;\n    INPUT   marital         \n            trnsgndr\n            _sex\n            _age5year\n            id              :$13.       ;\n    RUN;\n\n\n\nPrint Statement\n\n* Print header to check data;\nTITLE \"General Demographics\";\nPROC PRINT DATA = Impt.gen_dem (OBS = 6);\n    RUN;\n\nlstlog\n\n\n&lt;!DOCTYPE html&gt;\n\n\n\n\n\nSAS Output\n\n\n\n\n\n\n\nGeneral Demographics\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObs\n\n\nmarital\n\n\ntrnsgndr\n\n\n_sex\n\n\n_age5year\n\n\nid\n\n\n\n\n\n\n1\n\n\n1\n\n\n.\n\n\n2\n\n\n13\n\n\n1_2022000001\n\n\n\n\n2\n\n\n3\n\n\n.\n\n\n2\n\n\n13\n\n\n1_2022000002\n\n\n\n\n3\n\n\n1\n\n\n.\n\n\n2\n\n\n8\n\n\n1_2022000003\n\n\n\n\n4\n\n\n1\n\n\n.\n\n\n2\n\n\n14\n\n\n1_2022000004\n\n\n\n\n5\n\n\n1\n\n\n.\n\n\n2\n\n\n5\n\n\n1_2022000005\n\n\n\n\n6\n\n\n1\n\n\n.\n\n\n1\n\n\n13\n\n\n1_2022000006\n\n\n\n\n\n\n\n\n\n\n\u001415                                                         The SAS System                    Saturday, December 28, 2024 08:17:00 PM\n\n115        ods listing close;ods html5 (id=saspy_internal) file=_tomods1 options(bitmap_mode='inline') device=svg style=HTMLBlue;\n115      ! ods graphics on / outputfmt=png;\n116        \n117        * Print header to check data;\n118        TITLE \"General Demographics\";\n119        PROC PRINT DATA = Impt.gen_dem (OBS = 6);\n120         RUN;\n121        \n122        \n123        ods html5 (id=saspy_internal) close;ods listing;\n124        \n\u001416                                                         The SAS System                    Saturday, December 28, 2024 08:17:00 PM\n\n125        \n\n\n\n\n\n\nData Quality Check\nI noticed that there were all missing values for trnsgndr in that output.\nLet’s sort by that variable and double check to ensure we coded it correctly.\n\n* Check that we successfully have trnsgender values imported. This was rarer in the data set so we will sort by it;\nPROC SORT DATA = Impt.gen_dem\n    OUT = sorted_data;\n    BY DESCENDING trnsgndr;\n    RUN;\n\n\n* Print sorted data to check for correct importing;\nPROC PRINT DATA = sorted_data (OBS = 6);\n     RUN;\n\nlstlog\n\n\n&lt;!DOCTYPE html&gt;\n\n\n\n\n\nSAS Output\n\n\n\n\n\n\n\nGeneral Demographics\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObs\n\n\nmarital\n\n\ntrnsgndr\n\n\n_sex\n\n\n_age5year\n\n\nid\n\n\n\n\n\n\n1\n\n\n9\n\n\n9\n\n\n1\n\n\n12\n\n\n2_2022000020\n\n\n\n\n2\n\n\n1\n\n\n9\n\n\n1\n\n\n9\n\n\n2_2022000035\n\n\n\n\n3\n\n\n2\n\n\n9\n\n\n2\n\n\n9\n\n\n2_2022000126\n\n\n\n\n4\n\n\n1\n\n\n9\n\n\n2\n\n\n7\n\n\n2_2022000173\n\n\n\n\n5\n\n\n9\n\n\n9\n\n\n2\n\n\n9\n\n\n2_2022000352\n\n\n\n\n6\n\n\n1\n\n\n9\n\n\n2\n\n\n10\n\n\n2_2022000503\n\n\n\n\n\n\n\n\n\n\n\u001419                                                         The SAS System                    Saturday, December 28, 2024 08:17:00 PM\n\n142        ods listing close;ods html5 (id=saspy_internal) file=_tomods1 options(bitmap_mode='inline') device=svg style=HTMLBlue;\n142      ! ods graphics on / outputfmt=png;\n143        \n144        * Print sorted data to check for correct importing;\n145        PROC PRINT DATA = sorted_data (OBS = 6);\n146          RUN;\n147        \n148        \n149        ods html5 (id=saspy_internal) close;ods listing;\n150        \n\u001420                                                         The SAS System                    Saturday, December 28, 2024 08:17:00 PM\n\n151        \n\n\n\n\nThat looks good, we can move on."
  },
  {
    "objectID": "Data_Management_(SAS)/Project_1/Project_1_Importing.html#social-and-economic-status",
    "href": "Data_Management_(SAS)/Project_1/Project_1_Importing.html#social-and-economic-status",
    "title": "Project 1 - Import Data Into SAS",
    "section": "Social and Economic Status",
    "text": "Social and Economic Status\nRead in the social and economic status data set.\n\nImport Statement\nThis is a .csv file with the first row as the column names, thus we use MISSOVER DSD FIRSTOBS = 2 to denote a comma as the delimiter, and to start reading at the second row.\n\n* Read in socioeconomic data set;\nDATA Impt.socioec;\n    INFILE \"&CourseRoot/BRFSS/Data/1_Source/socioec.csv\" MISSOVER DSD FIRSTOBS = 2;\n    INPUT   educa       \n            employ1\n            income3\n            seqno       :$13.       ;\n    RUN;\n\n\n\nPrint Statement\n\n* Print header of data set;\nTITLE \"Social and Economic Status\";\nPROC PRINT DATA = Impt.socioec (OBS = 6);\n  RUN;\n\nlstlog\n\n\n&lt;!DOCTYPE html&gt;\n\n\n\n\n\nSAS Output\n\n\n\n\n\n\n\nSocial and Economic Status\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObs\n\n\neduca\n\n\nemploy1\n\n\nincome3\n\n\nseqno\n\n\n\n\n\n\n1\n\n\n6\n\n\n7\n\n\n99\n\n\n202-200-0001\n\n\n\n\n2\n\n\n4\n\n\n2\n\n\n5\n\n\n202-200-0002\n\n\n\n\n3\n\n\n6\n\n\n7\n\n\n10\n\n\n202-200-0003\n\n\n\n\n4\n\n\n4\n\n\n7\n\n\n77\n\n\n202-200-0004\n\n\n\n\n5\n\n\n5\n\n\n5\n\n\n5\n\n\n202-200-0005\n\n\n\n\n6\n\n\n4\n\n\n7\n\n\n99\n\n\n202-200-0006\n\n\n\n\n\n\n\n\n\n\n\u001423                                                         The SAS System                    Saturday, December 28, 2024 08:17:00 PM\n\n171        ods listing close;ods html5 (id=saspy_internal) file=_tomods1 options(bitmap_mode='inline') device=svg style=HTMLBlue;\n171      ! ods graphics on / outputfmt=png;\n172        \n173        * Print header of data set;\n174        TITLE \"Social and Economic Status\";\n175        PROC PRINT DATA = Impt.socioec (OBS = 6);\n176          RUN;\n177        \n178        \n179        ods html5 (id=saspy_internal) close;ods listing;\n180        \n\u001424                                                         The SAS System                    Saturday, December 28, 2024 08:17:00 PM\n\n181"
  },
  {
    "objectID": "Data_Management_(SAS)/Project_1/Project_1_Importing.html#insurance",
    "href": "Data_Management_(SAS)/Project_1/Project_1_Importing.html#insurance",
    "title": "Project 1 - Import Data Into SAS",
    "section": "Insurance",
    "text": "Insurance\nRead in the insurance data set.\n\nImport Statement\nThis is a .txt file with spaces as the delimiter. Let’s address.\n\n* Read in insurance data set;\nDATA Impt.insurance;\n    INFILE \"&CourseRoot/BRFSS/Data/1_Source/insurance.txt\" DELIMITER = \" \" MISSOVER DSD FIRSTOBS = 2;\n    INPUT   priminisr       \n            persdoc3\n            medcost1\n            checkup1\n            seqno           ;\n    RUN;\n\n\n\nPrint Statement\n\n# Print header of data set;\nTITLE \"Insurance\";\nPROC PRINT DATA = Impt.insurance (OBS = 6);\n  RUN;\n\nlstlog\n\n\n&lt;!DOCTYPE html&gt;\n\n\n\n\n\nSAS Output\n\n\n\n\n\n\n\nInsurance\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObs\n\n\npriminisr\n\n\npersdoc3\n\n\nmedcost1\n\n\ncheckup1\n\n\nseqno\n\n\n\n\n\n\n1\n\n\n99\n\n\n1\n\n\n2\n\n\n1\n\n\n2022000001\n\n\n\n\n2\n\n\n3\n\n\n2\n\n\n2\n\n\n8\n\n\n2022000002\n\n\n\n\n3\n\n\n1\n\n\n1\n\n\n2\n\n\n1\n\n\n2022000003\n\n\n\n\n4\n\n\n99\n\n\n1\n\n\n2\n\n\n1\n\n\n2022000004\n\n\n\n\n5\n\n\n7\n\n\n2\n\n\n2\n\n\n1\n\n\n2022000005\n\n\n\n\n6\n\n\n3\n\n\n1\n\n\n2\n\n\n1\n\n\n2022000006\n\n\n\n\n\n\n\n\n\n\n\u001427                                                         The SAS System                    Saturday, December 28, 2024 08:17:00 PM\n\n202        ods listing close;ods html5 (id=saspy_internal) file=_tomods1 options(bitmap_mode='inline') device=svg style=HTMLBlue;\n202      ! ods graphics on / outputfmt=png;\n203        \n204        # Print header of data set;\n           _\n           180\nERROR 180-322: Statement is not valid or it is used out of proper order.\n\n205        TITLE \"Insurance\";\n206        PROC PRINT DATA = Impt.insurance (OBS = 6);\n207          RUN;\n208        \n209        \n210        ods html5 (id=saspy_internal) close;ods listing;\n211        \n\u001428                                                         The SAS System                    Saturday, December 28, 2024 08:17:00 PM\n\n212"
  },
  {
    "objectID": "Data_Management_(SAS)/Project_1/Project_1_Importing.html#disabilities",
    "href": "Data_Management_(SAS)/Project_1/Project_1_Importing.html#disabilities",
    "title": "Project 1 - Import Data Into SAS",
    "section": "Disabilities",
    "text": "Disabilities\nRead in the disabilities data set.\n\nImport Statement\nThis is a .txt file with a delimiter of |. Let’s handle that.\n\n* Read in disability data set;\nDATA Impt.disability;\n    INFILE \"&CourseRoot/BRFSS/Data/1_Source/disability.txt\" DELIMITER = \"|\" MISSOVER DSD FIRSTOBS = 2;\n    INPUT   deaf\n            blind\n            decide\n            diffwalk\n            diffdres\n            diffalon\n            id          :$13.   ;\n    RUN;\n\n\n\nPrint Statement\n\n* Print header of data set;\nTITLE \"Disabilities\";\nPROC PRINT DATA = Impt.disability (OBS = 6);\n  RUN;\n\nlstlog\n\n\n&lt;!DOCTYPE html&gt;\n\n\n\n\n\nSAS Output\n\n\n\n\n\n\n\nDisabilities\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObs\n\n\ndeaf\n\n\nblind\n\n\ndecide\n\n\ndiffwalk\n\n\ndiffdres\n\n\ndiffalon\n\n\nid\n\n\n\n\n\n\n1\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n1_2022000001\n\n\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n1_2022000002\n\n\n\n\n3\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n1_2022000003\n\n\n\n\n4\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n1_2022000004\n\n\n\n\n5\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n1_2022000005\n\n\n\n\n6\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n1_2022000006\n\n\n\n\n\n\n\n\n\n\n\u001431                                                         The SAS System                    Saturday, December 28, 2024 08:17:00 PM\n\n235        ods listing close;ods html5 (id=saspy_internal) file=_tomods1 options(bitmap_mode='inline') device=svg style=HTMLBlue;\n235      ! ods graphics on / outputfmt=png;\n236        \n237        * Print header of data set;\n238        TITLE \"Disabilities\";\n239        PROC PRINT DATA = Impt.disability (OBS = 6);\n240          RUN;\n241        \n242        \n243        ods html5 (id=saspy_internal) close;ods listing;\n244        \n\u001432                                                         The SAS System                    Saturday, December 28, 2024 08:17:00 PM\n\n245"
  },
  {
    "objectID": "Data_Management_(SAS)/Project_1/Project_1_Importing.html#chronic-health-conditions",
    "href": "Data_Management_(SAS)/Project_1/Project_1_Importing.html#chronic-health-conditions",
    "title": "Project 1 - Import Data Into SAS",
    "section": "Chronic Health Conditions",
    "text": "Chronic Health Conditions\nRead in the chronic health conditions data set.\n\nImport Statement\nThis is a .dat file with a delimiter of ::. Thus we handle with a DATA statement.\n\n* Read in chronic health conditions data set;\nDATA    Impt.chronic;\n    INFILE  \"&CourseRoot/BRFSS/Data/1_Source/chronic.dat\" DLMSTR = \"::\" DSD;\n    INPUT   seqno           \n            asthma3\n            asthnow\n            chccopd3\n            diabetes4\n            cvdcrhd4\n            cvdinfr4        @@ ;\n    RUN;\n\n\n\nPrint Statement\n\n* Print header of data set;\nTITLE \"Chronic Health Conditions\";\nPROC PRINT DATA = Impt.chronic (OBS = 6);\n  RUN;\n\nlstlog\n\n\n&lt;!DOCTYPE html&gt;\n\n\n\n\n\nSAS Output\n\n\n\n\n\n\n\nChronic Health Conditions\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObs\n\n\nseqno\n\n\nasthma3\n\n\nasthnow\n\n\nchccopd3\n\n\ndiabetes4\n\n\ncvdcrhd4\n\n\ncvdinfr4\n\n\n\n\n\n\n1\n\n\n2022000001\n\n\n2\n\n\n.\n\n\n2\n\n\n1\n\n\n2\n\n\n2\n\n\n\n\n2\n\n\n2022000002\n\n\n2\n\n\n.\n\n\n2\n\n\n3\n\n\n2\n\n\n2\n\n\n\n\n3\n\n\n2022000003\n\n\n2\n\n\n.\n\n\n2\n\n\n3\n\n\n2\n\n\n2\n\n\n\n\n4\n\n\n2022000004\n\n\n1\n\n\n1\n\n\n2\n\n\n3\n\n\n2\n\n\n2\n\n\n\n\n5\n\n\n2022000005\n\n\n2\n\n\n.\n\n\n2\n\n\n3\n\n\n2\n\n\n2\n\n\n\n\n6\n\n\n2022000006\n\n\n2\n\n\n.\n\n\n2\n\n\n1\n\n\n1\n\n\n2\n\n\n\n\n\n\n\n\n\n\n\u001435                                                         The SAS System                    Saturday, December 28, 2024 08:17:00 PM\n\n268        ods listing close;ods html5 (id=saspy_internal) file=_tomods1 options(bitmap_mode='inline') device=svg style=HTMLBlue;\n268      ! ods graphics on / outputfmt=png;\n269        \n270        * Print header of data set;\n271        TITLE \"Chronic Health Conditions\";\n272        PROC PRINT DATA = Impt.chronic (OBS = 6);\n273          RUN;\n274        \n275        \n276        ods html5 (id=saspy_internal) close;ods listing;\n277        \n\u001436                                                         The SAS System                    Saturday, December 28, 2024 08:17:00 PM\n\n278"
  },
  {
    "objectID": "Data_Management_(SAS)/Project_1/Project_1_Importing.html#mental-health",
    "href": "Data_Management_(SAS)/Project_1/Project_1_Importing.html#mental-health",
    "title": "Project 1 - Import Data Into SAS",
    "section": "Mental Health",
    "text": "Mental Health\nRead in the mental health data set.\n\nImport Statement\nThis is just another .csv we’ve already handled before.\n\n* Read in mental health data set;\nDATA    Impt.mentalhealth;\n    INFILE  \"&CourseRoot/BRFSS/Data/1_Source/mentalhealth.csv\" MISSOVER DSD FIRSTOBS=2;\n    INPUT   addepev3            \n            lsatisfy\n            emtsuprt\n            sdhisoft\n            id              :$13.   ;\n    RUN;\n\n\n\nPrint Statement\n\n* Print header of data set;\nTITLE \"Mental Health\";\nPROC PRINT DATA = Impt.mentalhealth (OBS = 6);\n  RUN;\n\nlstlog\n\n\n&lt;!DOCTYPE html&gt;\n\n\n\n\n\nSAS Output\n\n\n\n\n\n\n\nMental Health\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObs\n\n\naddepev3\n\n\nlsatisfy\n\n\nemtsuprt\n\n\nsdhisoft\n\n\nid\n\n\n\n\n\n\n1\n\n\n2\n\n\n1\n\n\n1\n\n\n5\n\n\n1_2022000001\n\n\n\n\n2\n\n\n2\n\n\n1\n\n\n1\n\n\n5\n\n\n1_2022000002\n\n\n\n\n3\n\n\n2\n\n\n2\n\n\n2\n\n\n3\n\n\n1_2022000003\n\n\n\n\n4\n\n\n2\n\n\n1\n\n\n1\n\n\n3\n\n\n1_2022000004\n\n\n\n\n5\n\n\n2\n\n\n1\n\n\n1\n\n\n5\n\n\n1_2022000005\n\n\n\n\n6\n\n\n2\n\n\n2\n\n\n2\n\n\n5\n\n\n1_2022000006\n\n\n\n\n\n\n\n\n\n\n\u001439                                                         The SAS System                    Saturday, December 28, 2024 08:17:00 PM\n\n299        ods listing close;ods html5 (id=saspy_internal) file=_tomods1 options(bitmap_mode='inline') device=svg style=HTMLBlue;\n299      ! ods graphics on / outputfmt=png;\n300        \n301        * Print header of data set;\n302        TITLE \"Mental Health\";\n303        PROC PRINT DATA = Impt.mentalhealth (OBS = 6);\n304          RUN;\n305        \n306        \n307        ods html5 (id=saspy_internal) close;ods listing;\n308        \n\u001440                                                         The SAS System                    Saturday, December 28, 2024 08:17:00 PM\n\n309"
  },
  {
    "objectID": "Data_Management_(SAS)/Project_1/Project_1_Importing.html#cancer",
    "href": "Data_Management_(SAS)/Project_1/Project_1_Importing.html#cancer",
    "title": "Project 1 - Import Data Into SAS",
    "section": "Cancer",
    "text": "Cancer\nRead in the cancer data set.\n\nImport Statement\nThis is a .xls file, thus we use a PROC IMPORT statement with the DBMS as XLS.\n\n* Read in cancer data set;\nPROC IMPORT\n        DATAFILE    = \"&CourseRoot/BRFSS/Data/1_Source/cancer.xls\"\n        OUT         = Impt.cancer\n        DBMS        = XLS\n        REPLACE;\n    RUN;\n\n\n\nPrint Statement\n\n* Print header of data set;\nTITLE \"Cancer\";\nPROC PRINT DATA = Impt.cancer (OBS = 6);\n  RUN;\n\nlstlog\n\n\n&lt;!DOCTYPE html&gt;\n\n\n\n\n\nSAS Output\n\n\n\n\n\n\n\nCancer\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObs\n\n\nCNCRDIFF\n\n\nCNCRAGE\n\n\nCNCRTYP2\n\n\nid\n\n\n\n\n\n\n1\n\n\n.\n\n\n.\n\n\n.\n\n\n6_2022000075\n\n\n\n\n2\n\n\n.\n\n\n.\n\n\n.\n\n\n6_2022000076\n\n\n\n\n3\n\n\n.\n\n\n.\n\n\n.\n\n\n6_2022000077\n\n\n\n\n4\n\n\n.\n\n\n.\n\n\n.\n\n\n6_2022000078\n\n\n\n\n5\n\n\n.\n\n\n.\n\n\n.\n\n\n6_2022000079\n\n\n\n\n6\n\n\n.\n\n\n.\n\n\n.\n\n\n6_2022000080\n\n\n\n\n\n\n\n\n\n\n\u001443                                                         The SAS System                    Saturday, December 28, 2024 08:17:00 PM\n\n328        ods listing close;ods html5 (id=saspy_internal) file=_tomods1 options(bitmap_mode='inline') device=svg style=HTMLBlue;\n328      ! ods graphics on / outputfmt=png;\n329        \n330        * Print header of data set;\n331        TITLE \"Cancer\";\n332        PROC PRINT DATA = Impt.cancer (OBS = 6);\n333          RUN;\n334        \n335        \n336        ods html5 (id=saspy_internal) close;ods listing;\n337        \n\u001444                                                         The SAS System                    Saturday, December 28, 2024 08:17:00 PM\n\n338        \n\n\n\n\n\n\nData Quality Check\nThat’s a lot of missing data points, but that makes sense for cancer data since instances of cancer must be rare.\nLet’s sort that data and print it again to be sure.\n\n* Sort the data;\nPROC SORT DATA = Impt.cancer\n    OUT = sorted_data;\n    BY DESCENDING CNCRDIFF;\n    RUN;\n    \n* Print the data;\nTITLE \"Sorted Data - Quality Check\";\nPROC PRINT DATA = sorted_data (OBS = 6);\n  RUN;\n\nlstlog\n\n\n&lt;!DOCTYPE html&gt;\n\n\n\n\n\nSAS Output\n\n\n\n\n\n\n\nSorted Data - Quality Check\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObs\n\n\nCNCRDIFF\n\n\nCNCRAGE\n\n\nCNCRTYP2\n\n\nid\n\n\n\n\n\n\n1\n\n\n9\n\n\n.\n\n\n.\n\n\n8_2022000168\n\n\n\n\n2\n\n\n9\n\n\n.\n\n\n.\n\n\n8_2022001164\n\n\n\n\n3\n\n\n9\n\n\n.\n\n\n.\n\n\n8_2022001249\n\n\n\n\n4\n\n\n9\n\n\n.\n\n\n.\n\n\n8_2022001934\n\n\n\n\n5\n\n\n9\n\n\n.\n\n\n.\n\n\n8_2022002358\n\n\n\n\n6\n\n\n9\n\n\n.\n\n\n.\n\n\n8_2022002377\n\n\n\n\n\n\n\n\n\n\n\u001445                                                         The SAS System                    Saturday, December 28, 2024 08:17:00 PM\n\n341        ods listing close;ods html5 (id=saspy_internal) file=_tomods1 options(bitmap_mode='inline') device=svg style=HTMLBlue;\n341      ! ods graphics on / outputfmt=png;\n342        \n343        * Sort the data;\n344        PROC SORT DATA = Impt.cancer\n345         OUT = sorted_data;\n346         BY DESCENDING CNCRDIFF;\n347         RUN;\n348         \n349        * Print the data;\n350        TITLE \"Sorted Data - Quality Check\";\n351        PROC PRINT DATA = sorted_data (OBS = 6);\n352          RUN;\n353        \n354        \n355        ods html5 (id=saspy_internal) close;ods listing;\n356        \n\u001446                                                         The SAS System                    Saturday, December 28, 2024 08:17:00 PM\n\n357        \n\n\n\n\nLooks good."
  },
  {
    "objectID": "Data_Management_(SAS)/Project_1/Project_1_Importing.html#obesity",
    "href": "Data_Management_(SAS)/Project_1/Project_1_Importing.html#obesity",
    "title": "Project 1 - Import Data Into SAS",
    "section": "Obesity",
    "text": "Obesity\nRead in the obesity data set.\n\nImport Statement\nThis is a .txt file with a delimiter of *. Let’s handle that.\n\n* Read in the data set;\nDATA    Impt.obesity;\n    INFILE  \"&CourseRoot/BRFSS/Data/1_Source/obesity.txt\" DELIMITER = \"*\" MISSOVER DSD FIRSTOBS = 2;\n    INPUT   seqno       :$13.           \n            weight      \n            height      ;\n    RUN;\n\n\n\nPrint Statement\n\n* Print header of data set;\nPROC PRINT DATA = Impt.obesity (OBS = 6);\n  RUN;\n\nlstlog\n\n\n&lt;!DOCTYPE html&gt;\n\n\n\n\n\nSAS Output\n\n\n\n\n\n\n\nSorted Data - Quality Check\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObs\n\n\nseqno\n\n\nweight\n\n\nheight\n\n\n\n\n\n\n1\n\n\n2022000001\n\n\n9999\n\n\n9999\n\n\n\n\n2\n\n\n2022000002\n\n\n150\n\n\n503\n\n\n\n\n3\n\n\n2022000003\n\n\n140\n\n\n502\n\n\n\n\n4\n\n\n2022000004\n\n\n140\n\n\n505\n\n\n\n\n5\n\n\n2022000005\n\n\n119\n\n\n502\n\n\n\n\n6\n\n\n2022000006\n\n\n187\n\n\n511\n\n\n\n\n\n\n\n\n\n\n\u001449                                                         The SAS System                    Saturday, December 28, 2024 08:17:00 PM\n\n376        ods listing close;ods html5 (id=saspy_internal) file=_tomods1 options(bitmap_mode='inline') device=svg style=HTMLBlue;\n376      ! ods graphics on / outputfmt=png;\n377        \n378        * Print header of data set;\n379        PROC PRINT DATA = Impt.obesity (OBS = 6);\n380          RUN;\n381        \n382        \n383        ods html5 (id=saspy_internal) close;ods listing;\n384        \n\u001450                                                         The SAS System                    Saturday, December 28, 2024 08:17:00 PM\n\n385"
  },
  {
    "objectID": "Data_Management_(SAS)/Project_1/Project_1_Importing.html#covid",
    "href": "Data_Management_(SAS)/Project_1/Project_1_Importing.html#covid",
    "title": "Project 1 - Import Data Into SAS",
    "section": "Covid",
    "text": "Covid\nRead in the covid data set.\n\nImport Statement\nThis is just another .xlsx file.\n\n* Read in the covid data set;\nPROC IMPORT \n        DATAFILE    = \"&CourseRoot/BRFSS/Data/1_Source/covid.xlsx\"\n        OUT         = Impt.covid\n        DBMS        = XLSX\n        REPLACE;\n    RUN;\n\n\n\nPrint Statement\n\n* Print header of data set;\nTITLE \"Covid\";\nPROC PRINT DATA = Impt.covid (OBS = 6);\n  RUN;\n\nlstlog\n\n\n&lt;!DOCTYPE html&gt;\n\n\n\n\n\nSAS Output\n\n\n\n\n\n\n\nCovid\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObs\n\n\nCOVIDPOS\n\n\nCOVIDSMP\n\n\nCOVIDPRM\n\n\nseqno\n\n\n\n\n\n\n1\n\n\n2\n\n\n.\n\n\n.\n\n\n2022000001\n\n\n\n\n2\n\n\n2\n\n\n.\n\n\n.\n\n\n2022000002\n\n\n\n\n3\n\n\n1\n\n\n1\n\n\n9\n\n\n2022000003\n\n\n\n\n4\n\n\n2\n\n\n.\n\n\n.\n\n\n2022000004\n\n\n\n\n5\n\n\n2\n\n\n.\n\n\n.\n\n\n2022000005\n\n\n\n\n6\n\n\n2\n\n\n.\n\n\n.\n\n\n2022000006\n\n\n\n\n\n\n\n\n\n\n\u001453                                                         The SAS System                    Saturday, December 28, 2024 08:17:00 PM\n\n404        ods listing close;ods html5 (id=saspy_internal) file=_tomods1 options(bitmap_mode='inline') device=svg style=HTMLBlue;\n404      ! ods graphics on / outputfmt=png;\n405        \n406        * Print header of data set;\n407        TITLE \"Covid\";\n408        PROC PRINT DATA = Impt.covid (OBS = 6);\n409          RUN;\n410        \n411        \n412        ods html5 (id=saspy_internal) close;ods listing;\n413        \n\u001454                                                         The SAS System                    Saturday, December 28, 2024 08:17:00 PM\n\n414"
  },
  {
    "objectID": "Data_Management_(SAS)/Project_1/Project_1_Importing.html#general-health",
    "href": "Data_Management_(SAS)/Project_1/Project_1_Importing.html#general-health",
    "title": "Project 1 - Import Data Into SAS",
    "section": "General Health",
    "text": "General Health\nRead in the general health data.\n\nImport Statement\nThis is a .txt file.\n\n* Read in in general health data;\nDATA Impt.gen_health;\n    INFILE  \"&CourseRoot/BRFSS/Data/1_Source/gen_health.txt\";\n    INPUT   seqno           $1-11\n            gnhlth          12-14\n            menthlth        15-17\n            physhlth        18-20\n            exerany2        21-23\n            sleptim1                ;\n    RUN;\n\n\n\nPrint Statement\n\n* Print header of data set;\nTITLE \"General Health\";\nPROC PRINT DATA = Impt.gen_health (OBS = 6);\n  RUN;\n\nlstlog\n\n\n&lt;!DOCTYPE html&gt;\n\n\n\n\n\nSAS Output\n\n\n\n\n\n\n\nGeneral Health\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObs\n\n\nseqno\n\n\ngnhlth\n\n\nmenthlth\n\n\nphyshlth\n\n\nexerany2\n\n\nsleptim1\n\n\n\n\n\n\n1\n\n\n2022000001\n\n\n2\n\n\n88\n\n\n88\n\n\n2\n\n\n8\n\n\n\n\n2\n\n\n2022000002\n\n\n1\n\n\n88\n\n\n88\n\n\n2\n\n\n6\n\n\n\n\n3\n\n\n2022000003\n\n\n2\n\n\n2\n\n\n3\n\n\n1\n\n\n5\n\n\n\n\n4\n\n\n2022000004\n\n\n1\n\n\n88\n\n\n88\n\n\n1\n\n\n7\n\n\n\n\n5\n\n\n2022000005\n\n\n4\n\n\n2\n\n\n88\n\n\n1\n\n\n9\n\n\n\n\n6\n\n\n2022000006\n\n\n5\n\n\n1\n\n\n88\n\n\n2\n\n\n7\n\n\n\n\n\n\n\n\n\n\n\u001457                                                         The SAS System                    Saturday, December 28, 2024 08:17:00 PM\n\n436        ods listing close;ods html5 (id=saspy_internal) file=_tomods1 options(bitmap_mode='inline') device=svg style=HTMLBlue;\n436      ! ods graphics on / outputfmt=png;\n437        \n438        * Print header of data set;\n439        TITLE \"General Health\";\n440        PROC PRINT DATA = Impt.gen_health (OBS = 6);\n441          RUN;\n442        \n443        \n444        ods html5 (id=saspy_internal) close;ods listing;\n445        \n\u001458                                                         The SAS System                    Saturday, December 28, 2024 08:17:00 PM\n\n446"
  },
  {
    "objectID": "Data_Management_(SAS)/Project_1/Project_1_Importing.html#marijuana-use",
    "href": "Data_Management_(SAS)/Project_1/Project_1_Importing.html#marijuana-use",
    "title": "Project 1 - Import Data Into SAS",
    "section": "Marijuana Use",
    "text": "Marijuana Use\nRead in the marijuana data set\n\nImport Statement\nThis is a simple .xlsx file.\n\n* Read in marijuana data set;\nPROC IMPORT\n    DATAFILE    =  \"&CourseRoot/BRFSS/Data/1_Source/marijuana.xlsx\"\n    OUT         =   Impt.marijuana\n    DBMS        =   XLSX\n    REPLACE;\nRUN;\n\n\n\n\nPrint Statement\n\n* Print header of data set;\nTITLE \"Marijuana Use\";\nPROC PRINT DATA = Impt.marijuana (OBS = 6);\n  RUN;\n\nlstlog\n\n\n&lt;!DOCTYPE html&gt;\n\n\n\n\n\nSAS Output\n\n\n\n\n\n\n\nMarijuana Use\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObs\n\n\nMARIJAN1\n\n\nMARJSMOK\n\n\nMARJEAT\n\n\nMARJVAPE\n\n\nMARJDAB\n\n\nMARJOTHR\n\n\nid\n\n\n\n\n\n\n1\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n1_2022000001\n\n\n\n\n2\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n1_2022000002\n\n\n\n\n3\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n1_2022000003\n\n\n\n\n4\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n1_2022000004\n\n\n\n\n5\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n1_2022000005\n\n\n\n\n6\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n1_2022000006\n\n\n\n\n\n\n\n\n\n\n\u001461                                                         The SAS System                    Saturday, December 28, 2024 08:17:00 PM\n\n466        ods listing close;ods html5 (id=saspy_internal) file=_tomods1 options(bitmap_mode='inline') device=svg style=HTMLBlue;\n466      ! ods graphics on / outputfmt=png;\n467        \n468        * Print header of data set;\n469        TITLE \"Marijuana Use\";\n470        PROC PRINT DATA = Impt.marijuana (OBS = 6);\n471          RUN;\n472        \n473        \n474        ods html5 (id=saspy_internal) close;ods listing;\n475        \n\u001462                                                         The SAS System                    Saturday, December 28, 2024 08:17:00 PM\n\n476"
  },
  {
    "objectID": "Data_Management_(SAS)/Project_1/Project_1_Importing.html#smoking-status",
    "href": "Data_Management_(SAS)/Project_1/Project_1_Importing.html#smoking-status",
    "title": "Project 1 - Import Data Into SAS",
    "section": "Smoking Status",
    "text": "Smoking Status\nRead in the smoking status data set.\n\nImport Statement\nThis is a simple .xlsx file.\n\n* Read in the smoking status data set;\nPROC IMPORT \n    DATAFILE    = \"&CourseRoot/BRFSS/Data/1_Source/smoke.xlsx\"\n    OUT         = Impt.smoke\n    DBMS        = XLSX\n    REPLACE;\nRUN;\n\n\n\nPrint Statement\n\n* Print header of data set;\nTITLE \"Smoking Status\";\nPROC PRINT DATA = Impt.smoke (OBS = 6);\n  RUN;\n\nlstlog\n\n\n&lt;!DOCTYPE html&gt;\n\n\n\n\n\nSAS Output\n\n\n\n\n\n\n\nSmoking Status\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObs\n\n\nSEQNO\n\n\nSMOKE100\n\n\nSMOKDAY2\n\n\nUSENOW3\n\n\nECIGNOW2\n\n\nLCSFIRST\n\n\nLCSLAST\n\n\nLCSNUMCG\n\n\n\n\n\n\n1\n\n\n2022000001\n\n\n2\n\n\n.\n\n\n3\n\n\n4\n\n\n.\n\n\n.\n\n\n.\n\n\n\n\n2\n\n\n2022000002\n\n\n2\n\n\n.\n\n\n3\n\n\n1\n\n\n.\n\n\n.\n\n\n.\n\n\n\n\n3\n\n\n2022000003\n\n\n2\n\n\n.\n\n\n3\n\n\n1\n\n\n.\n\n\n.\n\n\n.\n\n\n\n\n4\n\n\n2022000004\n\n\n1\n\n\n2\n\n\n3\n\n\n1\n\n\n17\n\n\n999\n\n\n2\n\n\n\n\n5\n\n\n2022000005\n\n\n2\n\n\n.\n\n\n3\n\n\n1\n\n\n.\n\n\n.\n\n\n.\n\n\n\n\n6\n\n\n2022000006\n\n\n2\n\n\n.\n\n\n3\n\n\n1\n\n\n.\n\n\n.\n\n\n.\n\n\n\n\n\n\n\n\n\n\n\u001465                                                         The SAS System                    Saturday, December 28, 2024 08:17:00 PM\n\n495        ods listing close;ods html5 (id=saspy_internal) file=_tomods1 options(bitmap_mode='inline') device=svg style=HTMLBlue;\n495      ! ods graphics on / outputfmt=png;\n496        \n497        * Print header of data set;\n498        TITLE \"Smoking Status\";\n499        PROC PRINT DATA = Impt.smoke (OBS = 6);\n500          RUN;\n501        \n502        \n503        ods html5 (id=saspy_internal) close;ods listing;\n504        \n\u001466                                                         The SAS System                    Saturday, December 28, 2024 08:17:00 PM\n\n505"
  },
  {
    "objectID": "Data_Management_(SAS)/Project_1/Project_1_Importing.html#alcohol-use",
    "href": "Data_Management_(SAS)/Project_1/Project_1_Importing.html#alcohol-use",
    "title": "Project 1 - Import Data Into SAS",
    "section": "Alcohol Use",
    "text": "Alcohol Use\nRead in the alcohol use data set.\n\nImport Statement\nThis is a simple .txt file.\n\n* Read in the alcohol use data set;\nDATA Impt.alcohol;\n    INFILE  \"&CourseRoot/BRFSS/Data/1_Source/alcohol.txt\" MISSOVER DSD FIRSTOBS = 2;\n    INPUT   alcday4         \n            avedrnk3\n            drnk3ge5\n            drnkaby6\n            maxdrnks\n            id          :$13.   ;\n    RUN;\n\n\n\nPrint Statement\n\n* Print header of data set;\nTITLE \"Alcohol Use\";\nPROC PRINT DATA = Impt.alcohol (OBS = 6);\n  RUN;\n\nlstlog\n\n\n&lt;!DOCTYPE html&gt;\n\n\n\n\n\nSAS Output\n\n\n\n\n\n\n\nAlcohol Use\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObs\n\n\nalcday4\n\n\navedrnk3\n\n\ndrnk3ge5\n\n\ndrnkaby6\n\n\nmaxdrnks\n\n\nid\n\n\n\n\n\n\n1\n\n\n888\n\n\n.\n\n\n.\n\n\n.\n\n\n2\n\n\n1_2022000001\n\n\n\n\n2\n\n\n888\n\n\n.\n\n\n.\n\n\n.\n\n\n2\n\n\n1_2022000002\n\n\n\n\n3\n\n\n888\n\n\n.\n\n\n.\n\n\n.\n\n\n2\n\n\n1_2022000003\n\n\n\n\n4\n\n\n888\n\n\n.\n\n\n.\n\n\n.\n\n\n2\n\n\n1_2022000004\n\n\n\n\n5\n\n\n203\n\n\n2\n\n\n88\n\n\n2\n\n\n1\n\n\n1_2022000005\n\n\n\n\n6\n\n\n888\n\n\n.\n\n\n.\n\n\n.\n\n\n2\n\n\n1_2022000006\n\n\n\n\n\n\n\n\n\n\n\u001469                                                         The SAS System                    Saturday, December 28, 2024 08:17:00 PM\n\n527        ods listing close;ods html5 (id=saspy_internal) file=_tomods1 options(bitmap_mode='inline') device=svg style=HTMLBlue;\n527      ! ods graphics on / outputfmt=png;\n528        \n529        * Print header of data set;\n530        TITLE \"Alcohol Use\";\n531        PROC PRINT DATA = Impt.alcohol (OBS = 6);\n532          RUN;\n533        \n534        \n535        ods html5 (id=saspy_internal) close;ods listing;\n536        \n\u001470                                                         The SAS System                    Saturday, December 28, 2024 08:17:00 PM\n\n537"
  },
  {
    "objectID": "Data_Management_(SAS)/Project_1/Project_1_Importing.html#covid-vaccination",
    "href": "Data_Management_(SAS)/Project_1/Project_1_Importing.html#covid-vaccination",
    "title": "Project 1 - Import Data Into SAS",
    "section": "Covid Vaccination",
    "text": "Covid Vaccination\nRead in the covid vaccination data set.\n\nImport Statement\n\n* Read in covid vaccination data;\nDATA Impt.covid_vax;\n    INFILE  \"&CourseRoot/BRFSS/Data/1_Source/covid_vax.csv\" MISSOVER DSD FIRSTOBS = 2;\n    INPUT   covidva1\n            covidnu1\n            covidfs1\n            covidse1\n            seqno       ;\n    RUN;\n\n\n\nPrint Statement\n\n* Print header of data set;\nTITLE \"Covid Vaccination\";\nPROC PRINT DATA = Impt.covid_vax (OBS = 6);\n  RUN;\n\nlstlog\n\n\n&lt;!DOCTYPE html&gt;\n\n\n\n\n\nSAS Output\n\n\n\n\n\n\n\nCovid Vaccination\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObs\n\n\ncovidva1\n\n\ncovidnu1\n\n\ncovidfs1\n\n\ncovidse1\n\n\nseqno\n\n\n\n\n\n\n1\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n2022000001\n\n\n\n\n2\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n2022000002\n\n\n\n\n3\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n2022000003\n\n\n\n\n4\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n2022000004\n\n\n\n\n5\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n2022000005\n\n\n\n\n6\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n2022000006\n\n\n\n\n\n\n\n\n\n\n\u001473                                                         The SAS System                    Saturday, December 28, 2024 08:17:00 PM\n\n558        ods listing close;ods html5 (id=saspy_internal) file=_tomods1 options(bitmap_mode='inline') device=svg style=HTMLBlue;\n558      ! ods graphics on / outputfmt=png;\n559        \n560        * Print header of data set;\n561        TITLE \"Covid Vaccination\";\n562        PROC PRINT DATA = Impt.covid_vax (OBS = 6);\n563          RUN;\n564        \n565        \n566        ods html5 (id=saspy_internal) close;ods listing;\n567        \n\u001474                                                         The SAS System                    Saturday, December 28, 2024 08:17:00 PM\n\n568"
  },
  {
    "objectID": "Data_Management_(SAS)/Project_3/Project3.html",
    "href": "Data_Management_(SAS)/Project_3/Project3.html",
    "title": "Project 3 - Date Set Creation",
    "section": "",
    "text": "This is part three of the main project for BIOS 6680: Data Management Using SAS, in which we plan and carry out data management activities including data cleaning/quality assurance, data documentation, and analytic data set creation. A key goal of this project is to demonstrate reproducible research and reporting.\nThe purpose of this project is to write a SAS program to create a combined analysis data set.\nIn this project we use:\n\nAn array\nDO loops\nIF/THEN statements\nPROC TRANSPOSE\nThe following functions:\n\nSUM()\nTRIM()\nPUT()\nINPUT()\nSUBSTR(), MDY()\nINTCK()\nCOMPRESS()"
  },
  {
    "objectID": "Data_Management_(SAS)/Project_3/Project3.html#email-from-investigator",
    "href": "Data_Management_(SAS)/Project_3/Project3.html#email-from-investigator",
    "title": "Project 3 - Date Set Creation",
    "section": "Email from Investigator",
    "text": "Email from Investigator\n\n\n\n\n\n\nMain Research Question\nThe researcher’s main question is whether there is a significant difference in BMI between California and North Carolina.\n\n\nDesired Data Set\nThey are only interested in including the following states: California, Colorado, Connecticut, Delaware, New Mexico, North Carolina, and West Virginia.\nThe investigator has asked us to create and merge the following data sets:"
  },
  {
    "objectID": "Data_Management_(SAS)/Project_3/Project3.html#consolidate-demogrpahics-data-set",
    "href": "Data_Management_(SAS)/Project_3/Project3.html#consolidate-demogrpahics-data-set",
    "title": "Project 3 - Date Set Creation",
    "section": "Consolidate Demogrpahics Data Set",
    "text": "Consolidate Demogrpahics Data Set\nHere we will collect only the variables we created which the investigator asked for, and the variables needed for the final merge;\n\nCollect Variables\n\nTITLE \"Demographics Cleaned\";\nDATA Impt.Demographics_Final;\n    SET Impt.Demographics;\n    KEEP seqno RaceCd InsuranceCd EducationCd AgeCd IncomeCd Disabilities;\n    RENAME  RaceCd = Race \n            InsuranceCd = Insurance\n            EducationCd = Education\n            AgeCd = Age\n            IncomeCd = Income;\n  RUN;\n\n\n\nView Data\n\n* Examine data set;\nPROC PRINT DATA = Impt.DEMOGRAPHICS_FINAL (OBS = 10);\n    RUN;\n\nlstlog\n\n\n&lt;!DOCTYPE html&gt;\n\n\n\n\n\nSAS Output\n\n\n\n\n\n\n\nDemographics Cleaned\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObs\n\n\nseqno\n\n\nRace\n\n\nInsurance\n\n\nEducation\n\n\nAge\n\n\nIncome\n\n\nDisabilities\n\n\n\n\n\n\n1\n\n\n202-200-0075\n\n\nNon-Hispanic White\n\n\nPrivate/commercial/Medicare/Medigap\n\n\nSome College\n\n\n65+\n\n\n50k-&lt;100k\n\n\n0\n\n\n\n\n2\n\n\n202-200-0076\n\n\nHispanic\n\n\nPrivate/commercial/Medicare/Medigap\n\n\n4+ Year College Degree\n\n\n18-44\n\n\n100k-&lt;200k\n\n\n0\n\n\n\n\n3\n\n\n202-200-0077\n\n\nNon-Hispanic White\n\n\nPrivate/commercial/Medicare/Medigap\n\n\n4+ Year College Degree\n\n\n65+\n\n\n50k-&lt;100k\n\n\n0\n\n\n\n\n4\n\n\n202-200-0078\n\n\nNon-Hispanic Black\n\n\nOther\n\n\n4+ Year College Degree\n\n\n65+\n\n\n35-&lt;50K\n\n\n1\n\n\n\n\n5\n\n\n202-200-0079\n\n\nNon-Hispanic White\n\n\nPrivate/commercial/Medicare/Medigap\n\n\n4+ Year College Degree\n\n\n65+\n\n\n.\n\n\n1\n\n\n\n\n6\n\n\n202-200-0080\n\n\nOther\n\n\nPrivate/commercial/Medicare/Medigap\n\n\n4+ Year College Degree\n\n\n.\n\n\n.\n\n\n0\n\n\n\n\n7\n\n\n202-200-0081\n\n\nNon-Hispanic White\n\n\nPrivate/commercial/Medicare/Medigap\n\n\n4+ Year College Degree\n\n\n65+\n\n\n15-&lt;35k\n\n\n1\n\n\n\n\n8\n\n\n202-200-0082\n\n\nNon-Hispanic White\n\n\nOther\n\n\nGraduated High School\n\n\n45-64\n\n\n35-&lt;50K\n\n\n0\n\n\n\n\n9\n\n\n202-200-0083\n\n\nNon-Hispanic White\n\n\nPrivate/commercial/Medicare/Medigap\n\n\n4+ Year College Degree\n\n\n65+\n\n\n.\n\n\n0\n\n\n\n\n10\n\n\n202-200-0084\n\n\nNon-Hispanic White\n\n\nPrivate/commercial/Medicare/Medigap\n\n\n4+ Year College Degree\n\n\n65+\n\n\n15-&lt;35k\n\n\n0\n\n\n\n\n\n\n\n\n\n\n\u001459                                                         The SAS System                    Saturday, December 28, 2024 08:19:00 PM\n\n569        ods listing close;ods html5 (id=saspy_internal) file=_tomods1 options(bitmap_mode='inline') device=svg style=HTMLBlue;\n569      ! ods graphics on / outputfmt=png;\n570        \n571        * Examine data set;\n572        PROC PRINT DATA = Impt.DEMOGRAPHICS_FINAL (OBS = 10);\n573         RUN;\n574        \n575        \n576        ods html5 (id=saspy_internal) close;ods listing;\n577        \n\u001460                                                         The SAS System                    Saturday, December 28, 2024 08:19:00 PM\n\n578        \n\n\n\n\n\n\nDouble Check for Correct Formats and Labels\n\n* Double check for format;\nODS SELECT VARIABLES;\nPROC CONTENTS DATA = Impt.Demographics_Final;\n    RUN;\n\nlstlog\n\n\n&lt;!DOCTYPE html&gt;\n\n\n\n\n\nSAS Output\n\n\n\n\n\n\n\nDemographics Cleaned\n\n\n\n\nThe CONTENTS Procedure\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAlphabetic List of Variables and Attributes\n\n\n\n\n#\n\n\nVariable\n\n\nType\n\n\nLen\n\n\nFormat\n\n\nInformat\n\n\nLabel\n\n\n\n\n\n\n5\n\n\nAge\n\n\nNum\n\n\n8\n\n\nAGECD.\n\n\n \n\n\nAge\n\n\n\n\n7\n\n\nDisabilities\n\n\nNum\n\n\n8\n\n\nBEST.\n\n\n \n\n\nTotal Number of Disabilities\n\n\n\n\n4\n\n\nEducation\n\n\nNum\n\n\n8\n\n\nEDUCATIONCD.\n\n\n \n\n\nEducation\n\n\n\n\n6\n\n\nIncome\n\n\nNum\n\n\n8\n\n\nINCOMECD.\n\n\n \n\n\nIncome\n\n\n\n\n3\n\n\nInsurance\n\n\nNum\n\n\n8\n\n\nINSURANCECD.\n\n\n \n\n\nInsurance\n\n\n\n\n2\n\n\nRace\n\n\nNum\n\n\n8\n\n\nRACECD.\n\n\n \n\n\nRace\n\n\n\n\n1\n\n\nseqno\n\n\nChar\n\n\n12\n\n\n$12.\n\n\n$12.\n\n\nseqno\n\n\n\n\n\n\n\n\n\n\n\n\u001461                                                         The SAS System                    Saturday, December 28, 2024 08:19:00 PM\n\n581        ods listing close;ods html5 (id=saspy_internal) file=_tomods1 options(bitmap_mode='inline') device=svg style=HTMLBlue;\n581      ! ods graphics on / outputfmt=png;\n582        \n583        * Double check for format;\n584        ODS SELECT VARIABLES;\n585        PROC CONTENTS DATA = Impt.Demographics_Final;\n586         RUN;\n587        \n588        \n589        ods html5 (id=saspy_internal) close;ods listing;\n590        \n\u001462                                                         The SAS System                    Saturday, December 28, 2024 08:19:00 PM\n\n591"
  },
  {
    "objectID": "Data_Management_(SAS)/Project_3/Project3.html#consolidate-outcomes-data-set",
    "href": "Data_Management_(SAS)/Project_3/Project3.html#consolidate-outcomes-data-set",
    "title": "Project 3 - Date Set Creation",
    "section": "Consolidate Outcomes Data Set",
    "text": "Consolidate Outcomes Data Set\nHere we will collect only the variables we created which the investigator asked for, and the variables needed for the final merge.\n\nCollect Variables\n\nTITLE \"Outcomes: Final Data Set\";\nDATA Impt.Outcomes_Final;\n    SET Impt.Outcomes;\n    KEEP SEQNO CHDattackCd AsthmaCd BMI BMIcat CovidCd CancerCd CancerAge;\n    RENAME  CHDattackCd = CHDattack\n            AsthmaCd = Asthma\n            CovidCd = Covid\n            CancerCd = Cancer;\n    RUN;\n\n\n\nView Data\n\n* Print head;\nPROC PRINT DATA = Impt.Outcomes_Final (OBS = 10);\n    RUN;\n    \n* Double check formats;\nODS SELECT VARIABLES;\nPROC CONTENTS DATA = Impt.Outcomes_Final;\n    RUN;\n\nlstlog\n\n\n&lt;!DOCTYPE html&gt;\n\n\n\n\n\nSAS Output\n\n\n\n\n\n\n\nOutcomes: Final Data Set\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObs\n\n\nSEQNO\n\n\nCHDattack\n\n\nAsthma\n\n\nBMI\n\n\nBMIcat\n\n\nCovid\n\n\nCancer\n\n\nCancerAge\n\n\n\n\n\n\n1\n\n\n2022000075\n\n\nNo\n\n\nNever\n\n\n27.2210\n\n\nOverweight\n\n\nNo\n\n\nNo\n\n\n.\n\n\n\n\n2\n\n\n2022000076\n\n\nNo\n\n\nNever\n\n\n26.7554\n\n\nOverweight\n\n\nNo\n\n\nNo\n\n\n.\n\n\n\n\n3\n\n\n2022000077\n\n\nNo\n\n\nFormer\n\n\n21.9214\n\n\nNormal Weight\n\n\nNo\n\n\nNo\n\n\n.\n\n\n\n\n4\n\n\n2022000078\n\n\nNo\n\n\nNever\n\n\n24.5327\n\n\nNormal Weight\n\n\nNo\n\n\nNo\n\n\n.\n\n\n\n\n5\n\n\n2022000079\n\n\nNo\n\n\nNever\n\n\n22.8914\n\n\nNormal Weight\n\n\nNo\n\n\nNo\n\n\n.\n\n\n\n\n6\n\n\n2022000080\n\n\nNo\n\n\nNever\n\n\n.\n\n\n.\n\n\nNo\n\n\nNo\n\n\n.\n\n\n\n\n7\n\n\n2022000081\n\n\nYes\n\n\nNever\n\n\n20.5234\n\n\nNormal Weight\n\n\nNo\n\n\nNo\n\n\n.\n\n\n\n\n8\n\n\n2022000082\n\n\nYes\n\n\nNever\n\n\n25.1377\n\n\nOverweight\n\n\nYes\n\n\nNo\n\n\n.\n\n\n\n\n9\n\n\n2022000083\n\n\nNo\n\n\nNever\n\n\n25.5953\n\n\nOverweight\n\n\nYes\n\n\nNo\n\n\n.\n\n\n\n\n10\n\n\n2022000084\n\n\nNo\n\n\nCurrent\n\n\n.\n\n\n.\n\n\nNo\n\n\nNo\n\n\n.\n\n\n\n\n\n\n\n\n\n\nOutcomes: Final Data Set\n\n\n\n\nThe CONTENTS Procedure\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAlphabetic List of Variables and Attributes\n\n\n\n\n#\n\n\nVariable\n\n\nType\n\n\nLen\n\n\nFormat\n\n\nInformat\n\n\nLabel\n\n\n\n\n\n\n3\n\n\nAsthma\n\n\nNum\n\n\n8\n\n\nASTHMACD.\n\n\n \n\n\nAsthma Status\n\n\n\n\n4\n\n\nBMI\n\n\nNum\n\n\n8\n\n\n \n\n\n \n\n\nPatient BMI\n\n\n\n\n5\n\n\nBMIcat\n\n\nNum\n\n\n8\n\n\nBMICAT.\n\n\n \n\n\nBMI Category\n\n\n\n\n2\n\n\nCHDattack\n\n\nNum\n\n\n8\n\n\nCHDATTACKCD.\n\n\n \n\n\nCHD or Heart Attack\n\n\n\n\n7\n\n\nCancer\n\n\nNum\n\n\n8\n\n\nCANCERCD.\n\n\n \n\n\nEver had Cancer\n\n\n\n\n8\n\n\nCancerAge\n\n\nNum\n\n\n8\n\n\nCANCERAGE.\n\n\n \n\n\nAge at first cancer diagnosis\n\n\n\n\n6\n\n\nCovid\n\n\nNum\n\n\n8\n\n\nCOVIDCD.\n\n\n \n\n\nEver had Covid\n\n\n\n\n1\n\n\nSEQNO\n\n\nNum\n\n\n8\n\n\nBEST12.\n\n\nBEST32.\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\u0014107                                                        The SAS System                    Saturday, December 28, 2024 08:19:00 PM\n\n976        ods listing close;ods html5 (id=saspy_internal) file=_tomods1 options(bitmap_mode='inline') device=svg style=HTMLBlue;\n976      ! ods graphics on / outputfmt=png;\n977        \n978        * Print head;\n979        PROC PRINT DATA = Impt.Outcomes_Final (OBS = 10);\n980         RUN;\n981         \n982        * Double check formats;\n983        ODS SELECT VARIABLES;\n984        PROC CONTENTS DATA = Impt.Outcomes_Final;\n985         RUN;\n986        \n987        \n988        ods html5 (id=saspy_internal) close;ods listing;\n989        \n\u0014108                                                        The SAS System                    Saturday, December 28, 2024 08:19:00 PM\n\n990"
  },
  {
    "objectID": "Data_Management_(SAS)/Project_3/Project3.html#consolidate-risk-factors-data-set",
    "href": "Data_Management_(SAS)/Project_3/Project3.html#consolidate-risk-factors-data-set",
    "title": "Project 3 - Date Set Creation",
    "section": "Consolidate Risk Factors Data Set",
    "text": "Consolidate Risk Factors Data Set\nHere we will collect only the variables we created which the investigator asked for, and the variables needed for the final merge.\n\nCollect Variables\n\nTITLE \"Risk Factors: Final Data Set\";\nDATA Impt.Risk_Final;\n    SET Impt.Risk;\n    KEEP id PhysHealth MentHealth SmokeStat SmokeNum SmokeTime Alcohol CovidVaxMonths MarijuanaPerc Marijuana;\n    RUN;\n\n\n\nView the Data Set\n\n* Print head;\nPROC PRINT DATA = Impt.Risk_Final (OBS = 10);\n    RUN;\n    \n* Double check formats;\nTITLE \"Risk Factors: Check Formats\";\nODS SELECT VARIABLES;\nPROC CONTENTS DATA = Impt.Risk_Final;\n    RUN;\n\nlstlog\n\n\n&lt;!DOCTYPE html&gt;\n\n\n\n\n\nSAS Output\n\n\n\n\n\n\n\nRisk Factors: Final Data Set\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObs\n\n\nid\n\n\nPhysHealth\n\n\nMentHealth\n\n\nSmokeStat\n\n\nSmokeTime\n\n\nAlcohol\n\n\nCovidVaxMonths\n\n\nMarijuanaPerc\n\n\nMarijuana\n\n\nSmokeNum\n\n\n\n\n\n\n1\n\n\n1_2022000001\n\n\n0.00000\n\n\n0\n\n\nNever Smoker\n\n\n0\n\n\nNo\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n\n\n2\n\n\n1_2022000002\n\n\n0.00000\n\n\n0\n\n\nNever Smoker\n\n\n0\n\n\nNo\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n\n\n3\n\n\n1_2022000003\n\n\n6.66667\n\n\n10\n\n\nNever Smoker\n\n\n0\n\n\nNo\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n\n\n4\n\n\n1_2022000004\n\n\n0.00000\n\n\n0\n\n\nCurrent Smoker\n\n\n.\n\n\nNo\n\n\n.\n\n\n.\n\n\n.\n\n\n0.10\n\n\n\n\n5\n\n\n1_2022000005\n\n\n6.66667\n\n\n0\n\n\nNever Smoker\n\n\n0\n\n\nYes\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n\n\n6\n\n\n1_2022000006\n\n\n3.33333\n\n\n0\n\n\nNever Smoker\n\n\n0\n\n\nNo\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n\n\n7\n\n\n1_2022000007\n\n\n0.00000\n\n\n0\n\n\nFormer Smoker\n\n\n42\n\n\nYes\n\n\n.\n\n\n.\n\n\n.\n\n\n1.75\n\n\n\n\n8\n\n\n1_2022000008\n\n\n0.00000\n\n\n0\n\n\nNever Smoker\n\n\n0\n\n\nNo\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n\n\n9\n\n\n1_2022000009\n\n\n0.00000\n\n\n0\n\n\nFormer Smoker\n\n\n1\n\n\nNo\n\n\n.\n\n\n.\n\n\n.\n\n\n0.25\n\n\n\n\n10\n\n\n1_2022000010\n\n\n3.33333\n\n\n0\n\n\nNever Smoker\n\n\n0\n\n\nYes\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n\n\n\n\n\n\n\n\nRisk Factors: Check Formats\n\n\n\n\nThe CONTENTS Procedure\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAlphabetic List of Variables and Attributes\n\n\n\n\n#\n\n\nVariable\n\n\nType\n\n\nLen\n\n\nFormat\n\n\nInformat\n\n\nLabel\n\n\n\n\n\n\n6\n\n\nAlcohol\n\n\nNum\n\n\n8\n\n\nALCOHOL.\n\n\n \n\n\nConsumed alcohol in past 30 days\n\n\n\n\n7\n\n\nCovidVaxMonths\n\n\nNum\n\n\n8\n\n\n \n\n\n \n\n\nNumber of months between first and second covid vaccination\n\n\n\n\n9\n\n\nMarijuana\n\n\nNum\n\n\n8\n\n\nMARIJUANA.\n\n\n \n\n\nMarijuana use in past 30 days\n\n\n\n\n8\n\n\nMarijuanaPerc\n\n\nNum\n\n\n8\n\n\n \n\n\n \n\n\nPercent of 30 days with marijuana use\n\n\n\n\n3\n\n\nMentHealth\n\n\nNum\n\n\n8\n\n\n \n\n\n \n\n\nPercent of 30 days with mental health not good\n\n\n\n\n2\n\n\nPhysHealth\n\n\nNum\n\n\n8\n\n\n \n\n\n \n\n\nPercent of 30 days with physical health not good\n\n\n\n\n10\n\n\nSmokeNum\n\n\nNum\n\n\n8\n\n\n \n\n\n \n\n\nNumer of Cigarette Packs Per Day\n\n\n\n\n4\n\n\nSmokeStat\n\n\nNum\n\n\n8\n\n\nSMOKESTAT.\n\n\n \n\n\nSmoking Status\n\n\n\n\n5\n\n\nSmokeTime\n\n\nNum\n\n\n8\n\n\n \n\n\n \n\n\nHow many years smoked\n\n\n\n\n1\n\n\nid\n\n\nChar\n\n\n13\n\n\n$13.\n\n\n$13.\n\n\nid\n\n\n\n\n\n\n\n\n\n\n\n\u0014153                                                        The SAS System                    Saturday, December 28, 2024 08:19:00 PM\n\n1377       ods listing close;ods html5 (id=saspy_internal) file=_tomods1 options(bitmap_mode='inline') device=svg style=HTMLBlue;\n1377     ! ods graphics on / outputfmt=png;\n1378       \n1379       * Print head;\n1380       PROC PRINT DATA = Impt.Risk_Final (OBS = 10);\n1381        RUN;\n1382        \n1383       * Double check formats;\n1384       TITLE \"Risk Factors: Check Formats\";\n1385       ODS SELECT VARIABLES;\n1386       PROC CONTENTS DATA = Impt.Risk_Final;\n1387        RUN;\n1388       \n1389       \n1390       ods html5 (id=saspy_internal) close;ods listing;\n1391       \n\u0014154                                                        The SAS System                    Saturday, December 28, 2024 08:19:00 PM\n\n1392"
  },
  {
    "objectID": "Data_Management_(SAS)/Project_3/Project3.html#create-date-variable-in-reference-table",
    "href": "Data_Management_(SAS)/Project_3/Project3.html#create-date-variable-in-reference-table",
    "title": "Project 3 - Date Set Creation",
    "section": "Create Date Variable in Reference Table",
    "text": "Create Date Variable in Reference Table\n\nCreate Variable\n\n* Create and Format date variable;\nDATA Impt.Ref_Final;\n    SET Impt.ref_table_long;\n    IntvDate = MDY(IMONTH, IDAY, IYEAR);\n    ATTRIB IntvDate LABEL = \"Interview Date\" FORMAT = MMDDYY10.;\n    KEEP id SEQNO IntvDate;\n    RUN;\n\n\n\nDouble Check for Correct Conversion\n\n* Double check by printing;\nPROC PRINT DATA = Impt.Ref_Final (OBS = 10);\n    RUN;\n\nlstlog\n\n\n&lt;!DOCTYPE html&gt;\n\n\n\n\n\nSAS Output\n\n\n\n\n\n\n\nRisk Factors: Check Formats\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObs\n\n\nid\n\n\nSEQNO\n\n\nIntvDate\n\n\n\n\n\n\n1\n\n\n6_2022000075\n\n\n2022000075\n\n\n07/05/2022\n\n\n\n\n2\n\n\n6_2022000076\n\n\n2022000076\n\n\n06/28/2022\n\n\n\n\n3\n\n\n6_2022000077\n\n\n2022000077\n\n\n06/27/2022\n\n\n\n\n4\n\n\n6_2022000078\n\n\n2022000078\n\n\n06/29/2022\n\n\n\n\n5\n\n\n6_2022000079\n\n\n2022000079\n\n\n07/05/2022\n\n\n\n\n6\n\n\n6_2022000080\n\n\n2022000080\n\n\n07/05/2022\n\n\n\n\n7\n\n\n6_2022000081\n\n\n2022000081\n\n\n07/09/2022\n\n\n\n\n8\n\n\n6_2022000082\n\n\n2022000082\n\n\n06/28/2022\n\n\n\n\n9\n\n\n6_2022000083\n\n\n2022000083\n\n\n07/04/2022\n\n\n\n\n10\n\n\n6_2022000084\n\n\n2022000084\n\n\n07/09/2022\n\n\n\n\n\n\n\n\n\n\n\u0014157                                                        The SAS System                    Saturday, December 28, 2024 08:19:00 PM\n\n1411       ods listing close;ods html5 (id=saspy_internal) file=_tomods1 options(bitmap_mode='inline') device=svg style=HTMLBlue;\n1411     ! ods graphics on / outputfmt=png;\n1412       \n1413       * Double check by printing;\n1414       PROC PRINT DATA = Impt.Ref_Final (OBS = 10);\n1415        RUN;\n1416       \n1417       \n1418       ods html5 (id=saspy_internal) close;ods listing;\n1419       \n\u0014158                                                        The SAS System                    Saturday, December 28, 2024 08:19:00 PM\n\n1420"
  },
  {
    "objectID": "Data_Management_(SAS)/Project_3/Project3.html#merge-reference-table-demographics-and-outcomes-on-seqno",
    "href": "Data_Management_(SAS)/Project_3/Project3.html#merge-reference-table-demographics-and-outcomes-on-seqno",
    "title": "Project 3 - Date Set Creation",
    "section": "Merge Reference Table, Demographics, and Outcomes on SEQNO",
    "text": "Merge Reference Table, Demographics, and Outcomes on SEQNO\n\nEnsure Same Format and Type for SEQNO\n\n* Convert SEQNO in the reference table to numeric and SORT;\nTITLE \"Final Data Set\";\nDATA Impt.Ref_Final;\n    SET Impt.Ref_Final;\n    SEQNO_num = INPUT(SEQNO, BEST12.);\n    FORMAT SEQNO_num BEST12.;\n    DROP SEQNO;\n    RENAME SEQNO_NUM = SEQNO;\n    RUN;\n\n\n* Correct the SEQNO format in the demographics data set;\nDATA Impt.Demographics_Final;\n    SET Impt.Demographics_Final;\n    SEQNO = COMPRESS(SEQNO, \"-\");\n    SEQNO_num = INPUT(SEQNO, BEST12.);\n    FORMAT SEQNO_num BEST12.;\n    DROP SEQNO;\n    RENAME SEQNO_num = SEQNO;\n    RUN;\n\n\n\nSort Data Sets\n\n* Sort Demographics_Final by SEQNO;\nPROC SORT DATA = Impt.Demographics_Final;\n    BY SEQNO;\n    RUN;\n\n* Sort Outcomes_Final by SEQNO;\nPROC SORT DATA = Impt.Outcomes_Final;\n    BY SEQNO;\n    RUN;    \n\n* Sort Ref_Final by SEQNO;\nPROC SORT DATA = Impt.Ref_Final;\n    BY SEQNO;\n    RUN;\n\n\n\nPerform Merge on SEQNO\n\n* First merge by SEQNO;\nDATA Impt.BRFSS_Final;\n    MERGE Impt.Demographics_Final (IN = a)\n          Impt.Ref_Final (IN = b)\n          Impt.Outcomes_Final (IN = c);\n    BY SEQNO;\n    IF a AND b AND c;\n    RUN;"
  },
  {
    "objectID": "Data_Management_(SAS)/Project_3/Project3.html#merge-risk-factors-on-id",
    "href": "Data_Management_(SAS)/Project_3/Project3.html#merge-risk-factors-on-id",
    "title": "Project 3 - Date Set Creation",
    "section": "Merge Risk Factors on ID",
    "text": "Merge Risk Factors on ID\n\nSort by ID\n\n* Sort by ID;\nPROC SORT DATA = Impt.BRFSS_Final;\n    BY id;\n    RUN;\n    \n* Sort by ID;\nPROC SORT DATA = impt.Risk_Final;\n    BY id;\n    RUN;\n\n\n\nPerform Merge on ID\n\n* Then merge by Id;\nDATA Impt.BRFSS_Final;\n    MERGE Impt.BRFSS_Final (IN = a)\n          Impt.Risk_Final (IN = b); \n    BY id;\n    IF a AND b;\n    RUN;"
  },
  {
    "objectID": "Data_Management_(SAS)/Project_3/Project3.html#clean-final-data-set",
    "href": "Data_Management_(SAS)/Project_3/Project3.html#clean-final-data-set",
    "title": "Project 3 - Date Set Creation",
    "section": "Clean Final Data Set",
    "text": "Clean Final Data Set\n\n* Rearrange and drop extraneous variables for a cleaner data set;\nDATA Impt.BRFSS_Final;\n    RETAIN id SEQNO IntvDate;\n    SET Impt.BRFSS_Final;\n    DROP SEQNO_OUTCOMES SEQNO_REF SEQNO_DEMO id_RISK id SEQNO;\n    ATTRIB SEQNO LABEL = \"Sequence Number\";\n    ATTRIB id LABEL = \"ID\";\n    RUN;"
  },
  {
    "objectID": "Data_Visualization/Cancer_Animations/Gapminder_Cancer_Animations.html",
    "href": "Data_Visualization/Cancer_Animations/Gapminder_Cancer_Animations.html",
    "title": "Animated Plot - Global Lung Cancer Rates",
    "section": "",
    "text": "In this mini-project we perform data tidying, joining, and ggplot animation to answer a public health research question using data sets from the gapminder data base.\n\n\n\nI am interested in trends in lung cancer in men over time (because the data sets from Gapminder are split by sex).\n\nHypothesis: I predict that over time, as life expectancy increases, lung cancer rates will increase.\n\nAdditionally, as GDP increases, cancer rates should decrease.\n\n\nThis research question is inspired by a TED talk which discussed how society has progressed in medicine so much that we have “killed all other killers”, allowing cancer to become more common, as longer life expectancies increase the chances of cancerous mutations occurring."
  },
  {
    "objectID": "Data_Visualization/Cancer_Animations/Gapminder_Cancer_Animations.html#research-question",
    "href": "Data_Visualization/Cancer_Animations/Gapminder_Cancer_Animations.html#research-question",
    "title": "Animated Plot - Global Lung Cancer Rates",
    "section": "",
    "text": "I am interested in trends in lung cancer in men over time (because the data sets from Gapminder are split by sex).\n\nHypothesis: I predict that over time, as life expectancy increases, lung cancer rates will increase.\n\nAdditionally, as GDP increases, cancer rates should decrease.\n\n\nThis research question is inspired by a TED talk which discussed how society has progressed in medicine so much that we have “killed all other killers”, allowing cancer to become more common, as longer life expectancies increase the chances of cancerous mutations occurring."
  },
  {
    "objectID": "Data_Visualization/Cancer_Animations/Gapminder_Cancer_Animations.html#load-libraries",
    "href": "Data_Visualization/Cancer_Animations/Gapminder_Cancer_Animations.html#load-libraries",
    "title": "Animated Plot - Global Lung Cancer Rates",
    "section": "Load Libraries",
    "text": "Load Libraries\n\nlibrary(gapminder)\nlibrary(tidyverse)\nlibrary(ggdark) # Used for fun dark themes for plots\nlibrary(kableExtra)\nlibrary(gganimate)\n\nAll data sets are from Gapminder.org\n\nRead in Lung Cancer Data\n\n# Read in data set\ndata_cancer &lt;- read_csv(r\"(C:\\Users\\sviea\\Documents\\Portfolio\\Data_Visualization\\Cancer_Animations\\RawData\\lung_cancer_new_cases_per_100000_men.csv)\")\n\nRows: 207 Columns: 68\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (1): country\ndbl (67): 1953, 1954, 1955, 1956, 1957, 1958, 1959, 1960, 1961, 1962, 1963, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# Examine data\npretty_print(head(data_cancer))\n\n\n\n\ncountry\n1953\n1954\n1955\n1956\n1957\n1958\n1959\n1960\n1961\n1962\n1963\n1964\n1965\n1966\n1967\n1968\n1969\n1970\n1971\n1972\n1973\n1974\n1975\n1976\n1977\n1978\n1979\n1980\n1981\n1982\n1983\n1984\n1985\n1986\n1987\n1988\n1989\n1990\n1991\n1992\n1993\n1994\n1995\n1996\n1997\n1998\n1999\n2000\n2001\n2002\n2003\n2004\n2005\n2006\n2007\n2008\n2009\n2010\n2011\n2012\n2013\n2014\n2015\n2016\n2017\n2018\n2019\n\n\n\n\nAfghanistan\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n21.5\n21.1\n20.8\n20.5\n20.4\n20.2\n20.0\n19.9\n19.8\n19.7\n19.8\n19.9\n19.8\n19.9\n20.1\n20.0\n20.0\n19.9\n19.9\n19.8\n19.8\n19.7\n19.6\n19.5\n19.5\n19.4\n19.4\n19.3\n19.3\n19.2\n\n\nAngola\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n24.8\n24.8\n24.8\n24.9\n24.8\n24.6\n24.2\n24.1\n24.3\n24.4\n24.1\n23.7\n22.9\n22.6\n23.0\n22.5\n22.7\n22.7\n23.0\n23.4\n23.6\n23.8\n24.1\n24.1\n23.7\n23.7\n23.6\n23.6\n23.7\n23.8\n\n\nAlbania\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n52.5\n52.0\n49.0\n46.6\n43.8\n45.1\n46.8\n47.3\n47.1\n46.4\n46.1\n43.8\n44.1\n45.2\n44.8\n42.8\n40.3\n38.7\n39.3\n39.3\n39.5\n40.6\n41.1\n41.8\n43.1\n44.4\n44.7\n44.9\n45.1\n45.2\n\n\nAndorra\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n75.2\n75.4\n75.7\n74.7\n73.8\n73.0\n72.2\n71.7\n71.3\n70.9\n70.4\n70.0\n69.7\n69.6\n69.8\n69.2\n68.8\n68.3\n67.8\n67.3\n66.5\n66.2\n65.6\n65.3\n64.8\n63.4\n63.1\n62.8\n62.5\n62.1\n\n\nUAE\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n25.9\n26.1\n26.4\n26.7\n27.5\n27.9\n28.4\n28.9\n28.9\n29.0\n30.2\n30.4\n30.5\n30.1\n29.9\n29.5\n28.2\n27.7\n26.6\n25.9\n25.3\n24.8\n24.2\n23.8\n23.3\n22.9\n22.5\n22.2\n21.8\n21.5\n\n\nArgentina\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n56.8\n56.2\n57.5\n56.0\n55.2\n54.6\n55.4\n55.2\n54.9\n54.9\n51.8\n51.2\n51.8\n50.9\n48.5\n47.4\n46.8\n47.8\n45.8\n44.5\n43.4\n43.0\n42.3\n41.3\n39.5\n39.1\n40.2\n40.3\n39.8\n39.8\n\n\n\n\n\n\n\nWe have NA values for 1953-1989. Let’s just get rid of those now.\n\n# Remove unnecessary columns with missing values\ndata_cancer &lt;- data_cancer |&gt; \n  select(-c(\"1953\":\"1989\"))\n\n# Double check for correct removal\npretty_print(head(data_cancer))\n\n\n\n\ncountry\n1990\n1991\n1992\n1993\n1994\n1995\n1996\n1997\n1998\n1999\n2000\n2001\n2002\n2003\n2004\n2005\n2006\n2007\n2008\n2009\n2010\n2011\n2012\n2013\n2014\n2015\n2016\n2017\n2018\n2019\n\n\n\n\nAfghanistan\n21.5\n21.1\n20.8\n20.5\n20.4\n20.2\n20.0\n19.9\n19.8\n19.7\n19.8\n19.9\n19.8\n19.9\n20.1\n20.0\n20.0\n19.9\n19.9\n19.8\n19.8\n19.7\n19.6\n19.5\n19.5\n19.4\n19.4\n19.3\n19.3\n19.2\n\n\nAngola\n24.8\n24.8\n24.8\n24.9\n24.8\n24.6\n24.2\n24.1\n24.3\n24.4\n24.1\n23.7\n22.9\n22.6\n23.0\n22.5\n22.7\n22.7\n23.0\n23.4\n23.6\n23.8\n24.1\n24.1\n23.7\n23.7\n23.6\n23.6\n23.7\n23.8\n\n\nAlbania\n52.5\n52.0\n49.0\n46.6\n43.8\n45.1\n46.8\n47.3\n47.1\n46.4\n46.1\n43.8\n44.1\n45.2\n44.8\n42.8\n40.3\n38.7\n39.3\n39.3\n39.5\n40.6\n41.1\n41.8\n43.1\n44.4\n44.7\n44.9\n45.1\n45.2\n\n\nAndorra\n75.2\n75.4\n75.7\n74.7\n73.8\n73.0\n72.2\n71.7\n71.3\n70.9\n70.4\n70.0\n69.7\n69.6\n69.8\n69.2\n68.8\n68.3\n67.8\n67.3\n66.5\n66.2\n65.6\n65.3\n64.8\n63.4\n63.1\n62.8\n62.5\n62.1\n\n\nUAE\n25.9\n26.1\n26.4\n26.7\n27.5\n27.9\n28.4\n28.9\n28.9\n29.0\n30.2\n30.4\n30.5\n30.1\n29.9\n29.5\n28.2\n27.7\n26.6\n25.9\n25.3\n24.8\n24.2\n23.8\n23.3\n22.9\n22.5\n22.2\n21.8\n21.5\n\n\nArgentina\n56.8\n56.2\n57.5\n56.0\n55.2\n54.6\n55.4\n55.2\n54.9\n54.9\n51.8\n51.2\n51.8\n50.9\n48.5\n47.4\n46.8\n47.8\n45.8\n44.5\n43.4\n43.0\n42.3\n41.3\n39.5\n39.1\n40.2\n40.3\n39.8\n39.8\n\n\n\n\n\n\n\n\n\nRead in GDP Per Capita Data\nThe gapminder data set that comes with the tidyverse package only goes up to 2007. To get a more complete data set we will read in the GDP per capita data set downloaded from the gapminder website.\n\n# Read in data set\ndata_gdp &lt;- read_csv(r\"(C:\\Users\\sviea\\Documents\\Portfolio\\Data_Visualization\\Cancer_Animations\\RawData\\gdp_pcap.csv)\")\n\nRows: 195 Columns: 302\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (199): country, 1901, 1903, 1905, 1906, 1907, 1908, 1909, 1910, 1911, 19...\ndbl (103): 1800, 1801, 1802, 1803, 1804, 1805, 1806, 1807, 1808, 1809, 1810,...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# Filter data set\ndata_gdp &lt;- data_gdp |&gt; \n  select(c(\"country\", \"1990\":\"2019\"))\n\n# Examine data\npretty_print(head(data_gdp))\n\n\n\n\ncountry\n1990\n1991\n1992\n1993\n1994\n1995\n1996\n1997\n1998\n1999\n2000\n2001\n2002\n2003\n2004\n2005\n2006\n2007\n2008\n2009\n2010\n2011\n2012\n2013\n2014\n2015\n2016\n2017\n2018\n2019\n\n\n\n\nAfghanistan\n1850\n1710\n1650\n1140\n855\n1280\n1220\n1180\n1130\n1090\n1070\n979\n1230\n1260\n1240\n1330\n1360\n1530\n1560\n1820\n2030\n1960\n2120\n2170\n2150\n2110\n2100\n2100\n2060\n2080\n\n\nAngola\n4000\n4050\n3810\n2890\n2920\n3340\n3770\n4000\n4150\n4190\n4270\n4380\n4900\n4950\n5370\n6040\n6590\n7310\n7860\n7650\n7690\n7660\n8010\n8100\n8180\n7970\n7490\n7220\n6880\n6600\n\n\nAlbania\n4750\n3460\n3260\n3600\n3910\n4410\n4830\n4330\n4750\n5410\n5840\n6390\n6710\n7130\n7560\n8030\n8560\n9140\n9910\n10.3k\n10.7k\n11.1k\n11.2k\n11.4k\n11.6k\n11.9k\n12.3k\n12.8k\n13.3k\n13.7k\n\n\nAndorra\n40.4k\n40k\n39k\n37.5k\n37.5k\n38k\n39.7k\n43.6k\n45.4k\n47.2k\n47k\n47.4k\n47.7k\n51.9k\n54.9k\n60.4k\n65.2k\n66.5k\n63.7k\n64.5k\n60.5k\n60.9k\n57.9k\n55.7k\n56.6k\n56.5k\n57.5k\n56.4k\n56.2k\n56.3k\n\n\nUAE\n72.1k\n72k\n73.2k\n73.4k\n77.6k\n81.9k\n84.6k\n88.6k\n86k\n85.7k\n92k\n90.1k\n89.2k\n93.6k\n98.8k\n97.3k\n94.1k\n81.7k\n71.1k\n59k\n56.4k\n59.2k\n59.7k\n62.1k\n64.1k\n67.8k\n70.9k\n70.9k\n71.2k\n71.5k\n\n\nArgentina\n13.4k\n14.5k\n15.6k\n16.6k\n17.4k\n16.7k\n17.5k\n18.7k\n19.3k\n18.5k\n18.2k\n17.2k\n15.2k\n16.4k\n17.7k\n19.1k\n20.5k\n22.2k\n22.9k\n21.3k\n23.4k\n24.6k\n24k\n24.3k\n23.5k\n23.9k\n23.1k\n23.5k\n22.7k\n22k\n\n\n\n\n\n\n\nNote: We have mixed data representation here! Some variables have a “k” to represent 10,000. We will have to process this.\n\n\nRead in Life Expectancy Data\n\n# Read in data set\ndata_lex &lt;- read_csv(r\"(C:\\Users\\sviea\\Documents\\R for Data Science\\lex.csv)\")\n\nRows: 196 Columns: 302\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr   (1): country\ndbl (301): 1800, 1801, 1802, 1803, 1804, 1805, 1806, 1807, 1808, 1809, 1810,...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# Filter data set\ndata_lex &lt;- data_lex |&gt; \n  select(c(\"country\", \"1990\":\"2019\"))\n\n# Examine data\npretty_print(head(data_lex))\n\n\n\n\ncountry\n1990\n1991\n1992\n1993\n1994\n1995\n1996\n1997\n1998\n1999\n2000\n2001\n2002\n2003\n2004\n2005\n2006\n2007\n2008\n2009\n2010\n2011\n2012\n2013\n2014\n2015\n2016\n2017\n2018\n2019\n\n\n\n\nAfghanistan\n53.8\n53.8\n54.2\n54.4\n53.9\n54.3\n54.7\n54.5\n53.3\n54.7\n54.7\n54.8\n55.5\n56.5\n57.1\n57.6\n58.0\n58.5\n59.2\n59.9\n60.5\n61.0\n61.4\n61.9\n61.9\n61.9\n62.0\n62.9\n62.7\n63.3\n\n\nAngola\n49.7\n50.3\n50.3\n49.0\n50.3\n51.2\n51.7\n51.6\n50.6\n51.9\n52.8\n53.4\n54.5\n55.1\n55.5\n56.4\n57.0\n58.0\n58.8\n59.5\n60.2\n60.8\n61.4\n62.1\n63.0\n63.5\n63.9\n64.2\n64.6\n65.1\n\n\nAlbania\n72.8\n72.6\n73.2\n73.8\n74.6\n74.6\n74.5\n72.9\n74.8\n75.1\n75.4\n76.0\n75.9\n75.6\n75.8\n76.2\n76.9\n77.5\n77.6\n78.0\n78.1\n78.1\n78.2\n78.3\n78.2\n78.1\n78.2\n78.3\n78.4\n78.5\n\n\nAndorra\n79.0\n79.1\n79.2\n79.3\n79.5\n79.8\n80.0\n80.2\n80.4\n80.6\n80.8\n80.9\n81.1\n81.2\n81.3\n81.4\n81.5\n81.7\n81.8\n81.8\n81.8\n81.9\n81.9\n82.0\n82.0\n82.0\n82.1\n82.1\n82.1\n82.2\n\n\nUAE\n68.7\n68.7\n68.8\n68.8\n68.7\n68.8\n68.9\n69.0\n69.2\n69.2\n69.1\n69.2\n69.4\n69.3\n69.1\n69.2\n69.5\n70.0\n70.4\n70.6\n70.8\n71.0\n71.2\n71.6\n73.0\n73.2\n73.4\n73.5\n73.7\n73.9\n\n\nArgentina\n72.5\n72.7\n72.8\n73.0\n73.4\n73.4\n73.5\n73.6\n73.7\n73.8\n74.2\n74.3\n74.3\n74.4\n74.9\n75.3\n75.4\n75.3\n75.7\n75.8\n75.9\n76.0\n76.2\n76.3\n76.5\n76.5\n76.2\n76.3\n76.5\n76.6"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Sean Vieau",
    "section": "",
    "text": "My Quarto Website"
  },
  {
    "objectID": "Project_1_Regression/Project_1_Regression_SAS/Project_1_Regression_SAS.html",
    "href": "Project_1_Regression/Project_1_Regression_SAS/Project_1_Regression_SAS.html",
    "title": "Project 1 - Regression (SAS)",
    "section": "",
    "text": "The aim of the current study is to assess whether a new gel treatment for gum disease results in lower whole-mouth average pocket depth and attachment loss after one year. Average pocket depth and attachment loss were taken at baseline, and participants were assigned into one of five groups (1 = placebo, 2 = no treatment, 3 = low concentration, 4 = medium concentration, 5 = high concentration) and instructed to apply the gel to their gums twice a day. After 1-year, average pocket depth and attachment loss were recorded again. Data was received as a .csv file containing treatment level, average pocket depth and attachment loss at baseline and at one-year, demographic information, gender, age, number of sites measured, and smoking status. The clinical hypothesis is that average pocket depth and attachment loss in participants who applied the gel will be lower compared to participants who did not. Gender, age, ethnicity, and smoking status will be investigated as potential covariates.\nThe project description provided by the PI is available below:\n\n\nThis document was created in RStudio with Quarto.\nTo code in SAS we must first connect RStudio to the SAS server.\n\n# This code connects to SAS On Demand\nlibrary(configSAS)\nconfigSAS::set_sas_engine()\n\nUsing SAS Config named: oda\nSAS Connection established. Subprocess id is 6028\n\n#  This code allows you to run ```{sas}``` chunks\nsas = knitr::opts_chunk$get(\"sas\")\n\n\n\n\n\n\nWe begin by making our library for the project.\n\n* Create library;\n%LET CourseRoot = /home/u63376223/sasuser.v94/Advanced Data Analysis;\nLIBNAME Proj1 \"&CourseRoot/Project 1\";\n\n\n\n\nThen we will import the dataset\n\n* Import the dataset;\nPROC IMPORT\n    DATAFILE = \"&CourseRoot/Project 1/Project1_data.csv\"\n    OUT = Proj1.raw_data\n    REPLACE;\n    RUN;\n\n\n\n\nWe have missing values in this data set as ‘NA’. SAS will not be able to handle those. We need to convert them to missing values in SAS format (“” for characters and . for numeric);\n\n* Fix missing value format for attachment loss;\nDATA Proj1.data;\n    SET Proj1.raw_data;\n    if attach1year = \"NA\" then attach1year = \"\";\n    attach1year = INPUT(attach1year, best32.);\n    RUN;\n\n* Fix missing value format for pocket depth change;\nDATA Proj1.data;\n    SET Proj1.data;\n    if pd1year = \"NA\" then pd1year = \"\";\n    pd1year = INPUT(pd1year, best32.);\n    RUN;\n\n\n/* Convert 'NA' to missing values and ensure numeric format for multiple variables */\n/* Code from ChatGPT */\nDATA Proj1.data;\n    SET Proj1.data;\n    \n    /* Define arrays for character and numeric variables */\n    array char_vars[2] attach1year pd1year ; /* Add more variables as needed */\n    array num_vars[2] attach1year_num pd1year_num; /* Corresponding numeric variables */\n    \n    /* Loop through the arrays to handle 'NA' and convert to numeric */\n    do i = 1 to dim(char_vars);\n        if char_vars[i] = 'NA' then char_vars[i] = '';\n        num_vars[i] = input(char_vars[i], best32.);\n    end;\n    \n    /* Drop the original character variables and rename the numeric ones */\n    drop attach1year pd1year i; /* Add more variables as needed */\n    rename attach1year_num = attach1year pd1year_num = pd1year; /* Corresponding renaming */\nRUN;\n\n\n\n\nWe will then create labels for the categorical variables.\n\n* Create format;\nPROC FORMAT;\n    VALUE trtgroup_fmt\n        1 = \"Placebo\"\n        2 = \"Control\"\n        3 = \"Low\"\n        4 = \"Medium\"\n        5 = \"High\";\n    VALUE gender_fmt\n        1 = \"Male\"\n        2 = \"Female\";\n    VALUE race_fmt\n        1 = \"Native American\"\n        2 = \"African American\"\n        4 = \"White\"\n        5 = \"Asian\";\n    VALUE smoker_fmt\n        0 = \"Non-Smoker\"\n        1 = \"Smoker\";\n    RUN;\n\n* Apply format;\nDATA Proj1.data;\n    SET Proj1.data;\n    FORMAT  trtgroup trtgroup_fmt.\n            gender gender_fmt.\n            race race_fmt.\n            smoker smoker_fmt.;\n    RUN;\n\nLet’s also change the labels of all variable names so they are capitalized and make our output look more professional.\n\n* Change labels to capitalize variable names;\nDATA Proj1.data;\n    SET Proj1.data;\n    LABEL \n        id = \"ID\"\n        trtgroup = \"Treatment Group\"\n        gender = \"Gender\"\n        race= \"Race\"\n        age = \"Age\"\n        smoker = \"Smoking Status\"\n        sites = \"Sites\"\n        attachbase = \"Attachment Loss Baseline\"\n        attach1year = \"Attachment Loss One Year\"\n        pdbase = \"Pocket Depth Baseline\"\n        pd1year = \"Pocket Depth One Year\"\n        ;\n    RUN;\n\n\n\n\nFinally let’s visualize the data set.\n\n* View Dataset;\nPROC PRINT DATA = Proj1.data (OBS = 10);\n    RUN;\n\nlstlog\n\n\n&lt;!DOCTYPE html&gt;\n\n\n\n\n\nSAS Output\n\n\n\n\n\n\n\nThe SAS System\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObs\n\n\nid\n\n\ntrtgroup\n\n\ngender\n\n\nrace\n\n\nage\n\n\nsmoker\n\n\nsites\n\n\nattachbase\n\n\npdbase\n\n\nattach1year\n\n\npd1year\n\n\n\n\n\n\n1\n\n\n101\n\n\nMedium\n\n\nFemale\n\n\nAsian\n\n\n44.57221082\n\n\nSmoker\n\n\n162\n\n\n2.432098765\n\n\n3.24691358\n\n\n2.57764\n\n\n3.40741\n\n\n\n\n2\n\n\n102\n\n\nHigh\n\n\nFemale\n\n\nAsian\n\n\n35.57289528\n\n\nSmoker\n\n\n162\n\n\n2.543209877\n\n\n3.00617284\n\n\n.\n\n\n.\n\n\n\n\n3\n\n\n103\n\n\nControl\n\n\nFemale\n\n\nAsian\n\n\n47.94524298\n\n\nSmoker\n\n\n144\n\n\n2.881944444\n\n\n3.118055556\n\n\n3.07639\n\n\n3.12500\n\n\n\n\n4\n\n\n104\n\n\nLow\n\n\nFemale\n\n\nAsian\n\n\n55.17864476\n\n\nSmoker\n\n\n138\n\n\n4.956521739\n\n\n5.217391304\n\n\n5.30435\n\n\n4.89130\n\n\n\n\n5\n\n\n105\n\n\nPlacebo\n\n\nFemale\n\n\nAfrican American\n\n\n43.79739904\n\n\nSmoker\n\n\n168\n\n\n1.773809524\n\n\n3.363095238\n\n\n1.45238\n\n\n2.89881\n\n\n\n\n6\n\n\n106\n\n\nMedium\n\n\nFemale\n\n\nAsian\n\n\n42.14921287\n\n\nSmoker\n\n\n168\n\n\n2.369047619\n\n\n3.910714286\n\n\n1.92262\n\n\n3.08333\n\n\n\n\n7\n\n\n107\n\n\nLow\n\n\nFemale\n\n\nAsian\n\n\n37.15811088\n\n\nSmoker\n\n\n156\n\n\n3.544871795\n\n\n3.897435897\n\n\n2.83974\n\n\n3.23718\n\n\n\n\n8\n\n\n108\n\n\nControl\n\n\nFemale\n\n\nAsian\n\n\n67.41957563\n\n\nSmoker\n\n\n162\n\n\n2.493827161\n\n\n3.481481482\n\n\n2.19136\n\n\n3.08025\n\n\n\n\n9\n\n\n109\n\n\nHigh\n\n\nFemale\n\n\nAfrican American\n\n\n40.59137577\n\n\nSmoker\n\n\n156\n\n\n1.92948718\n\n\n3.301282051\n\n\n1.62821\n\n\n2.71154\n\n\n\n\n10\n\n\n110\n\n\nPlacebo\n\n\nFemale\n\n\nAsian\n\n\n49.2128679\n\n\nSmoker\n\n\n162\n\n\n1.790123457\n\n\n3.277777778\n\n\n1.46296\n\n\n2.41975\n\n\n\n\n\n\n\n\n\n\n\u001417                                                         The SAS System                    Saturday, December 28, 2024 08:27:00 PM\n\n213        ods listing close;ods html5 (id=saspy_internal) file=_tomods1 options(bitmap_mode='inline') device=svg style=HTMLBlue;\n213      ! ods graphics on / outputfmt=png;\n214        \n215        * View Dataset;\n216        PROC PRINT DATA = Proj1.data (OBS = 10);\n217         RUN;\n218        \n219        \n220        ods html5 (id=saspy_internal) close;ods listing;\n221        \n\u001418                                                         The SAS System                    Saturday, December 28, 2024 08:27:00 PM\n\n222        \n\n\n\n\nOur data set is comprised of 130 observations, with 11 variables.\n\n\n\nThe variables in the provided data set are as follows:\n\nid: Patient ID\ntrtgroup: Treatment group (1 = placebo, 2 = no treatment control, 3 = low concentration, 4 = medium concentration, 5 = high concentration gel)\ngender: Gender (1 = male, 2 = female)\nrace: Race, (1 = Native American, 2 = African American, 3 = Not used, 4 = Asian, 5 = White)\nage: Age in years\nsmoker: Smoking status (0 = No, 1 = Yes)\nsites: Number of sites gum measurements were averaged from\nattachbase: Whole-mouth average attachment loss taken at base timepoint\nattach1year: Whole-mouth average attachment loss after 1 year\npdbase: Whole-mouth average pocket loss at base timepoint\npd1year: Whole-mouth average pocket loss after 1 year\n\n\n\n\n\n\n\nNote\n\n\n\nPocket depth and attachment loss are both measurements of how far the gums have pulled away from the teeth, hence smaller values are better"
  },
  {
    "objectID": "Project_1_Regression/Project_1_Regression_SAS/Project_1_Regression_SAS.html#load-sas",
    "href": "Project_1_Regression/Project_1_Regression_SAS/Project_1_Regression_SAS.html#load-sas",
    "title": "Project 1 - Regression (SAS)",
    "section": "",
    "text": "This document was created in RStudio with Quarto.\nTo code in SAS we must first connect RStudio to the SAS server.\n\n# This code connects to SAS On Demand\nlibrary(configSAS)\nconfigSAS::set_sas_engine()\n\nUsing SAS Config named: oda\nSAS Connection established. Subprocess id is 6028\n\n#  This code allows you to run ```{sas}``` chunks\nsas = knitr::opts_chunk$get(\"sas\")"
  },
  {
    "objectID": "Project_1_Regression/Project_1_Regression_SAS/Project_1_Regression_SAS.html#data-preparation",
    "href": "Project_1_Regression/Project_1_Regression_SAS/Project_1_Regression_SAS.html#data-preparation",
    "title": "Project 1 - Regression (SAS)",
    "section": "",
    "text": "We begin by making our library for the project.\n\n* Create library;\n%LET CourseRoot = /home/u63376223/sasuser.v94/Advanced Data Analysis;\nLIBNAME Proj1 \"&CourseRoot/Project 1\";\n\n\n\n\nThen we will import the dataset\n\n* Import the dataset;\nPROC IMPORT\n    DATAFILE = \"&CourseRoot/Project 1/Project1_data.csv\"\n    OUT = Proj1.raw_data\n    REPLACE;\n    RUN;\n\n\n\n\nWe have missing values in this data set as ‘NA’. SAS will not be able to handle those. We need to convert them to missing values in SAS format (“” for characters and . for numeric);\n\n* Fix missing value format for attachment loss;\nDATA Proj1.data;\n    SET Proj1.raw_data;\n    if attach1year = \"NA\" then attach1year = \"\";\n    attach1year = INPUT(attach1year, best32.);\n    RUN;\n\n* Fix missing value format for pocket depth change;\nDATA Proj1.data;\n    SET Proj1.data;\n    if pd1year = \"NA\" then pd1year = \"\";\n    pd1year = INPUT(pd1year, best32.);\n    RUN;\n\n\n/* Convert 'NA' to missing values and ensure numeric format for multiple variables */\n/* Code from ChatGPT */\nDATA Proj1.data;\n    SET Proj1.data;\n    \n    /* Define arrays for character and numeric variables */\n    array char_vars[2] attach1year pd1year ; /* Add more variables as needed */\n    array num_vars[2] attach1year_num pd1year_num; /* Corresponding numeric variables */\n    \n    /* Loop through the arrays to handle 'NA' and convert to numeric */\n    do i = 1 to dim(char_vars);\n        if char_vars[i] = 'NA' then char_vars[i] = '';\n        num_vars[i] = input(char_vars[i], best32.);\n    end;\n    \n    /* Drop the original character variables and rename the numeric ones */\n    drop attach1year pd1year i; /* Add more variables as needed */\n    rename attach1year_num = attach1year pd1year_num = pd1year; /* Corresponding renaming */\nRUN;\n\n\n\n\nWe will then create labels for the categorical variables.\n\n* Create format;\nPROC FORMAT;\n    VALUE trtgroup_fmt\n        1 = \"Placebo\"\n        2 = \"Control\"\n        3 = \"Low\"\n        4 = \"Medium\"\n        5 = \"High\";\n    VALUE gender_fmt\n        1 = \"Male\"\n        2 = \"Female\";\n    VALUE race_fmt\n        1 = \"Native American\"\n        2 = \"African American\"\n        4 = \"White\"\n        5 = \"Asian\";\n    VALUE smoker_fmt\n        0 = \"Non-Smoker\"\n        1 = \"Smoker\";\n    RUN;\n\n* Apply format;\nDATA Proj1.data;\n    SET Proj1.data;\n    FORMAT  trtgroup trtgroup_fmt.\n            gender gender_fmt.\n            race race_fmt.\n            smoker smoker_fmt.;\n    RUN;\n\nLet’s also change the labels of all variable names so they are capitalized and make our output look more professional.\n\n* Change labels to capitalize variable names;\nDATA Proj1.data;\n    SET Proj1.data;\n    LABEL \n        id = \"ID\"\n        trtgroup = \"Treatment Group\"\n        gender = \"Gender\"\n        race= \"Race\"\n        age = \"Age\"\n        smoker = \"Smoking Status\"\n        sites = \"Sites\"\n        attachbase = \"Attachment Loss Baseline\"\n        attach1year = \"Attachment Loss One Year\"\n        pdbase = \"Pocket Depth Baseline\"\n        pd1year = \"Pocket Depth One Year\"\n        ;\n    RUN;\n\n\n\n\nFinally let’s visualize the data set.\n\n* View Dataset;\nPROC PRINT DATA = Proj1.data (OBS = 10);\n    RUN;\n\nlstlog\n\n\n&lt;!DOCTYPE html&gt;\n\n\n\n\n\nSAS Output\n\n\n\n\n\n\n\nThe SAS System\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObs\n\n\nid\n\n\ntrtgroup\n\n\ngender\n\n\nrace\n\n\nage\n\n\nsmoker\n\n\nsites\n\n\nattachbase\n\n\npdbase\n\n\nattach1year\n\n\npd1year\n\n\n\n\n\n\n1\n\n\n101\n\n\nMedium\n\n\nFemale\n\n\nAsian\n\n\n44.57221082\n\n\nSmoker\n\n\n162\n\n\n2.432098765\n\n\n3.24691358\n\n\n2.57764\n\n\n3.40741\n\n\n\n\n2\n\n\n102\n\n\nHigh\n\n\nFemale\n\n\nAsian\n\n\n35.57289528\n\n\nSmoker\n\n\n162\n\n\n2.543209877\n\n\n3.00617284\n\n\n.\n\n\n.\n\n\n\n\n3\n\n\n103\n\n\nControl\n\n\nFemale\n\n\nAsian\n\n\n47.94524298\n\n\nSmoker\n\n\n144\n\n\n2.881944444\n\n\n3.118055556\n\n\n3.07639\n\n\n3.12500\n\n\n\n\n4\n\n\n104\n\n\nLow\n\n\nFemale\n\n\nAsian\n\n\n55.17864476\n\n\nSmoker\n\n\n138\n\n\n4.956521739\n\n\n5.217391304\n\n\n5.30435\n\n\n4.89130\n\n\n\n\n5\n\n\n105\n\n\nPlacebo\n\n\nFemale\n\n\nAfrican American\n\n\n43.79739904\n\n\nSmoker\n\n\n168\n\n\n1.773809524\n\n\n3.363095238\n\n\n1.45238\n\n\n2.89881\n\n\n\n\n6\n\n\n106\n\n\nMedium\n\n\nFemale\n\n\nAsian\n\n\n42.14921287\n\n\nSmoker\n\n\n168\n\n\n2.369047619\n\n\n3.910714286\n\n\n1.92262\n\n\n3.08333\n\n\n\n\n7\n\n\n107\n\n\nLow\n\n\nFemale\n\n\nAsian\n\n\n37.15811088\n\n\nSmoker\n\n\n156\n\n\n3.544871795\n\n\n3.897435897\n\n\n2.83974\n\n\n3.23718\n\n\n\n\n8\n\n\n108\n\n\nControl\n\n\nFemale\n\n\nAsian\n\n\n67.41957563\n\n\nSmoker\n\n\n162\n\n\n2.493827161\n\n\n3.481481482\n\n\n2.19136\n\n\n3.08025\n\n\n\n\n9\n\n\n109\n\n\nHigh\n\n\nFemale\n\n\nAfrican American\n\n\n40.59137577\n\n\nSmoker\n\n\n156\n\n\n1.92948718\n\n\n3.301282051\n\n\n1.62821\n\n\n2.71154\n\n\n\n\n10\n\n\n110\n\n\nPlacebo\n\n\nFemale\n\n\nAsian\n\n\n49.2128679\n\n\nSmoker\n\n\n162\n\n\n1.790123457\n\n\n3.277777778\n\n\n1.46296\n\n\n2.41975\n\n\n\n\n\n\n\n\n\n\n\u001417                                                         The SAS System                    Saturday, December 28, 2024 08:27:00 PM\n\n213        ods listing close;ods html5 (id=saspy_internal) file=_tomods1 options(bitmap_mode='inline') device=svg style=HTMLBlue;\n213      ! ods graphics on / outputfmt=png;\n214        \n215        * View Dataset;\n216        PROC PRINT DATA = Proj1.data (OBS = 10);\n217         RUN;\n218        \n219        \n220        ods html5 (id=saspy_internal) close;ods listing;\n221        \n\u001418                                                         The SAS System                    Saturday, December 28, 2024 08:27:00 PM\n\n222        \n\n\n\n\nOur data set is comprised of 130 observations, with 11 variables.\n\n\n\nThe variables in the provided data set are as follows:\n\nid: Patient ID\ntrtgroup: Treatment group (1 = placebo, 2 = no treatment control, 3 = low concentration, 4 = medium concentration, 5 = high concentration gel)\ngender: Gender (1 = male, 2 = female)\nrace: Race, (1 = Native American, 2 = African American, 3 = Not used, 4 = Asian, 5 = White)\nage: Age in years\nsmoker: Smoking status (0 = No, 1 = Yes)\nsites: Number of sites gum measurements were averaged from\nattachbase: Whole-mouth average attachment loss taken at base timepoint\nattach1year: Whole-mouth average attachment loss after 1 year\npdbase: Whole-mouth average pocket loss at base timepoint\npd1year: Whole-mouth average pocket loss after 1 year\n\n\n\n\n\n\n\nNote\n\n\n\nPocket depth and attachment loss are both measurements of how far the gums have pulled away from the teeth, hence smaller values are better"
  },
  {
    "objectID": "Project_1_Regression/Project_1_Regression_SAS/Project_1_Regression_SAS.html#Descriptives",
    "href": "Project_1_Regression/Project_1_Regression_SAS/Project_1_Regression_SAS.html#Descriptives",
    "title": "Project 1 - Regression (SAS)",
    "section": "Descriptive Statistics",
    "text": "Descriptive Statistics\nHere we will acquire the descriptive statistics of our data set and create Table 1.\n\n* Generate Table 1;\nTITLE \"Table 1\";\nPROC TABULATE DATA=Proj1.data;\n    CLASS trtgroup gender race smoker;\n    VAR age sites attachbase attach1year pdbase pd1year attachchange pdchange;\n    TABLE  \n        (gender race smoker)*(n) (age sites attachbase attach1year pdbase pd1year attachchange pdchange)*(n mean std min max), \n        trtgroup ALL;\nRUN;\n\nlstlog\n\n\n&lt;!DOCTYPE html&gt;\n\n\n\n\n\nSAS Output\n\n\n\n\n\n\n\nTable 1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\nTreatment Group\n\n\nAll\n\n\n\n\nPlacebo\n\n\nControl\n\n\nLow\n\n\nMedium\n\n\nHigh\n\n\n\n\n\n\nGender\n\n\n \n\n\n11\n\n\n10\n\n\n11\n\n\n10\n\n\n11\n\n\n53\n\n\n\n\nMale\n\n\nN\n\n\n\n\nFemale\n\n\nN\n\n\n15\n\n\n16\n\n\n15\n\n\n15\n\n\n15\n\n\n76\n\n\n\n\nRace\n\n\n \n\n\n.\n\n\n1\n\n\n1\n\n\n.\n\n\n2\n\n\n4\n\n\n\n\nNative American\n\n\nN\n\n\n\n\nAfrican American\n\n\nN\n\n\n2\n\n\n1\n\n\n5\n\n\n.\n\n\n1\n\n\n9\n\n\n\n\nWhite\n\n\nN\n\n\n1\n\n\n1\n\n\n.\n\n\n1\n\n\n.\n\n\n3\n\n\n\n\nAsian\n\n\nN\n\n\n23\n\n\n23\n\n\n20\n\n\n24\n\n\n23\n\n\n113\n\n\n\n\nSmoking Status\n\n\n \n\n\n15\n\n\n17\n\n\n18\n\n\n14\n\n\n17\n\n\n81\n\n\n\n\nNon-Smoker\n\n\nN\n\n\n\n\nSmoker\n\n\nN\n\n\n11\n\n\n9\n\n\n8\n\n\n11\n\n\n9\n\n\n48\n\n\n\n\nAge\n\n\nN\n\n\n25\n\n\n26\n\n\n26\n\n\n25\n\n\n26\n\n\n128\n\n\n\n\nMean\n\n\n47.11\n\n\n50.75\n\n\n51.92\n\n\n49.05\n\n\n50.82\n\n\n49.96\n\n\n\n\nStd\n\n\n8.61\n\n\n9.90\n\n\n10.78\n\n\n9.69\n\n\n11.20\n\n\n10.07\n\n\n\n\nMin\n\n\n30.40\n\n\n36.13\n\n\n36.95\n\n\n28.57\n\n\n34.12\n\n\n28.57\n\n\n\n\nMax\n\n\n67.14\n\n\n73.25\n\n\n71.90\n\n\n70.89\n\n\n74.53\n\n\n74.53\n\n\n\n\nSites\n\n\nN\n\n\n26\n\n\n26\n\n\n26\n\n\n25\n\n\n26\n\n\n129\n\n\n\n\nMean\n\n\n159.69\n\n\n154.38\n\n\n160.62\n\n\n157.12\n\n\n157.38\n\n\n157.84\n\n\n\n\nStd\n\n\n10.05\n\n\n10.94\n\n\n8.54\n\n\n13.54\n\n\n9.65\n\n\n10.71\n\n\n\n\nMin\n\n\n138.00\n\n\n126.00\n\n\n138.00\n\n\n114.00\n\n\n138.00\n\n\n114.00\n\n\n\n\nMax\n\n\n168.00\n\n\n168.00\n\n\n168.00\n\n\n168.00\n\n\n168.00\n\n\n168.00\n\n\n\n\nAttachment Loss Baseline\n\n\nN\n\n\n26\n\n\n26\n\n\n26\n\n\n25\n\n\n26\n\n\n129\n\n\n\n\nMean\n\n\n1.79\n\n\n2.46\n\n\n2.07\n\n\n2.19\n\n\n2.24\n\n\n2.15\n\n\n\n\nStd\n\n\n0.65\n\n\n0.69\n\n\n0.99\n\n\n0.66\n\n\n0.86\n\n\n0.80\n\n\n\n\nMin\n\n\n0.90\n\n\n1.22\n\n\n0.90\n\n\n1.02\n\n\n1.26\n\n\n0.90\n\n\n\n\nMax\n\n\n3.64\n\n\n4.39\n\n\n4.96\n\n\n4.01\n\n\n5.09\n\n\n5.09\n\n\n\n\nAttachment Loss One Year\n\n\nN\n\n\n23\n\n\n23\n\n\n21\n\n\n19\n\n\n16\n\n\n102\n\n\n\n\nMean\n\n\n1.74\n\n\n2.33\n\n\n2.08\n\n\n2.27\n\n\n2.15\n\n\n2.11\n\n\n\n\nStd\n\n\n0.54\n\n\n0.55\n\n\n1.06\n\n\n0.66\n\n\n0.92\n\n\n0.77\n\n\n\n\nMin\n\n\n0.96\n\n\n1.46\n\n\n0.87\n\n\n1.35\n\n\n1.22\n\n\n0.87\n\n\n\n\nMax\n\n\n3.10\n\n\n3.49\n\n\n5.30\n\n\n3.83\n\n\n4.04\n\n\n5.30\n\n\n\n\nPocket Depth Baseline\n\n\nN\n\n\n26\n\n\n26\n\n\n26\n\n\n25\n\n\n26\n\n\n129\n\n\n\n\nMean\n\n\n3.09\n\n\n3.28\n\n\n3.17\n\n\n3.04\n\n\n3.11\n\n\n3.14\n\n\n\n\nStd\n\n\n0.37\n\n\n0.47\n\n\n0.59\n\n\n0.41\n\n\n0.27\n\n\n0.44\n\n\n\n\nMin\n\n\n2.47\n\n\n2.65\n\n\n2.26\n\n\n2.42\n\n\n2.62\n\n\n2.26\n\n\n\n\nMax\n\n\n4.08\n\n\n4.77\n\n\n5.22\n\n\n3.91\n\n\n3.60\n\n\n5.22\n\n\n\n\nPocket Depth One Year\n\n\nN\n\n\n23\n\n\n23\n\n\n21\n\n\n19\n\n\n16\n\n\n102\n\n\n\n\nMean\n\n\n2.75\n\n\n2.95\n\n\n3.02\n\n\n2.83\n\n\n2.80\n\n\n2.87\n\n\n\n\nStd\n\n\n0.48\n\n\n0.46\n\n\n0.58\n\n\n0.48\n\n\n0.42\n\n\n0.49\n\n\n\n\nMin\n\n\n1.96\n\n\n2.24\n\n\n2.16\n\n\n2.05\n\n\n2.04\n\n\n1.96\n\n\n\n\nMax\n\n\n3.75\n\n\n4.07\n\n\n4.89\n\n\n3.78\n\n\n3.40\n\n\n4.89\n\n\n\n\nAttachment Loss Change\n\n\nN\n\n\n23\n\n\n23\n\n\n21\n\n\n19\n\n\n16\n\n\n102\n\n\n\n\nMean\n\n\n-0.09\n\n\n-0.22\n\n\n-0.02\n\n\n-0.00\n\n\n-0.16\n\n\n-0.10\n\n\n\n\nStd\n\n\n0.24\n\n\n0.28\n\n\n0.27\n\n\n0.24\n\n\n0.33\n\n\n0.28\n\n\n\n\nMin\n\n\n-0.60\n\n\n-0.90\n\n\n-0.71\n\n\n-0.45\n\n\n-1.05\n\n\n-1.05\n\n\n\n\nMax\n\n\n0.45\n\n\n0.19\n\n\n0.35\n\n\n0.34\n\n\n0.20\n\n\n0.45\n\n\n\n\nPocket Depth Change\n\n\nN\n\n\n23\n\n\n23\n\n\n21\n\n\n19\n\n\n16\n\n\n102\n\n\n\n\nMean\n\n\n-0.35\n\n\n-0.34\n\n\n-0.21\n\n\n-0.21\n\n\n-0.38\n\n\n-0.30\n\n\n\n\nStd\n\n\n0.28\n\n\n0.23\n\n\n0.28\n\n\n0.28\n\n\n0.24\n\n\n0.27\n\n\n\n\nMin\n\n\n-0.86\n\n\n-0.76\n\n\n-0.66\n\n\n-0.83\n\n\n-0.85\n\n\n-0.86\n\n\n\n\nMax\n\n\n0.16\n\n\n0.01\n\n\n0.46\n\n\n0.17\n\n\n0.05\n\n\n0.46\n\n\n\n\n\n\n\n\n\n\n\n\u001431                                                         The SAS System                    Saturday, December 28, 2024 08:27:00 PM\n\n328        ods listing close;ods html5 (id=saspy_internal) file=_tomods1 options(bitmap_mode='inline') device=svg style=HTMLBlue;\n328      ! ods graphics on / outputfmt=png;\n329        \n330        * Generate Table 1;\n331        TITLE \"Table 1\";\n332        PROC TABULATE DATA=Proj1.data;\n333            CLASS trtgroup gender race smoker;\n334            VAR age sites attachbase attach1year pdbase pd1year attachchange pdchange;\n335            TABLE\n336                (gender race smoker)*(n) (age sites attachbase attach1year pdbase pd1year attachchange pdchange)*(n mean std min\n336      ! max),\n337                trtgroup ALL;\n338        RUN;\n339        \n340        \n341        ods html5 (id=saspy_internal) close;ods listing;\n342        \n\u001432                                                         The SAS System                    Saturday, December 28, 2024 08:27:00 PM\n\n343"
  },
  {
    "objectID": "Project_1_Regression/Project_1_Regression_SAS/Project_1_Regression_SAS.html#Preliminary",
    "href": "Project_1_Regression/Project_1_Regression_SAS/Project_1_Regression_SAS.html#Preliminary",
    "title": "Project 1 - Regression (SAS)",
    "section": "Preliminary Evaluation of Assumptions",
    "text": "Preliminary Evaluation of Assumptions\nBefore we begin digging into the data, let’s take a closer look at the relationships between our variables.\nWe will first run a correlation matrix to assess the correlations between our IVs. Then we will explore any high correlations that which could affect our analysis.\nFinally we will make histograms of attachment loss and pocket depth change to gauge whether they will be normally distributed or if we need to run some transformations.\n\nCorrelation MatrixNormality of Dependent Variables\n\n\nFirst we will begin by making a correlation matrix to assess whether any of our IVs are related to each other (multicollinearity). This will inform which variables to incorporate into the final model.\n\n* Create and visualize the correlation matrix;\nTITLE \"Correlation Matrix\";\nODS GRAPHICS ON;\nPROC CORR DATA = Proj1.data PLOTS = MATRIX(HISTOGRAM);\n    VAR age sites attachbase attach1year pdbase pd1year attachchange pdchange;\n    RUN;\nODS GRAPHICS OFF;\n\nlstlog\n\n\n&lt;!DOCTYPE html&gt;\n\n\n\n\n\nSAS Output\n\n\n\n\n\n\n\nCorrelation Matrix\n\n\n\n\nThe CORR Procedure\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n8 Variables:\n\n\nage sites attachbase attach1year pdbase pd1year attachchange pdchange\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSimple Statistics\n\n\n\n\nVariable\n\n\nN\n\n\nMean\n\n\nStd Dev\n\n\nSum\n\n\nMinimum\n\n\nMaximum\n\n\nLabel\n\n\n\n\n\n\nage\n\n\n129\n\n\n49.94347\n\n\n10.03233\n\n\n6443\n\n\n28.57221\n\n\n74.53251\n\n\nAge\n\n\n\n\nsites\n\n\n130\n\n\n157.50769\n\n\n11.34125\n\n\n20476\n\n\n114.00000\n\n\n168.00000\n\n\nSites\n\n\n\n\nattachbase\n\n\n130\n\n\n2.14608\n\n\n0.79705\n\n\n278.98979\n\n\n0.89506\n\n\n5.08929\n\n\nAttachment Loss Baseline\n\n\n\n\nattach1year\n\n\n103\n\n\n2.10139\n\n\n0.77188\n\n\n216.44302\n\n\n0.86538\n\n\n5.30435\n\n\nAttachment Loss One Year\n\n\n\n\npdbase\n\n\n130\n\n\n3.13837\n\n\n0.43672\n\n\n407.98821\n\n\n2.26282\n\n\n5.21739\n\n\nPocket Depth Baseline\n\n\n\n\npd1year\n\n\n103\n\n\n2.87516\n\n\n0.48755\n\n\n296.14175\n\n\n1.96429\n\n\n4.89130\n\n\nPocket Depth One Year\n\n\n\n\nattachchange\n\n\n103\n\n\n-0.09945\n\n\n0.27603\n\n\n-10.24346\n\n\n-1.04762\n\n\n0.45238\n\n\nAttachment Loss Change\n\n\n\n\npdchange\n\n\n103\n\n\n-0.29435\n\n\n0.26761\n\n\n-30.31849\n\n\n-0.85802\n\n\n0.45513\n\n\nPocket Depth Change\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPearson Correlation CoefficientsProb &gt; |r| under H0: Rho=0Number of Observations\n\n\n\n\n \n\n\nage\n\n\nsites\n\n\nattachbase\n\n\nattach1year\n\n\npdbase\n\n\npd1year\n\n\nattachchange\n\n\npdchange\n\n\n\n\n\n\n\n\nage\n\n\nAge\n\n\n\n\n\n\n1.00000\n\n\n \n\n\n129\n\n\n\n\n\n\n-0.11266\n\n\n0.2037\n\n\n129\n\n\n\n\n\n\n0.12162\n\n\n0.1698\n\n\n129\n\n\n\n\n\n\n0.08605\n\n\n0.3898\n\n\n102\n\n\n\n\n\n\n-0.11097\n\n\n0.2106\n\n\n129\n\n\n\n\n\n\n-0.12526\n\n\n0.2097\n\n\n102\n\n\n\n\n\n\n-0.17458\n\n\n0.0793\n\n\n102\n\n\n\n\n\n\n-0.07422\n\n\n0.4585\n\n\n102\n\n\n\n\n\n\n\n\nsites\n\n\nSites\n\n\n\n\n\n\n-0.11266\n\n\n0.2037\n\n\n129\n\n\n\n\n\n\n1.00000\n\n\n \n\n\n130\n\n\n\n\n\n\n-0.39708\n\n\n&lt;.0001\n\n\n130\n\n\n\n\n\n\n-0.36885\n\n\n0.0001\n\n\n103\n\n\n\n\n\n\n-0.16634\n\n\n0.0586\n\n\n130\n\n\n\n\n\n\n-0.18852\n\n\n0.0565\n\n\n103\n\n\n\n\n\n\n0.16209\n\n\n0.1019\n\n\n103\n\n\n\n\n\n\n-0.04812\n\n\n0.6293\n\n\n103\n\n\n\n\n\n\n\n\nattachbase\n\n\nAttachment Loss Baseline\n\n\n\n\n\n\n0.12162\n\n\n0.1698\n\n\n129\n\n\n\n\n\n\n-0.39708\n\n\n&lt;.0001\n\n\n130\n\n\n\n\n\n\n1.00000\n\n\n \n\n\n130\n\n\n\n\n\n\n0.94556\n\n\n&lt;.0001\n\n\n103\n\n\n\n\n\n\n0.58869\n\n\n&lt;.0001\n\n\n130\n\n\n\n\n\n\n0.54983\n\n\n&lt;.0001\n\n\n103\n\n\n\n\n\n\n-0.41437\n\n\n&lt;.0001\n\n\n103\n\n\n\n\n\n\n-0.03676\n\n\n0.7124\n\n\n103\n\n\n\n\n\n\n\n\nattach1year\n\n\nAttachment Loss One Year\n\n\n\n\n\n\n0.08605\n\n\n0.3898\n\n\n102\n\n\n\n\n\n\n-0.36885\n\n\n0.0001\n\n\n103\n\n\n\n\n\n\n0.94556\n\n\n&lt;.0001\n\n\n103\n\n\n\n\n\n\n1.00000\n\n\n \n\n\n103\n\n\n\n\n\n\n0.60498\n\n\n&lt;.0001\n\n\n103\n\n\n\n\n\n\n0.66049\n\n\n&lt;.0001\n\n\n103\n\n\n\n\n\n\n-0.09561\n\n\n0.3367\n\n\n103\n\n\n\n\n\n\n0.15128\n\n\n0.1272\n\n\n103\n\n\n\n\n\n\n\n\npdbase\n\n\nPocket Depth Baseline\n\n\n\n\n\n\n-0.11097\n\n\n0.2106\n\n\n129\n\n\n\n\n\n\n-0.16634\n\n\n0.0586\n\n\n130\n\n\n\n\n\n\n0.58869\n\n\n&lt;.0001\n\n\n130\n\n\n\n\n\n\n0.60498\n\n\n&lt;.0001\n\n\n103\n\n\n\n\n\n\n1.00000\n\n\n \n\n\n130\n\n\n\n\n\n\n0.84327\n\n\n&lt;.0001\n\n\n103\n\n\n\n\n\n\n-0.13472\n\n\n0.1749\n\n\n103\n\n\n\n\n\n\n-0.20267\n\n\n0.0401\n\n\n103\n\n\n\n\n\n\n\n\npd1year\n\n\nPocket Depth One Year\n\n\n\n\n\n\n-0.12526\n\n\n0.2097\n\n\n102\n\n\n\n\n\n\n-0.18852\n\n\n0.0565\n\n\n103\n\n\n\n\n\n\n0.54983\n\n\n&lt;.0001\n\n\n103\n\n\n\n\n\n\n0.66049\n\n\n&lt;.0001\n\n\n103\n\n\n\n\n\n\n0.84327\n\n\n&lt;.0001\n\n\n103\n\n\n\n\n\n\n1.00000\n\n\n \n\n\n103\n\n\n\n\n\n\n0.16531\n\n\n0.0952\n\n\n103\n\n\n\n\n\n\n0.35543\n\n\n0.0002\n\n\n103\n\n\n\n\n\n\n\n\nattachchange\n\n\nAttachment Loss Change\n\n\n\n\n\n\n-0.17458\n\n\n0.0793\n\n\n102\n\n\n\n\n\n\n0.16209\n\n\n0.1019\n\n\n103\n\n\n\n\n\n\n-0.41437\n\n\n&lt;.0001\n\n\n103\n\n\n\n\n\n\n-0.09561\n\n\n0.3367\n\n\n103\n\n\n\n\n\n\n-0.13472\n\n\n0.1749\n\n\n103\n\n\n\n\n\n\n0.16531\n\n\n0.0952\n\n\n103\n\n\n\n\n\n\n1.00000\n\n\n \n\n\n103\n\n\n\n\n\n\n0.53546\n\n\n&lt;.0001\n\n\n103\n\n\n\n\n\n\n\n\npdchange\n\n\nPocket Depth Change\n\n\n\n\n\n\n-0.07422\n\n\n0.4585\n\n\n102\n\n\n\n\n\n\n-0.04812\n\n\n0.6293\n\n\n103\n\n\n\n\n\n\n-0.03676\n\n\n0.7124\n\n\n103\n\n\n\n\n\n\n0.15128\n\n\n0.1272\n\n\n103\n\n\n\n\n\n\n-0.20267\n\n\n0.0401\n\n\n103\n\n\n\n\n\n\n0.35543\n\n\n0.0002\n\n\n103\n\n\n\n\n\n\n0.53546\n\n\n&lt;.0001\n\n\n103\n\n\n\n\n\n\n1.00000\n\n\n \n\n\n103\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001433                                                         The SAS System                    Saturday, December 28, 2024 08:27:00 PM\n\n346        ods listing close;ods html5 (id=saspy_internal) file=_tomods1 options(bitmap_mode='inline') device=svg style=HTMLBlue;\n346      ! ods graphics on / outputfmt=png;\n347        \n348        * Create and visualize the correlation matrix;\n349        TITLE \"Correlation Matrix\";\n350        ODS GRAPHICS ON;\n351        PROC CORR DATA = Proj1.data PLOTS = MATRIX(HISTOGRAM);\n352         VAR age sites attachbase attach1year pdbase pd1year attachchange pdchange;\n353         RUN;\n354        ODS GRAPHICS OFF;\n355        \n356        \n357        ods html5 (id=saspy_internal) close;ods listing;\n358        \n\u001434                                                         The SAS System                    Saturday, December 28, 2024 08:27:00 PM\n\n359        \n\n\n\n\nVisually, we can see a cluster of high positive correlations between all of attachment loss and pocket depth at baseline and 1 year. This makes sense, as all measurements were taken from the same sites in the gums. This will be explored later (See Exploratory Data Analysis - Dependent Variables)\nBack to top of tabset\n\n\nHere we will simply plot the histograms of attachment loss and pocket depth changes scores, to assess if they appear normally distributed or if we will have to perform a transformation of some kind.\n\n* Assess Normality of Dependent Variables (attachchange, pdchange);\n* Note: Here we would use the Kolmogorov-Smirnov test, since our sample size is &gt; 50\n    if the sample size was &lt; 50 you would use Shapiro Wilks test;\nTITLE \"Normality Assesment\";\nPROC UNIVARIATE DATA = Proj1.data;\n    VAR attachchange pdchange;\n    HISTOGRAM attachchange pdchange / NORMAL;\n    QQPLOT attachchange pdchange / NORMAL (MU=EST SIGMA = EST);\n    RUN;\n\nlstlog\n\n\n&lt;!DOCTYPE html&gt;\n\n\n\n\n\nSAS Output\n\n\n\n\n\n\n\nNormality Assesment\n\n\n\n\nThe UNIVARIATE Procedure\n\n\nVariable: attachchange (Attachment Loss Change)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMoments\n\n\n\n\n\n\nN\n\n\n103\n\n\nSum Weights\n\n\n103\n\n\n\n\nMean\n\n\n-0.099451\n\n\nSum Observations\n\n\n-10.243457\n\n\n\n\nStd Deviation\n\n\n0.27603039\n\n\nVariance\n\n\n0.07619277\n\n\n\n\nSkewness\n\n\n-0.7927997\n\n\nKurtosis\n\n\n1.03785174\n\n\n\n\nUncorrected SS\n\n\n8.79038543\n\n\nCorrected SS\n\n\n7.77166295\n\n\n\n\nCoeff Variation\n\n\n-277.55404\n\n\nStd Error Mean\n\n\n0.02719808\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBasic Statistical Measures\n\n\n\n\nLocation\n\n\nVariability\n\n\n\n\n\n\nMean\n\n\n-0.09945\n\n\nStd Deviation\n\n\n0.27603\n\n\n\n\nMedian\n\n\n-0.06790\n\n\nVariance\n\n\n0.07619\n\n\n\n\nMode\n\n\n-0.18519\n\n\nRange\n\n\n1.50000\n\n\n\n\n \n\n\n \n\n\nInterquartile Range\n\n\n0.36684\n\n\n\n\n\n\nNote: The mode displayed is the smallest of 3 modes with a count of 2.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTests for Location: Mu0=0\n\n\n\n\nTest\n\n\nStatistic\n\n\np Value\n\n\n\n\n\n\nStudent's t\n\n\nt\n\n\n-3.65655\n\n\nPr &gt; |t|\n\n\n0.0004\n\n\n\n\nSign\n\n\nM\n\n\n-12.5\n\n\nPr &gt;= |M|\n\n\n0.0176\n\n\n\n\nSigned Rank\n\n\nS\n\n\n-926.5\n\n\nPr &gt;= |S|\n\n\n0.0020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuantiles (Definition 5)\n\n\n\n\nLevel\n\n\nQuantile\n\n\n\n\n\n\n100% Max\n\n\n0.4523810\n\n\n\n\n99%\n\n\n0.3478261\n\n\n\n\n95%\n\n\n0.3148148\n\n\n\n\n90%\n\n\n0.2222222\n\n\n\n\n75% Q3\n\n\n0.0952381\n\n\n\n\n50% Median\n\n\n-0.0679012\n\n\n\n\n25% Q1\n\n\n-0.2716049\n\n\n\n\n10%\n\n\n-0.4487179\n\n\n\n\n5%\n\n\n-0.5434783\n\n\n\n\n1%\n\n\n-0.9008889\n\n\n\n\n0% Min\n\n\n-1.0476190\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExtreme Observations\n\n\n\n\nLowest\n\n\nHighest\n\n\n\n\nValue\n\n\nObs\n\n\nValue\n\n\nObs\n\n\n\n\n\n\n-1.047619\n\n\n67\n\n\n0.320513\n\n\n16\n\n\n\n\n-0.900889\n\n\n44\n\n\n0.327160\n\n\n64\n\n\n\n\n-0.810000\n\n\n46\n\n\n0.339286\n\n\n34\n\n\n\n\n-0.705128\n\n\n7\n\n\n0.347826\n\n\n4\n\n\n\n\n-0.598765\n\n\n43\n\n\n0.452381\n\n\n13\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMissing Values\n\n\n\n\nMissingValue\n\n\nCount\n\n\nPercent Of\n\n\n\n\nAll Obs\n\n\nMissing Obs\n\n\n\n\n\n\n.\n\n\n27\n\n\n20.77\n\n\n100.00\n\n\n\n\n\n\n\n\nNormality Assesment\n\n\n\n\nThe UNIVARIATE Procedure\n\n\n\n\n\n\n\n\n\n\n\nNormality Assesment\n\n\n\n\nThe UNIVARIATE Procedure\n\n\nFitted Normal Distribution for attachchange (Attachment Loss Change)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nParameters for Normal Distribution\n\n\n\n\nParameter\n\n\nSymbol\n\n\nEstimate\n\n\n\n\n\n\nMean\n\n\nMu\n\n\n-0.09945\n\n\n\n\nStd Dev\n\n\nSigma\n\n\n0.27603\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGoodness-of-Fit Tests for Normal Distribution\n\n\n\n\nTest\n\n\nStatistic\n\n\np Value\n\n\n\n\n\n\nKolmogorov-Smirnov\n\n\nD\n\n\n0.08091601\n\n\nPr &gt; D\n\n\n0.095\n\n\n\n\nCramer-von Mises\n\n\nW-Sq\n\n\n0.12605262\n\n\nPr &gt; W-Sq\n\n\n0.049\n\n\n\n\nAnderson-Darling\n\n\nA-Sq\n\n\n0.79367755\n\n\nPr &gt; A-Sq\n\n\n0.040\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuantiles for Normal Distribution\n\n\n\n\nPercent\n\n\nQuantile\n\n\n\n\nObserved\n\n\nEstimated\n\n\n\n\n\n\n1.0\n\n\n-0.90089\n\n\n-0.74159\n\n\n\n\n5.0\n\n\n-0.54348\n\n\n-0.55348\n\n\n\n\n10.0\n\n\n-0.44872\n\n\n-0.45320\n\n\n\n\n25.0\n\n\n-0.27160\n\n\n-0.28563\n\n\n\n\n50.0\n\n\n-0.06790\n\n\n-0.09945\n\n\n\n\n75.0\n\n\n0.09524\n\n\n0.08673\n\n\n\n\n90.0\n\n\n0.22222\n\n\n0.25430\n\n\n\n\n95.0\n\n\n0.31481\n\n\n0.35458\n\n\n\n\n99.0\n\n\n0.34783\n\n\n0.54269\n\n\n\n\n\n\n\n\n\n\nNormality Assesment\n\n\n\n\nThe UNIVARIATE Procedure\n\n\n\n\n\n\n\n\n\n\n\n\n\nNormality Assesment\n\n\n\n\nThe UNIVARIATE Procedure\n\n\nVariable: pdchange (Pocket Depth Change)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMoments\n\n\n\n\n\n\nN\n\n\n103\n\n\nSum Weights\n\n\n103\n\n\n\n\nMean\n\n\n-0.2943543\n\n\nSum Observations\n\n\n-30.318488\n\n\n\n\nStd Deviation\n\n\n0.2676105\n\n\nVariance\n\n\n0.07161538\n\n\n\n\nSkewness\n\n\n0.07023632\n\n\nKurtosis\n\n\n-0.2191411\n\n\n\n\nUncorrected SS\n\n\n16.2291445\n\n\nCorrected SS\n\n\n7.30476874\n\n\n\n\nCoeff Variation\n\n\n-90.914434\n\n\nStd Error Mean\n\n\n0.02636845\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBasic Statistical Measures\n\n\n\n\nLocation\n\n\nVariability\n\n\n\n\n\n\nMean\n\n\n-0.29435\n\n\nStd Deviation\n\n\n0.26761\n\n\n\n\nMedian\n\n\n-0.28395\n\n\nVariance\n\n\n0.07162\n\n\n\n\nMode\n\n\n-0.50000\n\n\nRange\n\n\n1.31315\n\n\n\n\n \n\n\n \n\n\nInterquartile Range\n\n\n0.38448\n\n\n\n\n\n\nNote: The mode displayed is the smallest of 3 modes with a count of 2.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTests for Location: Mu0=0\n\n\n\n\nTest\n\n\nStatistic\n\n\np Value\n\n\n\n\n\n\nStudent's t\n\n\nt\n\n\n-11.1631\n\n\nPr &gt; |t|\n\n\n&lt;.0001\n\n\n\n\nSign\n\n\nM\n\n\n-35.5\n\n\nPr &gt;= |M|\n\n\n&lt;.0001\n\n\n\n\nSigned Rank\n\n\nS\n\n\n-2352\n\n\nPr &gt;= |S|\n\n\n&lt;.0001\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuantiles (Definition 5)\n\n\n\n\nLevel\n\n\nQuantile\n\n\n\n\n\n\n100% Max\n\n\n0.4551282\n\n\n\n\n99%\n\n\n0.2976190\n\n\n\n\n95%\n\n\n0.1604938\n\n\n\n\n90%\n\n\n0.0476190\n\n\n\n\n75% Q3\n\n\n-0.0952381\n\n\n\n\n50% Median\n\n\n-0.2839506\n\n\n\n\n25% Q1\n\n\n-0.4797178\n\n\n\n\n10%\n\n\n-0.6845238\n\n\n\n\n5%\n\n\n-0.7283951\n\n\n\n\n1%\n\n\n-0.8452381\n\n\n\n\n0% Min\n\n\n-0.8580247\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExtreme Observations\n\n\n\n\nLowest\n\n\nHighest\n\n\n\n\nValue\n\n\nObs\n\n\nValue\n\n\nObs\n\n\n\n\n\n\n-0.858025\n\n\n10\n\n\n0.160494\n\n\n89\n\n\n\n\n-0.845238\n\n\n18\n\n\n0.160714\n\n\n130\n\n\n\n\n-0.827381\n\n\n6\n\n\n0.174638\n\n\n21\n\n\n\n\n-0.759259\n\n\n66\n\n\n0.297619\n\n\n37\n\n\n\n\n-0.749094\n\n\n54\n\n\n0.455128\n\n\n99\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMissing Values\n\n\n\n\nMissingValue\n\n\nCount\n\n\nPercent Of\n\n\n\n\nAll Obs\n\n\nMissing Obs\n\n\n\n\n\n\n.\n\n\n27\n\n\n20.77\n\n\n100.00\n\n\n\n\n\n\n\n\nNormality Assesment\n\n\n\n\nThe UNIVARIATE Procedure\n\n\n\n\n\n\n\n\n\n\n\nNormality Assesment\n\n\n\n\nThe UNIVARIATE Procedure\n\n\nFitted Normal Distribution for pdchange (Pocket Depth Change)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nParameters for Normal Distribution\n\n\n\n\nParameter\n\n\nSymbol\n\n\nEstimate\n\n\n\n\n\n\nMean\n\n\nMu\n\n\n-0.29435\n\n\n\n\nStd Dev\n\n\nSigma\n\n\n0.267611\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGoodness-of-Fit Tests for Normal Distribution\n\n\n\n\nTest\n\n\nStatistic\n\n\np Value\n\n\n\n\n\n\nKolmogorov-Smirnov\n\n\nD\n\n\n0.04137085\n\n\nPr &gt; D\n\n\n&gt;0.150\n\n\n\n\nCramer-von Mises\n\n\nW-Sq\n\n\n0.02560441\n\n\nPr &gt; W-Sq\n\n\n&gt;0.250\n\n\n\n\nAnderson-Darling\n\n\nA-Sq\n\n\n0.19521495\n\n\nPr &gt; A-Sq\n\n\n&gt;0.250\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuantiles for Normal Distribution\n\n\n\n\nPercent\n\n\nQuantile\n\n\n\n\nObserved\n\n\nEstimated\n\n\n\n\n\n\n1.0\n\n\n-0.84524\n\n\n-0.91691\n\n\n\n\n5.0\n\n\n-0.72840\n\n\n-0.73453\n\n\n\n\n10.0\n\n\n-0.68452\n\n\n-0.63731\n\n\n\n\n25.0\n\n\n-0.47972\n\n\n-0.47485\n\n\n\n\n50.0\n\n\n-0.28395\n\n\n-0.29435\n\n\n\n\n75.0\n\n\n-0.09524\n\n\n-0.11385\n\n\n\n\n90.0\n\n\n0.04762\n\n\n0.04860\n\n\n\n\n95.0\n\n\n0.16049\n\n\n0.14583\n\n\n\n\n99.0\n\n\n0.29762\n\n\n0.32820\n\n\n\n\n\n\n\n\n\n\nNormality Assesment\n\n\n\n\nThe UNIVARIATE Procedure\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001435                                                         The SAS System                    Saturday, December 28, 2024 08:27:00 PM\n\n362        ods listing close;ods html5 (id=saspy_internal) file=_tomods1 options(bitmap_mode='inline') device=svg style=HTMLBlue;\n362      ! ods graphics on / outputfmt=png;\n363        \n364        * Assess Normality of Dependent Variables (attachchange, pdchange);\n365        * Note: Here we would use the Kolmogorov-Smirnov test, since our sample size is &gt; 50\n366         if the sample size was &lt; 50 you would use Shapiro Wilks test;\n367        TITLE \"Normality Assesment\";\n368        PROC UNIVARIATE DATA = Proj1.data;\n369         VAR attachchange pdchange;\n370         HISTOGRAM attachchange pdchange / NORMAL;\n371         QQPLOT attachchange pdchange / NORMAL (MU=EST SIGMA = EST);\n372         RUN;\n373        \n374        \n375        ods html5 (id=saspy_internal) close;ods listing;\n376        \n\u001436                                                         The SAS System                    Saturday, December 28, 2024 08:27:00 PM\n\n377        \n\n\n\n\n\nConclusion\nBoth attachment loss and pocket depth change appear to be normally distributed and will not need to be transformed. Attachment loss change is slightly left-tailed but this could be due to outliers, or may just not impact the analysis.\nBack to top of tabset"
  },
  {
    "objectID": "Project_1_Regression/Project_1_Regression_SAS/Project_1_Regression_SAS.html#Exploratory",
    "href": "Project_1_Regression/Project_1_Regression_SAS/Project_1_Regression_SAS.html#Exploratory",
    "title": "Project 1 - Regression (SAS)",
    "section": "Exploratory Data Analysis",
    "text": "Exploratory Data Analysis\nHere I will plot the data and perform a number of simple linear regressions to examine relationships between variables in order to determine which covariates to include in the model.\n\nPrimary Explanatory VariableCovariatesSummary\n\n\n\n5 Treatment Groups\n\n\nLet’s examine if there appears to be a difference in the average attachment loss and pocket depth change according to treatment level\n\n* Boxplot of attachchange vs treatment condition;\nTITLE \"Primary Attachment Loss Change by Treatment Condition\";\nPROC SGPLOT DATA = Proj1.data;\n    VBOX attachchange / CATEGORY = trtgroup;\n    RUN;\n\n* Boxplot of pdchange vs treatment condition;\nTITLE \"Primary Pocket Depth Change by Treatment Condition\";\nPROC SGPLOT DATA = Proj1.data;\n    VBOX pdchange / CATEGORY = trtgroup;\n    RUN;\n\nlstlog\n\n\n&lt;!DOCTYPE html&gt;\n\n\n\n\n\nSAS Output\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001437                                                         The SAS System                    Saturday, December 28, 2024 08:27:00 PM\n\n380        ods listing close;ods html5 (id=saspy_internal) file=_tomods1 options(bitmap_mode='inline') device=svg style=HTMLBlue;\n380      ! ods graphics on / outputfmt=png;\n381        \n382        * Boxplot of attachchange vs treatment condition;\n383        TITLE \"Primary Attachment Loss Change by Treatment Condition\";\n384        PROC SGPLOT DATA = Proj1.data;\n385         VBOX attachchange / CATEGORY = trtgroup;\n386         RUN;\n387        \n388        * Boxplot of pdchange vs treatment condition;\n389        TITLE \"Primary Pocket Depth Change by Treatment Condition\";\n390        PROC SGPLOT DATA = Proj1.data;\n391         VBOX pdchange / CATEGORY = trtgroup;\n392         RUN;\n393        \n394        \n395        ods html5 (id=saspy_internal) close;ods listing;\n396        \n\u001438                                                         The SAS System                    Saturday, December 28, 2024 08:27:00 PM\n\n397        \n\n\n\n\n\nAttachment Loss Change\nVisually, there does not appear to be a difference between the treatment group levels (low, medium, high concentration gel) compared to each other. Additionally, there does not seem to be a difference between the treatment groups and the placebo for attachment loss change.\n\n\nPocket Depth Change\nInterestingly, the high concentration condition appears to have had a negative effect. Additionally, there seems to be a difference in the treatment groups compared to the no treatment control, but NOT when compared to the placebo. The exception is the low concentration condition compared to the placebo when looking at pocket depth change. Further analysis will reveal whether these differences are statistically significant or not.\nBack to top of tabset\n\n\n\n\n\n\nNow we will exlore the relationships between our potential covariates and attachment loss and pocket depth change\n\nGenderAgeSitesRaceSmoking Status\n\n\nIt is conceivable that there are sex differences in regards to gum health. Let’s examine if there is a difference in attachment loss or pocket depth change based on gender\n\n* Create boxplot of gender vs attachmnent loss change;\nTITLE \"Boxplot of Gender vs Attachment Loss Change\";\nPROC SGPLOT DATA = Proj1.data;\n    VBOX attachchange / CATEGORY = gender;\n    RUN;\n    \n* Create boxplot of gender vs pocket depth change;\nTITLE \"Boxplot of Gender vs Pocket Depth Change\";\nPROC SGPLOT DATA = Proj1.data;\n    VBOX pdchange / CATEGORY = gender;\n    RUN;\n    \n* Note: Males may have more pocket depth change than females\n    Run a t-test to assess;\nPROC TTEST DATA = Proj1.data;\n    CLASS gender;\n    VAR pdchange;\n    RUN;\n\nlstlog\n\n\n&lt;!DOCTYPE html&gt;\n\n\n\n\n\nSAS Output\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBoxplot of Gender vs Pocket Depth Change\n\n\n\n\nThe TTEST Procedure\n\n\n \n\n\nVariable: pdchange (Pocket Depth Change)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ngender\n\n\nMethod\n\n\nN\n\n\nMean\n\n\nStd Dev\n\n\nStd Err\n\n\nMinimum\n\n\nMaximum\n\n\n\n\n\n\nMale\n\n\n \n\n\n36\n\n\n-0.2151\n\n\n0.2362\n\n\n0.0394\n\n\n-0.6964\n\n\n0.4551\n\n\n\n\nFemale\n\n\n \n\n\n67\n\n\n-0.3369\n\n\n0.2754\n\n\n0.0336\n\n\n-0.8580\n\n\n0.2976\n\n\n\n\nDiff (1-2)\n\n\nPooled\n\n\n \n\n\n0.1219\n\n\n0.2625\n\n\n0.0542\n\n\n \n\n\n \n\n\n\n\nDiff (1-2)\n\n\nSatterthwaite\n\n\n \n\n\n0.1219\n\n\n \n\n\n0.0518\n\n\n \n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ngender\n\n\nMethod\n\n\nMean\n\n\n95% CL Mean\n\n\nStd Dev\n\n\n95% CL Std Dev\n\n\n\n\n\n\nMale\n\n\n \n\n\n-0.2151\n\n\n-0.2950\n\n\n-0.1352\n\n\n0.2362\n\n\n0.1916\n\n\n0.3081\n\n\n\n\nFemale\n\n\n \n\n\n-0.3369\n\n\n-0.4041\n\n\n-0.2698\n\n\n0.2754\n\n\n0.2353\n\n\n0.3319\n\n\n\n\nDiff (1-2)\n\n\nPooled\n\n\n0.1219\n\n\n0.0143\n\n\n0.2295\n\n\n0.2625\n\n\n0.2307\n\n\n0.3044\n\n\n\n\nDiff (1-2)\n\n\nSatterthwaite\n\n\n0.1219\n\n\n0.0188\n\n\n0.2249\n\n\n \n\n\n \n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMethod\n\n\nVariances\n\n\nDF\n\n\nt Value\n\n\nPr &gt; |t|\n\n\n\n\n\n\nPooled\n\n\nEqual\n\n\n101\n\n\n2.25\n\n\n0.0268\n\n\n\n\nSatterthwaite\n\n\nUnequal\n\n\n81.683\n\n\n2.35\n\n\n0.0210\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEquality of Variances\n\n\n\n\nMethod\n\n\nNum DF\n\n\nDen DF\n\n\nF Value\n\n\nPr &gt; F\n\n\n\n\n\n\nFolded F\n\n\n66\n\n\n35\n\n\n1.36\n\n\n0.3250\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001439                                                         The SAS System                    Saturday, December 28, 2024 08:27:00 PM\n\n400        ods listing close;ods html5 (id=saspy_internal) file=_tomods1 options(bitmap_mode='inline') device=svg style=HTMLBlue;\n400      ! ods graphics on / outputfmt=png;\n401        \n402        * Create boxplot of gender vs attachmnent loss change;\n403        TITLE \"Boxplot of Gender vs Attachment Loss Change\";\n404        PROC SGPLOT DATA = Proj1.data;\n405         VBOX attachchange / CATEGORY = gender;\n406         RUN;\n407         \n408        * Create boxplot of gender vs pocket depth change;\n409        TITLE \"Boxplot of Gender vs Pocket Depth Change\";\n410        PROC SGPLOT DATA = Proj1.data;\n411         VBOX pdchange / CATEGORY = gender;\n412         RUN;\n413         \n414        * Note: Males may have more pocket depth change than females\n415         Run a t-test to assess;\n416        PROC TTEST DATA = Proj1.data;\n417         CLASS gender;\n418         VAR pdchange;\n419         RUN;\n420        \n421        \n422        ods html5 (id=saspy_internal) close;ods listing;\n423        \n\u001440                                                         The SAS System                    Saturday, December 28, 2024 08:27:00 PM\n\n424        \n\n\n\n\nThere is a significant difference in attachment loss change score based on gender (t = 2.0502, p = 0.04299)\nThere is also a statistically significant difference in pocket depth change based on gender (t = 2.2626, p = 0.02641).\nTherefore I will include gender as a covariate in the analysis!\nBack to top of tabset\n\n\n\n* Create scatterplot of age vs attachment loss change;\nTITLE \"Age vs Attachment Loss Change\";\nPROC SGPLOT DATA = Proj1.data;\n    SCATTER X = age y = attachchange / GROUP = trtgroup;\n    RUN;\n    \n* Create scatterplot of sites vs pocket depth change;\nTITLE \"Age vs Pocket Depth Change\";\nPROC SGPLOT DATA = Proj1.data;\n    SCATTER X = age y = pdchange / GROUP = trtgroup;\n    RUN;\n\nlstlog\n\n\n&lt;!DOCTYPE html&gt;\n\n\n\n\n\nSAS Output\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001441                                                         The SAS System                    Saturday, December 28, 2024 08:27:00 PM\n\n427        ods listing close;ods html5 (id=saspy_internal) file=_tomods1 options(bitmap_mode='inline') device=svg style=HTMLBlue;\n427      ! ods graphics on / outputfmt=png;\n428        \n429        * Create scatterplot of age vs attachment loss change;\n430        TITLE \"Age vs Attachment Loss Change\";\n431        PROC SGPLOT DATA = Proj1.data;\n432         SCATTER X = age y = attachchange / GROUP = trtgroup;\n433         RUN;\n434         \n435        * Create scatterplot of sites vs pocket depth change;\n436        TITLE \"Age vs Pocket Depth Change\";\n437        PROC SGPLOT DATA = Proj1.data;\n438         SCATTER X = age y = pdchange / GROUP = trtgroup;\n439         RUN;\n440        \n441        \n442        ods html5 (id=saspy_internal) close;ods listing;\n443        \n\u001442                                                         The SAS System                    Saturday, December 28, 2024 08:27:00 PM\n\n444        \n\n\n\n\nThere does not appear to be any kind of linear relationship between age and attachment loss change or pocket depth change. I will run a model where I include age, but it will likely be removed in the final model.\nBack to top of tabset\n\n\nIs there any relationship between the number of sites measured from and attachment loss change or pocket depth change (e.g. a lower number of sites could lead to less accurate readings)? If so this could be something to include in our data analysis\n\n* Create scatterplot of sites vs attachment loss change;\nTITLE \"Sites vs Attachment Loss Change\";\nPROC SGPLOT DATA = Proj1.data;\n    SCATTER X = sites y = attachchange / GROUP = trtgroup;\n    RUN;\n    \n* Create scatterplot of sites vs pocket depth change;\nTITLE \"Sites vs Pocket Depth Change\";\nPROC SGPLOT DATA = Proj1.data;\n    SCATTER X = sites y = pdchange / GROUP = trtgroup;\n    RUN;\n\nlstlog\n\n\n&lt;!DOCTYPE html&gt;\n\n\n\n\n\nSAS Output\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001443                                                         The SAS System                    Saturday, December 28, 2024 08:27:00 PM\n\n447        ods listing close;ods html5 (id=saspy_internal) file=_tomods1 options(bitmap_mode='inline') device=svg style=HTMLBlue;\n447      ! ods graphics on / outputfmt=png;\n448        \n449        * Create scatterplot of sites vs attachment loss change;\n450        TITLE \"Sites vs Attachment Loss Change\";\n451        PROC SGPLOT DATA = Proj1.data;\n452         SCATTER X = sites y = attachchange / GROUP = trtgroup;\n453         RUN;\n454         \n455        * Create scatterplot of sites vs pocket depth change;\n456        TITLE \"Sites vs Pocket Depth Change\";\n457        PROC SGPLOT DATA = Proj1.data;\n458         SCATTER X = sites y = pdchange / GROUP = trtgroup;\n459         RUN;\n460        \n461        \n462        ods html5 (id=saspy_internal) close;ods listing;\n463        \n\u001444                                                         The SAS System                    Saturday, December 28, 2024 08:27:00 PM\n\n464        \n\n\n\n\nThere does not seem to be a dramatic difference in attachment loss or pocket depth change based on the number of sites measured from. It’s a bit hard to tell with those two low site subjects, but the points are similar enough to the rest of the dataset that I do not think we have to worry about site number in our analysis.\nBack to top of tabset\n\n\n\n* Create boxplot of race vs attachment loss change;\nTITLE \"Race vs Attachment Loss Change\";\nPROC SGPLOT DATA = Proj1.data;\n    VBOX attachchange / CATEGORY = race;\n    RUN;\n    \n* Create boxplot of race vs pocket depth change;\nTITLE \"Race vs Pocket Depth Change\";\nPROC SGPLOT DATA = Proj1.data;\n    VBOX pdchange / CATEGORY = race;\n    RUN;\n\nlstlog\n\n\n&lt;!DOCTYPE html&gt;\n\n\n\n\n\nSAS Output\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001445                                                         The SAS System                    Saturday, December 28, 2024 08:27:00 PM\n\n467        ods listing close;ods html5 (id=saspy_internal) file=_tomods1 options(bitmap_mode='inline') device=svg style=HTMLBlue;\n467      ! ods graphics on / outputfmt=png;\n468        \n469        * Create boxplot of race vs attachment loss change;\n470        TITLE \"Race vs Attachment Loss Change\";\n471        PROC SGPLOT DATA = Proj1.data;\n472         VBOX attachchange / CATEGORY = race;\n473         RUN;\n474         \n475        * Create boxplot of race vs pocket depth change;\n476        TITLE \"Race vs Pocket Depth Change\";\n477        PROC SGPLOT DATA = Proj1.data;\n478         VBOX pdchange / CATEGORY = race;\n479         RUN;\n480        \n481        \n482        ods html5 (id=saspy_internal) close;ods listing;\n483        \n\u001446                                                         The SAS System                    Saturday, December 28, 2024 08:27:00 PM\n\n484        \n\n\n\n\nThere does not seem to be much of a difference in attachment loss or pocket depth based on race. MAYBE African Americans (2) have more pocket depth loss compared to Asians(4), but the sample size was very small for both races (88.1% of the sample identified as White)\nBack to top of tabset\n\n\nWhether the participant is a smoker or not likely has a dramatic effect on gum health. Let’s assess\n\nTITLE \"Smoking Status vs Attachment Loss Change\";\nPROC SGPLOT DATA = Proj1.data;\n    VBOX attachchange / CATEGORY = smoker;\n    RUN;\n\n* Create boxplot of smoking status vs pocket depth change;\nTITLE \"Smoking Status vs Pocket Depth Change\";\nPROC SGPLOT DATA = Proj1.data;\n    VBOX pdchange / CATEGORY = smoker;\n    RUN;\n\nlstlog\n\n\n&lt;!DOCTYPE html&gt;\n\n\n\n\n\nSAS Output\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001447                                                         The SAS System                    Saturday, December 28, 2024 08:27:00 PM\n\n487        ods listing close;ods html5 (id=saspy_internal) file=_tomods1 options(bitmap_mode='inline') device=svg style=HTMLBlue;\n487      ! ods graphics on / outputfmt=png;\n488        \n489        TITLE \"Smoking Status vs Attachment Loss Change\";\n490        PROC SGPLOT DATA = Proj1.data;\n491         VBOX attachchange / CATEGORY = smoker;\n492         RUN;\n493        \n494        * Create boxplot of smoking status vs pocket depth change;\n495        TITLE \"Smoking Status vs Pocket Depth Change\";\n496        PROC SGPLOT DATA = Proj1.data;\n497         VBOX pdchange / CATEGORY = smoker;\n498         RUN;\n499        \n500        \n501        ods html5 (id=saspy_internal) close;ods listing;\n502        \n\u001448                                                         The SAS System                    Saturday, December 28, 2024 08:27:00 PM\n\n503        \n\n\n\n\nThere does not appear to be any difference in attachment loss or pocket depth change based on smoking status. I will test a model with smoking status included, but it will likely be dropped in the final model.\nBack to top of tabset\n\n\n\n\n\n\nGender\nGender is related to attachment loss and pocket depth change, and as such will be included in the final model.\n\n\nSupressor / Counfound Variables\nA model will be tested with all covariates to assess for possible suppressor / confound variables. But the other potential covariates of race, age, sites, and smoking status were not related to attachment loss or pocket depth change by themselves.\nBack to top of tabset"
  },
  {
    "objectID": "Project_1_Regression/Project_1_Regression_SAS/Project_1_Regression_SAS.html#Model_1",
    "href": "Project_1_Regression/Project_1_Regression_SAS/Project_1_Regression_SAS.html#Model_1",
    "title": "Project 1 - Regression (SAS)",
    "section": "Running the Model",
    "text": "Running the Model\nFor ease of interpretation I will be performing this analysis as two separate linear regressions, one for each outcome variable of either attachment loss or pocket depth change. I will begin with a simple linear regression only including the PEV and either attachment loss change or pocket depth change.\n\nAttachment Loss ChangePocket Depth ChangePost-Hoc AnalysisEvaluating Assumptions\n\n\nWe start by constructing a model with attachment loss change as the dependent variable, and treatment group as the independent variable.\nFirst we need to create the dummy variables needed for these analyses.\n\n* Create Dummy Variables;\nTITLE \"Create Dummy Variables\";\nDATA Proj1.data;\n    SET Proj1.data;\n    IF trtgroup ne . THEN DO;\n        placebo = (trtgroup = 1);\n        control = (trtgroup = 2);\n        low     = (trtgroup = 3);\n        medium  = (trtgroup = 4);\n        high    = (trtgroup = 5);\n    END;\nRUN;\n\n* Check Dummy Variables Created Correctly;\nPROC PRINT DATA = Proj1.data (OBS = 6);\n    RUN;\n\nlstlog\n\n\n&lt;!DOCTYPE html&gt;\n\n\n\n\n\nSAS Output\n\n\n\n\n\n\n\nCreate Dummy Variables\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObs\n\n\nid\n\n\ntrtgroup\n\n\ngender\n\n\nrace\n\n\nage\n\n\nsmoker\n\n\nsites\n\n\nattachbase\n\n\npdbase\n\n\nattach1year\n\n\npd1year\n\n\nattachchange\n\n\npdchange\n\n\nplacebo\n\n\ncontrol\n\n\nlow\n\n\nmedium\n\n\nhigh\n\n\n\n\n\n\n1\n\n\n101\n\n\nMedium\n\n\nFemale\n\n\nAsian\n\n\n44.57221082\n\n\nSmoker\n\n\n162\n\n\n2.432098765\n\n\n3.24691358\n\n\n2.57764\n\n\n3.40741\n\n\n0.14554\n\n\n0.16049\n\n\n0\n\n\n0\n\n\n0\n\n\n1\n\n\n0\n\n\n\n\n2\n\n\n102\n\n\nHigh\n\n\nFemale\n\n\nAsian\n\n\n35.57289528\n\n\nSmoker\n\n\n162\n\n\n2.543209877\n\n\n3.00617284\n\n\n.\n\n\n.\n\n\n.\n\n\n.\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n1\n\n\n\n\n3\n\n\n103\n\n\nControl\n\n\nFemale\n\n\nAsian\n\n\n47.94524298\n\n\nSmoker\n\n\n144\n\n\n2.881944444\n\n\n3.118055556\n\n\n3.07639\n\n\n3.12500\n\n\n0.19444\n\n\n0.00694\n\n\n0\n\n\n1\n\n\n0\n\n\n0\n\n\n0\n\n\n\n\n4\n\n\n104\n\n\nLow\n\n\nFemale\n\n\nAsian\n\n\n55.17864476\n\n\nSmoker\n\n\n138\n\n\n4.956521739\n\n\n5.217391304\n\n\n5.30435\n\n\n4.89130\n\n\n0.34783\n\n\n-0.32609\n\n\n0\n\n\n0\n\n\n1\n\n\n0\n\n\n0\n\n\n\n\n5\n\n\n105\n\n\nPlacebo\n\n\nFemale\n\n\nAfrican American\n\n\n43.79739904\n\n\nSmoker\n\n\n168\n\n\n1.773809524\n\n\n3.363095238\n\n\n1.45238\n\n\n2.89881\n\n\n-0.32143\n\n\n-0.46429\n\n\n1\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n\n\n6\n\n\n106\n\n\nMedium\n\n\nFemale\n\n\nAsian\n\n\n42.14921287\n\n\nSmoker\n\n\n168\n\n\n2.369047619\n\n\n3.910714286\n\n\n1.92262\n\n\n3.08333\n\n\n-0.44643\n\n\n-0.82738\n\n\n0\n\n\n0\n\n\n0\n\n\n1\n\n\n0\n\n\n\n\n\n\n\n\n\n\n\u001449                                                         The SAS System                    Saturday, December 28, 2024 08:27:00 PM\n\n506        ods listing close;ods html5 (id=saspy_internal) file=_tomods1 options(bitmap_mode='inline') device=svg style=HTMLBlue;\n506      ! ods graphics on / outputfmt=png;\n507        \n508        * Create Dummy Variables;\n509        TITLE \"Create Dummy Variables\";\n510        DATA Proj1.data;\n511            SET Proj1.data;\n512            IF trtgroup ne . THEN DO;\n513                placebo = (trtgroup = 1);\n514                control = (trtgroup = 2);\n515                low     = (trtgroup = 3);\n516                medium  = (trtgroup = 4);\n517                high    = (trtgroup = 5);\n518         END;\n519        RUN;\n520        \n521        * Check Dummy Variables Created Correctly;\n522        PROC PRINT DATA = Proj1.data (OBS = 6);\n523         RUN;\n524        \n525        \n526        ods html5 (id=saspy_internal) close;ods listing;\n527        \n\u001450                                                         The SAS System                    Saturday, December 28, 2024 08:27:00 PM\n\n528        \n\n\n\n\n\n* Run regression of attachment loss by treatment condition with control as reference;\nTITLE \"SLR of Attachment Loss Change by Treatment Condition - Control as Reference\";\nPROC REG DATA = Proj1.data;\n    MODEL attachchange = placebo low medium high;\n    RUN;\n\nlstlog\n\n\n&lt;!DOCTYPE html&gt;\n\n\n\n\n\nSAS Output\n\n\n\n\n\n\n\nSLR of Attachment Loss Change by Treatment Condition - Control as Reference\n\n\n\n\nThe REG Procedure\n\n\nModel: MODEL1\n\n\nDependent Variable: attachchange Attachment Loss Change\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNumber of Observations Read\n\n\n130\n\n\n\n\nNumber of Observations Used\n\n\n103\n\n\n\n\nNumber of Observations with Missing Values\n\n\n27\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnalysis of Variance\n\n\n\n\nSource\n\n\nDF\n\n\nSum ofSquares\n\n\nMeanSquare\n\n\nF Value\n\n\nPr &gt; F\n\n\n\n\n\n\nModel\n\n\n4\n\n\n0.72806\n\n\n0.18202\n\n\n2.53\n\n\n0.0451\n\n\n\n\nError\n\n\n98\n\n\n7.04360\n\n\n0.07187\n\n\n \n\n\n \n\n\n\n\nCorrected Total\n\n\n102\n\n\n7.77166\n\n\n \n\n\n \n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRoot MSE\n\n\n0.26809\n\n\nR-Square\n\n\n0.0937\n\n\n\n\nDependent Mean\n\n\n-0.09945\n\n\nAdj R-Sq\n\n\n0.0567\n\n\n\n\nCoeff Var\n\n\n-269.57211\n\n\n \n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nParameter Estimates\n\n\n\n\nVariable\n\n\nLabel\n\n\nDF\n\n\nParameterEstimate\n\n\nStandardError\n\n\nt Value\n\n\nPr &gt; |t|\n\n\n\n\n\n\nIntercept\n\n\nIntercept\n\n\n1\n\n\n-0.22169\n\n\n0.05590\n\n\n-3.97\n\n\n0.0001\n\n\n\n\nplacebo\n\n\n \n\n\n1\n\n\n0.13462\n\n\n0.07906\n\n\n1.70\n\n\n0.0918\n\n\n\n\nlow\n\n\n \n\n\n1\n\n\n0.20388\n\n\n0.08092\n\n\n2.52\n\n\n0.0134\n\n\n\n\nmedium\n\n\n \n\n\n1\n\n\n0.21514\n\n\n0.08197\n\n\n2.62\n\n\n0.0101\n\n\n\n\nhigh\n\n\n \n\n\n1\n\n\n0.05690\n\n\n0.08728\n\n\n0.65\n\n\n0.5159\n\n\n\n\n\n\n\n\n\n\nSLR of Attachment Loss Change by Treatment Condition - Control as Reference\n\n\n\n\nThe REG Procedure\n\n\nModel: MODEL1\n\n\nDependent Variable: attachchange Attachment Loss Change\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001451                                                         The SAS System                    Saturday, December 28, 2024 08:27:00 PM\n\n531        ods listing close;ods html5 (id=saspy_internal) file=_tomods1 options(bitmap_mode='inline') device=svg style=HTMLBlue;\n531      ! ods graphics on / outputfmt=png;\n532        \n533        * Run regression of attachment loss by treatment condition with control as reference;\n534        TITLE \"SLR of Attachment Loss Change by Treatment Condition - Control as Reference\";\n535        PROC REG DATA = Proj1.data;\n536         MODEL attachchange = placebo low medium high;\n537         RUN;\n538        \n539        \n540        ods html5 (id=saspy_internal) close;ods listing;\n541        \n\u001452                                                         The SAS System                    Saturday, December 28, 2024 08:27:00 PM\n\n542        \n\n\n\n\n\nInterpretation\nThe overall model is significant (F(4,98) = 2.53, p = 0.0451). Looking at the individual t-statistics, we see that - following adjustment for multiple pairwise comparisons - the placebo group (p = 0.549), low concentration group (p = 0.0668), and high concentration group (p = 1.000) are not statistically different from the control group. The medium concentration group is approaching significance (p = 0.0503).\n\n\nChange Reference to Placebo Group\nThat was with the no treatment control group as the reference. Let’s see if any of our treatment conditions were different from the placebo group.\n\nTITLE \"SLR of Attachment Loss Change by Treatment Condition - Placebo as Reference\";\nPROC REG DATA = Proj1.data;\n    MODEL attachchange = control low medium high;\n    RUN;\n\nlstlog\n\n\n&lt;!DOCTYPE html&gt;\n\n\n\n\n\nSAS Output\n\n\n\n\n\n\n\n\n\n\nSLR of Attachment Loss Change by Treatment Condition - Placebo as Reference\n\n\n\n\nThe REG Procedure\n\n\nModel: MODEL1\n\n\nDependent Variable: attachchange Attachment Loss Change\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNumber of Observations Read\n\n\n130\n\n\n\n\nNumber of Observations Used\n\n\n103\n\n\n\n\nNumber of Observations with Missing Values\n\n\n27\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnalysis of Variance\n\n\n\n\nSource\n\n\nDF\n\n\nSum ofSquares\n\n\nMeanSquare\n\n\nF Value\n\n\nPr &gt; F\n\n\n\n\n\n\nModel\n\n\n4\n\n\n0.72806\n\n\n0.18202\n\n\n2.53\n\n\n0.0451\n\n\n\n\nError\n\n\n98\n\n\n7.04360\n\n\n0.07187\n\n\n \n\n\n \n\n\n\n\nCorrected Total\n\n\n102\n\n\n7.77166\n\n\n \n\n\n \n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRoot MSE\n\n\n0.26809\n\n\nR-Square\n\n\n0.0937\n\n\n\n\nDependent Mean\n\n\n-0.09945\n\n\nAdj R-Sq\n\n\n0.0567\n\n\n\n\nCoeff Var\n\n\n-269.57211\n\n\n \n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nParameter Estimates\n\n\n\n\nVariable\n\n\nLabel\n\n\nDF\n\n\nParameterEstimate\n\n\nStandardError\n\n\nt Value\n\n\nPr &gt; |t|\n\n\n\n\n\n\nIntercept\n\n\nIntercept\n\n\n1\n\n\n-0.08707\n\n\n0.05590\n\n\n-1.56\n\n\n0.1225\n\n\n\n\ncontrol\n\n\n \n\n\n1\n\n\n-0.13462\n\n\n0.07906\n\n\n-1.70\n\n\n0.0918\n\n\n\n\nlow\n\n\n \n\n\n1\n\n\n0.06926\n\n\n0.08092\n\n\n0.86\n\n\n0.3941\n\n\n\n\nmedium\n\n\n \n\n\n1\n\n\n0.08052\n\n\n0.08197\n\n\n0.98\n\n\n0.3284\n\n\n\n\nhigh\n\n\n \n\n\n1\n\n\n-0.07772\n\n\n0.08728\n\n\n-0.89\n\n\n0.3754\n\n\n\n\n\n\n\n\n\n\nSLR of Attachment Loss Change by Treatment Condition - Placebo as Reference\n\n\n\n\nThe REG Procedure\n\n\nModel: MODEL1\n\n\nDependent Variable: attachchange Attachment Loss Change\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001453                                                         The SAS System                    Saturday, December 28, 2024 08:27:00 PM\n\n545        ods listing close;ods html5 (id=saspy_internal) file=_tomods1 options(bitmap_mode='inline') device=svg style=HTMLBlue;\n545      ! ods graphics on / outputfmt=png;\n546        \n547        TITLE \"SLR of Attachment Loss Change by Treatment Condition - Placebo as Reference\";\n548        PROC REG DATA = Proj1.data;\n549         MODEL attachchange = control low medium high;\n550         RUN;\n551        \n552        \n553        ods html5 (id=saspy_internal) close;ods listing;\n554        \n\u001454                                                         The SAS System                    Saturday, December 28, 2024 08:27:00 PM\n\n555        \n\n\n\n\nAfter applying a bonferroni correction, none of the groups are significantly different from the placebo group (p &gt; .05).\n\n\nConclusion\nWhile the overall model was significant, the only groups that were statistically different from the no treatment control were the low and medium concentration gel groups (p &lt; 0.05). However, since this was a placebo-controlled RCT, and none of the treatment groups were significantly different from the placebo group, we can conclude that we fail to reject the null hypothesis that the average attachment loss over 1 year is the same between all groups.\nBack to top of tabset\n\n\n\nNow we will create our model with pocket depth change as the dependent variable and treatment group as the independent variable.\n\n* Simple Linear Regression Predicting Pocket Depth Change by Treatment Condition;\nTITLE \"SLR of Pocket Depth Change by Treatment Condition - Control as Reference\";\nPROC REG DATA = Proj1.data;\n    MODEL pdchange = placebo low medium high;\n    RUN;\n\nlstlog\n\n\n&lt;!DOCTYPE html&gt;\n\n\n\n\n\nSAS Output\n\n\n\n\n\n\n\n\n\n\nSLR of Pocket Depth Change by Treatment Condition - Control as Reference\n\n\n\n\nThe REG Procedure\n\n\nModel: MODEL1\n\n\nDependent Variable: pdchange Pocket Depth Change\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNumber of Observations Read\n\n\n130\n\n\n\n\nNumber of Observations Used\n\n\n103\n\n\n\n\nNumber of Observations with Missing Values\n\n\n27\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnalysis of Variance\n\n\n\n\nSource\n\n\nDF\n\n\nSum ofSquares\n\n\nMeanSquare\n\n\nF Value\n\n\nPr &gt; F\n\n\n\n\n\n\nModel\n\n\n4\n\n\n0.57020\n\n\n0.14255\n\n\n2.07\n\n\n0.0899\n\n\n\n\nError\n\n\n98\n\n\n6.73457\n\n\n0.06872\n\n\n \n\n\n \n\n\n\n\nCorrected Total\n\n\n102\n\n\n7.30477\n\n\n \n\n\n \n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRoot MSE\n\n\n0.26215\n\n\nR-Square\n\n\n0.0781\n\n\n\n\nDependent Mean\n\n\n-0.29435\n\n\nAdj R-Sq\n\n\n0.0404\n\n\n\n\nCoeff Var\n\n\n-89.05770\n\n\n \n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nParameter Estimates\n\n\n\n\nVariable\n\n\nLabel\n\n\nDF\n\n\nParameterEstimate\n\n\nStandardError\n\n\nt Value\n\n\nPr &gt; |t|\n\n\n\n\n\n\nIntercept\n\n\nIntercept\n\n\n1\n\n\n-0.33817\n\n\n0.05466\n\n\n-6.19\n\n\n&lt;.0001\n\n\n\n\nplacebo\n\n\n \n\n\n1\n\n\n-0.01152\n\n\n0.07730\n\n\n-0.15\n\n\n0.8818\n\n\n\n\nlow\n\n\n \n\n\n1\n\n\n0.13200\n\n\n0.07912\n\n\n1.67\n\n\n0.0984\n\n\n\n\nmedium\n\n\n \n\n\n1\n\n\n0.13562\n\n\n0.08015\n\n\n1.69\n\n\n0.0938\n\n\n\n\nhigh\n\n\n \n\n\n1\n\n\n-0.04413\n\n\n0.08534\n\n\n-0.52\n\n\n0.6063\n\n\n\n\n\n\n\n\n\n\nSLR of Pocket Depth Change by Treatment Condition - Control as Reference\n\n\n\n\nThe REG Procedure\n\n\nModel: MODEL1\n\n\nDependent Variable: pdchange Pocket Depth Change\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001455                                                         The SAS System                    Saturday, December 28, 2024 08:27:00 PM\n\n558        ods listing close;ods html5 (id=saspy_internal) file=_tomods1 options(bitmap_mode='inline') device=svg style=HTMLBlue;\n558      ! ods graphics on / outputfmt=png;\n559        \n560        * Simple Linear Regression Predicting Pocket Depth Change by Treatment Condition;\n561        TITLE \"SLR of Pocket Depth Change by Treatment Condition - Control as Reference\";\n562        PROC REG DATA = Proj1.data;\n563         MODEL pdchange = placebo low medium high;\n564         RUN;\n565        \n566        \n567        ods html5 (id=saspy_internal) close;ods listing;\n568        \n\u001456                                                         The SAS System                    Saturday, December 28, 2024 08:27:00 PM\n\n569        \n\n\n\n\nThe overall model is not statistically significant (F(4,98)= 2.074, p = 0.0899). Based on this model, it appears that the gel treatment has no effect on pocket depth change after 1 year.\nBack to top of tabset\n\n\nWe did not find a main effect based on treatment group. However I am still interested if including any of the covariates changes the results.\n\nModel Including Gender\n\n* Try a model of attachment loss change by treatment condition, controlling for gender;\nTITLE \"MLR of Attachment Loss Change by Treatment Condition and Gender\";\nPROC REG DATA = Proj1.data;\n    MODEL attachchange = placebo low medium high gender;\n    RUN;\n\nlstlog\n\n\n&lt;!DOCTYPE html&gt;\n\n\n\n\n\nSAS Output\n\n\n\n\n\n\n\n\n\n\nMLR of Attachment Loss Change by Treatment Condition and Gender\n\n\n\n\nThe REG Procedure\n\n\nModel: MODEL1\n\n\nDependent Variable: attachchange Attachment Loss Change\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNumber of Observations Read\n\n\n130\n\n\n\n\nNumber of Observations Used\n\n\n103\n\n\n\n\nNumber of Observations with Missing Values\n\n\n27\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnalysis of Variance\n\n\n\n\nSource\n\n\nDF\n\n\nSum ofSquares\n\n\nMeanSquare\n\n\nF Value\n\n\nPr &gt; F\n\n\n\n\n\n\nModel\n\n\n5\n\n\n0.89852\n\n\n0.17970\n\n\n2.54\n\n\n0.0335\n\n\n\n\nError\n\n\n97\n\n\n6.87314\n\n\n0.07086\n\n\n \n\n\n \n\n\n\n\nCorrected Total\n\n\n102\n\n\n7.77166\n\n\n \n\n\n \n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRoot MSE\n\n\n0.26619\n\n\nR-Square\n\n\n0.1156\n\n\n\n\nDependent Mean\n\n\n-0.09945\n\n\nAdj R-Sq\n\n\n0.0700\n\n\n\n\nCoeff Var\n\n\n-267.65934\n\n\n \n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nParameter Estimates\n\n\n\n\nVariable\n\n\nLabel\n\n\nDF\n\n\nParameterEstimate\n\n\nStandardError\n\n\nt Value\n\n\nPr &gt; |t|\n\n\n\n\n\n\nIntercept\n\n\nIntercept\n\n\n1\n\n\n-0.07460\n\n\n0.10988\n\n\n-0.68\n\n\n0.4988\n\n\n\n\nplacebo\n\n\n \n\n\n1\n\n\n0.12330\n\n\n0.07883\n\n\n1.56\n\n\n0.1210\n\n\n\n\nlow\n\n\n \n\n\n1\n\n\n0.19310\n\n\n0.08064\n\n\n2.39\n\n\n0.0186\n\n\n\n\nmedium\n\n\n \n\n\n1\n\n\n0.21118\n\n\n0.08143\n\n\n2.59\n\n\n0.0110\n\n\n\n\nhigh\n\n\n \n\n\n1\n\n\n0.06704\n\n\n0.08690\n\n\n0.77\n\n\n0.4423\n\n\n\n\ngender\n\n\nGender\n\n\n1\n\n\n-0.08675\n\n\n0.05593\n\n\n-1.55\n\n\n0.1242\n\n\n\n\n\n\n\n\n\n\nMLR of Attachment Loss Change by Treatment Condition and Gender\n\n\n\n\nThe REG Procedure\n\n\nModel: MODEL1\n\n\nDependent Variable: attachchange Attachment Loss Change\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001457                                                         The SAS System                    Saturday, December 28, 2024 08:27:00 PM\n\n572        ods listing close;ods html5 (id=saspy_internal) file=_tomods1 options(bitmap_mode='inline') device=svg style=HTMLBlue;\n572      ! ods graphics on / outputfmt=png;\n573        \n574        * Try a model of attachment loss change by treatment condition, controlling for gender;\n575        TITLE \"MLR of Attachment Loss Change by Treatment Condition and Gender\";\n576        PROC REG DATA = Proj1.data;\n577         MODEL attachchange = placebo low medium high gender;\n578         RUN;\n579        \n580        \n581        ods html5 (id=saspy_internal) close;ods listing;\n582        \n\u001458                                                         The SAS System                    Saturday, December 28, 2024 08:27:00 PM\n\n583        \n\n\n\n\nA model including gender does not seem to help anything. What about with all covariates?\n\n\nSaturated Model\n\nTITLE \"MLR of Pocket Depth Change by Treatment Condition and Gender\";\nPROC REG DATA = Proj1.data;\n    MODEL pdchange = placebo low medium high gender;\n    RUN;\n\nlstlog\n\n\n&lt;!DOCTYPE html&gt;\n\n\n\n\n\nSAS Output\n\n\n\n\n\n\n\n\n\n\nMLR of Pocket Depth Change by Treatment Condition and Gender\n\n\n\n\nThe REG Procedure\n\n\nModel: MODEL1\n\n\nDependent Variable: pdchange Pocket Depth Change\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNumber of Observations Read\n\n\n130\n\n\n\n\nNumber of Observations Used\n\n\n103\n\n\n\n\nNumber of Observations with Missing Values\n\n\n27\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnalysis of Variance\n\n\n\n\nSource\n\n\nDF\n\n\nSum ofSquares\n\n\nMeanSquare\n\n\nF Value\n\n\nPr &gt; F\n\n\n\n\n\n\nModel\n\n\n5\n\n\n0.85534\n\n\n0.17107\n\n\n2.57\n\n\n0.0313\n\n\n\n\nError\n\n\n97\n\n\n6.44943\n\n\n0.06649\n\n\n \n\n\n \n\n\n\n\nCorrected Total\n\n\n102\n\n\n7.30477\n\n\n \n\n\n \n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRoot MSE\n\n\n0.25785\n\n\nR-Square\n\n\n0.1171\n\n\n\n\nDependent Mean\n\n\n-0.29435\n\n\nAdj R-Sq\n\n\n0.0716\n\n\n\n\nCoeff Var\n\n\n-87.60007\n\n\n \n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nParameter Estimates\n\n\n\n\nVariable\n\n\nLabel\n\n\nDF\n\n\nParameterEstimate\n\n\nStandardError\n\n\nt Value\n\n\nPr &gt; |t|\n\n\n\n\n\n\nIntercept\n\n\nIntercept\n\n\n1\n\n\n-0.14793\n\n\n0.10644\n\n\n-1.39\n\n\n0.1678\n\n\n\n\nplacebo\n\n\n \n\n\n1\n\n\n-0.02615\n\n\n0.07636\n\n\n-0.34\n\n\n0.7327\n\n\n\n\nlow\n\n\n \n\n\n1\n\n\n0.11806\n\n\n0.07812\n\n\n1.51\n\n\n0.1339\n\n\n\n\nmedium\n\n\n \n\n\n1\n\n\n0.13050\n\n\n0.07888\n\n\n1.65\n\n\n0.1013\n\n\n\n\nhigh\n\n\n \n\n\n1\n\n\n-0.03102\n\n\n0.08418\n\n\n-0.37\n\n\n0.7133\n\n\n\n\ngender\n\n\nGender\n\n\n1\n\n\n-0.11219\n\n\n0.05418\n\n\n-2.07\n\n\n0.0410\n\n\n\n\n\n\n\n\n\n\nMLR of Pocket Depth Change by Treatment Condition and Gender\n\n\n\n\nThe REG Procedure\n\n\nModel: MODEL1\n\n\nDependent Variable: pdchange Pocket Depth Change\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001459                                                         The SAS System                    Saturday, December 28, 2024 08:27:00 PM\n\n586        ods listing close;ods html5 (id=saspy_internal) file=_tomods1 options(bitmap_mode='inline') device=svg style=HTMLBlue;\n586      ! ods graphics on / outputfmt=png;\n587        \n588        TITLE \"MLR of Pocket Depth Change by Treatment Condition and Gender\";\n589        PROC REG DATA = Proj1.data;\n590         MODEL pdchange = placebo low medium high gender;\n591         RUN;\n592        \n593        \n594        ods html5 (id=saspy_internal) close;ods listing;\n595        \n\u001460                                                         The SAS System                    Saturday, December 28, 2024 08:27:00 PM\n\n596        \n\n\n\n\nNope. Interestingly the best predictor of attachment loss at 1 year is attachment loss at baseline. The results for pocket depth are likely the same.\n\n\n\nThe beauty of SAS is that the output to check all of the assumptions is included in the output of each model!\nThus we have seen that in each model so far, all of our assumptions have been met!"
  },
  {
    "objectID": "Project_2/Project_2_R/Code/Project2LMM.html",
    "href": "Project_2/Project_2_R/Code/Project2LMM.html",
    "title": "Linear Mixed Model",
    "section": "",
    "text": "The objective of this project is to gain experience in the application of Linear Mixed Models for statistical analysis.\nThis is the exact same data set as Project 2. Please see Project 2 - MLR with Confounding and Interaction for a detailed description of the data cleaning process.\n\n\nThe aim of the current study is to assess how treatment response differs for HIV+ patients 2 years after initiating Highly Active Antiretroviral Therapy (HAART) based on hard drug usage (such as heroin or cocaine). This study is of particular scientific interest because it is unclear whether the use of hard drugs inhibits the immune system in humans; treatment strategies may differ based on these results. The researchers are interested in comparing subjects who never used hard drugs to current hard drug users (those that use hard drugs at year 2) or previous hard drug users (those who used drugs at year 0 or 1). Outcomes of interest are: viral load (HIV copies in a mL of blood), CD4+ T cell count (a measure of immunologic health), and aggregate physical and quality of life scores from the SF-36.\nThe clinical hypothesis is that, if hard drugs inhibit the immune system in humans, subjects who currently or previously used hard drugs will have higher viral load and lower CD4+ T cell counts than those who never used hard drugs. Additionally, the researchers are interested in knowing if potential differences between the drug use groups can be explained by differences in adherence to the treatment regimen. The researchers are agnostic on how quality of life changes after treatment, since side effects of the treatment are significant.\nThe project description provided by the PI is available below:"
  },
  {
    "objectID": "Project_2/Project_2_R/Code/Project2LMM.html#project-description",
    "href": "Project_2/Project_2_R/Code/Project2LMM.html#project-description",
    "title": "Linear Mixed Model",
    "section": "",
    "text": "The aim of the current study is to assess how treatment response differs for HIV+ patients 2 years after initiating Highly Active Antiretroviral Therapy (HAART) based on hard drug usage (such as heroin or cocaine). This study is of particular scientific interest because it is unclear whether the use of hard drugs inhibits the immune system in humans; treatment strategies may differ based on these results. The researchers are interested in comparing subjects who never used hard drugs to current hard drug users (those that use hard drugs at year 2) or previous hard drug users (those who used drugs at year 0 or 1). Outcomes of interest are: viral load (HIV copies in a mL of blood), CD4+ T cell count (a measure of immunologic health), and aggregate physical and quality of life scores from the SF-36.\nThe clinical hypothesis is that, if hard drugs inhibit the immune system in humans, subjects who currently or previously used hard drugs will have higher viral load and lower CD4+ T cell counts than those who never used hard drugs. Additionally, the researchers are interested in knowing if potential differences between the drug use groups can be explained by differences in adherence to the treatment regimen. The researchers are agnostic on how quality of life changes after treatment, since side effects of the treatment are significant.\nThe project description provided by the PI is available below:"
  },
  {
    "objectID": "Project_2/Project_2_R/Code/Project2LMM.html#Spaghetti",
    "href": "Project_2/Project_2_R/Code/Project2LMM.html#Spaghetti",
    "title": "Linear Mixed Model",
    "section": "Spaghetti Plots",
    "text": "Spaghetti Plots\nIn order to get an initial feel for the data, we will create some spaghetti plots to visualize each outcome variable for all patients over 2 years.\n\nViral LoadCD4+ T Cell CountMental QOLPhysical QOLSummary\n\n\nLet’s examine log viral load since that is our main PEV.\n\n# Create log vload for 8 year dataset\ndata &lt;- data |&gt; \n  mutate(VLOAD_log = log(VLOAD))\n\n# Turn newid into a factor\ndata &lt;- data |&gt; \n  mutate(newid = factor(newid),\n         ADH = factor(ADH))\n  \n# Create spaghetti plot for log viral load\nggplot(data, aes(y = VLOAD_log, x = years, group = newid)) +\n  geom_line(alpha = 0.1) +\n  theme_minimal() +\n  labs(title = \"Spaghetti Plot of Log Viral Load\",\n       x = \"Year\",\n       y = \"Log Viral Load (copies/mL)\")\n\n\n\n\n\n\n\n\nThat’s interesting, we can see that for most patients, they had a decrease in log viral load over the course of the study.\nI wonder if those patients with higher viral loads tend to be those who are hard drug users. Let’s explore.\n\n# Create spaghetti plot for log viral load\nggplot(data, aes(y = VLOAD_log, x = years, group = newid, color = as.factor(hard_drugs))) +\n  geom_line(alpha = 0.2) +\n  theme_minimal() +\n  labs(title = \"Spaghetti Plot of Log Viral Load\",\n       x = \"Year\",\n       y = \"Log Viral Load (copies/mL)\")\n\nWarning: Removed 86 rows containing missing values or values outside the scale range\n(`geom_line()`).\n\n\n\n\n\n\n\n\n\nHuh, that’s interesting. There seem to really only be a few hard drug users in the sample. Additionally, if patients used hard drugs, they seem to have consistenly used them throughout the study and never quit.\n\ntable(data$hard_drugs)\n\n\n   0    1 \n3215  406 \n\n\nAbout 12.6% of visits were by patients who used hard drugs since the previous visit.\nThese plots are interesting, but there are too many data points to extrapolate any more meaningful information from them.\nLet’s move on to plotting the averages of each outcome variable based on hard drug use and protocol adherence.\nTop of Tabset\n\n\nLet’s examine what happened to leukocyte count over the course of the study.\n\n# Create log vload for 8 year dataset\ndata &lt;- data |&gt; \n  mutate(VLOAD_log = log(VLOAD))\n\n# Turn newid into a factor\ndata &lt;- data |&gt; \n  mutate(newid = factor(newid),\n         ADH = factor(ADH))\n  \n# Create spaghetti plot for log viral load\nggplot(data, aes(y = LEU3N, x = years, group = newid)) +\n  geom_line(alpha = 0.1) +\n  theme_minimal() +\n  labs(title = \"Spaghetti Plot of CD4+ T Cell Count\",\n       x = \"Year\",\n       y = \"CD4+ T Cell Count\")\n\n\n\n\n\n\n\n\nFor the most part it increasd, with a lot of variation for some patients.\nTop of Tabset\n\n\nNow let’s examine how mental QOL changed over time.\n\n# Create log vload for 8 year dataset\ndata &lt;- data |&gt; \n  mutate(VLOAD_log = log(VLOAD))\n\n# Turn newid into a factor\ndata &lt;- data |&gt; \n  mutate(newid = factor(newid),\n         ADH = factor(ADH))\n  \n# Create spaghetti plot for log viral load\nggplot(data, aes(y = AGG_MENT, x = years, group = newid)) +\n  geom_line(alpha = 0.1) +\n  theme_minimal() +\n  labs(title = \"Spaghetti Plot of Mental QOL\",\n       x = \"Year\",\n       y = \"Mental QOL\")\n\n\n\n\n\n\n\n\nIt stayed high for the most part but with a LOT of variation for some patients.\nThis looks like good evidnece to include random slopes into our model for mental QOL.\nTop of Tabset\n\n\nFinally let’s assess how physical QOL changed over time.\n\n# Create log vload for 8 year data set\ndata &lt;- data |&gt; \n  mutate(VLOAD_log = log(VLOAD))\n\n# Turn newid into a factor\ndata &lt;- data |&gt; \n  mutate(newid = factor(newid),\n         ADH = factor(ADH))\n  \n# Create spaghetti plot for log viral load\nggplot(data, aes(y = AGG_PHYS, x = years, group = newid)) +\n  geom_line(alpha = 0.1) +\n  theme_minimal() +\n  labs(title = \"Spaghetti Plot of Physical QOL\",\n       x = \"Year\",\n       y = \"Physical QOL\")\n\n\n\n\n\n\n\n\nJust like mental QOL, it was high for the most part, but with a lot of variation within patients.\nThis is good evidence for including random slopes into our model for physical QOL.\nTop of Tabset\n\n\nWe saw over the 8 year course of the study an overall decrease in viral load and increase in CD4+ T Cell count.\nThe mental and physical QOL scores appeared remain about the same overall, but with a lot of variation within patients. We will include random slopes into these models to account for this variation."
  },
  {
    "objectID": "Project_2/Project_2_R/Code/Project2LMM.html#Bivariate",
    "href": "Project_2/Project_2_R/Code/Project2LMM.html#Bivariate",
    "title": "Linear Mixed Model",
    "section": "Bivariate Relationships",
    "text": "Bivariate Relationships\nSpaghetti plots were informative to see general trends in each outcome variable over the course of the study, but they don’t give us a fine tuned look into the important bivariate relationships between the outcome variables and our two primary explanatory variables: hard_drugs_grp and ADH_HIGHVSLOW.\nTo do that, we will plot the average of each outcome group by our PEVs.\n\nMain Outcome Variables\n\nViral LoadCD4+ T Cell CountMental QOLPhysical QOLSummary\n\n\n\nViral Load by Hard Drugs Group\n\n# calculate the average vload log at each year for each hard drugs use group\nsummary_data &lt;- data_2 |&gt; \n  group_by(hard_drugs_grp, years) |&gt; \n  summarize(avg_VLOAD_log = mean(VLOAD_log, na.rm = TRUE))\n\n# Plot\nggplot(summary_data, aes(x=years, y= avg_VLOAD_log, color = hard_drugs_grp)) +\n  geom_line(size = 1) + \n  scale_color_brewer(palette = \"Pastel2\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nInterestingly, it appears that previous users have the lowest average log viral load over 2 years. This difference may not be statistically significant however.\n\n\nViral Load by Adherence\n\n# calculate the average vload log at each year for each hard drugs use group\nsummary_data &lt;- data_2 |&gt; \n  group_by(ADH_HIGHVSLOW, years) |&gt; \n  summarize(avg_VLOAD_log = mean(VLOAD_log, na.rm = TRUE))\n\n# Plot\nggplot(summary_data, aes(x=years, y= avg_VLOAD_log, color = ADH_HIGHVSLOW)) +\n  geom_line(size = 1) +\n  theme_minimal() +\n  scale_color_brewer(palette = \"Pastel2\")\n\n\n\n\n\n\n\n\nThis shows the expected result, that those with high adherence have lower viral load. Additionally, they have different slopes, so there’s an interaction with time here.\n\n\nSummary\nThose with low adherence have a higher average log viral load. Interestingly, previous users have a lower log viral load, but this may not be statistically significant in the full model.\nTop of Tabset\n\n\n\n\nCD4+ T Cell Count by Hard Drug Use\n\n# Calculate the average CD4+ T Cell Count at each year for each hard drugs use group\nsummary_data &lt;- data_2 |&gt; \n  group_by(hard_drugs_grp, years) |&gt; \n  summarize(avg_LEU3N = mean(LEU3N, na.rm = TRUE))\n\n# Plot\nggplot(summary_data, aes(x=years, y= avg_LEU3N, color = hard_drugs_grp)) +\n  geom_line(size = 1) +\n  theme_minimal() +\n  scale_color_brewer(palette = \"Pastel2\")\n\n\n\n\n\n\n\n\nHere it appears that current drug users have the highest leukocyte count, and previous drug users have the lowest. This is not really as expected. Never users should have the highest white blood cell count since they are the healthiest.\n\n\nCD4+ T Cell Count by Adherence\n\n# Calculate the average CD4+ T Cell Count at each year by adherence\nsummary_data &lt;- data_2 |&gt; \n  group_by(ADH_HIGHVSLOW, years) |&gt; \n  summarize(avg_LEU3N = mean(LEU3N, na.rm = TRUE))\n\n# Plot\nggplot(summary_data, aes(x=years, y= avg_LEU3N, color = ADH_HIGHVSLOW)) +\n  geom_line(size = 1) +\n  theme_minimal() +\n  scale_color_brewer(palette = \"Pastel2\")\n\n\n\n\n\n\n\n\nThose with low adherence have lower leukocyte count. That’s expected. The slope does not change by adherence group.\n\n\nSummary\nPrevious hard drug users and those with low adherence have lower white blood cell count.\nTop of Tabset\n\n\n\n\nMental QOL by Hard Drug Use\n\n# Calculate the average mental QOL at each year for each hard drugs use group\nsummary_data &lt;- data_2 |&gt; \n  group_by(hard_drugs_grp, years) |&gt; \n  summarize(avg_AGG_MENT = mean(AGG_MENT, na.rm = TRUE))\n\n# Plot\nggplot(summary_data, aes(x=years, y= avg_AGG_MENT, color = hard_drugs_grp)) +\n  geom_line(size = 1) +\n  theme_minimal() +\n  scale_color_brewer(palette = \"Pastel2\")\n\n\n\n\n\n\n\n\nThis shows that previous users have the lowest average mental QOL, and never users the highest. This is as expected, especially if we consider that previous users are likely dealing with the side effects of recently quitting hard drugs.\n\n\nMental QOL by Adherence\n\n# Calculate the average mental QOL at each year by adherence\nsummary_data &lt;- data_2 |&gt; \n  group_by(ADH_HIGHVSLOW, years) |&gt; \n  summarize(avg_AGG_MENT = mean(AGG_MENT, na.rm = TRUE))\n\n# Plot\nggplot(summary_data, aes(x=years, y= avg_AGG_MENT, color = ADH_HIGHVSLOW)) +\n  geom_line(size = 1) +\n  theme_minimal() +\n  scale_color_brewer(palette = \"Pastel2\")\n\n\n\n\n\n\n\n\nThose with high adherence have a drastically higher mental QOL score. That’s expected.\n\n\nSummary\nThose with low adherence have worse mental QOL, and previous users have worse mental QOL.\nTop of Tabset\n\n\n\n\nPhysical QOL by Hard Drug Use\n\n# Calculate the average physical QOL at each year for each hard drugs use group\nsummary_data &lt;- data_2 |&gt; \n  group_by(hard_drugs_grp, years) |&gt; \n  summarize(avg_AGG_PHYS = mean(AGG_PHYS, na.rm = TRUE))\n\n# Plot\nggplot(summary_data, aes(x=years, y= avg_AGG_PHYS, color = hard_drugs_grp)) +\n  geom_line(size = 1) +\n  theme_minimal() +\n  scale_color_brewer(palette = \"Pastel2\")\n\n\n\n\n\n\n\n\nOverall it appears that never users have higher physical QOL across the study. While initally high, the physical QOL for current users plummets after year 1. Additionally, previous users have the lowest physical QOL, which also makes sense with the understanding we’ve mentioned that previous users are dealing with the side effects of quitting and are lacking their coping mechanism.\n\n\nPhysical QOL by Adherence\n\n# Calculate the average physical QOL at each year for each hard drugs use group\nsummary_data &lt;- data_2 |&gt; \n  group_by(ADH_HIGHVSLOW, years) |&gt; \n  summarize(avg_AGG_PHYS = mean(AGG_PHYS, na.rm = TRUE))\n\n# Plot\nggplot(summary_data, aes(x=years, y= avg_AGG_PHYS, color = ADH_HIGHVSLOW)) +\n  geom_line(size = 1) +\n  theme_minimal() +\n  scale_color_brewer(palette = \"Pastel2\")\n\n\n\n\n\n\n\n\n\n\nSummary\nThose with low adherence have worse physical QOL, and previous users have worse physical QOL.\nTop of Tabset\n\n\n\nFor hard drug use group, previous users have:\n\nThe lowest average log viral load\nThe lowest leukocyte count, and highest mental and physical QOL.\n\nFor adherence, those with low adherence have:\n\nThe highest average log viral load\nThe lowest leukocyte count, and lowest mental and physical QOL.\n\nTop of Tabset\n\n\n\n\n\nCovariates\n\nDepressionFrailty Related PhenotypeCollege Education\n\n\nDepression was a confounder in the model for mental QOL and must be accounted for.\n\n# Calculate the average depression at each year for each hard drugs use group\nsummary_data &lt;- data_2 |&gt; \n  group_by(hard_drugs_grp, years) |&gt; \n  summarize(avg_CESD = mean(CESD, na.rm = TRUE))\n\n# Plot\nggplot(summary_data, aes(x=years, y= avg_CESD, color = hard_drugs_grp)) +\n  geom_line(size = 1) +\n  theme_minimal() +\n  scale_color_brewer(palette = \"Pastel2\")\n\n\n\n\n\n\n\n\nWe see the expected result, previous drug users are more depressed.\n\n# Calculate the average mental QOL at each year by adherence\nsummary_data &lt;- data_2 |&gt; \n  group_by(ADH_HIGHVSLOW, years) |&gt; \n  summarize(avg_CESD = mean(CESD, na.rm = TRUE))\n\n# Plot\nggplot(summary_data, aes(x=years, y= avg_CESD, color = ADH_HIGHVSLOW)) +\n  geom_line(size = 1) +\n  theme_minimal() +\n  scale_color_brewer(palette = \"Pastel2\")\n\n\n\n\n\n\n\n\nWe see the expected result, those with low adherence were more depressed.\nTop of Tabset\n\n\nFrailty Related Phenotype is a precision variable for physical QOL, and was previously significant for CD4+ T Cell Count.\n\n# Calculate the average CD4+ T Cell Count for FRP\nsummary_data &lt;- data_2 |&gt; \n  group_by(FRP, years) |&gt; \n  summarize(avg_LEU3N = mean(LEU3N, na.rm = TRUE))\n\n# Plot\nggplot(summary_data, aes(x=years, y= avg_LEU3N, color = FRP)) +\n  geom_line(size = 1) +\n  theme_minimal() +\n  scale_color_brewer(palette = \"Pastel2\")\n\n\n\n\n\n\n\n\nThose with a frailty related phenotype have lower average CD4+ T Cell Count.\n\n# Calculate the average Physical QOL for FRP\nsummary_data &lt;- data_2 |&gt; \n  group_by(FRP, years) |&gt; \n  summarize(avg_AGG_PHYS = mean(AGG_PHYS, na.rm = TRUE))\n\n# Plot\nggplot(summary_data, aes(x=years, y= avg_AGG_PHYS, color = FRP)) +\n  geom_line(size = 1) +\n  theme_minimal() +\n  scale_color_brewer(palette = \"Pastel2\")\n\n\n\n\n\n\n\n\nThey also have drastically lower physical QOL.\nTop of Tabset\n\n\nThis was significant for the model with VLOAD_log. Let’s assess.\n\n# Calculate the average VLOAD log by college education\nsummary_data &lt;- data_2 |&gt; \n  group_by(EDUC_COLLEGE, years) |&gt; \n  summarize(avg_VLOAD_log = mean(VLOAD_log, na.rm = TRUE))\n\n# Plot\nggplot(summary_data, aes(x=years, y= avg_VLOAD_log, color = EDUC_COLLEGE)) +\n  geom_line(size = 1) +\n  theme_minimal() +\n  scale_color_brewer(palette = \"Pastel2\")\n\n\n\n\n\n\n\n\nThose with no college have slightly higher average log vload. This does not appear to be significantly different.\n\n# Calculate the average LEU3N by college education\nsummary_data &lt;- data_2 |&gt; \n  group_by(EDUC_COLLEGE, years) |&gt; \n  summarize(avg_LEU3N = mean(LEU3N, na.rm = TRUE))\n\n# Plot\nggplot(summary_data, aes(x=years, y= avg_LEU3N, color = EDUC_COLLEGE)) +\n  geom_line(size = 1) +\n  theme_minimal() +\n  scale_color_brewer(palette = \"Pastel2\")\n\n\n\n\n\n\n\n\nThose with no college had lower white blood cell count.\n\n# Calculate the average VLOAD log by college education\nsummary_data &lt;- data_2 |&gt; \n  group_by(EDUC_COLLEGE, years) |&gt; \n  summarize(avg_AGG_PHYS = mean(AGG_PHYS, na.rm = TRUE))\n\n# Plot\nggplot(summary_data, aes(x=years, y= avg_AGG_PHYS, color = EDUC_COLLEGE)) +\n  geom_line(size = 1) +\n  theme_minimal() +\n  scale_color_brewer(palette = \"Pastel2\")\n\n\n\n\n\n\n\n\nThose with no college have lower physical QOL.\n\n# Calculate the mental QOL by college education\nsummary_data &lt;- data_2 |&gt; \n  group_by(EDUC_COLLEGE, years) |&gt; \n  summarize(avg_AGG_MENT = mean(AGG_MENT, na.rm = TRUE))\n\n# Plot\nggplot(summary_data, aes(x=years, y= avg_AGG_MENT, color = EDUC_COLLEGE)) +\n  geom_line(size = 1) +\n  theme_minimal() +\n  scale_color_brewer(palette = \"Pastel2\")\n\n\n\n\n\n\n\n\nThose with no college also appear to have lower mental QOL.\nReally it appears that now, since we are plotting and modelling data longitudinally instead of with change score, EDUC_COLLEGE should be included in every model as a precision variable.\nTop of Tabset"
  },
  {
    "objectID": "Project_2/Project_2_R/Code/Project2LMM.html#Viral",
    "href": "Project_2/Project_2_R/Code/Project2LMM.html#Viral",
    "title": "Linear Mixed Model",
    "section": "Log Viral Load",
    "text": "Log Viral Load\nBased on the previous interactive model selection in Project 2, the final covariates for this model were hard_drugs_grp, and ADH, and EDUC_COLLEGE\n\nModel SelectionAnalysisInterpretationVisualization\n\n\nTo answer the researcher’s main question, we will run a model of hard_drugs_grp, ADH_HIGHVSLOW, years, and an interaction between hard_drugs_grp*years to see if the slopes differ based on hard drug usage.\nNote: An unstructured covariance matrix is not needed here since we are only using random intercepts.\n\n# Ensure 'hard_drugs_grp' is a factor\ndata_2$hard_drugs_grp &lt;- as.factor(data_2$hard_drugs_grp)\n\n# Relevel the factor, setting \"Never User\" as the reference level\ndata_2$hard_drugs_grp &lt;- relevel(data_2$hard_drugs_grp, ref = \"Never User\")\n\n# Create LMM\nmodel_VLOAD1 &lt;- lmer(VLOAD_log ~ hard_drugs_grp + ADH_HIGHVSLOW + years + hard_drugs_grp*years + (1|newid), data = data_2)\n\n# Get model summary\nsummary(model_VLOAD1)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: VLOAD_log ~ hard_drugs_grp + ADH_HIGHVSLOW + years + hard_drugs_grp *  \n    years + (1 | newid)\n   Data: data_2\n\nREML criterion at convergence: 7864.3\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-2.86075 -0.55033  0.02914  0.59685  2.85626 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n newid    (Intercept) 2.856    1.690   \n Residual             5.883    2.425   \nNumber of obs: 1601, groups:  newid, 541\n\nFixed effects:\n                                    Estimate Std. Error         df t value\n(Intercept)                          9.81995    0.13770 1112.94700  71.315\nhard_drugs_grpCurrent User          -0.42074    0.38385 1154.65848  -1.096\nhard_drugs_grpPrevious User         -0.69690    0.43200 1154.95449  -1.613\nADH_HIGHVSLOWLow Adherence           0.43019    0.31315  525.18166   1.374\nyears                               -3.13791    0.08277 1054.68010 -37.910\nhard_drugs_grpCurrent User:years     0.39401    0.23638 1050.22268   1.667\nhard_drugs_grpPrevious User:years    0.48772    0.26608 1050.09154   1.833\n                                             Pr(&gt;|t|)    \n(Intercept)                       &lt;0.0000000000000002 ***\nhard_drugs_grpCurrent User                     0.2733    \nhard_drugs_grpPrevious User                    0.1070    \nADH_HIGHVSLOWLow Adherence                     0.1701    \nyears                             &lt;0.0000000000000002 ***\nhard_drugs_grpCurrent User:years               0.0958 .  \nhard_drugs_grpPrevious User:years              0.0671 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) hr__CU hr__PU ADH_HA years  h__CU:\nhrd_drgs_CU -0.346                                   \nhrd_drgs_PU -0.296  0.107                            \nADH_HIGHVSA -0.236  0.030 -0.019                     \nyears       -0.596  0.214  0.190  0.000              \nhrd_drg_CU:  0.209 -0.615 -0.066  0.000 -0.350       \nhrd_drg_PU:  0.185 -0.066 -0.615  0.000 -0.311  0.109\n\n\nChange reference level\n\n# Ensure 'hard_drugs_grp' is a factor\ndata_2$hard_drugs_grp &lt;- as.factor(data_2$hard_drugs_grp)\n\n# Relevel the factor, setting \"Never User\" as the reference level\ndata_2$hard_drugs_grp &lt;- relevel(data_2$hard_drugs_grp, ref = \"Previous User\")\n\n# Create LMM\nmodel_VLOAD1 &lt;- lmer(VLOAD_log ~ hard_drugs_grp + ADH_HIGHVSLOW + years + hard_drugs_grp*years + (1|newid), data = data_2)\n\n# Get model summary\nsummary(model_VLOAD1)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: VLOAD_log ~ hard_drugs_grp + ADH_HIGHVSLOW + years + hard_drugs_grp *  \n    years + (1 | newid)\n   Data: data_2\n\nREML criterion at convergence: 7864.3\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-2.86075 -0.55033  0.02914  0.59685  2.85626 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n newid    (Intercept) 2.856    1.690   \n Residual             5.883    2.425   \nNumber of obs: 1601, groups:  newid, 541\n\nFixed effects:\n                                  Estimate Std. Error        df t value\n(Intercept)                         9.1230     0.4127 1146.7510  22.105\nhard_drugs_grpNever User            0.6969     0.4320 1154.9545   1.613\nhard_drugs_grpCurrent User          0.2762     0.5462 1153.8155   0.506\nADH_HIGHVSLOWLow Adherence          0.4302     0.3131  525.1817   1.374\nyears                              -2.6502     0.2529 1049.6005 -10.480\nhard_drugs_grpNever User:years     -0.4877     0.2661 1050.0915  -1.833\nhard_drugs_grpCurrent User:years   -0.0937     0.3361 1049.6005  -0.279\n                                            Pr(&gt;|t|)    \n(Intercept)                      &lt;0.0000000000000002 ***\nhard_drugs_grpNever User                      0.1070    \nhard_drugs_grpCurrent User                    0.6132    \nADH_HIGHVSLOWLow Adherence                    0.1701    \nyears                            &lt;0.0000000000000002 ***\nhard_drugs_grpNever User:years                0.0671 .  \nhard_drugs_grpCurrent User:years              0.7805    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) hr__NU hr__CU ADH_HA years  h__NU:\nhrd_drgs_NU -0.948                                   \nhrd_drgs_CU -0.752  0.715                            \nADH_HIGHVSA -0.099  0.019  0.037                     \nyears       -0.613  0.585  0.463  0.000              \nhrd_drg_NU:  0.582 -0.615 -0.440  0.000 -0.950       \nhrd_drg_CU:  0.461 -0.440 -0.615  0.000 -0.752  0.715\n\n\nThe interaction is not significant. Let’s run the model with out it.\n\n# Relevel the factor, setting \"Never User\" as the reference level\ndata_2$hard_drugs_grp &lt;- relevel(data_2$hard_drugs_grp, ref = \"Never User\")\n\n# Create LMM\nmodel_VLOAD2 &lt;- lmer(VLOAD_log ~ hard_drugs_grp + ADH_HIGHVSLOW + years + (1|newid), data = data_2)\n\n# Get model summary\nsummary(model_VLOAD2)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: VLOAD_log ~ hard_drugs_grp + ADH_HIGHVSLOW + years + (1 | newid)\n   Data: data_2\n\nREML criterion at convergence: 7867.9\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-2.82180 -0.55906  0.01816  0.58112  2.81516 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n newid    (Intercept) 2.850    1.688   \n Residual             5.902    2.429   \nNumber of obs: 1601, groups:  newid, 541\n\nFixed effects:\n                              Estimate Std. Error         df t value\n(Intercept)                    9.73462    0.13285 1010.37558  73.277\nhard_drugs_grpPrevious User   -0.20992    0.34055  522.69614  -0.616\nhard_drugs_grpCurrent User    -0.02746    0.30267  522.94637  -0.091\nADH_HIGHVSLOWLow Adherence     0.43023    0.31318  525.10679   1.374\nyears                         -3.05185    0.07425 1055.62327 -41.105\n                                       Pr(&gt;|t|)    \n(Intercept)                 &lt;0.0000000000000002 ***\nhard_drugs_grpPrevious User               0.538    \nhard_drugs_grpCurrent User                0.928    \nADH_HIGHVSLOWLow Adherence                0.170    \nyears                       &lt;0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) hr__PU hr__CU ADH_HA\nhrd_drgs_PU -0.240                     \nhrd_drgs_CU -0.286  0.107              \nADH_HIGHVSA -0.245 -0.024  0.039       \nyears       -0.554 -0.002 -0.002  0.000\n\n\nThat looks pretty good, however, we did see that high adherence vs low adherence had differing slopes when we plotted them. Let’s include an interaction term for adherence.\n\n# Relevel the factor, setting \"Never User\" as the reference level\ndata_2$hard_drugs_grp &lt;- relevel(data_2$hard_drugs_grp, ref = \"Never User\")\n\n# Create LMM\nmodel_VLOAD3 &lt;- lmer(VLOAD_log ~ hard_drugs_grp + ADH_HIGHVSLOW + years + ADH_HIGHVSLOW*years + (1|newid), data = data_2)\n\n# Get model summary\nsummary(model_VLOAD3)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: VLOAD_log ~ hard_drugs_grp + ADH_HIGHVSLOW + years + ADH_HIGHVSLOW *  \n    years + (1 | newid)\n   Data: data_2\n\nREML criterion at convergence: 7852.6\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-2.77922 -0.55214  0.02611  0.58116  2.47095 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n newid    (Intercept) 2.882    1.698   \n Residual             5.816    2.412   \nNumber of obs: 1601, groups:  newid, 541\n\nFixed effects:\n                                   Estimate Std. Error         df t value\n(Intercept)                         9.83455    0.13483 1052.37684  72.938\nhard_drugs_grpPrevious User        -0.21036    0.34066  522.78164  -0.618\nhard_drugs_grpCurrent User         -0.02743    0.30276  523.02950  -0.091\nADH_HIGHVSLOWLow Adherence         -0.55205    0.39593 1148.27523  -1.394\nyears                              -3.15231    0.07776 1054.58618 -40.541\nADH_HIGHVSLOWLow Adherence:years    0.98968    0.24398 1054.61543   4.056\n                                             Pr(&gt;|t|)    \n(Intercept)                      &lt; 0.0000000000000002 ***\nhard_drugs_grpPrevious User                     0.537    \nhard_drugs_grpCurrent User                      0.928    \nADH_HIGHVSLOWLow Adherence                      0.163    \nyears                            &lt; 0.0000000000000002 ***\nADH_HIGHVSLOWLow Adherence:years            0.0000535 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n                 (Intr) hr__PU hr__CU ADH_HIGHVSLOWLwA years \nhrd_drgs_PU      -0.236                                      \nhrd_drgs_CU      -0.282  0.107                               \nADH_HIGHVSLOWLwA -0.303 -0.019  0.030                        \nyears            -0.572 -0.002 -0.002  0.195                 \nADH_HIGHVSLOWLA:  0.182  0.000  0.000 -0.612           -0.319\n\n\nWe have a significant interaciton. Let’s perform model selection based on AIC and BIC.\n\n# Model selection with AIC and BIC\nAIC(model_VLOAD1, model_VLOAD2, model_VLOAD3)\n\n             df      AIC\nmodel_VLOAD1  9 7882.253\nmodel_VLOAD2  7 7881.917\nmodel_VLOAD3  8 7868.561\n\nBIC(model_VLOAD1, model_VLOAD2, model_VLOAD3)\n\n             df      BIC\nmodel_VLOAD1  9 7930.659\nmodel_VLOAD2  7 7919.566\nmodel_VLOAD3  8 7911.588\n\n\nAIC and BIC both prefer model 3, with the ADH_HIGHVSLOW*years interaction. Thus that is our final model.\nTop of Tabset\n\n\nBased on model selection, the final model for log viral load includes hard_drugs_grp, ADH_HIGHVSLOW, years, and an interaction between ADH_HIGHVSLOW*years.\n\n# Relevel the factor, setting \"Never User\" as the reference level\ndata_2$hard_drugs_grp &lt;- relevel(data_2$hard_drugs_grp, ref = \"Never User\")\n\n# Create LMM\nmodel_VLOAD1 &lt;- lmer(VLOAD_log ~ hard_drugs_grp + ADH_HIGHVSLOW + years + ADH_HIGHVSLOW*years + (1|newid), data = data_2)\n\n# Get model summary\nsummary(model_VLOAD1)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: VLOAD_log ~ hard_drugs_grp + ADH_HIGHVSLOW + years + ADH_HIGHVSLOW *  \n    years + (1 | newid)\n   Data: data_2\n\nREML criterion at convergence: 7852.6\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-2.77922 -0.55214  0.02611  0.58116  2.47095 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n newid    (Intercept) 2.882    1.698   \n Residual             5.816    2.412   \nNumber of obs: 1601, groups:  newid, 541\n\nFixed effects:\n                                   Estimate Std. Error         df t value\n(Intercept)                         9.83455    0.13483 1052.37684  72.938\nhard_drugs_grpPrevious User        -0.21036    0.34066  522.78164  -0.618\nhard_drugs_grpCurrent User         -0.02743    0.30276  523.02950  -0.091\nADH_HIGHVSLOWLow Adherence         -0.55205    0.39593 1148.27523  -1.394\nyears                              -3.15231    0.07776 1054.58618 -40.541\nADH_HIGHVSLOWLow Adherence:years    0.98968    0.24398 1054.61543   4.056\n                                             Pr(&gt;|t|)    \n(Intercept)                      &lt; 0.0000000000000002 ***\nhard_drugs_grpPrevious User                     0.537    \nhard_drugs_grpCurrent User                      0.928    \nADH_HIGHVSLOWLow Adherence                      0.163    \nyears                            &lt; 0.0000000000000002 ***\nADH_HIGHVSLOWLow Adherence:years            0.0000535 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n                 (Intr) hr__PU hr__CU ADH_HIGHVSLOWLwA years \nhrd_drgs_PU      -0.236                                      \nhrd_drgs_CU      -0.282  0.107                               \nADH_HIGHVSLOWLwA -0.303 -0.019  0.030                        \nyears            -0.572 -0.002 -0.002  0.195                 \nADH_HIGHVSLOWLA:  0.182  0.000  0.000 -0.612           -0.319\n\npretty_print(confint(model_VLOAD1))\n\nComputing profile confidence intervals ...\n\n\n\n\n\n\n2.5 %\n97.5 %\n\n\n\n\n.sig01\n1.5112544\n1.8703721\n\n\n.sigma\n2.3099745\n2.5160618\n\n\n(Intercept)\n9.5707668\n10.0982986\n\n\nhard_drugs_grpPrevious User\n-0.8767521\n0.4559816\n\n\nhard_drugs_grpCurrent User\n-0.6196802\n0.5647824\n\n\nADH_HIGHVSLOWLow Adherence\n-1.3265854\n0.2225392\n\n\nyears\n-3.3047431\n-2.9999461\n\n\nADH_HIGHVSLOWLow Adherence:years\n0.5114453\n1.4678017\n\n\n\n\n\n\n\nAfter controlling for adherence and between subject means (intercepts), there was no difference in average log viral load for current (p = 0.93, 95% CI: -0.62, 0.56) and previous (p = 0.54, 95% CI: -0.88, 0.46) hard drug users compared to never users.\nAfter controlling for hard drug use and between subject means, those with low adherence did not differ significantly in average log viral load compared to those with high adherence (p = 0.16, 95% CI: -1.32, 0.22).\nThere was a significant decrease in log viral load over time while controlling for subject-specific means, hard drug use, and adherence, with an average decrease of 3.15 log copies/mL per year (p &lt; 0.0001, 95% CI: -3.30, -3.00).\nSubject ID contributes greatly to log viral load, as it accounts for 2.882 / (2.882 + 5.816) = 0.331 or 33.1% of the variance in log viral load.\nThe slope for viral load over time differed significantly between those with low and high adherence. Those with low adherence had on average a 0.99 increase in log viral load each year compared to those with high adherence (p &lt; 0.0001, 95% CI: 0.51, 1.47).\n\nChange Reference Group\n\n# Relevel the factor, setting \"Never User\" as the reference level\ndata_2$hard_drugs_grp &lt;- relevel(data_2$hard_drugs_grp, ref = \"Previous User\")\n\n# Create LMM\nmodel_VLOAD1 &lt;- lmer(VLOAD_log ~ hard_drugs_grp + ADH_HIGHVSLOW + years + ADH_HIGHVSLOW*years + (1|newid), data = data_2)\n\n# Get model summary\nsummary(model_VLOAD1)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: VLOAD_log ~ hard_drugs_grp + ADH_HIGHVSLOW + years + ADH_HIGHVSLOW *  \n    years + (1 | newid)\n   Data: data_2\n\nREML criterion at convergence: 7852.6\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-2.77922 -0.55214  0.02611  0.58116  2.47095 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n newid    (Intercept) 2.882    1.698   \n Residual             5.816    2.412   \nNumber of obs: 1601, groups:  newid, 541\n\nFixed effects:\n                                   Estimate Std. Error         df t value\n(Intercept)                         9.62419    0.33541  582.65751  28.694\nhard_drugs_grpNever User            0.21036    0.34066  522.78172   0.618\nhard_drugs_grpCurrent User          0.18294    0.43075  522.08749   0.425\nADH_HIGHVSLOWLow Adherence         -0.55205    0.39593 1148.27547  -1.394\nyears                              -3.15231    0.07776 1054.58612 -40.541\nADH_HIGHVSLOWLow Adherence:years    0.98968    0.24398 1054.61537   4.056\n                                             Pr(&gt;|t|)    \n(Intercept)                      &lt; 0.0000000000000002 ***\nhard_drugs_grpNever User                        0.537    \nhard_drugs_grpCurrent User                      0.671    \nADH_HIGHVSLOWLow Adherence                      0.163    \nyears                            &lt; 0.0000000000000002 ***\nADH_HIGHVSLOWLow Adherence:years            0.0000535 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n                 (Intr) hr__NU hr__CU ADH_HIGHVSLOWLwA years \nhrd_drgs_NU      -0.921                                      \nhrd_drgs_CU      -0.731  0.715                               \nADH_HIGHVSLOWLwA -0.141  0.019  0.037                        \nyears            -0.232  0.002  0.000  0.195                 \nADH_HIGHVSLOWLA:  0.073  0.000  0.000 -0.612           -0.319\n\npretty_print(confint(model_VLOAD1))\n\nComputing profile confidence intervals ...\n\n\n\n\n\n\n2.5 %\n97.5 %\n\n\n\n\n.sig01\n1.5112544\n1.8703721\n\n\n.sigma\n2.3099745\n2.5160618\n\n\n(Intercept)\n8.9680979\n10.2802687\n\n\nhard_drugs_grpNever User\n-0.4559816\n0.8767521\n\n\nhard_drugs_grpCurrent User\n-0.6596545\n1.0255272\n\n\nADH_HIGHVSLOWLow Adherence\n-1.3265854\n0.2225392\n\n\nyears\n-3.3047431\n-2.9999461\n\n\nADH_HIGHVSLOWLow Adherence:years\n0.5114453\n1.4678017\n\n\n\n\n\n\n\nPrevious hard drug users did not differ significantly in average log viral load compared to current users (p = 0.67, 95% CI: -0.66, 1.03).\nTop of Tabset\n\n\n\n\nHard Drug Use Group\nAfter controlling for adherence and between subject means (intercepts), there was no difference in average log viral load for current (p = 0.93, 95% CI: -0.62, 0.56) and previous (p = 0.54, 95% CI: -0.88, 0.46) hard drug users compared to never users. Previous hard drug users did not differ significantly in average log viral load compared to current users (p = 0.67, 95% CI: -0.66, 1.03).\n\n\nAdherence\nAfter controlling for hard drug use and between subject means, those with low adherence did not differ significantly in average log viral load compared to those with high adherence (p = 0.16, 95% CI: -1.32, 0.22).\n\n\nTime\nThere was a significant decrease in log viral load over time while controlling for subject-specific means, hard drug use, and adherence, with an average decrease of 3.15 log copies/mL per year (p &lt; 0.0001, 95% CI: -3.30, -3.00).\n\n\nViral Load over Time by Adherence\nThe slope for viral load over time differed significantly between those with low and high adherence. Those with low adherence had on average a 0.99 increase in log viral load each year compared to those with high adherence (p &lt; 0.0001, 95% CI: 0.51, 1.47). The other in group comparisons were not significant (p &gt; 0.05).\n\n\nWithin Subject Means\nSubject ID contributes greatly to log viral load, as it accounts for 2.882 / (2.882 + 5.816) = 0.331 or 33.1% of the variance in log viral load.\nTop of Tabset\n\n\n\nWe can plot the predicted values of our model in order to visualize the interaction and understand it better.\n\n# Ungroup data_2\ndata_2 &lt;- data_2 |&gt; \n  ungroup()\n\n# Prepare data for plotting\ndata_2_VLOAD &lt;- data_2 |&gt;\n  select(newid, VLOAD_log, ADH_HIGHVSLOW, hard_drugs_grp, years) %&gt;%\n  filter(complete.cases(.)) |&gt; \n  mutate(fitted = predict(model_VLOAD3)) |&gt; \n  group_by(newid)\n\n# Plot the interaction\nggplot(data_2_VLOAD, aes(x = years, y = fitted, color = ADH_HIGHVSLOW)) +\n  geom_smooth(method = \"lm\", se = FALSE) + \n  labs(title = \"Predicted Log Viral Load over Years with Interaction by Adherence\",\n       x = \"Year\",\n       y = \"Predicted Log Viral Load\") +\n  theme_minimal() +\n  scale_color_brewer(palette = \"Pastel2\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nHere we can see that those with high adherence had a greater decrease in log viral load over time compared to those who had high adherence.\nWe can also see that the slopes over time were not different based on hard drug use.\n\n# Plot the interaction\nggplot(data_2_VLOAD, aes(x = years, y = fitted, color = hard_drugs_grp)) +\n  geom_smooth(method = \"lm\", se = FALSE) + \n  labs(title = \"Predicted Log Viral Load over Years by Hard Drug Use\",\n       x = \"Year\",\n       y = \"Predicted Log Viral Load\") +\n  theme_minimal() +\n  scale_color_brewer(palette = \"Pastel2\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nTop of Tabset"
  },
  {
    "objectID": "Project_2/Project_2_R/Code/Project2LMM.html#CD4",
    "href": "Project_2/Project_2_R/Code/Project2LMM.html#CD4",
    "title": "Linear Mixed Model",
    "section": "CD4+ T Cell Count",
    "text": "CD4+ T Cell Count\n\nModel SelectionAnalysisInterpretationVisualization\n\n\nFirst let us fit the full model of interest using a structured covariance matrix, and hard_drugs_grp, ADH_HIGHVSLOW, years, and the hard_drugs_grp*years interaction term.\n\n# Filter data set\ndata_LEU3N &lt;- data_2 |&gt; \n  select(newid, years, hard_drugs_grp, ADH_HIGHVSLOW, LEU3N) %&gt;% \n  filter(complete.cases(.))\n\n# Create LMM\nmodel_LEU3N1 &lt;- lme(LEU3N ~ hard_drugs_grp + ADH_HIGHVSLOW + years + hard_drugs_grp*years, \n                   random = list(newid = pdSymm(~ 1)), \n                   data = data_LEU3N)\n\n# Get model summary\nsummary(model_LEU3N1)\n\nLinear mixed-effects model fit by REML\n  Data: data_LEU3N \n       AIC      BIC    logLik\n  21085.29 21133.68 -10533.64\n\nRandom effects:\n Formula: ~1 | newid\n        (Intercept) Residual\nStdDev:    214.9289 115.5901\n\nFixed effects:  LEU3N ~ hard_drugs_grp + ADH_HIGHVSLOW + years + hard_drugs_grp *      years \n                                    Value Std.Error   DF   t-value p-value\n(Intercept)                      333.6108  35.54933 1061  9.384449  0.0000\nhard_drugs_grpNever User          47.3576  37.13030  538  1.275443  0.2027\nhard_drugs_grpCurrent User       163.7339  46.96733  538  3.486124  0.0005\nADH_HIGHVSLOWLow Adherence       -33.6629  32.05338  538 -1.050215  0.2941\nyears                             57.1508  12.05110 1061  4.742373  0.0000\nhard_drugs_grpNever User:years    32.3086  12.68127 1061  2.547745  0.0110\nhard_drugs_grpCurrent User:years -27.0024  16.01784 1061 -1.685768  0.0921\n Correlation: \n                                 (Intr) hr__NU hr__CU ADH_HA years  h__NU:\nhard_drugs_grpNever User         -0.947                                   \nhard_drugs_grpCurrent User       -0.752  0.716                            \nADH_HIGHVSLOWLow Adherence       -0.118  0.023  0.044                     \nyears                            -0.339  0.325  0.257  0.000              \nhard_drugs_grpNever User:years    0.322 -0.341 -0.244  0.000 -0.950       \nhard_drugs_grpCurrent User:years  0.255 -0.244 -0.341  0.000 -0.752  0.715\n\nStandardized Within-Group Residuals:\n       Min         Q1        Med         Q3        Max \n-3.9360359 -0.4855393 -0.0484215  0.4241892  5.4905529 \n\nNumber of Observations: 1606\nNumber of Groups: 542 \n\n\nThen let us run the same model but with an unstructured covariance matrix.\n\n# Create LMM\nmodel_LEU3N2 &lt;- lme(LEU3N ~ hard_drugs_grp + ADH_HIGHVSLOW + years + hard_drugs_grp*years, \n                   random = list(newid = pdSymm(~ years)), \n                   data = data_LEU3N)\n\n# Get model summary\nsummary(model_LEU3N2)\n\nLinear mixed-effects model fit by REML\n  Data: data_LEU3N \n       AIC      BIC    logLik\n  20935.06 20994.21 -10456.53\n\nRandom effects:\n Formula: ~years | newid\n Structure: General positive-definite\n            StdDev    Corr  \n(Intercept) 182.03482 (Intr)\nyears        60.68956 0.49  \nResidual     98.31914       \n\nFixed effects:  LEU3N ~ hard_drugs_grp + ADH_HIGHVSLOW + years + hard_drugs_grp *      years \n                                    Value Std.Error   DF   t-value p-value\n(Intercept)                      331.7197  30.16098 1061 10.998307  0.0000\nhard_drugs_grpNever User          47.7472  31.47512  538  1.516984  0.1299\nhard_drugs_grpCurrent User       164.6585  39.81726  538  4.135354  0.0000\nADH_HIGHVSLOWLow Adherence       -19.1640  28.89178  538 -0.663304  0.5074\nyears                             57.1508  13.60670 1061  4.200195  0.0000\nhard_drugs_grpNever User:years    32.1522  14.31691 1061  2.245750  0.0249\nhard_drugs_grpCurrent User:years -27.0024  18.08549 1061 -1.493040  0.1357\n Correlation: \n                                 (Intr) hr__NU hr__CU ADH_HA years  h__NU:\nhard_drugs_grpNever User         -0.946                                   \nhard_drugs_grpCurrent User       -0.751  0.716                            \nADH_HIGHVSLOWLow Adherence       -0.125  0.025  0.046                     \nyears                             0.031 -0.029 -0.023  0.000              \nhard_drugs_grpNever User:years   -0.029  0.031  0.022  0.000 -0.950       \nhard_drugs_grpCurrent User:years -0.023  0.022  0.031  0.000 -0.752  0.715\n\nStandardized Within-Group Residuals:\n        Min          Q1         Med          Q3         Max \n-3.01629771 -0.44050109 -0.04862548  0.37263572  4.08134359 \n\nNumber of Observations: 1606\nNumber of Groups: 542 \n\n\nAIC, BIC, and -log likelihood all prefer the unstructured covariance matrix is better, and thus we select it as our final model.\nTop of Tabset\n\n\nHere we perform the analysis with the final selected model, using an unstructured covariance matrix.\n\n# Create LMM\nmodel_LEU3N2 &lt;- lme(LEU3N ~ hard_drugs_grp + ADH_HIGHVSLOW + years + hard_drugs_grp*years, \n                   random = list(newid = pdSymm(~ years)), \n                   data = data_LEU3N)\n\n# Get model summary\nsummary(model_LEU3N2)\n\nLinear mixed-effects model fit by REML\n  Data: data_LEU3N \n       AIC      BIC    logLik\n  20935.06 20994.21 -10456.53\n\nRandom effects:\n Formula: ~years | newid\n Structure: General positive-definite\n            StdDev    Corr  \n(Intercept) 182.03482 (Intr)\nyears        60.68956 0.49  \nResidual     98.31914       \n\nFixed effects:  LEU3N ~ hard_drugs_grp + ADH_HIGHVSLOW + years + hard_drugs_grp *      years \n                                    Value Std.Error   DF   t-value p-value\n(Intercept)                      331.7197  30.16098 1061 10.998307  0.0000\nhard_drugs_grpNever User          47.7472  31.47512  538  1.516984  0.1299\nhard_drugs_grpCurrent User       164.6585  39.81726  538  4.135354  0.0000\nADH_HIGHVSLOWLow Adherence       -19.1640  28.89178  538 -0.663304  0.5074\nyears                             57.1508  13.60670 1061  4.200195  0.0000\nhard_drugs_grpNever User:years    32.1522  14.31691 1061  2.245750  0.0249\nhard_drugs_grpCurrent User:years -27.0024  18.08549 1061 -1.493040  0.1357\n Correlation: \n                                 (Intr) hr__NU hr__CU ADH_HA years  h__NU:\nhard_drugs_grpNever User         -0.946                                   \nhard_drugs_grpCurrent User       -0.751  0.716                            \nADH_HIGHVSLOWLow Adherence       -0.125  0.025  0.046                     \nyears                             0.031 -0.029 -0.023  0.000              \nhard_drugs_grpNever User:years   -0.029  0.031  0.022  0.000 -0.950       \nhard_drugs_grpCurrent User:years -0.023  0.022  0.031  0.000 -0.752  0.715\n\nStandardized Within-Group Residuals:\n        Min          Q1         Med          Q3         Max \n-3.01629771 -0.44050109 -0.04862548  0.37263572  4.08134359 \n\nNumber of Observations: 1606\nNumber of Groups: 542 \n\n# Get 95% CIs\nintervals(model_LEU3N2, level = 0.95)\n\nApproximate 95% confidence intervals\n\n Fixed effects:\n                                      lower      est.      upper\n(Intercept)                      272.537742 331.71968 390.901628\nhard_drugs_grpNever User         -14.081955  47.74724 109.576442\nhard_drugs_grpCurrent User        86.442127 164.65849 242.874849\nADH_HIGHVSLOWLow Adherence       -75.918561 -19.16404  37.590486\nyears                             30.451702  57.15081  83.849913\nhard_drugs_grpNever User:years     4.059531  32.15220  60.244878\nhard_drugs_grpCurrent User:years -62.489748 -27.00236   8.485025\n\n Random Effects:\n  Level: newid \n                             lower        est.       upper\nsd((Intercept))        168.7261224 182.0348208 196.3932763\nsd(years)               51.7123748  60.6895579  71.2251652\ncor((Intercept),years)   0.2792255   0.4900844   0.6558527\n\n Within-group standard error:\n    lower      est.     upper \n 92.55578  98.31914 104.44138 \n\n\nAfter controlling for adherence and between subject means (intercepts), there was no difference in average CD4+ T Cell count for previous users compared to never hard drug users (p = 0.13, 95% CI: -109.54, 14.08). However, current users had on average a 116.91 cell higher CD4+ T Cell count compared to never users (p &lt; 0.0001, 95% CI: 61.97, 171.85).\nAfter controlling for hard drug use and between subject means, those with low adherence did not differ significantly in average CD4+ T Cell count compared to those with high adherence (p = 0.51, 95% CI: -75.92, 37.59).\nThere was a significant increase in CD4+ T Cell count over time while controlling for subject-specific means, hard drug use, and adherence, with an average increase of 89.30 log copies/mL per year (p &lt; 0.0001, 95% CI: -3.30, -3.00).\nThe slope for CD4+ T Cell count over time differed significantly based on hard drug use. On average, previous hard drug users had a 32.15 decrease in CD4+ T Cell count per year compared to never drug users (p = 0.025, 95% CI: -60.24, -4.06), and current users had a 59.15 decrease in CD4+ T Cell count each year compared to never hard drug users (p &lt; 0.0001, 95% CI: -84.11, -34.20).\n\nChange the Reference Group\n\n# Change reference level\ndata_LEU3N$hard_drugs_grp &lt;- relevel(data_LEU3N$hard_drugs_grp, ref = \"Previous User\")\n\n# Create LMM\nmodel_LEU3N2 &lt;- lme(LEU3N ~ hard_drugs_grp + ADH_HIGHVSLOW + years + hard_drugs_grp*years, \n                   random = list(newid = pdSymm(~ years)), \n                   data = data_LEU3N)\n\n# Get model summary\nsummary(model_LEU3N2)\n\nLinear mixed-effects model fit by REML\n  Data: data_LEU3N \n       AIC      BIC    logLik\n  20935.06 20994.21 -10456.53\n\nRandom effects:\n Formula: ~years | newid\n Structure: General positive-definite\n            StdDev    Corr  \n(Intercept) 182.03482 (Intr)\nyears        60.68956 0.49  \nResidual     98.31914       \n\nFixed effects:  LEU3N ~ hard_drugs_grp + ADH_HIGHVSLOW + years + hard_drugs_grp *      years \n                                    Value Std.Error   DF   t-value p-value\n(Intercept)                      331.7197  30.16098 1061 10.998307  0.0000\nhard_drugs_grpNever User          47.7472  31.47512  538  1.516984  0.1299\nhard_drugs_grpCurrent User       164.6585  39.81726  538  4.135354  0.0000\nADH_HIGHVSLOWLow Adherence       -19.1640  28.89178  538 -0.663304  0.5074\nyears                             57.1508  13.60670 1061  4.200195  0.0000\nhard_drugs_grpNever User:years    32.1522  14.31691 1061  2.245750  0.0249\nhard_drugs_grpCurrent User:years -27.0024  18.08549 1061 -1.493040  0.1357\n Correlation: \n                                 (Intr) hr__NU hr__CU ADH_HA years  h__NU:\nhard_drugs_grpNever User         -0.946                                   \nhard_drugs_grpCurrent User       -0.751  0.716                            \nADH_HIGHVSLOWLow Adherence       -0.125  0.025  0.046                     \nyears                             0.031 -0.029 -0.023  0.000              \nhard_drugs_grpNever User:years   -0.029  0.031  0.022  0.000 -0.950       \nhard_drugs_grpCurrent User:years -0.023  0.022  0.031  0.000 -0.752  0.715\n\nStandardized Within-Group Residuals:\n        Min          Q1         Med          Q3         Max \n-3.01629771 -0.44050109 -0.04862548  0.37263572  4.08134359 \n\nNumber of Observations: 1606\nNumber of Groups: 542 \n\n# Get 95% CIs\nintervals(model_LEU3N2, level = 0.95)\n\nApproximate 95% confidence intervals\n\n Fixed effects:\n                                      lower      est.      upper\n(Intercept)                      272.537742 331.71968 390.901628\nhard_drugs_grpNever User         -14.081955  47.74724 109.576442\nhard_drugs_grpCurrent User        86.442127 164.65849 242.874849\nADH_HIGHVSLOWLow Adherence       -75.918561 -19.16404  37.590486\nyears                             30.451702  57.15081  83.849913\nhard_drugs_grpNever User:years     4.059531  32.15220  60.244878\nhard_drugs_grpCurrent User:years -62.489748 -27.00236   8.485025\n\n Random Effects:\n  Level: newid \n                             lower        est.       upper\nsd((Intercept))        168.7261224 182.0348208 196.3932763\nsd(years)               51.7123748  60.6895579  71.2251652\ncor((Intercept),years)   0.2792255   0.4900844   0.6558527\n\n Within-group standard error:\n    lower      est.     upper \n 92.55578  98.31914 104.44138 \n\n\nPrevious hard drug users had on average a 164.66 lower CD4+ T Cell count compared to current users (p &lt; 0.0001, 95% CI: 86.44, 242.87).\nThe slope for CD4+ T Cell count over time did not differ between previous and current hard drug users (p = 0.14, 95% CI: -62.49, 8.49).\nTop of Tabset\n\n\n\n\nHard Drug Use\nAfter controlling for adherence and between subject means (intercepts), there was no difference in average CD4+ T Cell count for previous users compared to never hard drug users (p = 0.13, 95% CI: -109.54, 14.08). However, current users had on average a 116.91 cell higher CD4+ T Cell count compared to never users (p &lt; 0.0001, 95% CI: 61.97, 171.85). Previous hard drug users had on average a 164.66 higher CD4+ T Cell count compared to current users (p &lt; 0.0001, 95% CI: 86.44, 242.87).\n\n\nAdherence\nAfter controlling for hard drug use and between subject means, those with low adherence did not differ significantly in average CD4+ T Cell count compared to those with high adherence (p = 0.51, 95% CI: -75.92, 37.59).\n\n\nCD4+ T Cell Count by Hard Drug Use\nThe slope for CD4+ T Cell count over time differed significantly based on hard drug use. On average, previous hard drug users had a 32.15 decrease in CD4+ T Cell count per year compared to never drug users (p = 0.025, 95% CI: -60.24, -4.06), and current users had a 59.15 decrease in CD4+ T Cell count each year compared to never hard drug users (p &lt; 0.0001, 95% CI: -84.11, -34.20). The slope for CD4+ T Cell count over time did not differ between previous and current hard drug users (p = 0.14, 95% CI: -62.49, 8.49).\n\n\nTime\nThere was a significant increase in CD4+ T Cell count over time while controlling for subject-specific means, hard drug use, and adherence, with an average increase of 89.30 log copies/mL per year (p &lt; 0.0001, 95% CI: -3.30, -3.00).\nTop of Tabset\n\n\n\n\n# Prepare data for plotting\ndata_LEU3N &lt;- data_LEU3N |&gt;\n  mutate(fitted = predict(model_LEU3N1))\n\n# Plot the interaction\nggplot(data_LEU3N, aes(x = years, y = fitted, color = hard_drugs_grp)) +\n  geom_smooth(method = \"lm\", se = FALSE) + \n  labs(title = \"Predicted CD4+ T Cell Coount over Years by Hard Drug Use\",\n       x = \"Year\",\n       y = \"Predicted CD4+ T Cell Count\") +\n  theme_minimal() +\n  scale_color_brewer(palette = \"Pastel2\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nHere we see the interaction base on hard drug use. Never drug users had a higher increase in CD4+ T Cell count each year compared to previous and current hard drug users.\n\n# Prepare data for plotting\ndata_LEU3N &lt;- data_LEU3N |&gt;\n  mutate(fitted = predict(model_LEU3N1))\n\n# Plot the interaction\nggplot(data_LEU3N, aes(x = years, y = fitted, color = ADH_HIGHVSLOW)) +\n  geom_smooth(method = \"lm\", se = FALSE) + \n  labs(title = \"Predicted CD4+ T Cell Coount over Years by Hard Drug Use\",\n       x = \"Year\",\n       y = \"Predicted CD4+ T Cell Count\") +\n  theme_minimal() +\n  scale_color_brewer(palette = \"Pastel2\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nLikewise, here we see that thosewith low adherence had lower CD4+ T Cell count compared to those with high adherence, but the differences over time did not differ.\nTop of Tabset"
  },
  {
    "objectID": "Project_2/Project_2_R/Code/Project2LMM.html#Ment",
    "href": "Project_2/Project_2_R/Code/Project2LMM.html#Ment",
    "title": "Linear Mixed Model",
    "section": "Mental QOL",
    "text": "Mental QOL\nWe will be fitting the full model with hard_drugs_grp, ADH_HIGHVSLOW, years, CESD, and the hard_drugs_grp*years interaction.\nWe will also use an unstructured covariance matrix to account for unequal variances.\nThis can be visualized here.\n\nggplot(data_2, aes(x = AGG_MENT, y = ADH_HIGHVSLOW, fill = ADH_HIGHVSLOW)) +\n  geom_density_ridges(alpha = 0.7, scale = 1) +\n  theme_ridges() +\n  scale_color_brewer(palette = \"Pastel2\") +\n  labs(title = \"Ridge Plot of Mental QOL by Hard Drug Use\",\n       x = \"Mental QOL\",\n       y = \"Hard Drug Use\") +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\nModel SelectionAnalysisInterpretationVisualization\n\n\nFit the model.\n\n# Create the data set for mental QOL\ndata_MENT &lt;- data_2 |&gt; \n  select(newid, AGG_MENT, hard_drugs_grp, ADH_HIGHVSLOW, years, CESD) %&gt;%\n  filter(complete.cases(.))\n\n# Change reference level\ndata_MENT$hard_drugs_grp &lt;- relevel(data_MENT$hard_drugs_grp, ref = \"Never User\")\n\n# Create LMM\nmodel_MENT1 &lt;- lme(AGG_MENT ~ hard_drugs_grp + years + ADH_HIGHVSLOW + hard_drugs_grp*years + CESD,\n                   random = list(newid = pdSymm(~ years)), \n                   data = data_MENT)\n\n# Get model summary\nsummary(model_MENT1)\n\nLinear mixed-effects model fit by REML\n  Data: data_MENT \n       AIC      BIC    logLik\n  10848.59 10913.16 -5412.294\n\nRandom effects:\n Formula: ~years | newid\n Structure: General positive-definite\n            StdDev    Corr  \n(Intercept) 4.3455421 (Intr)\nyears       0.3717221 -1    \nResidual    6.0377346       \n\nFixed effects:  AGG_MENT ~ hard_drugs_grp + years + ADH_HIGHVSLOW + hard_drugs_grp *      years + CESD \n                                     Value Std.Error   DF   t-value p-value\n(Intercept)                       57.87606 0.4190797 1059 138.10276  0.0000\nhard_drugs_grpPrevious User        0.78568 1.0942518  546   0.71801  0.4731\nhard_drugs_grpCurrent User        -2.27986 0.9972527  546  -2.28614  0.0226\nyears                              0.30811 0.2073133 1059   1.48621  0.1375\nADH_HIGHVSLOWLow Adherence         0.85071 0.7612683  546   1.11749  0.2643\nCESD                              -0.91187 0.0180253 1059 -50.58831  0.0000\nhard_drugs_grpPrevious User:years -0.36332 0.6647696 1059  -0.54654  0.5848\nhard_drugs_grpCurrent User:years   1.69469 0.6106482 1059   2.77524  0.0056\n Correlation: \n                                  (Intr) hr__PU hr__CU years  ADH_HA CESD  \nhard_drugs_grpPrevious User       -0.184                                   \nhard_drugs_grpCurrent User        -0.291  0.100                            \nyears                             -0.581  0.193  0.225                     \nADH_HIGHVSLOWLow Adherence        -0.142 -0.014  0.021 -0.006              \nCESD                              -0.569 -0.104  0.029  0.091 -0.068       \nhard_drugs_grpPrevious User:years  0.159 -0.657 -0.069 -0.308 -0.001  0.011\nhard_drugs_grpCurrent User:years   0.230 -0.060 -0.679 -0.345  0.008 -0.089\n                                  h__PU:\nhard_drugs_grpPrevious User             \nhard_drugs_grpCurrent User              \nyears                                   \nADH_HIGHVSLOWLow Adherence              \nCESD                                    \nhard_drugs_grpPrevious User:years       \nhard_drugs_grpCurrent User:years   0.104\n\nStandardized Within-Group Residuals:\n        Min          Q1         Med          Q3         Max \n-3.44300701 -0.46895838  0.05349899  0.54923036  4.31899276 \n\nNumber of Observations: 1613\nNumber of Groups: 550 \n\n\nWe have a significant interaction betwee hard drug use and time.\nLet’s see use BIC and AIC to determine if we need that interaction term.\n\n# Create the data set for mental QOL\ndata_MENT &lt;- data_2 |&gt; \n  select(newid, AGG_MENT, hard_drugs_grp, ADH_HIGHVSLOW, years, CESD) %&gt;%\n  filter(complete.cases(.))\n\n# Change reference level\ndata_MENT$hard_drugs_grp &lt;- relevel(data_MENT$hard_drugs_grp, ref = \"Never User\")\n\n# Create LMM\nmodel_MENT1 &lt;- lme(AGG_MENT ~ hard_drugs_grp + years + ADH_HIGHVSLOW,\n                   random = list(newid = pdSymm(~ years)), \n                   data = data_MENT)\n\n# Get model summary\nsummary(model_MENT1)\n\nLinear mixed-effects model fit by REML\n  Data: data_MENT \n       AIC      BIC    logLik\n  12203.19 12251.63 -6092.593\n\nRandom effects:\n Formula: ~years | newid\n Structure: General positive-definite\n            StdDev    Corr  \n(Intercept) 11.438345 (Intr)\nyears        3.518495 -0.513\nResidual     7.212184       \n\nFixed effects:  AGG_MENT ~ hard_drugs_grp + years + ADH_HIGHVSLOW \n                               Value Std.Error   DF  t-value p-value\n(Intercept)                 46.02166 0.6294490 1062 73.11419  0.0000\nhard_drugs_grpPrevious User -4.91599 1.6803885  546 -2.92551  0.0036\nhard_drugs_grpCurrent User  -2.01880 1.4955259  546 -1.34989  0.1776\nyears                        1.10189 0.2683723 1062  4.10581  0.0000\nADH_HIGHVSLOWLow Adherence  -1.91815 1.5516904  546 -1.23617  0.2169\n Correlation: \n                            (Intr) hr__PU hr__CU years \nhard_drugs_grpPrevious User -0.245                     \nhard_drugs_grpCurrent User  -0.289  0.105              \nyears                       -0.516 -0.001 -0.006       \nADH_HIGHVSLOWLow Adherence  -0.249 -0.028  0.035  0.002\n\nStandardized Within-Group Residuals:\n        Min          Q1         Med          Q3         Max \n-3.39660950 -0.43259762  0.09274777  0.46245152  3.02485528 \n\nNumber of Observations: 1613\nNumber of Groups: 550 \n\n\nAIC, BIC, and logLikelihood all prefer the most simplified model without the interaction term. Under this model hard drug use and adherence are not significant predictors of mental QOL. This is because depression is a confounder.\nI will therefore include the model with depression and the interaction term, so we can still investigate the main research question.\nTherefore model_MENT1 is the final model.\nTop of Tabset\n\n\n\n# Create the data set for mental QOL\ndata_MENT &lt;- data_2 |&gt; \n  select(newid, AGG_MENT, hard_drugs_grp, ADH_HIGHVSLOW, years, CESD) %&gt;%\n  filter(complete.cases(.))\n\n# Change reference level\ndata_MENT$hard_drugs_grp &lt;- relevel(data_MENT$hard_drugs_grp, ref = \"Never User\")\n\n# Create LMM\nmodel_MENT1 &lt;- lme(AGG_MENT ~ hard_drugs_grp + years + ADH_HIGHVSLOW + hard_drugs_grp*years + CESD,\n                   random = list(newid = pdSymm(~ years)), \n                   data = data_MENT)\n\n# Get model summary\nsummary(model_MENT1)\n\nLinear mixed-effects model fit by REML\n  Data: data_MENT \n       AIC      BIC    logLik\n  10848.59 10913.16 -5412.294\n\nRandom effects:\n Formula: ~years | newid\n Structure: General positive-definite\n            StdDev    Corr  \n(Intercept) 4.3455421 (Intr)\nyears       0.3717221 -1    \nResidual    6.0377346       \n\nFixed effects:  AGG_MENT ~ hard_drugs_grp + years + ADH_HIGHVSLOW + hard_drugs_grp *      years + CESD \n                                     Value Std.Error   DF   t-value p-value\n(Intercept)                       57.87606 0.4190797 1059 138.10276  0.0000\nhard_drugs_grpPrevious User        0.78568 1.0942518  546   0.71801  0.4731\nhard_drugs_grpCurrent User        -2.27986 0.9972527  546  -2.28614  0.0226\nyears                              0.30811 0.2073133 1059   1.48621  0.1375\nADH_HIGHVSLOWLow Adherence         0.85071 0.7612683  546   1.11749  0.2643\nCESD                              -0.91187 0.0180253 1059 -50.58831  0.0000\nhard_drugs_grpPrevious User:years -0.36332 0.6647696 1059  -0.54654  0.5848\nhard_drugs_grpCurrent User:years   1.69469 0.6106482 1059   2.77524  0.0056\n Correlation: \n                                  (Intr) hr__PU hr__CU years  ADH_HA CESD  \nhard_drugs_grpPrevious User       -0.184                                   \nhard_drugs_grpCurrent User        -0.291  0.100                            \nyears                             -0.581  0.193  0.225                     \nADH_HIGHVSLOWLow Adherence        -0.142 -0.014  0.021 -0.006              \nCESD                              -0.569 -0.104  0.029  0.091 -0.068       \nhard_drugs_grpPrevious User:years  0.159 -0.657 -0.069 -0.308 -0.001  0.011\nhard_drugs_grpCurrent User:years   0.230 -0.060 -0.679 -0.345  0.008 -0.089\n                                  h__PU:\nhard_drugs_grpPrevious User             \nhard_drugs_grpCurrent User              \nyears                                   \nADH_HIGHVSLOWLow Adherence              \nCESD                                    \nhard_drugs_grpPrevious User:years       \nhard_drugs_grpCurrent User:years   0.104\n\nStandardized Within-Group Residuals:\n        Min          Q1         Med          Q3         Max \n-3.44300701 -0.46895838  0.05349899  0.54923036  4.31899276 \n\nNumber of Observations: 1613\nNumber of Groups: 550 \n\n# Get 95% CI's\nintervals(model_MENT1, which = \"fixed\")\n\nApproximate 95% confidence intervals\n\n Fixed effects:\n                                        lower       est.      upper\n(Intercept)                       57.05373769 57.8760586 58.6983795\nhard_drugs_grpPrevious User       -1.36377666  0.7856821  2.9351409\nhard_drugs_grpCurrent User        -4.23878095 -2.2798591 -0.3209373\nyears                             -0.09867992  0.3081115  0.7149030\nADH_HIGHVSLOWLow Adherence        -0.64466145  0.8507117  2.3460849\nCESD                              -0.94723782 -0.9118685 -0.8764992\nhard_drugs_grpPrevious User:years -1.66773753 -0.3633223  0.9410929\nhard_drugs_grpCurrent User:years   0.49647472  1.6946926  2.8929104\n\n\nAfter controlling for adherence and between subject means (intercepts), there was no difference in average mental QOL between previous and current hard drug users (p = 0.47, 95% CI: -1.36, 2.94). On average, current hard drug users had a mental QOL score that was 2.28 points lower than never drug users (p = 0.023, 95% CI: -4.24, -0.32).\nAfter controlling for hard drug use and between subject means, those with low adherence did not differ significantly in average mental QOL compared to those with high adherence (p = 0.26, 95% CI: -0.64, 2.35).\nThere was a significant no change in mental QOL over time while controlling for subject-specific means, hard drug use, and adherence, with an average increase of 0.31 points per year (p 0.14, 95% CI: -0.099, 0.71).\nThe slope for mental QOL over time differed significantly based on hard drug use. On average, current hard drug users had a 1.69 point increase in physical QOL per year compared to never drug users (p = 0.056, 95% CI: 0.50, 2.89). The slope for physical QOL over time did not differ between never and previous users (p = 0.58, 95% CI: -1.67, 0.94).\nControlling for adherence, hard drug use, and time, depression was a significant predictor of mental QOL. On average, mental QOL score decreased by 0.91 points for every 1 point increase in depression score (p &lt; 0.0001, 95% CI: -0.95, -0.88).\n\nChange the Reference Level\n\n# Create the data set for mental QOL\ndata_MENT &lt;- data_2 |&gt; \n  select(newid, AGG_MENT, hard_drugs_grp, ADH_HIGHVSLOW, years, CESD) %&gt;%\n  filter(complete.cases(.))\n\n# Change reference level\ndata_MENT$hard_drugs_grp &lt;- relevel(data_MENT$hard_drugs_grp, ref = \"Previous User\")\n\n# Create LMM\nmodel_MENT1 &lt;- lme(AGG_MENT ~ hard_drugs_grp + years + ADH_HIGHVSLOW + hard_drugs_grp*years + CESD,\n                   random = list(newid = pdSymm(~ years)), \n                   data = data_MENT)\n\n# Get model summary\nsummary(model_MENT1)\n\nLinear mixed-effects model fit by REML\n  Data: data_MENT \n       AIC      BIC    logLik\n  10848.59 10913.16 -5412.294\n\nRandom effects:\n Formula: ~years | newid\n Structure: General positive-definite\n            StdDev    Corr  \n(Intercept) 4.3455460 (Intr)\nyears       0.3717269 -1    \nResidual    6.0377348       \n\nFixed effects:  AGG_MENT ~ hard_drugs_grp + years + ADH_HIGHVSLOW + hard_drugs_grp *      years + CESD \n                                    Value Std.Error   DF   t-value p-value\n(Intercept)                      58.66174 1.0974260 1059  53.45394  0.0000\nhard_drugs_grpNever User         -0.78568 1.0942522  546  -0.71801  0.4731\nhard_drugs_grpCurrent User       -3.06554 1.4047201  546  -2.18231  0.0295\nyears                            -0.05521 0.6324041 1059  -0.08730  0.9304\nADH_HIGHVSLOWLow Adherence        0.85071 0.7612681  546   1.11749  0.2643\nCESD                             -0.91187 0.0180253 1059 -50.58831  0.0000\nhard_drugs_grpNever User:years    0.36332 0.6647696 1059   0.54654  0.5848\nhard_drugs_grpCurrent User:years  2.05801 0.8546116 1059   2.40813  0.0162\n Correlation: \n                                 (Intr) hr__NU hr__CU years  ADH_HA CESD  \nhard_drugs_grpNever User         -0.927                                   \nhard_drugs_grpCurrent User       -0.730  0.708                            \nyears                            -0.635  0.628  0.490                     \nADH_HIGHVSLOWLow Adherence       -0.068  0.014  0.026 -0.003              \nCESD                             -0.320  0.104  0.101  0.042 -0.068       \nhard_drugs_grpNever User:years    0.595 -0.657 -0.463 -0.950  0.001 -0.011\nhard_drugs_grpCurrent User:years  0.483 -0.469 -0.672 -0.742  0.006 -0.072\n                                 h__NU:\nhard_drugs_grpNever User               \nhard_drugs_grpCurrent User             \nyears                                  \nADH_HIGHVSLOWLow Adherence             \nCESD                                   \nhard_drugs_grpNever User:years         \nhard_drugs_grpCurrent User:years  0.704\n\nStandardized Within-Group Residuals:\n        Min          Q1         Med          Q3         Max \n-3.44300908 -0.46895846  0.05349847  0.54923016  4.31899084 \n\nNumber of Observations: 1613\nNumber of Groups: 550 \n\n# Get 95% CI's\nintervals(model_MENT1, which = \"fixed\")\n\nApproximate 95% confidence intervals\n\n Fixed effects:\n                                      lower        est.      upper\n(Intercept)                      56.5083643 58.66174078 60.8151173\nhard_drugs_grpNever User         -2.9351417 -0.78568215  1.3637774\nhard_drugs_grpCurrent User       -5.8248585 -3.06554112 -0.3062238\nyears                            -1.2961183 -0.05521077  1.1856968\nADH_HIGHVSLOWLow Adherence       -0.6446619  0.85071103  2.3460840\nCESD                             -0.9472378 -0.91186850 -0.8764992\nhard_drugs_grpNever User:years   -0.9410930  0.36332233  1.6677377\nhard_drugs_grpCurrent User:years  0.3810903  2.05801476  3.7349392\n\n\nCurrent users had on average a 3.07 point lower mental QOL score compared to previous users (p = 0.03, 95% CI: 5.82, -0.31).\nThe slope for mental QOL over time also differed by hard drug use group. On average, current hard drug users had a 2.06 point increase in mental QOL compared to previous users (p = 0.016, 95% CI: 0.38, 3.73).\nTop of Tabset\n\n\n\n\nHard Drug Use\nAfter controlling for adherence and between subject means (intercepts), there was no difference in average mental QOL between never and previous hard drug users (p = 0.47, 95% CI: -1.36, 2.94). On average, current hard drug users had a mental QOL score that was 2.28 points lower than never drug users (p = 0.023, 95% CI: -4.24, -0.32) and 3.07 point lower mental QOL score compared to previous users (p = 0.03, 95% CI: 5.82, -0.31).\n\n\nAdherence\nAfter controlling for hard drug use and between subject means, those with low adherence did not differ significantly in average mental QOL compared to those with high adherence (p = 0.26, 95% CI: -0.64, 2.35)\n\n\nPhysical Quality of life by Hard Drug Use over Time\nThe slope for mental QOL over time differed significantly based on hard drug use. On average, current hard drug users had a 1.69 point increase in physical QOL per year compared to never drug users (p = 0.056, 95% CI: 0.50, 2.89). The slope for physical QOL over time did not differ between never and previous users (p = 0.58, 95% CI: -1.67, 0.94). On average, current hard drug users had a 2.06 point increase in mental QOL compared to previous users (p = 0.016, 95% CI: 0.38, 3.73).\n\n\nTime\nThere was a significant no change in mental QOL over time while controlling for subject-specific means, hard drug use, and adherence, with an average increase of 0.31 points per year (p 0.14, 95% CI: -0.099, 0.71).\n\n\nDepression\nControlling for adherence, hard drug use, and time, depression was a significant predictor of mental QOL. On average, mental QOL score decreased by 0.91 points for every 1 point increase in depression score (p &lt; 0.0001, 95% CI: -0.95, -0.88).\nTop of Tabset\n\n\n\nLet’s examine the main interaction for physical QOL by hard drug use group over time.\n\n# Get fitted values for plotting\ndata_MENT &lt;- data_MENT %&gt;%\n  mutate(fitted = predict(model_MENT1))\n\n# Plot the interaction\nggplot(data_MENT, aes(x = years, y = fitted, color = hard_drugs_grp)) +\n  geom_smooth(method = \"lm\", se = FALSE) + \n  labs(title = \"Predicted Mental QOL over Years by Hard Drug Use\",\n       x = \"Year\",\n       y = \"Predicted Mental QOL\") +\n  theme_minimal() +\n  scale_color_brewer(palette = \"Pastel2\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nHere we see the main effect that current users had lower mental QOL compared to previous and never users. The slopes were significantly different between current and previous users, but this was likely not significant following correction for pairwise comparisons."
  },
  {
    "objectID": "Project_2/Project_2_R/Code/Project2LMM.html#Phys",
    "href": "Project_2/Project_2_R/Code/Project2LMM.html#Phys",
    "title": "Linear Mixed Model",
    "section": "Physical QOL",
    "text": "Physical QOL\n\nModel SelectionAnalysisInterpretationVisualization\n\n\nWe will begin by fitting the full model predicting physical QOL, including as covariates: hard_drugs_grp, ADH_HIGHVSLOW, FRP, years, and the hard_drugs_grp*years interaction term.\nWe know that we do not meet the assumption of equality of variances (as pictured below), so we will employ an unstructured covariance matrix to account for this.\n\nggplot(data_2, aes(x = AGG_PHYS, y = hard_drugs_grp, fill = hard_drugs_grp)) +\n  geom_density_ridges(alpha = 0.7, scale = 1) +\n  theme_ridges() +\n  scale_color_brewer(palette = \"Pastel2\") +\n  labs(title = \"Ridge Plot of AGG_PHYS by hard_drugs_grp\",\n       x = \"AGG_PHYS\",\n       y = \"hard_drugs_grp\") +\n  theme(legend.position = \"none\")\n\nPicking joint bandwidth of 2.83\n\n\nWarning: Removed 10 rows containing non-finite outside the scale range\n(`stat_density_ridges()`).\n\n\n\n\n\n\n\n\n\nLet’s fit the model.\n\n# Filter data set\ndata_PHYS &lt;- data_2 |&gt; \n  select(newid, years, hard_drugs_grp, ADH_HIGHVSLOW, AGG_PHYS, FRP) %&gt;%\n  filter(complete.cases(.))\n\n# Fit LMM\nmodel_PHYS1 &lt;- lme(AGG_PHYS ~ hard_drugs_grp + ADH_HIGHVSLOW + years + hard_drugs_grp*years + FRP,\n                   random = list(newid = pdSymm(~ years)), \n                   data = data_PHYS)\n\n# Examine summary\nsummary(model_PHYS1)\n\nLinear mixed-effects model fit by REML\n  Data: data_PHYS \n       AIC      BIC    logLik\n  11148.49 11213.26 -5562.246\n\nRandom effects:\n Formula: ~years | newid\n Structure: General positive-definite\n            StdDev    Corr  \n(Intercept) 6.2152343 (Intr)\nyears       0.4360586 1     \nResidual    5.4292458       \n\nFixed effects:  AGG_PHYS ~ hard_drugs_grp + ADH_HIGHVSLOW + years + hard_drugs_grp *      years + FRP \n                                     Value Std.Error   DF   t-value p-value\n(Intercept)                       49.65356 1.1832279 1086  41.96449  0.0000\nhard_drugs_grpNever User           2.01176 1.2326815  546   1.63202  0.1033\nhard_drugs_grpCurrent User         3.08028 1.5625444  546   1.97132  0.0492\nADH_HIGHVSLOWLow Adherence        -1.71602 1.0419602  546  -1.64691  0.1002\nyears                             -1.38631 0.5706312 1086  -2.42943  0.0153\nFRPYes                           -14.03646 0.7584225 1086 -18.50745  0.0000\nhard_drugs_grpNever User:years     0.80634 0.5998861 1086   1.34416  0.1792\nhard_drugs_grpCurrent User:years  -1.01094 0.7605046 1086  -1.32931  0.1840\n Correlation: \n                                 (Intr) hr__NU hr__CU ADH_HA years  FRPYes\nhard_drugs_grpNever User         -0.948                                   \nhard_drugs_grpCurrent User       -0.753  0.717                            \nADH_HIGHVSLOWLow Adherence       -0.115  0.025  0.043                     \nyears                            -0.392  0.374  0.297  0.000              \nFRPYes                           -0.075  0.039  0.064  0.003  0.058       \nhard_drugs_grpNever User:years    0.373 -0.393 -0.283  0.000 -0.952 -0.061\nhard_drugs_grpCurrent User:years  0.298 -0.283 -0.395  0.000 -0.753 -0.093\n                                 h__NU:\nhard_drugs_grpNever User               \nhard_drugs_grpCurrent User             \nADH_HIGHVSLOWLow Adherence             \nyears                                  \nFRPYes                                 \nhard_drugs_grpNever User:years         \nhard_drugs_grpCurrent User:years  0.717\n\nStandardized Within-Group Residuals:\n        Min          Q1         Med          Q3         Max \n-3.95690610 -0.45956434  0.05295024  0.53606641  2.83142953 \n\nNumber of Observations: 1640\nNumber of Groups: 550 \n\n\nLet’s also check if that adherence by years interaction is significant.\n\n# Fit LMM\nmodel_PHYS2 &lt;- lme(AGG_PHYS ~ hard_drugs_grp + ADH_HIGHVSLOW + years + hard_drugs_grp*years + FRP + ADH_HIGHVSLOW*years,\n                   random = list(newid = pdSymm(~ years)), \n                   data = data_PHYS)\n\n# Examine summary\nsummary(model_PHYS2)\n\nLinear mixed-effects model fit by REML\n  Data: data_PHYS \n       AIC      BIC    logLik\n  11136.41 11206.57 -5555.206\n\nRandom effects:\n Formula: ~years | newid\n Structure: General positive-definite\n            StdDev    Corr  \n(Intercept) 6.2348253 (Intr)\nyears       0.4295268 1     \nResidual    5.3969297       \n\nFixed effects:  AGG_PHYS ~ hard_drugs_grp + ADH_HIGHVSLOW + years + hard_drugs_grp *      years + FRP + ADH_HIGHVSLOW * years \n                                     Value Std.Error   DF   t-value p-value\n(Intercept)                       49.43982 1.1841316 1085  41.75196  0.0000\nhard_drugs_grpNever User           2.06036 1.2322716  546   1.67200  0.0951\nhard_drugs_grpCurrent User         3.18660 1.5621690  546   2.03986  0.0418\nADH_HIGHVSLOWLow Adherence        -0.09878 1.1316120  546  -0.08730  0.9305\nyears                             -1.12156 0.5716636 1085  -1.96192  0.0500\nFRPYes                           -14.01236 0.7546578 1085 -18.56783  0.0000\nhard_drugs_grpNever User:years     0.74548 0.5964647 1085   1.24983  0.2116\nhard_drugs_grpCurrent User:years  -1.14212 0.7566907 1085  -1.50936  0.1315\nADH_HIGHVSLOWLow Adherence:years  -2.02171 0.5499329 1085  -3.67628  0.0002\n Correlation: \n                                 (Intr) hr__NU hr__CU ADH_HIGHVSLOWLwA years \nhard_drugs_grpNever User         -0.947                                      \nhard_drugs_grpCurrent User       -0.753  0.717                               \nADH_HIGHVSLOWLow Adherence       -0.125  0.027  0.046                        \nyears                            -0.392  0.370  0.295  0.049                 \nFRPYes                           -0.074  0.038  0.064  0.000            0.057\nhard_drugs_grpNever User:years    0.372 -0.391 -0.282 -0.010           -0.947\nhard_drugs_grpCurrent User:years  0.297 -0.281 -0.393 -0.018           -0.752\nADH_HIGHVSLOWLow Adherence:years  0.048 -0.010 -0.018 -0.389           -0.125\n                                 FRPYes h__NU: h__CU:\nhard_drugs_grpNever User                             \nhard_drugs_grpCurrent User                           \nADH_HIGHVSLOWLow Adherence                           \nyears                                                \nFRPYes                                               \nhard_drugs_grpNever User:years   -0.060              \nhard_drugs_grpCurrent User:years -0.093  0.717       \nADH_HIGHVSLOWLow Adherence:years  0.006  0.027  0.046\n\nStandardized Within-Group Residuals:\n        Min          Q1         Med          Q3         Max \n-4.01686500 -0.48052970  0.06048571  0.54335717  2.85109147 \n\nNumber of Observations: 1640\nNumber of Groups: 550 \n\n\nAIC, BIC, and logLik all prefer the model with both interactions. This will be our final model.\nTop of Tabset\n\n\n\n# Change reference level\ndata_PHYS$hard_drugs_grp &lt;- relevel(data_PHYS$hard_drugs_grp, ref = \"Never User\")\n\n# Fit LMM\nmodel_PHYS2 &lt;- lme(AGG_PHYS ~ hard_drugs_grp + ADH_HIGHVSLOW + years + hard_drugs_grp*years + FRP + ADH_HIGHVSLOW*years,\n                   random = list(newid = pdSymm(~ years)), \n                   data = data_PHYS)\n\n# Examine summary\nsummary(model_PHYS2)\n\nLinear mixed-effects model fit by REML\n  Data: data_PHYS \n       AIC      BIC    logLik\n  11136.41 11206.57 -5555.206\n\nRandom effects:\n Formula: ~years | newid\n Structure: General positive-definite\n            StdDev   Corr  \n(Intercept) 6.234822 (Intr)\nyears       0.429531 1     \nResidual    5.396929       \n\nFixed effects:  AGG_PHYS ~ hard_drugs_grp + ADH_HIGHVSLOW + years + hard_drugs_grp *      years + FRP + ADH_HIGHVSLOW * years \n                                      Value Std.Error   DF   t-value p-value\n(Intercept)                        51.50018 0.3965111 1085 129.88333  0.0000\nhard_drugs_grpPrevious User        -2.06036 1.2322712  546  -1.67200  0.0951\nhard_drugs_grpCurrent User          1.12624 1.0950838  546   1.02846  0.3042\nADH_HIGHVSLOWLow Adherence         -0.09878 1.1316116  546  -0.08730  0.9305\nyears                              -0.37608 0.1915536 1085  -1.96331  0.0499\nFRPYes                            -14.01235 0.7546578 1085 -18.56782  0.0000\nhard_drugs_grpPrevious User:years  -0.74548 0.5964647 1085  -1.24983  0.2116\nhard_drugs_grpCurrent User:years   -1.88760 0.5300278 1085  -3.56132  0.0004\nADH_HIGHVSLOWLow Adherence:years   -2.02171 0.5499329 1085  -3.67628  0.0002\n Correlation: \n                                  (Intr) hr__PU hr__CU ADH_HIGHVSLOWLwA years \nhard_drugs_grpPrevious User       -0.280                                      \nhard_drugs_grpCurrent User        -0.343  0.103                               \nADH_HIGHVSLOWLow Adherence        -0.289 -0.027  0.036                        \nyears                             -0.386  0.112  0.132  0.113                 \nFRPYes                            -0.102 -0.038  0.048  0.000           -0.019\nhard_drugs_grpPrevious User:years  0.105 -0.391 -0.038  0.010           -0.287\nhard_drugs_grpCurrent User:years   0.139 -0.038 -0.391 -0.014           -0.340\nADH_HIGHVSLOWLow Adherence:years   0.112  0.010 -0.014 -0.389           -0.290\n                                  FRPYes h__PU: h__CU:\nhard_drugs_grpPrevious User                           \nhard_drugs_grpCurrent User                            \nADH_HIGHVSLOWLow Adherence                            \nyears                                                 \nFRPYes                                                \nhard_drugs_grpPrevious User:years  0.060              \nhard_drugs_grpCurrent User:years  -0.065  0.101       \nADH_HIGHVSLOWLow Adherence:years   0.006 -0.027  0.035\n\nStandardized Within-Group Residuals:\n        Min          Q1         Med          Q3         Max \n-4.01686316 -0.48052935  0.06048594  0.54335666  2.85109182 \n\nNumber of Observations: 1640\nNumber of Groups: 550 \n\n# Get 95% CI's\nintervals(model_PHYS2, which = \"fixed\")\n\nApproximate 95% confidence intervals\n\n Fixed effects:\n                                        lower         est.          upper\n(Intercept)                        50.7221620  51.50017727  52.2781925500\nhard_drugs_grpPrevious User        -4.4809294  -2.06035669   0.3602160769\nhard_drugs_grpCurrent User         -1.0248484   1.12624474   3.2773378408\nADH_HIGHVSLOWLow Adherence         -2.3216295  -0.09878414   2.1240612074\nyears                              -0.7519374  -0.37607985  -0.0002223435\nFRPYes                            -15.4931073 -14.01235342 -12.5315995323\nhard_drugs_grpPrevious User:years  -1.9158339  -0.74547894   0.4248760243\nhard_drugs_grpCurrent User:years   -2.9275931  -1.88759751  -0.8476019473\nADH_HIGHVSLOWLow Adherence:years   -3.1007595  -2.02170706  -0.9426546025\n\n\nAfter controlling for adherence and between subject means (intercepts), there was no difference in average physical QOL between never hard drug users and previous users (p = 0.95, 95% CI: -4.48, 0.36) and current users (p = 0.30, 95% CI: -1.02, 3.27).\nAfter controlling for hard drug use and between subject means, those with low adherence did not differ significantly in average physical QOL compared to those with high adherence (p = 0.93, 95% CI: -2.32, 2.12).\nThere was a significant decrease in physical QOL over time while controlling for subject-specific means, hard drug use, and adherence, with an average decrease of 0.38 points per year (p &lt; 0.05, 95% CI: -0.75, -0.002). However this value was borderline significant and may not be a true relationshi, especially following correction for multiple pairwise comparisons.\nThe slope for physical QOL over time differed significantly based on hard drug use. On average, current hard drug users had a 1.89 point decrease in physical QOL per year compared to never drug users (p = 0.0004, 95% CI: -2.93, -0.85). The slope for physical QOL over time did not differ between never and previous users (p = 0.2116, 95% CI: -1.916, 0.42).\nThe slope for physical QOL over time also differed significantly based on adherence. On average, those with low adherence had a 2.02 point decrease in physical QOL per year compared to those with high adherence (p = 0.0002, 95% CI: -3.10, -0.94).\nAfter controlling for hard drug user, adherence, and their interactions over time, and between subject means, those with a frailty related phenotype had on average a physical QOL score that was 14.01 points lower than those without a frailty related phenotype (p &lt; 0.0001, 95% CI: -15.49, -12.53).\n\nChange Reference Level\n\n# Change reference level\ndata_PHYS$hard_drugs_grp &lt;- relevel(data_PHYS$hard_drugs_grp, ref = \"Previous User\")\n\n# Fit LMM\nmodel_PHYS2 &lt;- lme(AGG_PHYS ~ hard_drugs_grp + ADH_HIGHVSLOW + years + hard_drugs_grp*years + FRP + ADH_HIGHVSLOW*years,\n                   random = list(newid = pdSymm(~ years)), \n                   data = data_PHYS)\n\n# Examine summary\nsummary(model_PHYS2)\n\nLinear mixed-effects model fit by REML\n  Data: data_PHYS \n       AIC      BIC    logLik\n  11136.41 11206.57 -5555.206\n\nRandom effects:\n Formula: ~years | newid\n Structure: General positive-definite\n            StdDev    Corr  \n(Intercept) 6.2348253 (Intr)\nyears       0.4295268 1     \nResidual    5.3969297       \n\nFixed effects:  AGG_PHYS ~ hard_drugs_grp + ADH_HIGHVSLOW + years + hard_drugs_grp *      years + FRP + ADH_HIGHVSLOW * years \n                                     Value Std.Error   DF   t-value p-value\n(Intercept)                       49.43982 1.1841316 1085  41.75196  0.0000\nhard_drugs_grpNever User           2.06036 1.2322716  546   1.67200  0.0951\nhard_drugs_grpCurrent User         3.18660 1.5621690  546   2.03986  0.0418\nADH_HIGHVSLOWLow Adherence        -0.09878 1.1316120  546  -0.08730  0.9305\nyears                             -1.12156 0.5716636 1085  -1.96192  0.0500\nFRPYes                           -14.01236 0.7546578 1085 -18.56783  0.0000\nhard_drugs_grpNever User:years     0.74548 0.5964647 1085   1.24983  0.2116\nhard_drugs_grpCurrent User:years  -1.14212 0.7566907 1085  -1.50936  0.1315\nADH_HIGHVSLOWLow Adherence:years  -2.02171 0.5499329 1085  -3.67628  0.0002\n Correlation: \n                                 (Intr) hr__NU hr__CU ADH_HIGHVSLOWLwA years \nhard_drugs_grpNever User         -0.947                                      \nhard_drugs_grpCurrent User       -0.753  0.717                               \nADH_HIGHVSLOWLow Adherence       -0.125  0.027  0.046                        \nyears                            -0.392  0.370  0.295  0.049                 \nFRPYes                           -0.074  0.038  0.064  0.000            0.057\nhard_drugs_grpNever User:years    0.372 -0.391 -0.282 -0.010           -0.947\nhard_drugs_grpCurrent User:years  0.297 -0.281 -0.393 -0.018           -0.752\nADH_HIGHVSLOWLow Adherence:years  0.048 -0.010 -0.018 -0.389           -0.125\n                                 FRPYes h__NU: h__CU:\nhard_drugs_grpNever User                             \nhard_drugs_grpCurrent User                           \nADH_HIGHVSLOWLow Adherence                           \nyears                                                \nFRPYes                                               \nhard_drugs_grpNever User:years   -0.060              \nhard_drugs_grpCurrent User:years -0.093  0.717       \nADH_HIGHVSLOWLow Adherence:years  0.006  0.027  0.046\n\nStandardized Within-Group Residuals:\n        Min          Q1         Med          Q3         Max \n-4.01686500 -0.48052970  0.06048571  0.54335717  2.85109147 \n\nNumber of Observations: 1640\nNumber of Groups: 550 \n\n# Get 95% CI's\nintervals(model_PHYS2, which = \"fixed\")\n\nApproximate 95% confidence intervals\n\n Fixed effects:\n                                       lower         est.          upper\n(Intercept)                       47.1163737  49.43982096  51.7632681681\nhard_drugs_grpNever User          -0.3602171   2.06035649   4.4809301203\nhard_drugs_grpCurrent User         0.1180039   3.18660100   6.2551980783\nADH_HIGHVSLOWLow Adherence        -2.3216303  -0.09878418   2.1240619640\nyears                             -2.2432503  -1.12155894   0.0001324472\nFRPYes                           -15.4931105 -14.01235656 -12.5316026397\nhard_drugs_grpNever User:years    -0.4248758   0.74547909   1.9158339765\nhard_drugs_grpCurrent User:years  -2.6268611  -1.14211827   0.3426245694\nADH_HIGHVSLOWLow Adherence:years  -3.1007594  -2.02170698  -0.9426546059\n\n\nCurrent hard drug users had on average a physical QOL score that was 3.19 points higher than previous users (p = 0.042, 95% CI: 0.12, 6.26).\nThe slope for physical QOL over time did not differ between current and previous hard drug users (p = 0.13, 95% CI: -2.63, 0.34).\nTop of Tabest\n\n\n\n\nHard Drug Use\nAfter controlling for adherence and between subject means (intercepts), there was no difference in average physical QOL between never hard drug users and previous users (p = 0.95, 95% CI: -4.48, 0.36) and current users (p = 0.30, 95% CI: -1.02, 3.27). Current hard drug users had on average a physical QOL score that was 3.19 points higher than previous users (p = 0.042, 95% CI: 0.12, 6.26).\n\n\nAdherence\nAfter controlling for hard drug use and between subject means, those with low adherence did not differ significantly in average physical QOL compared to those with high adherence (p = 0.93, 95% CI: -2.32, 2.12).\n\n\nPhysical QOL by Hard Drug Use over Time\nThe slope for physical QOL over time differed significantly based on hard drug use. On average, current hard drug users had a 1.89 point decrease in physical QOL per year compared to never drug users (p = 0.0004, 95% CI: -2.93, -0.85). The slope for physical QOL over time did not differ between never and previous users (p = 0.2116, 95% CI: -1.916, 0.42). The slope for physical QOL over time did not differ between current and previous hard drug users (p = 0.13, 95% CI: -2.63, 0.34).\n\n\nPhysical QOL by Adherence over Time\nThe slope for physical QOL over time also differed significantly based on adherence. On average, those with low adherence had a 2.02 point decrease in physical QOL per year compared to those with high adherence (p = 0.0002, 95% CI: -3.10, -0.94).\n\n\nTime\nThere was a significant decrease in physical QOL over time while controlling for subject-specific means, hard drug use, and adherence, with an average decrease of 0.38 points per year (p &lt; 0.05, 95% CI: -0.75, -0.002). However this value was borderline significant and may not be a true relationshi, especially following correction for multiple pairwise comparisons.\n\n\nFrailty Related Phenotype\nAfter controlling for hard drug user, adherence, and their interactions over time, and between subject means, those with a frailty related phenotype had on average a physical QOL score that was 14.01 points lower than those without a frailty related phenotype (p &lt; 0.0001, 95% CI: -15.49, -12.53).\nTop of Tabset\n\n\n\nTop of Tabset\nFirst we examine the interaction between hard drug use and time for physical QOL.\n\n# Get fitted values for plotting\ndata_PHYS &lt;- data_PHYS |&gt;\n  mutate(fitted = predict(model_PHYS2))\n\n# Plot the interaction\nggplot(data_PHYS, aes(x = years, y = fitted, color = hard_drugs_grp)) +\n  geom_smooth(method = \"lm\", se = FALSE) + \n  labs(title = \"Predicted Physical QOL over Years by Hard Drug Use\",\n       x = \"Year\",\n       y = \"Predicted Physical QOL\") +\n  theme_minimal() +\n  scale_color_brewer(palette = \"Pastel2\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nFrom this plot, we can see the main interaction that current hard drug users had a steeper negative slope compared to never users. This difference was not statistically significant between current and previous users, although it looks like it should be.\nThat is, current drug users had more of a decline in physical QOL over time compared to previous and current drug users.\nLet’s then examine the interaction for adherence over time.\n\n# Plot the interaction\nggplot(data_PHYS, aes(x = years, y = fitted, color = ADH_HIGHVSLOW)) +\n  geom_smooth(method = \"lm\", se = FALSE) + \n  labs(title = \"Predicted Physical QOL over Years by Adherence\",\n       x = \"Year\",\n       y = \"Predicted Physical QOL\") +\n  theme_minimal() +\n  scale_color_brewer(palette = \"Pastel2\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nLikewise, we can see the main interaction here: those with low adherence had a steeper slope compared to those with high adherence. That is, if a patient had low adherence, they had more of a decline in physical QOL compared to those with high adherence."
  },
  {
    "objectID": "Project_2/Project_2_R/Code/Project2LMM.html#linear-mixed-model-vs-multiple-linear-regression",
    "href": "Project_2/Project_2_R/Code/Project2LMM.html#linear-mixed-model-vs-multiple-linear-regression",
    "title": "Linear Mixed Model",
    "section": "Linear Mixed Model vs Multiple Linear Regression",
    "text": "Linear Mixed Model vs Multiple Linear Regression\nFinally, we learned how much better suited a linear mixed model is to handling repeated measures than a multiple linear regression with change scores!"
  },
  {
    "objectID": "Project_2/Project_2_R/Code/Project2LMM.html#depression-interaction",
    "href": "Project_2/Project_2_R/Code/Project2LMM.html#depression-interaction",
    "title": "Linear Mixed Model",
    "section": "Depression Interaction",
    "text": "Depression Interaction\nThere was also a significant interaction between hard drug use and depression in this model.\nHowever, this model shows that current drug users have a steeper slope between depression and mental QOL.\nThat is, current hard drug users are more susceptible, where they are more vulnerable to depression impacting their mental QOL over time compared to previous and never hard drug users! BIC and AIC did not prefer this model but I thought it was interesting nonetheless!\n\n# Create the data set for mental QOL\ndata_MENT &lt;- data_2 |&gt; \n  select(newid, AGG_MENT, hard_drugs_grp, ADH_HIGHVSLOW, years, CESD) %&gt;%\n  filter(complete.cases(.))\n\n# Change reference level\ndata_MENT$hard_drugs_grp &lt;- relevel(data_MENT$hard_drugs_grp, ref = \"Never User\")\n\n# Create LMM\nmodel_MENT1 &lt;- lme(AGG_MENT ~ hard_drugs_grp + years + ADH_HIGHVSLOW + hard_drugs_grp*years + CESD + CESD*hard_drugs_grp,\n                   random = list(newid = pdSymm(~ years)), \n                   data = data_MENT)\n\n# Get model summary\nsummary(model_MENT1)\n\nLinear mixed-effects model fit by REML\n  Data: data_MENT \n       AIC      BIC    logLik\n  10844.23 10919.54 -5408.113\n\nRandom effects:\n Formula: ~years | newid\n Structure: General positive-definite\n            StdDev    Corr  \n(Intercept) 4.1384977 (Intr)\nyears       0.3342611 -1    \nResidual    6.0663122       \n\nFixed effects:  AGG_MENT ~ hard_drugs_grp + years + ADH_HIGHVSLOW + hard_drugs_grp *      years + CESD + CESD * hard_drugs_grp \n                                     Value Std.Error   DF   t-value p-value\n(Intercept)                       57.74963 0.4321094 1057 133.64588  0.0000\nhard_drugs_grpPrevious User       -0.69949 1.5642457  546  -0.44718  0.6549\nhard_drugs_grpCurrent User         0.50375 1.2412273  546   0.40585  0.6850\nyears                              0.31936 0.2082929 1057   1.53322  0.1255\nADH_HIGHVSLOWLow Adherence         0.67075 0.7563589  546   0.88682  0.3756\nCESD                              -0.90113 0.0198060 1057 -45.49799  0.0000\nhard_drugs_grpPrevious User:years -0.25476 0.6726451 1057  -0.37874  0.7050\nhard_drugs_grpCurrent User:years   2.12165 0.6216420 1057   3.41298  0.0007\nhard_drugs_grpPrevious User:CESD   0.07152 0.0593113 1057   1.20588  0.2281\nhard_drugs_grpCurrent User:CESD   -0.23461 0.0630276 1057  -3.72238  0.0002\n Correlation: \n                                  (Intr) hr__PU hr__CU years  ADH_HA CESD  \nhard_drugs_grpPrevious User       -0.287                                   \nhard_drugs_grpCurrent User        -0.342  0.096                            \nyears                             -0.571  0.158  0.199                     \nADH_HIGHVSLOWLow Adherence        -0.169  0.113  0.020  0.000              \nCESD                              -0.618  0.170  0.215  0.100 -0.008       \nhard_drugs_grpPrevious User:years  0.181 -0.543 -0.062 -0.310 -0.022 -0.031\nhard_drugs_grpCurrent User:years   0.191 -0.052 -0.433 -0.335  0.004 -0.034\nhard_drugs_grpPrevious User:CESD   0.235 -0.724 -0.075 -0.033 -0.169 -0.333\nhard_drugs_grpCurrent User:CESD    0.195 -0.054 -0.610 -0.032 -0.004 -0.314\n                                  hr__PU: hr__CU: h__PU:C\nhard_drugs_grpPrevious User                              \nhard_drugs_grpCurrent User                               \nyears                                                    \nADH_HIGHVSLOWLow Adherence                               \nCESD                                                     \nhard_drugs_grpPrevious User:years                        \nhard_drugs_grpCurrent User:years   0.104                 \nhard_drugs_grpPrevious User:CESD   0.125   0.011         \nhard_drugs_grpCurrent User:CESD    0.010  -0.166   0.106 \n\nStandardized Within-Group Residuals:\n       Min         Q1        Med         Q3        Max \n-3.4472746 -0.4726675  0.0637846  0.5380510  4.2762014 \n\nNumber of Observations: 1613\nNumber of Groups: 550 \n\n# Get 95% CI's\nintervals(model_MENT1, which = \"fixed\")\n\nApproximate 95% confidence intervals\n\n Fixed effects:\n                                        lower        est.      upper\n(Intercept)                       56.90174517 57.74963486 58.5975246\nhard_drugs_grpPrevious User       -3.77216947 -0.69949301  2.3731834\nhard_drugs_grpCurrent User        -1.93441598  0.50374943  2.9419148\nyears                             -0.08935586  0.31935877  0.7280734\nADH_HIGHVSLOWLow Adherence        -0.81497599  0.67075364  2.1564833\nCESD                              -0.93999435 -0.90113090 -0.8622674\nhard_drugs_grpPrevious User:years -1.57462726 -0.25475569  1.0651159\nhard_drugs_grpCurrent User:years   0.90186189  2.12165458  3.3414473\nhard_drugs_grpPrevious User:CESD  -0.04485871  0.07152266  0.1879040\nhard_drugs_grpCurrent User:CESD   -0.35828592 -0.23461252 -0.1109391\n\n# Get fitted values for plotting\ndata_MENT &lt;- data_MENT %&gt;%\n  mutate(fitted = predict(model_MENT1))\n\n# Plot the interaction\nggplot(data_MENT, aes(x = CESD, y = fitted, color = hard_drugs_grp)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) + \n  labs(title = \"Predicted Mental QOL over Years by Hard Drug Use and Depression Interaction\",\n       x = \"Depression Score\",\n       y = \"Predicted Mental QOL\") +\n  theme_minimal() +\n  scale_color_brewer(palette = \"Pastel2\")\n\n\n\n\n\n\n\n# Change reference level\ndata_MENT$hard_drugs_grp &lt;- relevel(data_MENT$hard_drugs_grp, ref = \"Previous User\")\n\n# Create LMM\nmodel_MENT1 &lt;- lme(AGG_MENT ~ hard_drugs_grp + years + ADH_HIGHVSLOW + hard_drugs_grp*years + CESD + CESD*hard_drugs_grp,\n                   random = list(newid = pdSymm(~ years)), \n                   data = data_MENT)\n\n# Get model summary\nsummary(model_MENT1)\n\nLinear mixed-effects model fit by REML\n  Data: data_MENT \n       AIC      BIC    logLik\n  10844.23 10919.54 -5408.113\n\nRandom effects:\n Formula: ~years | newid\n Structure: General positive-definite\n            StdDev    Corr  \n(Intercept) 4.1384922 (Intr)\nyears       0.3342554 -1    \nResidual    6.0663122       \n\nFixed effects:  AGG_MENT ~ hard_drugs_grp + years + ADH_HIGHVSLOW + hard_drugs_grp *      years + CESD + CESD * hard_drugs_grp \n                                    Value Std.Error   DF   t-value p-value\n(Intercept)                      57.05014 1.4983205 1057  38.07606  0.0000\nhard_drugs_grpNever User          0.69950 1.5642456  546   0.44718  0.6549\nhard_drugs_grpCurrent User        1.20324 1.9015356  546   0.63278  0.5271\nyears                             0.06460 0.6395843 1057   0.10101  0.9196\nADH_HIGHVSLOWLow Adherence        0.67075 0.7563590  546   0.88682  0.3756\nCESD                             -0.82961 0.0559357 1057 -14.83146  0.0000\nhard_drugs_grpNever User:years    0.25476 0.6726451 1057   0.37874  0.7050\nhard_drugs_grpCurrent User:years  2.37641 0.8672870 1057   2.74005  0.0062\nhard_drugs_grpNever User:CESD    -0.07152 0.0593114 1057  -1.20589  0.2281\nhard_drugs_grpCurrent User:CESD  -0.30614 0.0818578 1057  -3.73984  0.0002\n Correlation: \n                                 (Intr) hr__NU hr__CU years  ADH_HA CESD  \nhard_drugs_grpNever User         -0.961                                   \nhard_drugs_grpCurrent User       -0.790  0.760                            \nyears                            -0.542  0.520  0.427                     \nADH_HIGHVSLOWLow Adherence        0.069 -0.113 -0.080 -0.023              \nCESD                             -0.730  0.708  0.580  0.127 -0.182       \nhard_drugs_grpNever User:years    0.515 -0.543 -0.406 -0.951  0.022 -0.121\nhard_drugs_grpCurrent User:years  0.400 -0.384 -0.487 -0.738  0.020 -0.094\nhard_drugs_grpNever User:CESD     0.688 -0.724 -0.547 -0.120  0.169 -0.943\nhard_drugs_grpCurrent User:CESD   0.499 -0.483 -0.669 -0.087  0.119 -0.682\n                                 hr__NU: hr__CU: h__NU:C\nhard_drugs_grpNever User                                \nhard_drugs_grpCurrent User                              \nyears                                                   \nADH_HIGHVSLOWLow Adherence                              \nCESD                                                    \nhard_drugs_grpNever User:years                          \nhard_drugs_grpCurrent User:years  0.701                 \nhard_drugs_grpNever User:CESD     0.125   0.089         \nhard_drugs_grpCurrent User:CESD   0.083  -0.033   0.643 \n\nStandardized Within-Group Residuals:\n        Min          Q1         Med          Q3         Max \n-3.44727213 -0.47266743  0.06378434  0.53805090  4.27620351 \n\nNumber of Observations: 1613\nNumber of Groups: 550"
  },
  {
    "objectID": "Project_2/Project_2_R/Code/Project2LMM.html#animated-plots",
    "href": "Project_2/Project_2_R/Code/Project2LMM.html#animated-plots",
    "title": "Linear Mixed Model",
    "section": "Animated Plots",
    "text": "Animated Plots\n\nLog Viral Load\n\nlibrary(gifski)\nlibrary(png)\nlibrary(gganimate)\n\ndata &lt;- drop_na(data)\n\ndata |&gt; \n  group_by(hard_drugs, years) |&gt; \n  summarise(avg_VLOAD_log = mean(VLOAD_log, na.rm = T)) |&gt; \n  ggplot(aes(y = avg_VLOAD_log, x = years, color = as.factor(hard_drugs))) +\n  geom_line(size = 1) +\n  theme_minimal() +\n  labs(title = \"Average Viral Load Over Time for hard Drug Users\",\n       x = \"Year\",\n       y = \"Averaeg Log Viral Load\") +\n  transition_reveal(years) +\n  scale_color_brewer(palette = \"Pastel2\")\n\n`summarise()` has grouped output by 'hard_drugs'. You can override using the\n`.groups` argument.\n`geom_line()`: Each group consists of only one observation. ℹ Do you need to\nadjust the group aesthetic?\n`geom_line()`: Each group consists of only one observation. ℹ Do you need to\nadjust the group aesthetic?"
  },
  {
    "objectID": "Project_3_Predictive_Modelling/Project_3_R/Code/images/Megan_Watson_Project-01.html",
    "href": "Project_3_Predictive_Modelling/Project_3_R/Code/images/Megan_Watson_Project-01.html",
    "title": "Data Checking - Statistical Consulting",
    "section": "",
    "text": "This is the Quarto markdown for the live consultation in BIOS 6621: Statistical Consulting.\nThis is a research project encompassing the entire data analysis pipeline, from initial consultation to scope of work writing, execution of statistical analysis, and completion of project deliverables."
  },
  {
    "objectID": "Project_3_Predictive_Modelling/Project_3_R/Code/images/Megan_Watson_Project-01.html#general-information",
    "href": "Project_3_Predictive_Modelling/Project_3_R/Code/images/Megan_Watson_Project-01.html#general-information",
    "title": "Data Checking - Statistical Consulting",
    "section": "General Information",
    "text": "General Information\n\n\n\n\n\n\n\n\n\nInvestigator\nMegan Watson\nDate\nSeptember 30, 2024\n\n\nProject Title\nQuality Improvement into Standard Practice: Persistent Practice Changes and Decreased Length of Stay for 3 years following a Medical ICU Physical Therapy Quality Improvement Project"
  },
  {
    "objectID": "Project_3_Predictive_Modelling/Project_3_R/Code/images/Megan_Watson_Project-01.html#project-description",
    "href": "Project_3_Predictive_Modelling/Project_3_R/Code/images/Megan_Watson_Project-01.html#project-description",
    "title": "Data Checking - Statistical Consulting",
    "section": "Project Description",
    "text": "Project Description\n\nBackground\n             The objective of this study is to assess the impact of a MICU physical therapy (PT) quality improvement (QI) project on physical therapist practice, and quantify the changes in length of stay (LOS) and mechanical ventilation (MV) time. The goals of the QI Initiative were to increase percentage of MICU admissions receiving PT, decrease time from MICU admission to first PT, and increase the frequency of PT visits. The main question the researcher came to us with was: how to correctly model the longitudinal data in the study?\n\n\nStudy Design\n              This was a retrospective cohort study, comparing 3 main time periods: Pre-QI Initiative (before April 2015), During QI Initiative (April 2015-Dec 2015), and After-QI Initiative (Jan 2016 and later). The QI initiative was deployed during the 9-month period between April 2015 and December 2015, during which time there were education and staffing changes. Patients were stratified as either having mechanical ventilation or not having mechanical ventilation. The primary outcomes of interest were MICU LOS and time on MV. Secondary outcomes of interest were the non-ICU LOS, total hospital LOS, and discharge location.\n\n\nDescription of the Data\n              Data was received as a de-identified .csv. Data points consist of: Admit and Discharge Time, PT Consult Time, Admit to Consult Time, First Therapy Time, Consult to Therapy Time, Admit to Therapy Time, Total Therapy Visits, Days with Therapy Visits, Average, min, and Max Therapy Length, Admit and Discharge Date/Time, Total MV Days, Hospital LOS, Patient Age, Sex, “Problem List”, and Codes (presumably ICD9/10).\n\n\nAnticipated Sample Size / Study Population\n              The sample size is N = ~3000-4000, consisting of patients admitted to the MICU with a LOS between 2 and 30 days (I.e. exclude &lt; 2 days or &gt; 30 days). This was a single site study performed here at Colorado University.\n\n\nHypothesis\n              The hypothesis for the study was that LOS and time on MV would decrease when comparing the pre- and intra-initiative time periods, and when comparing the pre- and post-initiative time periods. Clinical significance for this outcome is considered as a 1-2 day change in LOS. The intended end product of this project is a publication in a PT journal.\n\n\nAnalysis Plan (Sketch)\n              Data should first be examined for missingness, as this has not been performed. Similarly, repeat MICU admissions should be explored and controlled for to ensure independence of observations. Death during MICU stay should also be examined and adjusted for if needed. The analysis plan discussed was to utilize splines to capture the trajectory of LOS or time on MV over time by fitting smooth curves to the data. Linear contrasts should be performed to assess if there is a significant difference in LOS or time on MV when comparing two timepoints (i.e. pre- and intra-, pre- and post- , and post- and intra-QI Initiative. Supplementary analyses should be performed to assess secondary outcomes of interest of non-ICU LOS and total hospital LOS."
  },
  {
    "objectID": "Project_3_Predictive_Modelling/Project_3_R/Code/images/Megan_Watson_Project-01.html#project-deliverables",
    "href": "Project_3_Predictive_Modelling/Project_3_R/Code/images/Megan_Watson_Project-01.html#project-deliverables",
    "title": "Data Checking - Statistical Consulting",
    "section": "Project Deliverables",
    "text": "Project Deliverables\n             At this stage, Megan has only asked for insight into what method to use to account for variance in time for her analysis. There are currently no other deliverables.\nTimeline / Deadlines\nAt this time, Megan has not asked for data analysis efforts from us.\nShould this change, the anticipated project start date will be 1 week after our next meeting with her, where revisions to the Comprehensive Analysis Report are also expected to be completed.  The assigned analysts will reach out to the investigator to confirm the project timeframe and the project start date may be altered if necessary. 2 weeks will be allowed for initial data analysis by the assigned analyst.\nA follow-up meeting is to occur no later than 1 month after the next meeting with Megan. During the follow-up meeting, the project team will discuss and preliminary results of the analysis, mockup the desired tables and figures (1-2 descriptive tables, 1-2 analysis results tables and up to 4 graphs), and discuss next steps. During the follow-up meeting, the team will establish a meeting schedule for the remainder of the project. \nEstimated Cost\nThis represents our best estimate of the effort needed to complete the project. Should the scope of work change substantially, the analyst(s) will discuss changes with the investigator and issue a new or amended project agreement.\n\n\n\n\nEffort\n\n\n\n\nMonths\nPhD Faculty\nMaster’s Faculty\nStudent\n\n\n\n\n\n\n\n\n1\n20%\n0%\n80%\n\n\n\n\nTotal Costs\n’Bout Tree Fiddy"
  },
  {
    "objectID": "Project_3_Predictive_Modelling/Project_3_R/Code/images/Megan_Watson_Project-01.html#Outcome_Variables",
    "href": "Project_3_Predictive_Modelling/Project_3_R/Code/images/Megan_Watson_Project-01.html#Outcome_Variables",
    "title": "Data Checking - Statistical Consulting",
    "section": "Outcome Variables",
    "text": "Outcome Variables\n\nMechanical VentilationMICU Length of StayHospital Length of StayNon-ICU Length of StayDischarge Location\n\n\nFirst we will begin with mechanical ventilation time (MV)\n\n# Create histogram and qqplot\ndistr_plots(data_ex, \"MV_Total\")\n\nWarning: `aes_string()` was deprecated in ggplot2 3.0.0.\nℹ Please use tidy evaluation idioms with `aes()`.\nℹ See also `vignette(\"ggplot2-in-packages\")` for more information.\n\n\nWarning: Removed 3166 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\nWarning: Removed 3166 rows containing non-finite outside the scale range\n(`stat_qq()`).\n\n\nWarning: Removed 3166 rows containing non-finite outside the scale range\n(`stat_qq_line()`).\n\n\n\n\n\n\n\n\n\nThat’s some severe right skewness! Is that logarithmic? Let’s try and transform and see what happens.\n\n# Perform a log transform of mechanical ventilation time and plot\ndata_ex$MV_Total_log &lt;- log(data_ex$MV_Total)\n\n# Create histogram and qqplot\ndistr_plots(data_ex, \"MV_Total_log\", 10)\n\nWarning: Removed 3166 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\nWarning: Removed 3166 rows containing non-finite outside the scale range\n(`stat_qq()`).\n\n\nWarning: Removed 3166 rows containing non-finite outside the scale range\n(`stat_qq_line()`).\n\n\n\n\n\n\n\n\n\nThat’s better. Maybe try a square root transformation?\n\n# Perform a sqrt transformation and plot\ndata_ex$MV_Total_sqrt &lt;- sqrt(data_ex$MV_Total)\n\n# Create histogram and qqplot\ndistr_plots(data_ex, \"MV_Total_sqrt\", 10)\n\nWarning: Removed 3166 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\nWarning: Removed 3166 rows containing non-finite outside the scale range\n(`stat_qq()`).\n\n\nWarning: Removed 3166 rows containing non-finite outside the scale range\n(`stat_qq_line()`).\n\n\n\n\n\n\n\n\n\nThat looks worse. Out of curiosity I’m going to try using the bestNormalize package I’ve been learning to see if it can recommend the best tranformation.\n\nBNobject &lt;- bestNormalize(data_ex$MV_Total)\n\nWarning: `progress_estimated()` was deprecated in dplyr 1.0.0.\nℹ The deprecated feature was likely used in the bestNormalize package.\n  Please report the issue to the authors.\n\nBNobject\n\nBest Normalizing transformation with 3264 Observations\n Estimated Normality Statistics (Pearson P / df, lower =&gt; more normal):\n - arcsinh(x): 24.5999\n - Box-Cox: 24.7093\n - Center+scale: 24.0263\n - Double Reversed Log_b(x+a): 25.2848\n - Exp(x): 344.4062\n - Log_b(x+a): 24.6074\n - orderNorm (ORQ): 24.2041\n - sqrt(x + a): 24.3073\n - Yeo-Johnson: 24.8438\nEstimation method: Out-of-sample via CV with 10 folds and 5 repeats\n \nBased off these, bestNormalize chose:\ncenter_scale(x) Transformation with 3264 nonmissing obs.\n Estimated statistics:\n - mean (before standardization) = 6.102022 \n - sd (before standardization) = 5.111536 \n\n\nThe short explanation of how these values work is that the value closest to 1 is the best transformation, but these are all VERY far from one. So that didn’t help.\n\nBoxplot\n\n#Create boxplot to assess for outliers\nggplot(data_ex, aes(y = MV_Total_log)) +\n  geom_boxplot() +\n  labs(title = \"Boxplot of MV Total Log\",\n       y = \"MV Total Log\")\n\nWarning: Removed 3166 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\n\n\n\n\nThere are also no outliers for MV_Total_Log!\n\n\nSummary\nIt looks like the best option we have is a log transformation of MV TOTAL.\nAlternatively, we will likely end up running a Poisson or Negative Binomial regression to handle this data.\nTop of Tabset\n\n\n\n\n# Create histogram and qqplot\ndistr_plots(data_ex, \"Unit_LOS\", 10)\n\n\n\n\n\n\n\n\nThat’s super skewed as well! Let’s try a log transform.\n\n# Perform log transform of MICU LOS\ndata_ex$Unit_LOS_log &lt;- log(data_ex$Unit_LOS)\n\n# Create histogram and qqplot\ndistr_plots(data_ex, \"Unit_LOS_log\", 10)\n\n\n\n\n\n\n\n\nWe’ve got more of an S shape going there. I wonder if a BoxCox transformation is more appropriate?\n\nlibrary(bestNormalize)\n# Use bestNormalize to try to find best transformation\nBNobject &lt;- bestNormalize(data_ex$Unit_LOS)\nBNobject\n\nBest Normalizing transformation with 6430 Observations\n Estimated Normality Statistics (Pearson P / df, lower =&gt; more normal):\n - arcsinh(x): 9.0156\n - Box-Cox: 5.6921\n - Center+scale: 34.1187\n - Double Reversed Log_b(x+a): 58.3866\n - Log_b(x+a): 9.0324\n - orderNorm (ORQ): 1.3715\n - sqrt(x + a): 20.0209\n - Yeo-Johnson: 5.7055\nEstimation method: Out-of-sample via CV with 10 folds and 5 repeats\n \nBased off these, bestNormalize chose:\norderNorm Transformation with 6430 nonmissing obs and ties\n - 515 unique values \n - Original quantiles:\n    0%    25%    50%    75%   100% \n 48.00  66.00  95.00 166.75 720.00 \n\n# Perform boxcox transformation\ndata_ex$Unit_LOS_boxcox &lt;- boxcox(data_ex$Unit_LOS)$x.t\n\n# Create histogram and qqplot\ndistr_plots(data_ex, \"Unit_LOS_boxcox\", 25)\n\n\n\n\n\n\n\n\n\n# Create boxplot to assess for outliers\nggplot(data_ex, aes(y = Unit_LOS_boxcox)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\nThere are no outliers for boxcox Unit LOS.\nThat histogram looks better at least, and we don’t have any outliers. The bestNormalize package offers ways to back transform after you run your analysis, but I haven’t gotten that far in learning it yet. It looks like a boxcox transformation is a candidate however.\n\nSummary\nWill come back to this. I think there is a specific link function we use in this situation when we have an s-shaped qq plot like that.\nTop of Tabset\n\n\n\n\n# Create histogram and qqplot\ndistr_plots(data_ex, \"`HOSPITAL LOS`\", 25)\n\n\n\n\n\n\n\n\nHospital LOS also looks logarithmic.\nLet’s transform and assess.\n\n# Perform log transform \ndata_ex$HOSPITAL_LOS_log &lt;- log(data_ex$`HOSPITAL LOS`)\n\n# Create histogram and qqplot\ndistr_plots(data_ex, \"HOSPITAL_LOS_log\", 25)\n\n\n\n\n\n\n\nggplot(data_ex, aes(y = HOSPITAL_LOS_log)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\nLooks good! There are a handful of outliers in there, but the data looks normal and those values will likely be included.\n\nSummary\nHOSPITAL LOS is normal after a log transform. We will either perform that or a Poisson or Negative Binomial Top of Tabset\n\n\n\nWe must compute NON_ICU_LOS by subtracting Unit_LOS from HOSPITAL LOS.\n\n# Compute variable for non icu LOS.\ndata_ex &lt;- data_ex |&gt; \n  mutate(NON_ICU_LOS = `HOSPITAL LOS` - Unit_LOS)\n\n# Create histogram and qqplot\ndistr_plots(data_ex, \"NON_ICU_LOS\", 25)\n\n\n\n\n\n\n\n\nAs expected, that looks exponential.\n\n# Perform log transform\ndata_ex$NON_ICU_LOS_log &lt;- log(data_ex$NON_ICU_LOS)\n\n# Create histogram and qqplot\ndistr_plots(data_ex, \"NON_ICU_LOS_log\", 25)\n\nWarning: Removed 106 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\nWarning: Removed 106 rows containing non-finite outside the scale range\n(`stat_qq()`).\n\n\nWarning: Removed 106 rows containing non-finite outside the scale range\n(`stat_qq_line()`).\n\n\n\n\n\n\n\n\n\nThat’s bimodal. Odd.\nLet’s try a square root transform\n\n# Perform log transform\ndata_ex$NON_ICU_LOS_sqrt &lt;- sqrt(data_ex$NON_ICU_LOS)\n\n# Create histogram and qqplot\ndistr_plots(data_ex, \"NON_ICU_LOS_sqrt\", 25)\n\n\n\n\n\n\n\n\nLooks crummy.\nLet’s try bestNormalize\n\nBNobject &lt;- bestNormalize(data_ex$NON_ICU_LOS)\nBNobject\n\nBest Normalizing transformation with 6430 Observations\n Estimated Normality Statistics (Pearson P / df, lower =&gt; more normal):\n - arcsinh(x): 7.1688\n - Center+scale: 49.5692\n - Double Reversed Log_b(x+a): 60.9389\n - Log_b(x+a): 8.765\n - orderNorm (ORQ): 1.6173\n - sqrt(x + a): 10.5195\n - Yeo-Johnson: 5.5792\nEstimation method: Out-of-sample via CV with 10 folds and 5 repeats\n \nBased off these, bestNormalize chose:\norderNorm Transformation with 6430 nonmissing obs and ties\n - 1045 unique values \n - Original quantiles:\n    0%    25%    50%    75%   100% \n   0.0   32.0  110.5  284.0 9206.0 \n\n# Perform boxcox transformation\ndata_ex$NON_ICU_LOS_yeo &lt;- yeojohnson(data_ex$NON_ICU_LOS)$x.t\n\n# Create histogram and qqplot\ndistr_plots(data_ex, \"NON_ICU_LOS_yeo\", 25)\n\n\n\n\n\n\n\n\nLooks better! Still bimodal but much more normal.\n\nSummary\nLooks like Yeo-Johnson transformation may be the way to go.\nTop of Tabset\n\n\n\n\n# Examine discharge locations\npretty_print(table(data_ex$DISCH_DISP))\n\n\n\n\nVar1\nFreq\n\n\n\n\nAcute IP to RPCU\n5\n\n\nAdministrative Discharge\n1\n\n\nAdmitted as an Inpatient\n3\n\n\nAgainst Clinical Advice\n22\n\n\nAnother Health Care Institution Not Defined w/Planned Readmission\n1\n\n\nCourt/Law Enforcement\n22\n\n\nDischarge Lounge\n6\n\n\nDischarged to Other Facility\n23\n\n\nDischarged/transferred to a Designated Cancer Center or Children’s Hospital\n41\n\n\nDrug/Alch Detox D/C\n4\n\n\nExpired\n1137\n\n\nExpired in Medical Facility\n8\n\n\nExpired Readmit as Organ Donor\n7\n\n\nFederal Hospital\n43\n\n\nHome-Health Care Svc\n759\n\n\nHome or Self Care\n2417\n\n\nHome or Self Care w/Planned Readmission\n1\n\n\nHospice/Home\n213\n\n\nHospice/Medical Facility\n211\n\n\nIntermediate Care Facility\n14\n\n\nIV Therapy Provider\n1\n\n\nLeft Against Medical Advice\n49\n\n\nLong Term Care\n451\n\n\nMental Health Facility\n2\n\n\nNursing Facility\n37\n\n\nPsychiatric Hospital\n34\n\n\nRehab Facility\n249\n\n\nShort Term Hospital\n71\n\n\nSkilled Nursing Facility\n596\n\n\nSwing Bed\n1\n\n\nTRANSFER TO CANCER OR CHILDRENS\n1\n\n\n\n\n\n\n\nThere are a few options we have for to analyze discharge location.\n\nWe can spit on expired vs not expired\nWe can split on hom vs other\nWe can just look at all the groups with the largest N, and collapse the rest into a single variable of ‘other’. This would be\n\nExpired (1137), Home (3177), Hospice (424), long term care (451), rehab (249), skilled Nursing facility (596)\n\n\nWe will have to ask the researcher how she wants to group up these discharge locations! Keeping in mind that since we are looking at three different time periods, each group will essentially by a third of what is shown here."
  },
  {
    "objectID": "Project_3_Predictive_Modelling/Project_3_R/Code/images/Megan_Watson_Project-01.html#average-micu-los",
    "href": "Project_3_Predictive_Modelling/Project_3_R/Code/images/Megan_Watson_Project-01.html#average-micu-los",
    "title": "Data Checking - Statistical Consulting",
    "section": "Average MICU LOS",
    "text": "Average MICU LOS\n\n# Summarize and plot the data\ndata_ex |&gt; \n  group_by(Admit_MonthYear) |&gt; \n  summarise(avg_Unit_LOS = mean(Unit_LOS, na.rm = T)) |&gt; \n  ggplot(aes(x = Admit_MonthYear, y = avg_Unit_LOS)) +\n  geom_line() +\n  theme_minimal() +\n  geom_rect(data = highlight_periods, aes(xmin = start, xmax = end, ymin = -Inf, ymax = Inf), \n            inherit.aes = FALSE, fill = \"lightblue\", alpha = 0.2) +\n  labs(title = \"Average MICU LOS over Time\",\n       x = \"Admit Date\",\n       y = \"Avg MICU LOS\")\n\n\n\n\n\n\n\n\nIt does not appear that MICU_LOS has decreased across any of the time periods.\nThis is including patients that did not receive PT, so of course there’s no difference.\n\nMICU LOS over Time by Mechanical Ventilation\n\n# Dummy code MV status \ndata_ex$MV_YN &lt;- ifelse(is.na(data_ex$MV_Total), 0, 1)\n\n# Make it a factor for better plotting\ndata_ex &lt;- data_ex |&gt; \n  mutate(MV_YN = as.factor(MV_YN))\n\n# Summarize and plot the data\ndata_ex |&gt; \n  group_by(Admit_MonthYear, MV_YN) |&gt; \n  summarise(avg_Unit_LOS = mean(Unit_LOS, na.rm = T)) |&gt; \n  ggplot(aes(x = Admit_MonthYear, y = avg_Unit_LOS, color = MV_YN, group = MV_YN)) +\n  geom_line() +\n  theme_minimal() +\n  geom_rect(data = highlight_periods, aes(xmin = start, xmax = end, ymin = -Inf, ymax = Inf), \n            inherit.aes = FALSE, fill = \"lightblue\", alpha = 0.2) +\n  labs(title = \"Average MICU LOS over Time by Mechanical Ventilation\",\n       x = \"Admit Date\",\n       y = \"Avg MICU LOS\")\n\n`summarise()` has grouped output by 'Admit_MonthYear'. You can override using\nthe `.groups` argument.\n\n\n\n\n\n\n\n\n\nThis shows that those who were mechanically ventilated had a higher MICU length of stay. This does not appear to have differed in relation to the QI iniative.\nI am also including those that did not receive PT here, so that would explain the null finding.\n\ndata_ex &lt;- data_ex |&gt; \n  mutate(PT_YESNO = as.factor(PT_YESNO))\n\n# Summarize and plot the data\ndata_ex |&gt; \n  group_by(Admit_MonthYear, MV_YN, PT_YESNO) |&gt; \n  summarise(avg_Unit_LOS = mean(Unit_LOS, na.rm = T)) |&gt; \n  ggplot(aes(x = Admit_MonthYear, y = avg_Unit_LOS, color = PT_YESNO)) +\n  geom_point() +\n  geom_smooth(method = \"loess\", se = F) +\n  theme_minimal() +\n  geom_rect(data = highlight_periods, aes(xmin = start, xmax = end, ymin = -Inf, ymax = Inf), \n            inherit.aes = FALSE, fill = \"lightblue\", alpha = 0.2) +\n  labs(title = \"Average MICU LOS over Time by Mechanical Ventilation\",\n       x = \"Admit Date\",\n       y = \"Avg MICU LOS\") +\n  facet_wrap(~MV_YN)\n\n`summarise()` has grouped output by 'Admit_MonthYear', 'MV_YN'. You can\noverride using the `.groups` argument.\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nI think this might show an interaction. For those on mechanical ventilation, they seemed to decrease in MICU LOS more if they received PT.\nFor those not on MV, the PT did not seem to change their average LOS.\n\n\nAverage MICU LOS over Time by Physical Therapy\n\n# Make it a factor for better plotting\ndata_ex &lt;- data_ex |&gt; \n  mutate(PT_YESNO = as.factor(PT_YESNO))\n\n# Summarize and plot the data\ndata_ex |&gt; \n  group_by(Admit_MonthYear, PT_YESNO) |&gt; \n  summarise(avg_Unit_LOS = mean(Unit_LOS, na.rm = T)) |&gt; \n  filter(Admit_MonthYear &gt; as.Date(\"2012-01-01\")) |&gt; \n  ggplot(aes(x = Admit_MonthYear, y = avg_Unit_LOS, color = PT_YESNO, group = PT_YESNO)) +\n  geom_line() +\n  theme_minimal() +\n  geom_rect(data = highlight_periods, aes(xmin = start, xmax = end, ymin = -Inf, ymax = Inf), \n            inherit.aes = FALSE, fill = \"lightblue\", alpha = 0.2) +\n  labs(title = \"Average MICU LOS over Time by Physical Therapy\",\n       x = \"Admit Date\",\n       y = \"Avg MICU LOS\")\n\n`summarise()` has grouped output by 'Admit_MonthYear'. You can override using\nthe `.groups` argument.\n\n\n\n\n\n\n\n\n\n\n# Summarize and plot the data\ndata_ex |&gt; \n  group_by(Admit_MonthYear, PT_YESNO) |&gt; \n  summarise(avg_Unit_LOS = mean(Unit_LOS, na.rm = T)) |&gt; \n  filter(Admit_MonthYear &gt; as.Date(\"2012-01-01\")) |&gt; \n  ggplot(aes(x = Admit_MonthYear, y = avg_Unit_LOS, color = PT_YESNO, group = PT_YESNO)) +\n  geom_point() +\n  geom_smooth(method = \"loess\", se = F) +\n  theme_minimal() +\n  geom_rect(data = highlight_periods, aes(xmin = start, xmax = end, ymin = -Inf, ymax = Inf), \n            inherit.aes = FALSE, fill = \"lightblue\", alpha = 0.2) +\n  labs(title = \"Average MICU LOS over Time by Physical Therapy\",\n       x = \"Admit Date\",\n       y = \"Avg MICU LOS\")\n\n`summarise()` has grouped output by 'Admit_MonthYear'. You can override using\nthe `.groups` argument.\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nInteresting! It does look like average MICU length of stay decreased slighly immediately after the QI initiative. It also looks like it may be increasing slightly from then on.\nThat does appear that those received PT had a decrease before and after the iniative\nBut overall it looks like the QI may have successfuly decreased MICU LOS.\n\n\nFitting with Splines\n\nlibrary(splines)\n# Summarize and plot the data\ndata_ex |&gt; \n  group_by(Admit_MonthYear, PT_YESNO) |&gt; \n  summarise(avg_Unit_LOS = mean(Unit_LOS, na.rm = T)) |&gt; \n  filter(Admit_MonthYear &gt; as.Date(\"2012-01-01\")) |&gt; \n  ggplot(aes(x = Admit_MonthYear, y = avg_Unit_LOS, color = PT_YESNO, group = PT_YESNO)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", formula = y ~ ns(x, df = 4), se = F) +\n  theme_minimal() +\n  geom_rect(data = highlight_periods, aes(xmin = start, xmax = end, ymin = -Inf, ymax = Inf), \n            inherit.aes = FALSE, fill = \"lightblue\", alpha = 0.2) +\n  labs(title = \"Average MICU LOS over Time by Physical Therapy\",\n       x = \"Admit Date\",\n       y = \"Avg MICU LOS\")\n\n`summarise()` has grouped output by 'Admit_MonthYear'. You can override using\nthe `.groups` argument.\n\n\n\n\n\n\n\n\n\nLooking just at those who received PT\n\n# Make it a factor for better plotting\ndata_ex &lt;- data_ex |&gt; \n  mutate(PT_YESNO = as.factor(PT_YESNO))\n\n# Summarize and plot the data\ndata_ex |&gt; \n  group_by(Admit_MonthYear, PT_YESNO) |&gt; \n  summarise(avg_Unit_LOS = mean(Unit_LOS, na.rm = T)) |&gt; \n  filter(Admit_MonthYear &gt; as.Date(\"2012-01-01\")) |&gt; \n  filter(PT_YESNO == 1) |&gt; \n  ggplot(aes(x = Admit_MonthYear, y = avg_Unit_LOS, group = PT_YESNO)) +\n  geom_point() +\n  geom_smooth(method = \"loess\", se = F) +\n  theme_minimal() +\n  geom_rect(data = highlight_periods, aes(xmin = start, xmax = end, ymin = -Inf, ymax = Inf), \n            inherit.aes = FALSE, fill = \"lightblue\", alpha = 0.2) +\n  labs(title = \"Average MICU LOS over Time by Physical Therapy\",\n       x = \"Admit Date\",\n       y = \"Avg MICU LOS\")\n\n`summarise()` has grouped output by 'Admit_MonthYear'. You can override using\nthe `.groups` argument.\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nFitting a loess curve, it does appear that for those that receive PT, they had a decrease in average MICU LOS during the intervention compared to before. It also appears that they are increasing in average MICU following the interention, but this may still be lower compared to the pre-intervention period.\n:::"
  },
  {
    "objectID": "Project_3_Predictive_Modelling/Project_3_R/Code/images/Megan_Watson_Project-01.html#increase-frequency-of-pt-visits",
    "href": "Project_3_Predictive_Modelling/Project_3_R/Code/images/Megan_Watson_Project-01.html#increase-frequency-of-pt-visits",
    "title": "Data Checking - Statistical Consulting",
    "section": "Increase Frequency of PT Visits",
    "text": "Increase Frequency of PT Visits\n\n# Summarize and plot the data\ndata_ex |&gt; \n  group_by(Admit_MonthYear) |&gt; \n  summarise(avg_pct_therapy = mean(`PERCENT OF DAYS WITH THERAPY VISITS`, na.rm = T)) |&gt; \n  ggplot(aes(x = Admit_MonthYear, y = avg_pct_therapy)) +\n  geom_line() +\n  theme_minimal() +\n  geom_rect(data = highlight_periods, aes(xmin = start, xmax = end, ymin = -Inf, ymax = Inf), \n            inherit.aes = FALSE, fill = \"lightblue\", alpha = 0.2) +\n  labs(title = \"Average Percentage of Days with Physical Therapy Over Time\",\n       x = \"Admit Date\",\n       y = \"Average Days with Physical Therapy (%)\")\n\n\n\n\n\n\n\n\nInteresting! They successfully and drastically increased the percentage of days that patients had physical therapy (roughly from 10-20% pre-iniative to 50-60% post iniative). This percentage did seem to drop off over time during the post- initiative period however, though it is still higher than it was pre-initiative (now roughly 30-40%).\n\nFit with Loess\n\n# Summarize and plot the data\ndata_ex |&gt; \n  group_by(Admit_MonthYear) |&gt; \n  summarise(avg_pct_therapy = mean(`PERCENT OF DAYS WITH THERAPY VISITS`, na.rm = T)) |&gt; \n  ggplot(aes(x = Admit_MonthYear, y = avg_pct_therapy)) +\n  geom_point() +\n  geom_smooth(method = \"loess\", se = F) +\n  theme_minimal() +\n  geom_rect(data = highlight_periods, aes(xmin = start, xmax = end, ymin = -Inf, ymax = Inf), \n            inherit.aes = FALSE, fill = \"lightblue\", alpha = 0.2) +\n  labs(title = \"Average Percentage of Days with Physical Therapy Over Time\",\n       x = \"Admit Date\",\n       y = \"Average Days with Physical Therapy (%)\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\nFit with Splines\n\n# Summarize and plot the data\ndata_ex |&gt; \n  group_by(Admit_MonthYear) |&gt; \n  summarise(avg_pct_therapy = mean(`PERCENT OF DAYS WITH THERAPY VISITS`, na.rm = T)) |&gt; \n  ggplot(aes(x = Admit_MonthYear, y = avg_pct_therapy)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", formula = y ~ ns(x, df = 5), se = F) +\n  theme_minimal() +\n  geom_rect(data = highlight_periods, aes(xmin = start, xmax = end, ymin = -Inf, ymax = Inf), \n            inherit.aes = FALSE, fill = \"lightblue\", alpha = 0.2) +\n  labs(title = \"Average Percentage of Days with Physical Therapy Over Time\",\n       x = \"Admit Date\",\n       y = \"Average Days with Physical Therapy (%)\")"
  },
  {
    "objectID": "Project_3_Predictive_Modelling/Project_3_R/Code/images/Megan_Watson_Project-01.html#decrease-time-from-admit-to-therapy",
    "href": "Project_3_Predictive_Modelling/Project_3_R/Code/images/Megan_Watson_Project-01.html#decrease-time-from-admit-to-therapy",
    "title": "Data Checking - Statistical Consulting",
    "section": "Decrease Time from Admit to Therapy",
    "text": "Decrease Time from Admit to Therapy\n\n# Summarize and plot the data\ndata_ex |&gt; \n  group_by(Admit_MonthYear) |&gt; \n  summarise(avg_ADMIT_TO_TREAT = mean(`ADMIT TO TREAT`, na.rm = T)) |&gt; \n  ggplot(aes(x = Admit_MonthYear, y = avg_ADMIT_TO_TREAT)) +\n  geom_line() +\n  theme_minimal() +\n  geom_rect(data = highlight_periods, aes(xmin = start, xmax = end, ymin = -Inf, ymax = Inf), \n            inherit.aes = FALSE, fill = \"lightblue\", alpha = 0.2) +\n  annotate(\"text\", x = as.Date(\"2015-09-01\"), y = Inf, \n           label = \"QI-Initiative\", color = \"skyblue2\", size = 4, vjust = 1.5,) +\n  labs(title = \"Time from Admit to Therapy over Time\",\n       x = \"Admit Date\",\n       y = \"Time from Admit to Therapy\")\n\n\n\n\n\n\n\n\nThey successfully decreased the time from a patient’s first admit to their first physical therapy session.\nThis means that patients were receiving PT sooner.\n\nPlot with Loess\n\n# Summarize and plot the data\ndata_ex |&gt; \n  group_by(Admit_MonthYear) |&gt; \n  summarise(avg_ADMIT_TO_TREAT = mean(`ADMIT TO TREAT`, na.rm = T)) |&gt; \n  ggplot(aes(x = Admit_MonthYear, y = avg_ADMIT_TO_TREAT)) +\n  geom_point() +\n  geom_smooth(method = \"loess\", se = F) +\n  theme_minimal() +\n  geom_rect(data = highlight_periods, aes(xmin = start, xmax = end, ymin = -Inf, ymax = Inf), \n            inherit.aes = FALSE, fill = \"lightblue\", alpha = 0.2) +\n  annotate(\"text\", x = as.Date(\"2015-09-01\"), y = Inf, \n           label = \"QI-Initiative\", color = \"skyblue2\", size = 4, vjust = 1.5,) +\n  labs(title = \"Time from Admit to Therapy over Time\",\n       x = \"Admit Date\",\n       y = \"Time from Admit to Therapy\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nYeah looks crummier with loess, will likely delete."
  },
  {
    "objectID": "Project_3_Predictive_Modelling/Project_3_R/Code/images/Megan_Watson_Project-01.html#increase-percentage-of-admissions-receiving-physical-therapy",
    "href": "Project_3_Predictive_Modelling/Project_3_R/Code/images/Megan_Watson_Project-01.html#increase-percentage-of-admissions-receiving-physical-therapy",
    "title": "Data Checking - Statistical Consulting",
    "section": "Increase Percentage of Admissions Receiving Physical Therapy",
    "text": "Increase Percentage of Admissions Receiving Physical Therapy\n\n# Summarize and calculate percentages\nsummary_data &lt;- data_ex |&gt; \n  group_by(Admit_MonthYear, PT_YESNO) |&gt; \n  summarise(count = n()) |&gt; \n  mutate(total_count = sum(count)) |&gt; \n  ungroup() |&gt; \n  mutate(percentage = (count / total_count) * 100) |&gt; \n  filter(PT_YESNO == 1)\n\n`summarise()` has grouped output by 'Admit_MonthYear'. You can override using\nthe `.groups` argument.\n\n# Plot the data\nggplot(summary_data, aes(x = Admit_MonthYear, y = percentage)) +\n  geom_line() +\n  theme_minimal() +\n  geom_rect(data = highlight_periods, aes(xmin = start, xmax = end, ymin = -Inf, ymax = Inf), \n            inherit.aes = FALSE, fill = \"lightblue\", alpha = 0.2) +\n  annotate(\"text\", x = as.Date(\"2015-09-01\"), y = Inf, \n           label = \"QI-Initiative\", color = \"skyblue2\", size = 4, vjust = 1.5) +\n  labs(title = \"Percentage of Admissions Receiving Physical Therapy\",\n       x = \"Admit Date\",\n       y = \"Percentage\")\n\n\n\n\n\n\n\n\nThey also successfully increased the percentage of patients receiving PT from ~40% pre-initiative to ~80% during the initiative. This percentage then decreases to ~70% post-initiative.\n::::"
  },
  {
    "objectID": "Project_3_Predictive_Modelling/Project_3_R/Code/images/Megan_Watson_Project-01.html#mechanical-ventilation-1",
    "href": "Project_3_Predictive_Modelling/Project_3_R/Code/images/Megan_Watson_Project-01.html#mechanical-ventilation-1",
    "title": "Data Checking - Statistical Consulting",
    "section": "Mechanical Ventilation",
    "text": "Mechanical Ventilation\nThe researcher expressed that patients would be stratified on Mechanical ventilation v. NO mechanical ventilation. However, I do not see this variable. Let’s go ahead and create it, assuming that patients with NA for MV_TOTAL were simply never on mechanical ventilation.\n\n# Dummy code MV status \ndata_ex$MV_YN &lt;- ifelse(is.na(data_ex$MV_Total), 0, 1)\n\nLet’s check that worked as intended.\n\n# Filter down to variables of interest to verify dummy coding worked\ncheck &lt;- data_ex %&gt;%\n  select(id, MV_Total, MV_YN)\n\n# Pretty print\nkable(head(check), format = \"html\") %&gt;%\n  kable_styling(bootstrap_options = c(\"condensed\", \"stripe\", \"hover\"))\n\n\n\n\nid\nMV_Total\nMV_YN\n\n\n\n\n2\nNA\n0\n\n\n3\n13\n1\n\n\n4\nNA\n0\n\n\n5\n16\n1\n\n\n8\nNA\n0\n\n\n10\n2\n1\n\n\n\n\n\n\n\nLooks good."
  },
  {
    "objectID": "Project_3_Predictive_Modelling/Project_3_R/Code/images/Megan_Watson_Project-01.html#qi-initiative-time-period",
    "href": "Project_3_Predictive_Modelling/Project_3_R/Code/images/Megan_Watson_Project-01.html#qi-initiative-time-period",
    "title": "Data Checking - Statistical Consulting",
    "section": "QI-Initiative Time Period",
    "text": "QI-Initiative Time Period\nLet’s group participants into the time period that they were present during the initiative for.\n\n# Create new variable based on QI initiative time period.\ndata_ex &lt;- data_ex |&gt; \n  mutate(initiative = ifelse(Admit_MonthYear &lt; as.Date(\"2015-04-01\"), \"Pre\",\n                             ifelse(Admit_MonthYear &gt; as.Date(\"2015-12-01\"), \"Post\", \"During\")))\n\nLet’s double check we did that right.\n\n# Check counts\ntable(data_ex$initiative)\n\n\nDuring   Post    Pre \n   656   2860   2889 \n\n# Double check assigning\ndata_ex |&gt; \n  select(id, Admit_MonthYear, initiative) |&gt; \n  filter(Admit_MonthYear &lt; as.Date(\"2015-04-01\")) |&gt; \n  arrange(desc(Admit_MonthYear)) |&gt; \n  head() |&gt; \n  pretty_print()\n\n\n\n\nid\nAdmit_MonthYear\ninitiative\n\n\n\n\n5759\n2015-03-01\nPre\n\n\n5760\n2015-03-01\nPre\n\n\n5761\n2015-03-01\nPre\n\n\n5763\n2015-03-01\nPre\n\n\n5765\n2015-03-01\nPre\n\n\n5767\n2015-03-01\nPre\n\n\n\n\n\n\n# Double check assigning\ndata_ex |&gt; \n  select(id, Admit_MonthYear, initiative) |&gt; \n  filter(Admit_MonthYear &gt; as.Date(\"2015-04-01\")) |&gt; \n  arrange(Admit_MonthYear) |&gt; \n  head() |&gt; \n  pretty_print()\n\n\n\n\nid\nAdmit_MonthYear\ninitiative\n\n\n\n\n6094\n2015-05-01\nDuring\n\n\n6096\n2015-05-01\nDuring\n\n\n6099\n2015-05-01\nDuring\n\n\n6100\n2015-05-01\nDuring\n\n\n6105\n2015-05-01\nDuring\n\n\n6106\n2015-05-01\nDuring\n\n\n\n\n\n\n# Double check assigning\ndata_ex |&gt; \n  select(id, Admit_MonthYear, initiative) |&gt; \n  filter(Admit_MonthYear &gt;= as.Date(\"2015-12-01\")) |&gt; \n  arrange(Admit_MonthYear) |&gt; \n  head() |&gt; \n  pretty_print()\n\n\n\n\nid\nAdmit_MonthYear\ninitiative\n\n\n\n\n7167\n2015-12-01\nDuring\n\n\n7169\n2015-12-01\nDuring\n\n\n7171\n2015-12-01\nDuring\n\n\n7172\n2015-12-01\nDuring\n\n\n7174\n2015-12-01\nDuring\n\n\n7176\n2015-12-01\nDuring\n\n\n\n\n\n\n# Double check assigning\ndata_ex |&gt; \n  select(id, Admit_MonthYear, initiative) |&gt; \n  filter(Admit_MonthYear &gt; as.Date(\"2015-12-01\")) |&gt; \n  arrange(Admit_MonthYear) |&gt; \n  head() |&gt; \n  pretty_print()\n\n\n\n\nid\nAdmit_MonthYear\ninitiative\n\n\n\n\n7324\n2016-01-01\nPost\n\n\n7331\n2016-01-01\nPost\n\n\n7336\n2016-01-01\nPost\n\n\n7339\n2016-01-01\nPost\n\n\n7343\n2016-01-01\nPost\n\n\n7345\n2016-01-01\nPost\n\n\n\n\n\n\n\nLooks good.\n\nSome Graphing\n\ndata_ex &lt;- data_ex |&gt; \n  mutate(initiative = as.factor(initiative)) |&gt; \n  mutate(initiative = relevel(initiative, ref = \"Pre\"))\n\nggplot(data_ex, aes(x = PT_YESNO, y = Unit_LOS_log, fill = initiative)) +\n  geom_boxplot() +\n  theme_minimal() +\n  scale_fill_brewer(palette = \"Pastel2\")\n\n\n\n\n\n\n\n\nThis shows that MICU LOS decreased in the during and post QI initiative periods compared to the pre-iniatitive period.\n\ndata_ex |&gt; \n  filter(PT_YESNO == 1) |&gt; \n  ggplot(aes(x = initiative, y = Unit_LOS_log, fill = initiative)) +\n  geom_boxplot() +\n  theme_minimal() +\n  scale_fill_brewer(palette = \"Pastel2\")\n\n\n\n\n\n\n\n\n\ndata_ex |&gt; \n  filter(PT_YESNO == 1) |&gt; \n  ggplot(aes(x = initiative, y = Unit_LOS_log, fill = initiative)) +\n  geom_boxplot() +\n  theme_minimal() +\n  scale_fill_brewer(palette = \"Pastel2\")\n\n\n\n\n\n\n\n\nThere is an interaction here between PT and unit LOS and initiative.\nIn other words, you HAVE to filter based on PT\n\n\nMV Total\n\nggplot(data_ex, aes(x = PT_YESNO, y = MV_Total_log, fill = initiative)) +\n  geom_boxplot() +\n  theme_minimal() +\n  scale_fill_brewer(palette = \"Pastel2\")\n\nWarning: Removed 3149 rows containing non-finite outside the scale range\n(`stat_boxplot()`)."
  },
  {
    "objectID": "Project_3_Predictive_Modelling/Project_3_R/Code/images/Megan_Watson_Project-01.html#check-for-repeat-admissions",
    "href": "Project_3_Predictive_Modelling/Project_3_R/Code/images/Megan_Watson_Project-01.html#check-for-repeat-admissions",
    "title": "Data Checking - Statistical Consulting",
    "section": "Check for Repeat Admissions",
    "text": "Check for Repeat Admissions\n\n# Check for repeate admissions\nlength(unique(data_ex$id))\n\n[1] 6405\n\ndim(data_ex)\n\n[1] 6405   40\n\n\nThe number of unique patient id’s is the same number as the number of rows in our data set.\nSo we do not have repeate admissions in this data set and do not need to account for repeated measures! (We meet the assumption of independent observations)\n:::"
  },
  {
    "objectID": "Project_3_Predictive_Modelling/Project_3_R/Code/images/Megan_Watson_Project.html",
    "href": "Project_3_Predictive_Modelling/Project_3_R/Code/images/Megan_Watson_Project.html",
    "title": "Data Checking - Statistical Consulting",
    "section": "",
    "text": "This is the Quarto markdown for the live consultation in BIOS 6621: Statistical Consulting.\nThis is a research project encompassing the entire data analysis pipeline, from initial consultation to scope of work writing, execution of statistical analysis, and completion of project deliverables."
  },
  {
    "objectID": "Project_3_Predictive_Modelling/Project_3_R/Code/images/Megan_Watson_Project.html#general-information",
    "href": "Project_3_Predictive_Modelling/Project_3_R/Code/images/Megan_Watson_Project.html#general-information",
    "title": "Data Checking - Statistical Consulting",
    "section": "General Information",
    "text": "General Information\n\n\n\n\n\n\n\n\n\nInvestigator\nMegan Watson\nDate\nSeptember 30, 2024\n\n\nProject Title\nQuality Improvement into Standard Practice: Persistent Practice Changes and Decreased Length of Stay for 3 years following a Medical ICU Physical Therapy Quality Improvement Project"
  },
  {
    "objectID": "Project_3_Predictive_Modelling/Project_3_R/Code/images/Megan_Watson_Project.html#project-description",
    "href": "Project_3_Predictive_Modelling/Project_3_R/Code/images/Megan_Watson_Project.html#project-description",
    "title": "Data Checking - Statistical Consulting",
    "section": "Project Description",
    "text": "Project Description\n\nBackground\n             The objective of this study is to assess the impact of a MICU physical therapy (PT) quality improvement (QI) project on physical therapist practice, and quantify the changes in length of stay (LOS) and mechanical ventilation (MV) time. The goals of the QI Initiative were to increase percentage of MICU admissions receiving PT, decrease time from MICU admission to first PT, and increase the frequency of PT visits. The main question the researcher came to us with was: how to correctly model the longitudinal data in the study?\n\n\nStudy Design\n              This was a retrospective cohort study, comparing 3 main time periods: Pre-QI Initiative (before April 2015), During QI Initiative (April 2015-Dec 2015), and After-QI Initiative (Jan 2016 and later). The QI initiative was deployed during the 9-month period between April 2015 and December 2015, during which time there were education and staffing changes. Patients were stratified as either having mechanical ventilation or not having mechanical ventilation. The primary outcomes of interest were MICU LOS and time on MV. Secondary outcomes of interest were the non-ICU LOS, total hospital LOS, and discharge location.\n\n\nDescription of the Data\n              Data was received as a de-identified .csv. Data points consist of: Admit and Discharge Time, PT Consult Time, Admit to Consult Time, First Therapy Time, Consult to Therapy Time, Admit to Therapy Time, Total Therapy Visits, Days with Therapy Visits, Average, min, and Max Therapy Length, Admit and Discharge Date/Time, Total MV Days, Hospital LOS, Patient Age, Sex, “Problem List”, and Codes (presumably ICD9/10).\n\n\nAnticipated Sample Size / Study Population\n              The sample size is N = ~3000-4000, consisting of patients admitted to the MICU with a LOS between 2 and 30 days (I.e. exclude &lt; 2 days or &gt; 30 days). This was a single site study performed here at Colorado University.\n\n\nHypothesis\n              The hypothesis for the study was that LOS and time on MV would decrease when comparing the pre- and intra-initiative time periods, and when comparing the pre- and post-initiative time periods. Clinical significance for this outcome is considered as a 1-2 day change in LOS. The intended end product of this project is a publication in a PT journal.\n\n\nAnalysis Plan (Sketch)\n              Data should first be examined for missingness, as this has not been performed. Similarly, repeat MICU admissions should be explored and controlled for to ensure independence of observations. Death during MICU stay should also be examined and adjusted for if needed. The analysis plan discussed was to utilize splines to capture the trajectory of LOS or time on MV over time by fitting smooth curves to the data. Linear contrasts should be performed to assess if there is a significant difference in LOS or time on MV when comparing two timepoints (i.e. pre- and intra-, pre- and post- , and post- and intra-QI Initiative. Supplementary analyses should be performed to assess secondary outcomes of interest of non-ICU LOS and total hospital LOS."
  },
  {
    "objectID": "Project_3_Predictive_Modelling/Project_3_R/Code/images/Megan_Watson_Project.html#project-deliverables",
    "href": "Project_3_Predictive_Modelling/Project_3_R/Code/images/Megan_Watson_Project.html#project-deliverables",
    "title": "Data Checking - Statistical Consulting",
    "section": "Project Deliverables",
    "text": "Project Deliverables\n             At this stage, Megan has only asked for insight into what method to use to account for variance in time for her analysis. There are currently no other deliverables.\nTimeline / Deadlines\nAt this time, Megan has not asked for data analysis efforts from us.\nShould this change, the anticipated project start date will be 1 week after our next meeting with her, where revisions to the Comprehensive Analysis Report are also expected to be completed.  The assigned analysts will reach out to the investigator to confirm the project timeframe and the project start date may be altered if necessary. 2 weeks will be allowed for initial data analysis by the assigned analyst.\nA follow-up meeting is to occur no later than 1 month after the next meeting with Megan. During the follow-up meeting, the project team will discuss and preliminary results of the analysis, mockup the desired tables and figures (1-2 descriptive tables, 1-2 analysis results tables and up to 4 graphs), and discuss next steps. During the follow-up meeting, the team will establish a meeting schedule for the remainder of the project. \nEstimated Cost\nThis represents our best estimate of the effort needed to complete the project. Should the scope of work change substantially, the analyst(s) will discuss changes with the investigator and issue a new or amended project agreement.\n\n\n\n\nEffort\n\n\n\n\nMonths\nPhD Faculty\nMaster’s Faculty\nStudent\n\n\n\n\n\n\n\n\n1\n20%\n0%\n80%\n\n\n\n\nTotal Costs\n’Bout Tree Fiddy"
  },
  {
    "objectID": "Project_3_Predictive_Modelling/Project_3_R/Code/images/Megan_Watson_Project.html#Outcome_Variables",
    "href": "Project_3_Predictive_Modelling/Project_3_R/Code/images/Megan_Watson_Project.html#Outcome_Variables",
    "title": "Data Checking - Statistical Consulting",
    "section": "Outcome Variables",
    "text": "Outcome Variables\n\nMechanical VentilationMICU Length of StayHospital Length of StayNon-ICU Length of StayDischarge Location\n\n\nFirst we will begin with mechanical ventilation time (MV)\n\n# Create histogram and qqplot\ndistr_plots(data_ex, \"MV_Total\")\n\nWarning: `aes_string()` was deprecated in ggplot2 3.0.0.\nℹ Please use tidy evaluation idioms with `aes()`.\nℹ See also `vignette(\"ggplot2-in-packages\")` for more information.\n\n\nWarning: Removed 3166 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\nWarning: Removed 3166 rows containing non-finite outside the scale range\n(`stat_qq()`).\n\n\nWarning: Removed 3166 rows containing non-finite outside the scale range\n(`stat_qq_line()`).\n\n\n\n\n\n\n\n\n\nThat’s some severe right skewness! Is that logarithmic? Let’s try and transform and see what happens.\n\n# Perform a log transform of mechanical ventilation time and plot\ndata_ex$MV_Total_log &lt;- log(data_ex$MV_Total)\n\n# Create histogram and qqplot\ndistr_plots(data_ex, \"MV_Total_log\", 10)\n\nWarning: Removed 3166 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\nWarning: Removed 3166 rows containing non-finite outside the scale range\n(`stat_qq()`).\n\n\nWarning: Removed 3166 rows containing non-finite outside the scale range\n(`stat_qq_line()`).\n\n\n\n\n\n\n\n\n\nThat’s better. Maybe try a square root transformation?\n\n# Perform a sqrt transformation and plot\ndata_ex$MV_Total_sqrt &lt;- sqrt(data_ex$MV_Total)\n\n# Create histogram and qqplot\ndistr_plots(data_ex, \"MV_Total_sqrt\", 10)\n\nWarning: Removed 3166 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\nWarning: Removed 3166 rows containing non-finite outside the scale range\n(`stat_qq()`).\n\n\nWarning: Removed 3166 rows containing non-finite outside the scale range\n(`stat_qq_line()`).\n\n\n\n\n\n\n\n\n\nThat looks worse. Out of curiosity I’m going to try using the bestNormalize package I’ve been learning to see if it can recommend the best tranformation.\n\nBNobject &lt;- bestNormalize(data_ex$MV_Total)\n\nWarning: `progress_estimated()` was deprecated in dplyr 1.0.0.\nℹ The deprecated feature was likely used in the bestNormalize package.\n  Please report the issue to the authors.\n\nBNobject\n\nBest Normalizing transformation with 3264 Observations\n Estimated Normality Statistics (Pearson P / df, lower =&gt; more normal):\n - arcsinh(x): 24.3719\n - Box-Cox: 24.5412\n - Center+scale: 23.8763\n - Double Reversed Log_b(x+a): 26.7038\n - Exp(x): 343.8966\n - Log_b(x+a): 24.393\n - orderNorm (ORQ): 24.1544\n - sqrt(x + a): 24.2678\n - Yeo-Johnson: 24.7983\nEstimation method: Out-of-sample via CV with 10 folds and 5 repeats\n \nBased off these, bestNormalize chose:\ncenter_scale(x) Transformation with 3264 nonmissing obs.\n Estimated statistics:\n - mean (before standardization) = 6.102022 \n - sd (before standardization) = 5.111536 \n\n\nThe short explanation of how these values work is that the value closest to 1 is the best transformation, but these are all VERY far from one. So that didn’t help.\n\nBoxplot\n\n#Create boxplot to assess for outliers\nggplot(data_ex, aes(y = MV_Total_log)) +\n  geom_boxplot() +\n  labs(title = \"Boxplot of MV Total Log\",\n       y = \"MV Total Log\")\n\nWarning: Removed 3166 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\n\n\n\n\nThere are also no outliers for MV_Total_Log!\n\n\nSummary\nIt looks like the best option we have is a log transformation of MV TOTAL.\nAlternatively, we will likely end up running a Poisson or Negative Binomial regression to handle this data.\nTop of Tabset\n\n\n\n\n# Create histogram and qqplot\ndistr_plots(data_ex, \"Unit_LOS\", 10)\n\n\n\n\n\n\n\n\nThat’s super skewed as well! Let’s try a log transform.\n\n# Perform log transform of MICU LOS\ndata_ex$Unit_LOS_log &lt;- log(data_ex$Unit_LOS)\n\n# Create histogram and qqplot\ndistr_plots(data_ex, \"Unit_LOS_log\", 10)\n\n\n\n\n\n\n\n\nWe’ve got more of an S shape going there. I wonder if a BoxCox transformation is more appropriate?\n\nlibrary(bestNormalize)\n# Use bestNormalize to try to find best transformation\nBNobject &lt;- bestNormalize(data_ex$Unit_LOS)\nBNobject\n\nBest Normalizing transformation with 6430 Observations\n Estimated Normality Statistics (Pearson P / df, lower =&gt; more normal):\n - arcsinh(x): 9.2659\n - Box-Cox: 5.7797\n - Center+scale: 34.0034\n - Double Reversed Log_b(x+a): 57.397\n - Log_b(x+a): 9.2634\n - orderNorm (ORQ): 1.4287\n - sqrt(x + a): 20.1311\n - Yeo-Johnson: 5.7206\nEstimation method: Out-of-sample via CV with 10 folds and 5 repeats\n \nBased off these, bestNormalize chose:\norderNorm Transformation with 6430 nonmissing obs and ties\n - 515 unique values \n - Original quantiles:\n    0%    25%    50%    75%   100% \n 48.00  66.00  95.00 166.75 720.00 \n\n# Perform boxcox transformation\ndata_ex$Unit_LOS_boxcox &lt;- boxcox(data_ex$Unit_LOS)$x.t\n\n# Create histogram and qqplot\ndistr_plots(data_ex, \"Unit_LOS_boxcox\", 25)\n\n\n\n\n\n\n\n\n\n# Create boxplot to assess for outliers\nggplot(data_ex, aes(y = Unit_LOS_boxcox)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\nThere are no outliers for boxcox Unit LOS.\nThat histogram looks better at least, and we don’t have any outliers. The bestNormalize package offers ways to back transform after you run your analysis, but I haven’t gotten that far in learning it yet. It looks like a boxcox transformation is a candidate however.\n\nSummary\nWill come back to this. I think there is a specific link function we use in this situation when we have an s-shaped qq plot like that.\nTop of Tabset\n\n\n\n\n# Create histogram and qqplot\ndistr_plots(data_ex, \"`HOSPITAL LOS`\", 25)\n\n\n\n\n\n\n\n\nHospital LOS also looks logarithmic.\nLet’s transform and assess.\n\n# Perform log transform \ndata_ex$HOSPITAL_LOS_log &lt;- log(data_ex$`HOSPITAL LOS`)\n\n# Create histogram and qqplot\ndistr_plots(data_ex, \"HOSPITAL_LOS_log\", 25)\n\n\n\n\n\n\n\nggplot(data_ex, aes(y = HOSPITAL_LOS_log)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\nLooks good! There are a handful of outliers in there, but the data looks normal and those values will likely be included.\n\nSummary\nHOSPITAL LOS is normal after a log transform. We will either perform that or a Poisson or Negative Binomial Top of Tabset\n\n\n\nWe must compute NON_ICU_LOS by subtracting Unit_LOS from HOSPITAL LOS.\n\n# Compute variable for non icu LOS.\ndata_ex &lt;- data_ex |&gt; \n  mutate(NON_ICU_LOS = `HOSPITAL LOS` - Unit_LOS)\n\n# Create histogram and qqplot\ndistr_plots(data_ex, \"NON_ICU_LOS\", 25)\n\n\n\n\n\n\n\n\nAs expected, that looks exponential.\n\n# Perform log transform\ndata_ex$NON_ICU_LOS_log &lt;- log(data_ex$NON_ICU_LOS)\n\n# Create histogram and qqplot\ndistr_plots(data_ex, \"NON_ICU_LOS_log\", 25)\n\nWarning: Removed 106 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\nWarning: Removed 106 rows containing non-finite outside the scale range\n(`stat_qq()`).\n\n\nWarning: Removed 106 rows containing non-finite outside the scale range\n(`stat_qq_line()`).\n\n\n\n\n\n\n\n\n\nThat’s bimodal. Odd.\nLet’s try a square root transform\n\n# Perform log transform\ndata_ex$NON_ICU_LOS_sqrt &lt;- sqrt(data_ex$NON_ICU_LOS)\n\n# Create histogram and qqplot\ndistr_plots(data_ex, \"NON_ICU_LOS_sqrt\", 25)\n\n\n\n\n\n\n\n\nLooks crummy.\nLet’s try bestNormalize\n\nBNobject &lt;- bestNormalize(data_ex$NON_ICU_LOS)\nBNobject\n\nBest Normalizing transformation with 6430 Observations\n Estimated Normality Statistics (Pearson P / df, lower =&gt; more normal):\n - arcsinh(x): 7.2026\n - Center+scale: 51.7918\n - Double Reversed Log_b(x+a): 57.9976\n - Log_b(x+a): 8.7389\n - orderNorm (ORQ): 1.5462\n - sqrt(x + a): 10.5123\n - Yeo-Johnson: 5.2059\nEstimation method: Out-of-sample via CV with 10 folds and 5 repeats\n \nBased off these, bestNormalize chose:\norderNorm Transformation with 6430 nonmissing obs and ties\n - 1045 unique values \n - Original quantiles:\n    0%    25%    50%    75%   100% \n   0.0   32.0  110.5  284.0 9206.0 \n\n# Perform boxcox transformation\ndata_ex$NON_ICU_LOS_yeo &lt;- yeojohnson(data_ex$NON_ICU_LOS)$x.t\n\n# Create histogram and qqplot\ndistr_plots(data_ex, \"NON_ICU_LOS_yeo\", 25)\n\n\n\n\n\n\n\n\nLooks better! Still bimodal but much more normal.\n\nSummary\nLooks like Yeo-Johnson transformation may be the way to go.\nTop of Tabset\n\n\n\n\n# Examine discharge locations\npretty_print(table(data_ex$DISCH_DISP))\n\n\n\n\nVar1\nFreq\n\n\n\n\nAcute IP to RPCU\n5\n\n\nAdministrative Discharge\n1\n\n\nAdmitted as an Inpatient\n3\n\n\nAgainst Clinical Advice\n22\n\n\nAnother Health Care Institution Not Defined w/Planned Readmission\n1\n\n\nCourt/Law Enforcement\n22\n\n\nDischarge Lounge\n6\n\n\nDischarged to Other Facility\n23\n\n\nDischarged/transferred to a Designated Cancer Center or Children’s Hospital\n41\n\n\nDrug/Alch Detox D/C\n4\n\n\nExpired\n1137\n\n\nExpired in Medical Facility\n8\n\n\nExpired Readmit as Organ Donor\n7\n\n\nFederal Hospital\n43\n\n\nHome-Health Care Svc\n759\n\n\nHome or Self Care\n2417\n\n\nHome or Self Care w/Planned Readmission\n1\n\n\nHospice/Home\n213\n\n\nHospice/Medical Facility\n211\n\n\nIntermediate Care Facility\n14\n\n\nIV Therapy Provider\n1\n\n\nLeft Against Medical Advice\n49\n\n\nLong Term Care\n451\n\n\nMental Health Facility\n2\n\n\nNursing Facility\n37\n\n\nPsychiatric Hospital\n34\n\n\nRehab Facility\n249\n\n\nShort Term Hospital\n71\n\n\nSkilled Nursing Facility\n596\n\n\nSwing Bed\n1\n\n\nTRANSFER TO CANCER OR CHILDRENS\n1\n\n\n\n\n\n\n\nThere are a few options we have for to analyze discharge location.\n\nWe can spit on expired vs not expired\nWe can split on hom vs other\nWe can just look at all the groups with the largest N, and collapse the rest into a single variable of ‘other’. This would be\n\nExpired (1137), Home (3177), Hospice (424), long term care (451), rehab (249), skilled Nursing facility (596)\n\n\nWe will have to ask the researcher how she wants to group up these discharge locations! Keeping in mind that since we are looking at three different time periods, each group will essentially by a third of what is shown here."
  },
  {
    "objectID": "Project_3_Predictive_Modelling/Project_3_R/Code/images/Megan_Watson_Project.html#average-micu-los",
    "href": "Project_3_Predictive_Modelling/Project_3_R/Code/images/Megan_Watson_Project.html#average-micu-los",
    "title": "Data Checking - Statistical Consulting",
    "section": "Average MICU LOS",
    "text": "Average MICU LOS\n\n# Summarize and plot the data\ndata_ex |&gt; \n  group_by(Admit_MonthYear) |&gt; \n  summarise(avg_Unit_LOS = mean(Unit_LOS, na.rm = T)) |&gt; \n  ggplot(aes(x = Admit_MonthYear, y = avg_Unit_LOS)) +\n  geom_line() +\n  theme_minimal() +\n  geom_rect(data = highlight_periods, aes(xmin = start, xmax = end, ymin = -Inf, ymax = Inf), \n            inherit.aes = FALSE, fill = \"lightblue\", alpha = 0.2) +\n  labs(title = \"Average MICU LOS over Time\",\n       x = \"Admit Date\",\n       y = \"Avg MICU LOS\")\n\n\n\n\n\n\n\n\nIt does not appear that MICU_LOS has decreased across any of the time periods.\nThis is including patients that did not receive PT, so of course there’s no difference.\n\nMICU LOS over Time by Mechanical Ventilation\n\n# Dummy code MV status \ndata_ex$MV_YN &lt;- ifelse(is.na(data_ex$MV_Total), 0, 1)\n\n# Make it a factor for better plotting\ndata_ex &lt;- data_ex |&gt; \n  mutate(MV_YN = as.factor(MV_YN))\n\n# Summarize and plot the data\ndata_ex |&gt; \n  group_by(Admit_MonthYear, MV_YN) |&gt; \n  summarise(avg_Unit_LOS = mean(Unit_LOS, na.rm = T)) |&gt; \n  ggplot(aes(x = Admit_MonthYear, y = avg_Unit_LOS, color = MV_YN, group = MV_YN)) +\n  geom_line() +\n  theme_minimal() +\n  geom_rect(data = highlight_periods, aes(xmin = start, xmax = end, ymin = -Inf, ymax = Inf), \n            inherit.aes = FALSE, fill = \"lightblue\", alpha = 0.2) +\n  labs(title = \"Average MICU LOS over Time by Mechanical Ventilation\",\n       x = \"Admit Date\",\n       y = \"Avg MICU LOS\")\n\n`summarise()` has grouped output by 'Admit_MonthYear'. You can override using\nthe `.groups` argument.\n\n\n\n\n\n\n\n\n\nThis shows that those who were mechanically ventilated had a higher MICU length of stay. This does not appear to have differed in relation to the QI iniative.\nI am also including those that did not receive PT here, so that would explain the null finding.\n\ndata_ex &lt;- data_ex |&gt; \n  mutate(PT_YESNO = as.factor(PT_YESNO))\n\n# Summarize and plot the data\ndata_ex |&gt; \n  group_by(Admit_MonthYear, MV_YN, PT_YESNO) |&gt; \n  summarise(avg_Unit_LOS = mean(Unit_LOS, na.rm = T)) |&gt; \n  ggplot(aes(x = Admit_MonthYear, y = avg_Unit_LOS, color = PT_YESNO)) +\n  geom_point() +\n  geom_smooth(method = \"loess\", se = F) +\n  theme_minimal() +\n  geom_rect(data = highlight_periods, aes(xmin = start, xmax = end, ymin = -Inf, ymax = Inf), \n            inherit.aes = FALSE, fill = \"lightblue\", alpha = 0.2) +\n  labs(title = \"Average MICU LOS over Time by Mechanical Ventilation\",\n       x = \"Admit Date\",\n       y = \"Avg MICU LOS\") +\n  facet_wrap(~MV_YN)\n\n`summarise()` has grouped output by 'Admit_MonthYear', 'MV_YN'. You can\noverride using the `.groups` argument.\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nI think this might show an interaction. For those on mechanical ventilation, they seemed to decrease in MICU LOS more if they received PT.\nFor those not on MV, the PT did not seem to change their average LOS.\n\n\nAverage MICU LOS over Time by Physical Therapy\n\n# Make it a factor for better plotting\ndata_ex &lt;- data_ex |&gt; \n  mutate(PT_YESNO = as.factor(PT_YESNO))\n\n# Summarize and plot the data\ndata_ex |&gt; \n  group_by(Admit_MonthYear, PT_YESNO) |&gt; \n  summarise(avg_Unit_LOS = mean(Unit_LOS, na.rm = T)) |&gt; \n  filter(Admit_MonthYear &gt; as.Date(\"2012-01-01\")) |&gt; \n  ggplot(aes(x = Admit_MonthYear, y = avg_Unit_LOS, color = PT_YESNO, group = PT_YESNO)) +\n  geom_line() +\n  theme_minimal() +\n  geom_rect(data = highlight_periods, aes(xmin = start, xmax = end, ymin = -Inf, ymax = Inf), \n            inherit.aes = FALSE, fill = \"lightblue\", alpha = 0.2) +\n  labs(title = \"Average MICU LOS over Time by Physical Therapy\",\n       x = \"Admit Date\",\n       y = \"Avg MICU LOS\")\n\n`summarise()` has grouped output by 'Admit_MonthYear'. You can override using\nthe `.groups` argument.\n\n\n\n\n\n\n\n\n\n\n# Summarize and plot the data\ndata_ex |&gt; \n  group_by(Admit_MonthYear, PT_YESNO) |&gt; \n  summarise(avg_Unit_LOS = mean(Unit_LOS, na.rm = T)) |&gt; \n  filter(Admit_MonthYear &gt; as.Date(\"2012-01-01\")) |&gt; \n  ggplot(aes(x = Admit_MonthYear, y = avg_Unit_LOS, color = PT_YESNO, group = PT_YESNO)) +\n  geom_point() +\n  geom_smooth(method = \"loess\", se = F) +\n  theme_minimal() +\n  geom_rect(data = highlight_periods, aes(xmin = start, xmax = end, ymin = -Inf, ymax = Inf), \n            inherit.aes = FALSE, fill = \"lightblue\", alpha = 0.2) +\n  labs(title = \"Average MICU LOS over Time by Physical Therapy\",\n       x = \"Admit Date\",\n       y = \"Avg MICU LOS\")\n\n`summarise()` has grouped output by 'Admit_MonthYear'. You can override using\nthe `.groups` argument.\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nInteresting! It does look like average MICU length of stay decreased slighly immediately after the QI initiative. It also looks like it may be increasing slightly from then on.\nThat does appear that those received PT had a decrease before and after the iniative\nBut overall it looks like the QI may have successfuly decreased MICU LOS.\n\n\nFitting with Splines\n\nlibrary(splines)\n# Summarize and plot the data\ndata_ex |&gt; \n  group_by(Admit_MonthYear, PT_YESNO) |&gt; \n  summarise(avg_Unit_LOS = mean(Unit_LOS, na.rm = T)) |&gt; \n  filter(Admit_MonthYear &gt; as.Date(\"2012-01-01\")) |&gt; \n  ggplot(aes(x = Admit_MonthYear, y = avg_Unit_LOS, color = PT_YESNO, group = PT_YESNO)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", formula = y ~ ns(x, df = 4), se = F) +\n  theme_minimal() +\n  geom_rect(data = highlight_periods, aes(xmin = start, xmax = end, ymin = -Inf, ymax = Inf), \n            inherit.aes = FALSE, fill = \"lightblue\", alpha = 0.2) +\n  labs(title = \"Average MICU LOS over Time by Physical Therapy\",\n       x = \"Admit Date\",\n       y = \"Avg MICU LOS\")\n\n`summarise()` has grouped output by 'Admit_MonthYear'. You can override using\nthe `.groups` argument.\n\n\n\n\n\n\n\n\n\nLooking just at those who received PT\n\n# Make it a factor for better plotting\ndata_ex &lt;- data_ex |&gt; \n  mutate(PT_YESNO = as.factor(PT_YESNO))\n\n# Summarize and plot the data\ndata_ex |&gt; \n  group_by(Admit_MonthYear, PT_YESNO) |&gt; \n  summarise(avg_Unit_LOS = mean(Unit_LOS, na.rm = T)) |&gt; \n  filter(Admit_MonthYear &gt; as.Date(\"2012-01-01\")) |&gt; \n  filter(PT_YESNO == 1) |&gt; \n  ggplot(aes(x = Admit_MonthYear, y = avg_Unit_LOS, group = PT_YESNO)) +\n  geom_point() +\n  geom_smooth(method = \"loess\", se = F) +\n  theme_minimal() +\n  geom_rect(data = highlight_periods, aes(xmin = start, xmax = end, ymin = -Inf, ymax = Inf), \n            inherit.aes = FALSE, fill = \"lightblue\", alpha = 0.2) +\n  labs(title = \"Average MICU LOS over Time by Physical Therapy\",\n       x = \"Admit Date\",\n       y = \"Avg MICU LOS\")\n\n`summarise()` has grouped output by 'Admit_MonthYear'. You can override using\nthe `.groups` argument.\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nFitting a loess curve, it does appear that for those that receive PT, they had a decrease in average MICU LOS during the intervention compared to before. It also appears that they are increasing in average MICU following the interention, but this may still be lower compared to the pre-intervention period.\n:::"
  },
  {
    "objectID": "Project_3_Predictive_Modelling/Project_3_R/Code/images/Megan_Watson_Project.html#increase-frequency-of-pt-visits",
    "href": "Project_3_Predictive_Modelling/Project_3_R/Code/images/Megan_Watson_Project.html#increase-frequency-of-pt-visits",
    "title": "Data Checking - Statistical Consulting",
    "section": "Increase Frequency of PT Visits",
    "text": "Increase Frequency of PT Visits\n\n# Summarize and plot the data\ndata_ex |&gt; \n  group_by(Admit_MonthYear) |&gt; \n  summarise(avg_pct_therapy = mean(`PERCENT OF DAYS WITH THERAPY VISITS`, na.rm = T)) |&gt; \n  ggplot(aes(x = Admit_MonthYear, y = avg_pct_therapy)) +\n  geom_line() +\n  theme_minimal() +\n  geom_rect(data = highlight_periods, aes(xmin = start, xmax = end, ymin = -Inf, ymax = Inf), \n            inherit.aes = FALSE, fill = \"lightblue\", alpha = 0.2) +\n  labs(title = \"Average Percentage of Days with Physical Therapy Over Time\",\n       x = \"Admit Date\",\n       y = \"Average Days with Physical Therapy (%)\")\n\n\n\n\n\n\n\n\nInteresting! They successfully and drastically increased the percentage of days that patients had physical therapy (roughly from 10-20% pre-iniative to 50-60% post iniative). This percentage did seem to drop off over time during the post- initiative period however, though it is still higher than it was pre-initiative (now roughly 30-40%).\n\nFit with Loess\n\n# Summarize and plot the data\ndata_ex |&gt; \n  group_by(Admit_MonthYear) |&gt; \n  summarise(avg_pct_therapy = mean(`PERCENT OF DAYS WITH THERAPY VISITS`, na.rm = T)) |&gt; \n  ggplot(aes(x = Admit_MonthYear, y = avg_pct_therapy)) +\n  geom_point() +\n  geom_smooth(method = \"loess\", se = F) +\n  theme_minimal() +\n  geom_rect(data = highlight_periods, aes(xmin = start, xmax = end, ymin = -Inf, ymax = Inf), \n            inherit.aes = FALSE, fill = \"lightblue\", alpha = 0.2) +\n  labs(title = \"Average Percentage of Days with Physical Therapy Over Time\",\n       x = \"Admit Date\",\n       y = \"Average Days with Physical Therapy (%)\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\nFit with Splines\n\n# Summarize and plot the data\ndata_ex |&gt; \n  group_by(Admit_MonthYear) |&gt; \n  summarise(avg_pct_therapy = mean(`PERCENT OF DAYS WITH THERAPY VISITS`, na.rm = T)) |&gt; \n  ggplot(aes(x = Admit_MonthYear, y = avg_pct_therapy)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", formula = y ~ ns(x, df = 5), se = F) +\n  theme_minimal() +\n  geom_rect(data = highlight_periods, aes(xmin = start, xmax = end, ymin = -Inf, ymax = Inf), \n            inherit.aes = FALSE, fill = \"lightblue\", alpha = 0.2) +\n  labs(title = \"Average Percentage of Days with Physical Therapy Over Time\",\n       x = \"Admit Date\",\n       y = \"Average Days with Physical Therapy (%)\")"
  },
  {
    "objectID": "Project_3_Predictive_Modelling/Project_3_R/Code/images/Megan_Watson_Project.html#decrease-time-from-admit-to-therapy",
    "href": "Project_3_Predictive_Modelling/Project_3_R/Code/images/Megan_Watson_Project.html#decrease-time-from-admit-to-therapy",
    "title": "Data Checking - Statistical Consulting",
    "section": "Decrease Time from Admit to Therapy",
    "text": "Decrease Time from Admit to Therapy\n\n# Summarize and plot the data\ndata_ex |&gt; \n  group_by(Admit_MonthYear) |&gt; \n  summarise(avg_ADMIT_TO_TREAT = mean(`ADMIT TO TREAT`, na.rm = T)) |&gt; \n  ggplot(aes(x = Admit_MonthYear, y = avg_ADMIT_TO_TREAT)) +\n  geom_line() +\n  theme_minimal() +\n  geom_rect(data = highlight_periods, aes(xmin = start, xmax = end, ymin = -Inf, ymax = Inf), \n            inherit.aes = FALSE, fill = \"lightblue\", alpha = 0.2) +\n  annotate(\"text\", x = as.Date(\"2015-09-01\"), y = Inf, \n           label = \"QI-Initiative\", color = \"skyblue2\", size = 4, vjust = 1.5,) +\n  labs(title = \"Time from Admit to Therapy over Time\",\n       x = \"Admit Date\",\n       y = \"Time from Admit to Therapy\")\n\n\n\n\n\n\n\n\nThey successfully decreased the time from a patient’s first admit to their first physical therapy session.\nThis means that patients were receiving PT sooner.\n\nPlot with Loess\n\n# Summarize and plot the data\ndata_ex |&gt; \n  group_by(Admit_MonthYear) |&gt; \n  summarise(avg_ADMIT_TO_TREAT = mean(`ADMIT TO TREAT`, na.rm = T)) |&gt; \n  ggplot(aes(x = Admit_MonthYear, y = avg_ADMIT_TO_TREAT)) +\n  geom_point() +\n  geom_smooth(method = \"loess\", se = F) +\n  theme_minimal() +\n  geom_rect(data = highlight_periods, aes(xmin = start, xmax = end, ymin = -Inf, ymax = Inf), \n            inherit.aes = FALSE, fill = \"lightblue\", alpha = 0.2) +\n  annotate(\"text\", x = as.Date(\"2015-09-01\"), y = Inf, \n           label = \"QI-Initiative\", color = \"skyblue2\", size = 4, vjust = 1.5,) +\n  labs(title = \"Time from Admit to Therapy over Time\",\n       x = \"Admit Date\",\n       y = \"Time from Admit to Therapy\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nYeah looks crummier with loess, will likely delete."
  },
  {
    "objectID": "Project_3_Predictive_Modelling/Project_3_R/Code/images/Megan_Watson_Project.html#increase-percentage-of-admissions-receiving-physical-therapy",
    "href": "Project_3_Predictive_Modelling/Project_3_R/Code/images/Megan_Watson_Project.html#increase-percentage-of-admissions-receiving-physical-therapy",
    "title": "Data Checking - Statistical Consulting",
    "section": "Increase Percentage of Admissions Receiving Physical Therapy",
    "text": "Increase Percentage of Admissions Receiving Physical Therapy\n\n# Summarize and calculate percentages\nsummary_data &lt;- data_ex |&gt; \n  group_by(Admit_MonthYear, PT_YESNO) |&gt; \n  summarise(count = n()) |&gt; \n  mutate(total_count = sum(count)) |&gt; \n  ungroup() |&gt; \n  mutate(percentage = (count / total_count) * 100) |&gt; \n  filter(PT_YESNO == 1)\n\n`summarise()` has grouped output by 'Admit_MonthYear'. You can override using\nthe `.groups` argument.\n\n# Plot the data\nggplot(summary_data, aes(x = Admit_MonthYear, y = percentage)) +\n  geom_line() +\n  theme_minimal() +\n  geom_rect(data = highlight_periods, aes(xmin = start, xmax = end, ymin = -Inf, ymax = Inf), \n            inherit.aes = FALSE, fill = \"lightblue\", alpha = 0.2) +\n  annotate(\"text\", x = as.Date(\"2015-09-01\"), y = Inf, \n           label = \"QI-Initiative\", color = \"skyblue2\", size = 4, vjust = 1.5) +\n  labs(title = \"Percentage of Admissions Receiving Physical Therapy\",\n       x = \"Admit Date\",\n       y = \"Percentage\")\n\n\n\n\n\n\n\n\nThey also successfully increased the percentage of patients receiving PT from ~40% pre-initiative to ~80% during the initiative. This percentage then decreases to ~70% post-initiative.\n::::"
  },
  {
    "objectID": "Project_3_Predictive_Modelling/Project_3_R/Code/images/Megan_Watson_Project.html#mechanical-ventilation-1",
    "href": "Project_3_Predictive_Modelling/Project_3_R/Code/images/Megan_Watson_Project.html#mechanical-ventilation-1",
    "title": "Data Checking - Statistical Consulting",
    "section": "Mechanical Ventilation",
    "text": "Mechanical Ventilation\nThe researcher expressed that patients would be stratified on Mechanical ventilation v. NO mechanical ventilation. However, I do not see this variable. Let’s go ahead and create it, assuming that patients with NA for MV_TOTAL were simply never on mechanical ventilation.\n\n# Dummy code MV status \ndata_ex$MV_YN &lt;- ifelse(is.na(data_ex$MV_Total), 0, 1)\n\nLet’s check that worked as intended.\n\n# Filter down to variables of interest to verify dummy coding worked\ncheck &lt;- data_ex %&gt;%\n  select(id, MV_Total, MV_YN)\n\n# Pretty print\nkable(head(check), format = \"html\") %&gt;%\n  kable_styling(bootstrap_options = c(\"condensed\", \"stripe\", \"hover\"))\n\n\n\n\nid\nMV_Total\nMV_YN\n\n\n\n\n2\nNA\n0\n\n\n3\n13\n1\n\n\n4\nNA\n0\n\n\n5\n16\n1\n\n\n8\nNA\n0\n\n\n10\n2\n1\n\n\n\n\n\n\n\nLooks good."
  },
  {
    "objectID": "Project_3_Predictive_Modelling/Project_3_R/Code/images/Megan_Watson_Project.html#qi-initiative-time-period",
    "href": "Project_3_Predictive_Modelling/Project_3_R/Code/images/Megan_Watson_Project.html#qi-initiative-time-period",
    "title": "Data Checking - Statistical Consulting",
    "section": "QI-Initiative Time Period",
    "text": "QI-Initiative Time Period\nLet’s group participants into the time period that they were present during the initiative for.\n\n# Create new variable based on QI initiative time period.\ndata_ex &lt;- data_ex |&gt; \n  mutate(initiative = ifelse(Admit_MonthYear &lt; as.Date(\"2015-04-01\"), \"Pre\",\n                             ifelse(Admit_MonthYear &gt; as.Date(\"2015-12-01\"), \"Post\", \"During\")))\n\nLet’s double check we did that right.\n\n# Check counts\ntable(data_ex$initiative)\n\n\nDuring   Post    Pre \n   656   2860   2889 \n\n# Double check assigning\ndata_ex |&gt; \n  select(id, Admit_MonthYear, initiative) |&gt; \n  filter(Admit_MonthYear &lt; as.Date(\"2015-04-01\")) |&gt; \n  arrange(desc(Admit_MonthYear)) |&gt; \n  head() |&gt; \n  pretty_print()\n\n\n\n\nid\nAdmit_MonthYear\ninitiative\n\n\n\n\n5759\n2015-03-01\nPre\n\n\n5760\n2015-03-01\nPre\n\n\n5761\n2015-03-01\nPre\n\n\n5763\n2015-03-01\nPre\n\n\n5765\n2015-03-01\nPre\n\n\n5767\n2015-03-01\nPre\n\n\n\n\n\n\n# Double check assigning\ndata_ex |&gt; \n  select(id, Admit_MonthYear, initiative) |&gt; \n  filter(Admit_MonthYear &gt; as.Date(\"2015-04-01\")) |&gt; \n  arrange(Admit_MonthYear) |&gt; \n  head() |&gt; \n  pretty_print()\n\n\n\n\nid\nAdmit_MonthYear\ninitiative\n\n\n\n\n6094\n2015-05-01\nDuring\n\n\n6096\n2015-05-01\nDuring\n\n\n6099\n2015-05-01\nDuring\n\n\n6100\n2015-05-01\nDuring\n\n\n6105\n2015-05-01\nDuring\n\n\n6106\n2015-05-01\nDuring\n\n\n\n\n\n\n# Double check assigning\ndata_ex |&gt; \n  select(id, Admit_MonthYear, initiative) |&gt; \n  filter(Admit_MonthYear &gt;= as.Date(\"2015-12-01\")) |&gt; \n  arrange(Admit_MonthYear) |&gt; \n  head() |&gt; \n  pretty_print()\n\n\n\n\nid\nAdmit_MonthYear\ninitiative\n\n\n\n\n7167\n2015-12-01\nDuring\n\n\n7169\n2015-12-01\nDuring\n\n\n7171\n2015-12-01\nDuring\n\n\n7172\n2015-12-01\nDuring\n\n\n7174\n2015-12-01\nDuring\n\n\n7176\n2015-12-01\nDuring\n\n\n\n\n\n\n# Double check assigning\ndata_ex |&gt; \n  select(id, Admit_MonthYear, initiative) |&gt; \n  filter(Admit_MonthYear &gt; as.Date(\"2015-12-01\")) |&gt; \n  arrange(Admit_MonthYear) |&gt; \n  head() |&gt; \n  pretty_print()\n\n\n\n\nid\nAdmit_MonthYear\ninitiative\n\n\n\n\n7324\n2016-01-01\nPost\n\n\n7331\n2016-01-01\nPost\n\n\n7336\n2016-01-01\nPost\n\n\n7339\n2016-01-01\nPost\n\n\n7343\n2016-01-01\nPost\n\n\n7345\n2016-01-01\nPost\n\n\n\n\n\n\n\nLooks good.\n\nSome Graphing\n\ndata_ex &lt;- data_ex |&gt; \n  mutate(initiative = as.factor(initiative)) |&gt; \n  mutate(initiative = relevel(initiative, ref = \"Pre\"))\n\nggplot(data_ex, aes(x = PT_YESNO, y = Unit_LOS_log, fill = initiative)) +\n  geom_boxplot() +\n  theme_minimal() +\n  scale_fill_brewer(palette = \"Pastel2\")\n\n\n\n\n\n\n\n\nThis shows that MICU LOS decreased in the during and post QI initiative periods compared to the pre-iniatitive period.\n\ndata_ex |&gt; \n  filter(PT_YESNO == 1) |&gt; \n  ggplot(aes(x = initiative, y = Unit_LOS_log, fill = initiative)) +\n  geom_boxplot() +\n  theme_minimal() +\n  scale_fill_brewer(palette = \"Pastel2\")\n\n\n\n\n\n\n\n\n\ndata_ex |&gt; \n  filter(PT_YESNO == 1) |&gt; \n  ggplot(aes(x = initiative, y = Unit_LOS_log, fill = initiative)) +\n  geom_boxplot() +\n  theme_minimal() +\n  scale_fill_brewer(palette = \"Pastel2\")\n\n\n\n\n\n\n\n\nThere is an interaction here between PT and unit LOS and initiative.\nIn other words, you HAVE to filter based on PT\n\n\nMV Total\n\nggplot(data_ex, aes(x = PT_YESNO, y = MV_Total_log, fill = initiative)) +\n  geom_boxplot() +\n  theme_minimal() +\n  scale_fill_brewer(palette = \"Pastel2\")\n\nWarning: Removed 3149 rows containing non-finite outside the scale range\n(`stat_boxplot()`)."
  },
  {
    "objectID": "Project_3_Predictive_Modelling/Project_3_R/Code/images/Megan_Watson_Project.html#check-for-repeat-admissions",
    "href": "Project_3_Predictive_Modelling/Project_3_R/Code/images/Megan_Watson_Project.html#check-for-repeat-admissions",
    "title": "Data Checking - Statistical Consulting",
    "section": "Check for Repeat Admissions",
    "text": "Check for Repeat Admissions\n\n# Check for repeate admissions\nlength(unique(data_ex$id))\n\n[1] 6405\n\ndim(data_ex)\n\n[1] 6405   40\n\n\nThe number of unique patient id’s is the same number as the number of rows in our data set.\nSo we do not have repeate admissions in this data set and do not need to account for repeated measures! (We meet the assumption of independent observations)\n:::"
  },
  {
    "objectID": "Project_3_Predictive_Modelling/Project_3_R/Code/Project_Submission.html",
    "href": "Project_3_Predictive_Modelling/Project_3_R/Code/Project_Submission.html",
    "title": "Predictive Modelling",
    "section": "",
    "text": "The aim of the current study is to investigate whether MRI image features extracted from baseline kidney images can enhance the prediction of disease progression in young Autosomal Dominant Polycystic Kidney Disease (ADPKD) patients. This study is important because recent literature has demonstrated the inclusion of MRI image features improves the prognostic accuracy of models in adults, but this has not yet been demonstrated in children. Improving prediction models could greatly assist in diagnosing and predicting kidney disease outcomes in children, which is more challenging than in adults and could lead to improved treatment.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe two clinical hypotheses for this study are that including MRI image features will improve model performance compared to using baseline kidney volume alone in models predicting\n\nPercentage change of total kidney volume growth\nClassification of a patient as having fast or slow progression of the disease."
  },
  {
    "objectID": "Project_3_Predictive_Modelling/Project_3_R/Code/Project_Submission.html#gabor-transform",
    "href": "Project_3_Predictive_Modelling/Project_3_R/Code/Project_Submission.html#gabor-transform",
    "title": "Predictive Modelling",
    "section": "Gabor Transform",
    "text": "Gabor Transform\n\nWhat is it\nThe Gabor Transform, named after Dennis Gabor, is a powerful technique for analyzing the texture of MRI images. By convolving the image with a Gabor filter, it becomes possible to discriminate between features based on intensity differences. This allows the target region and background to be differentiated if they possess distinct features, as they will exhibit different intensity levels. Moreover, the Gabor Transform has tunable parameters, such as the frequency of the sinusoidal wave, which can be adjusted to extract specific textures from the images. Higher frequencies are ideal for capturing fine textures, while lower frequencies are better suited for coarse textures.\n\n\nHow it Works\nThe Gabor Filter first applies a Gaussian Envelope to focus on a small region of the image. It then applies a sinusoidal wave that oscillates at a specific frequency and captures the variation in intensities at that frequency and orientation within that region.\n\n\nExample of Gabor Transform\n\n\n\nExample of applying a Gaussian, then Gabor, and Laplacian Filter (GGL) for differentiation between two different textures (+’s vs L’s) (2).\n\n\n\n\nImage Features Provided by the Gabor Transform\nIn general, Gabor functions can easily extract features of:\n\nSpatial Frequency (e.g. how often pixel intensity changes in a given area)\nDensity (e.g. concentration of features within a certain area)\nOrientation (e.g. Textures or edges at 0°, 45°, 90°, 135°)\nPhase (e.g. alignment/distance of features)\nEnergy (e.g. overall intensity)\n\nSources: 1, 2"
  },
  {
    "objectID": "Project_3_Predictive_Modelling/Project_3_R/Code/Project_Submission.html#gray-level-co-occurrence-matrix",
    "href": "Project_3_Predictive_Modelling/Project_3_R/Code/Project_Submission.html#gray-level-co-occurrence-matrix",
    "title": "Predictive Modelling",
    "section": "Gray Level Co-Occurrence Matrix",
    "text": "Gray Level Co-Occurrence Matrix\n\nWhat is it\nThe Gray Level Co-Occurrence Matrix (GLCM) is another powerful tool for extracting texture features from an MRI image. GLCM works under the assumption that the textural information in an image is contained in the “average” spatial relationship that the gray tones in the image have to each other. For example, when a small patch of the picture has little variation of features, the dominant property is tone; When a small patch of a picture has high variation, the dominant property is texture (3).\n\n\nHow it Works\nGLCM examines the spatial relationship between pairs of pixels in an image and calculates how often pairs of pixel values occur at a specified distance and orientation. It does this by computing a set of gray-tone spacial-dependence matrices for various angular relationships and distances between neighboring resolution cell pairs on the image (3).\n\n\nExample of GLCM\n\n\n\nExample of textural features extracted from two different land-use category images (3).\n\n\n\n\nImage Features Provided by GLCM\nIn general, GLCM provides information on the following features:\n\nHomogeneity\nLinear Structure\nContrast\nNumber and Nature of Boundaries\nComplexity of the Image"
  },
  {
    "objectID": "Project_3_Predictive_Modelling/Project_3_R/Code/Project_Submission.html#local-binary-pattern",
    "href": "Project_3_Predictive_Modelling/Project_3_R/Code/Project_Submission.html#local-binary-pattern",
    "title": "Predictive Modelling",
    "section": "Local Binary Pattern",
    "text": "Local Binary Pattern\n\nWhat is it\nThe Local Binary Pattern (LBP) is a third powerful method for extracting texture features from an image. An important and fundamental property of texture is how uniform the patterns are, and LBP captures this by detecting these uniform patterns in circular neighborhoods at any rotation and spatial resolution. LBP is rotation invariant, meaning it does not matter what rotation the image is at; it will always extract very similar features) (4).\n\n\nHow it Works\nLBP works by comparing the intensity of a central pixel with its neighboring pixels and encoding this relationship into a binary pattern. For each neighbor, if it’s intensity is greater or equal to the central pixel, it gets assigned a 1 (otherwise a 0). The binary values of all neighbors are then concatenated to form a binary number, and this number is converted into a decimal that represent the LBP for the central pixel (4).\n\n\nExample of LBP\n\n\n\nExample of the 36 unique comparisons that can be made between neighboring pixels (4).\n\n\n\n\nCode Information\n\n\n\n\n\n\n\nImage Features Provided by the LBP\nIn general, the LBP provides image features on:\n\nUniformity\nLocal Contrast\nTexture Description\nSpatial Patterns\nGray Level Distribution"
  },
  {
    "objectID": "Project_3_Predictive_Modelling/Project_3_R/Code/Project_Submission.html#study-design",
    "href": "Project_3_Predictive_Modelling/Project_3_R/Code/Project_Submission.html#study-design",
    "title": "Predictive Modelling",
    "section": "Study Design",
    "text": "Study Design\nThe investigators recruited 71 young patients with ADPKD and collected MRI data at baseline and after 3 years. Additionally, the height corrected kidney volume for each patient was collected at baseline and 3 years by a physician, and the percentage change calculated. Patients were classified as having slow or fast progression of the disease based on this percentage change. Image features were extracted from the baseline MRI images including 2 image features on kidney geometric information, 5 features based on Gabor transform, 2 features based on gray level co-occurrence matrix, 5 features based on image textures, and 5 features based on local binary pattern.\n\nStatistical Hypotheses\n\nA linear regression model predicting percentage change of total kidney volume growth including MRI image features and baseline kidney volume will have better performance than a model with baseline kidney volume alone, as determined by specificity, sensitivity, PPV,  NPV,  accuracy, and AUC.\nA logistic regression model predicting classification of disease progression as slow or fast including MRI image features and baseline kidney volume will have better performance than a model with baseline kidney volume alone, as determined by specificity, sensitivity, PPV,  NPV,  accuracy, and AUC."
  },
  {
    "objectID": "Project_3_Predictive_Modelling/Project_3_R/Code/Project_Submission.html#summary-2",
    "href": "Project_3_Predictive_Modelling/Project_3_R/Code/Project_Submission.html#summary-2",
    "title": "Predictive Modelling",
    "section": "Summary",
    "text": "Summary\n\nKidney Volume Change\nkidvol_change is approximately normally distributed following a log transform, with 2 potential outliers we will keep an eye on.\n\n\nSlow vs Fast Disease Progression\nWe have an even count between patients who had slow vs fast disease progression."
  },
  {
    "objectID": "Project_3_Predictive_Modelling/Project_3_R/Code/Project_Submission.html#glcm1",
    "href": "Project_3_Predictive_Modelling/Project_3_R/Code/Project_Submission.html#glcm1",
    "title": "Predictive Modelling",
    "section": "GLCM1",
    "text": "GLCM1\n\n# Make plots\ndistr_plots(data1_train, \"glcm1\", 10)\n\n\n\n\n\n\n\n\nglcm1 is approximately normally distributed.\nTop of Tabset"
  },
  {
    "objectID": "Project_3_Predictive_Modelling/Project_3_R/Code/Project_Submission.html#glcm2",
    "href": "Project_3_Predictive_Modelling/Project_3_R/Code/Project_Submission.html#glcm2",
    "title": "Predictive Modelling",
    "section": "GLCM2",
    "text": "GLCM2\n\n# Make plots\ndistr_plots(data1_train, \"glcm2\", 10)\n\n\n\n\n\n\n\n\nglcm2 may be logarithmic.\n\nLog Transform\n\n# Create log variable\ndata1_train &lt;- data1_train |&gt; \n  mutate(glcm2_log = log(glcm2))\n\n#| fig-height: 3.5\n#| fig-width: 8\n# Make plots\ndistr_plots(data1_train, \"glcm2_log\", 10)\n\n\n\n\n\n\n\n\n\n# Use bestNormalize R package to select the best transformation\nBNObject &lt;- bestNormalize(data1_train$glcm2)\nBNObject\n\nBest Normalizing transformation with 56 Observations\n Estimated Normality Statistics (Pearson P / df, lower =&gt; more normal):\n - arcsinh(x): 1.5713\n - Box-Cox: 1.4393\n - Center+scale: 2.778\n - Double Reversed Log_b(x+a): 3.388\n - Log_b(x+a): 1.5713\n - orderNorm (ORQ): 1.7147\n - sqrt(x + a): 1.6827\n - Yeo-Johnson: 1.4393\nEstimation method: Out-of-sample via CV with 10 folds and 5 repeats\n \nBased off these, bestNormalize chose:\nStandardized Box Cox Transformation with 56 nonmissing obs.:\n Estimated statistics:\n - lambda = 0.245653 \n - mean (before standardization) = 105.7341 \n - sd (before standardization) = 37.69292 \n\n\nIt appears that this is the best transformation we will get.\n\n\nSummary\nThe best transformation for glcm2 is a log transform.\nTop of Tabset"
  },
  {
    "objectID": "Project_3_Predictive_Modelling/Project_3_R/Code/Project_Submission.html#correlation-matrix-i",
    "href": "Project_3_Predictive_Modelling/Project_3_R/Code/Project_Submission.html#correlation-matrix-i",
    "title": "Predictive Modelling",
    "section": "Correlation Matrix I",
    "text": "Correlation Matrix I\nTo assess for the best engineering of features and select the most promising covariates for this predictive model, I will create the following correlation matrices.\n\nWith unaltered features & engineered features\nWith averages and interactions to aggregate features from the same class\nWith squared features\n\nThis process should uncover hidden relationships between features that are not immediately obvious (such as a squared feature being a significant predictor, but not in its original form).\nThis first matrix will contain the unaltered features in their original form, as well as the engineered features as determined by examination of distributions.\n\n# Control matrix for better output\nmatrix_order &lt;- c(\"kidvol_base\", \"kidvol_change\", \"kidvol_change_log\", \"progression\", \"geom1\", \"geom1_yeo\", \"geom2\", \"geom2_yeo\", \"gabor1\", \"gabor2\", \"gabor2_yeo\", \"gabor3\", \"gabor4\", \"gabor4_log\", \"gabor5\", \"glcm1\", \"glcm2\", \"glcm2_log\", \"txti1\", \"txti2\", \"txti3\", \"txti4\", \"txti4_log\", \"txti5\", \"txti5_log\", \"lbp1\", \"lbp2\", \"lbp3\", \"lbp4\", \"lbp4_log\", \"lbp5\", \"lbp5_log\")\n \n# Clean the output by making a trimmed dataset excluding extaneous variables\ndata_for_matrix &lt;- data1_train |&gt; \n  select(matrix_order)\n\nWarning: Using an external vector in selections was deprecated in tidyselect 1.1.0.\nℹ Please use `all_of()` or `any_of()` instead.\n  # Was:\n  data %&gt;% select(matrix_order)\n\n  # Now:\n  data %&gt;% select(all_of(matrix_order))\n\nSee &lt;https://tidyselect.r-lib.org/reference/faq-external-vector.html&gt;.\n\n# We factored our variables at the start. To make a correlation matrix we must reconvert those back to numeric\ndata_for_matrix &lt;- data.frame(lapply(data_for_matrix, function(x) if (is.factor(x)) as.numeric(x) else x))\n\n# Make a correlation matrix with all variables of the trimmed data set\ncorrelation_matrix &lt;- cor(data_for_matrix, use = \"complete.obs\")\n\n# Plot correlation matrix\ncorrplot(correlation_matrix, method = \"color\", addCoef.col = \"black\", number.cex = 0.4, insig = \"blank\")\n\n\n\n\n\n\n\n\nFrom this plot we can see a few relationships between our original and transformed features."
  },
  {
    "objectID": "Project_3_Predictive_Modelling/Project_3_R/Code/Project_Submission.html#Corr_One",
    "href": "Project_3_Predictive_Modelling/Project_3_R/Code/Project_Submission.html#Corr_One",
    "title": "Predictive Modelling",
    "section": "Main Observations",
    "text": "Main Observations\n\nGeometryGabor TransformGray Level Co-Occurence MatrixTxtiLocal Binary Pattern\n\n\ngeom1 (r = -0.17) and geom2_yeo (r = -0.15) have about equal correlation with kidvol_change\nTop of Tabset\n\n\nNone of the original or transformed gabor features are correlated to change in kidvol_change_log.\nInterestingly, gabor3 appears to have a strong correlation to the untransformed kidvol_change, but not with kidvol_change_log. Let’s assess.\n\n# Create plot\nggplot(data1_train, aes(x = gabor3, y = kidvol_change)) +\n  geom_smooth(method = \"lm\") +\n  geom_point() +\n  labs(title = \"Change in Kidney Volume by Gabor3\",\n       x = \"Gabor3\",\n       y = \"Change in Kidney Volume (%)\") +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nThis appears to be driven entirely by that outlier patient on the far left.\nWe previously determined that patients 37 and 51 were outliers on the boxplots on kidvol_change.\nLet’s assess how this relationship changes after removing them.\n\n# Remove outlier patients on kidvol change\ndata_out_rem &lt;- data1_train |&gt; \n  filter(!Subject_ID %in% c(37,51))\n\n# Plot\nggplot(data_out_rem, aes(x = gabor3, y = kidvol_change)) +\n  geom_smooth(method = \"lm\") +\n  geom_point() +\n  labs(title = \"Change in Kidney Volume by Gabor3\",\n       x = \"Gabor3\",\n       y = \"Change in Kidney Volume (%)\") +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nThe relationship now looks appropriately linear when these outlier patients are removed.\nWe can also see that taking the log of the change in kidney volume reduces the influence of these outliers and makes the correlation non significant\n\n# Create plot\nggplot(data1_train, aes(x = gabor3, y = kidvol_change_log)) +\n  geom_smooth(method = \"lm\") +\n  geom_point() +\n  labs(title = \"Change in Log Kidney Volume by Gabor3\",\n       x = \"Gabor3\",\n       y = \"Change in Log Kidney Volume (%)\") +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 1 row containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nNote: Patients 37 and 51 may be outliers and will need to be assessed using leverage and influence after the final model is fit!\n\n\nTop of Tabset\n\n\nNone of the MRI features from the gray level co-occurence matrix were strongly correlated with kidvol_change.\nTop of Tabset\n\n\ntxti2 is a strong predictor of change in kidvol regardless if whether kidvol is transformed or not.\n\n# Create plot\nggplot(data1_train, aes(x = txti2, y = kidvol_change)) +\n  geom_smooth(method = \"lm\") +\n  geom_point() +\n  labs(title = \"Change in Log Kidney Volume by Txti2\",\n       x = \"Txti2\",\n       y = \"Change in Log Kidney Volume (%)\") +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\ntxti2 is a strong predictor of change in kidney volume!\nTop of Tabset\n\n\n\nLbp2\nlbp2 is weakly correlated to change in kidney volume.\n\n# Create plot\nggplot(data1_train, aes(x = lbp2, y = kidvol_change)) +\n  geom_smooth(method = \"lm\") +\n  geom_point() +\n  labs(title = \"Change in Log Kidney Volume by lbp2\",\n       x = \"lbp2\",\n       y = \"Change in Log Kidney Volume (%)\") +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nmodel &lt;- lm(kidvol_change ~ lbp2, data = data1_train)\nsummary(model)\n\n\nCall:\nlm(formula = kidvol_change ~ lbp2, data = data1_train)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-11.584  -5.786  -1.331   2.369  28.519 \n\nCoefficients:\n            Estimate Std. Error t value         Pr(&gt;|t|)    \n(Intercept)    9.502      1.085   8.754 0.00000000000837 ***\nlbp2           2.343      1.945   1.205            0.234    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.926 on 52 degrees of freedom\nMultiple R-squared:  0.02716,   Adjusted R-squared:  0.008455 \nF-statistic: 1.452 on 1 and 52 DF,  p-value: 0.2337\n\n\n\n\nLbp4\nlbp4 after log transforming is weakly associated with change in kidney volume, but not strong enough to be significant.\n\n# Create plot\nggplot(data1_train, aes(x = lbp4_log, y = kidvol_change)) +\n  geom_smooth(method = \"lm\") +\n  geom_point() +\n  labs(title = \"Change in Log Kidney Volume by Lbp4\",\n       x = \"Log Lbp4\",\n       y = \"Change in Kidney Volume (%)\") +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\nLbp Log\nlbp5_log is strongly correlated to change in kidney volume.\n\n# Create plot\nggplot(data1_train, aes(x = lbp5_log, y = kidvol_change)) +\n  geom_smooth(method = \"lm\") +\n  geom_point() +\n  labs(title = \"Change in Log Kidney Volume by Log Lbp5\",\n       x = \"Log Lbp5\",\n       y = \"Change in Log Kidney Volume (%)\") +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nWe have a pretty strong negative relationship here, looks like it is slightly driven by those two outlier patients.\nTop of Tabset"
  },
  {
    "objectID": "Project_3_Predictive_Modelling/Project_3_R/Code/Project_Submission.html#summary-18",
    "href": "Project_3_Predictive_Modelling/Project_3_R/Code/Project_Submission.html#summary-18",
    "title": "Predictive Modelling",
    "section": "Summary",
    "text": "Summary\nThe features that are most promising as covariates at this point are:\n\ngeom2_yeo\ngabor3\ntxt12\nlbp5_log"
  },
  {
    "objectID": "Project_3_Predictive_Modelling/Project_3_R/Code/Project_Submission.html#averages-and-interaction-terms",
    "href": "Project_3_Predictive_Modelling/Project_3_R/Code/Project_Submission.html#averages-and-interaction-terms",
    "title": "Predictive Modelling",
    "section": "Averages and Interaction Terms",
    "text": "Averages and Interaction Terms\nWe saw in the correlation matrix that both geom1 and geom2 had pretty even correlations with change in kidney volume.\nIn the interest of discovering underlying patterns (and minimizing the number of features we have, since we have a limit of 5 based on our sample size), I will attempt to collapse these two variables by\n\nTaking their average\nTaking their interaction term\n\nI will also do the same for lbp2, and lbp5_log, which were correlated with the outcome.\n\nCreate Average and Interaction Terms\n\n# Create average and interaction terms\ndata1_train &lt;- data1_train |&gt; \n  mutate(geom_avg = (geom1_yeo + geom2_yeo)/2,\n         geom_int = geom1_yeo*geom2_yeo,\n         lbp_avg  = (lbp2 + lbp5_log),\n         lbp_int  = lbp2*lbp5_log)"
  },
  {
    "objectID": "Project_3_Predictive_Modelling/Project_3_R/Code/Project_Submission.html#correlation-matrix-ii",
    "href": "Project_3_Predictive_Modelling/Project_3_R/Code/Project_Submission.html#correlation-matrix-ii",
    "title": "Predictive Modelling",
    "section": "Correlation Matrix II",
    "text": "Correlation Matrix II\nNow we can rerun the correlation matrix and examine how those relationships changed\n\n# Control matrix for better output\nmatrix_order &lt;- c(\"kidvol_base\", \"kidvol_change\", \"kidvol_change_log\", \"progression\", \"geom1\", \"geom1_yeo\", \"geom2\", \"geom2_yeo\", \"geom_avg\", \"geom_int\", \"lbp1\", \"lbp2\", \"lbp3\", \"lbp4\", \"lbp4_log\", \"lbp5\", \"lbp5_log\", \"lbp_avg\", \"lbp_int\")\n \n# Clean the output by making a trimmed dataset excluding extaneous variables\ndata_for_matrix &lt;- data1_train |&gt; \n  select(matrix_order)\n\n# We factored our variables at the start. To make a correlation matrix we must reconvert those back to numeric\ndata_for_matrix &lt;- data.frame(lapply(data_for_matrix, function(x) if (is.factor(x)) as.numeric(x) else x))\n\n# Make a correlation matrix with all variables of the trimmed data set\ncorrelation_matrix &lt;- cor(data_for_matrix, use = \"complete.obs\")\n\n# Plot correlation matrix\ncorrplot(correlation_matrix, method = \"color\", addCoef.col = \"black\", number.cex = 0.6)"
  },
  {
    "objectID": "Project_3_Predictive_Modelling/Project_3_R/Code/Project_Submission.html#Corr_Two",
    "href": "Project_3_Predictive_Modelling/Project_3_R/Code/Project_Submission.html#Corr_Two",
    "title": "Predictive Modelling",
    "section": "Main Observations",
    "text": "Main Observations\n\nGeometryLocal Binary Pattern\n\n\ngeom_avg (r = 0.20) has a higher correlation coefficient than geom1_yeo and geom2_yeo alone, and will thus be used as an aggregate covariate to capture the geometry of the MRIs going forward.\n\n# Create plot\nggplot(data1_train, aes(x = geom_avg, y = kidvol_change)) +\n  geom_smooth(method = \"lm\") +\n  geom_point() +\n  labs(title = \"Change in Kidney Volume by geom_avg\",\n       x = \"geom_avg\",\n       y = \"Change in Kidney Volume (%)\") +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nNotably, this could be driven by those two outliers as mentioned previously.\nTop of Tabset\n\n\nThe average and interaction terms do not perform better than lbp5_log alone.\nTop of Tabset\n\n\n\n\nSummary\ngeom_avg increases the r by 0.03 over the individual non-combined features, and will be chosen as a candidate covariate.\nlbp_avg does not perform better than lbp5_log alone, and thus lbp5_log will be chosen as a candidate covariate in the final model."
  },
  {
    "objectID": "Project_3_Predictive_Modelling/Project_3_R/Code/Project_Submission.html#quadratic-terms",
    "href": "Project_3_Predictive_Modelling/Project_3_R/Code/Project_Submission.html#quadratic-terms",
    "title": "Predictive Modelling",
    "section": "Quadratic Terms",
    "text": "Quadratic Terms\nFinally, we will consider quadratic terms for each feature.\n\n# Create function to square specified columns and rename the new variables\nsquare_selected_variables &lt;- function(df, columns) {\n  df &lt;- df %&gt;%\n    mutate(across(all_of(columns), ~ .^2, .names = \"{.col}_square\"))\n  return(df)\n}\n\n# Choose columns to get squared\ncolumns &lt;- data1_train |&gt;\n  select(geom1, geom1_yeo, geom2, geom2_yeo, geom_avg, geom_int, gabor1, gabor2, gabor2_yeo, gabor3, gabor4, gabor4_log, gabor5, glcm1, glcm2, glcm2_log, txti1, txti2, txti3, txti4, txti4_log, txti5, txti5_log, lbp1, lbp2, lbp3, lbp4, lbp4_log, lbp5, lbp5_log, lbp_avg, lbp_int) |&gt;\n  colnames()\n\n\n# Square selected variables\ndata_square &lt;- square_selected_variables(data1_train, columns)"
  },
  {
    "objectID": "Project_3_Predictive_Modelling/Project_3_R/Code/Project_Submission.html#correlation-matrix-iii",
    "href": "Project_3_Predictive_Modelling/Project_3_R/Code/Project_Submission.html#correlation-matrix-iii",
    "title": "Predictive Modelling",
    "section": "Correlation Matrix III",
    "text": "Correlation Matrix III\n\n# Control matrix for better output\nmatrix_order &lt;- c(\"kidvol_base\", \"kidvol_change\", \"kidvol_change_log\", \"progression\", \"geom1_square\", \"geom1_yeo_square\", \"geom2_square\", \"geom2_yeo_square\", \"geom_avg_square\", \"geom_int_square\", \"gabor1_square\", \"gabor2_square\", \"gabor2_yeo_square\", \"gabor3_square\", \"gabor4_square\", \"gabor4_log_square\", \"gabor5_square\", \"glcm1_square\", \"glcm2_square\", \"glcm2_log_square\", \"txti1_square\", \"txti2_square\", \"txti3_square\", \"txti4_square\", \"txti4_log_square\", \"txti5_square\", \"txti5_log_square\", \"lbp1_square\", \"lbp2_square\", \"lbp3_square\", \"lbp4_square\", \"lbp4_log_square\", \"lbp5_square\", \"lbp5_log_square\", \"lbp_avg_square\", \"lbp_int_square\")\n \n# Clean the output by making a trimmed dataset excluding extraneous variables\ndata_for_matrix2 &lt;- data_square |&gt; \n  select(matrix_order)\n\n# We factored our variables at the start. To make a correlation matrix we must reconvert those back to numeric\ndata_for_matrix2 &lt;- data.frame(lapply(data_for_matrix2, function(x) if (is.factor(x)) as.numeric(x) else x))\n\n# Make a correlation matrix with all variables of the trimmed data set\ncorrelation_matrix &lt;- cor(data_for_matrix2, use = \"complete.obs\")\n\n# Plot correlation matrix\ncorrplot(correlation_matrix, method = \"color\", addCoef.col = \"black\", number.cex = 0.2)"
  },
  {
    "objectID": "Project_3_Predictive_Modelling/Project_3_R/Code/Project_Submission.html#Corr_Three",
    "href": "Project_3_Predictive_Modelling/Project_3_R/Code/Project_Submission.html#Corr_Three",
    "title": "Predictive Modelling",
    "section": "Main Observations",
    "text": "Main Observations\n\nGabor TransformGray Level Co-Occurence Matrix\n\n\n\nGabor3\nThe correlation coefficient for gabor3_square is higher than gabor3.\n\nggplot(data_square, aes(x = gabor3_square, y = kidvol_change)) +\n  geom_point() +\n  geom_smooth(method = \"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nLet’s compare if we remove those outliers\n\nggplot(data_out_rem, aes(x = gabor3**2, y = kidvol_change)) +\n  geom_point() +\n  geom_smooth(method = \"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nThis appears to be driven by that outlier patient.\nTop of Tabset\n\n\n\nglcm2_square has a strong correlation (r = 0.31). Let’s examine.\n\n# Create the plot\nggplot(data_square, aes(x = glcm2_square, y = kidvol_change)) +\n  geom_point() +\n  geom_smooth(method = \"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n# Check the model\nmodel &lt;- lm(kidvol_change ~ glcm2**2, data = data1_train)\nsummary(model)\n\n\nCall:\nlm(formula = kidvol_change ~ glcm2^2, data = data1_train)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-11.657  -5.766  -1.591   3.782  29.290 \n\nCoefficients:\n                Estimate   Std. Error t value    Pr(&gt;|t|)    \n(Intercept) 7.8715892217 1.4117081716   5.576 0.000000892 ***\nglcm2       0.0000013538 0.0000008435   1.605       0.115    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.844 on 52 degrees of freedom\nMultiple R-squared:  0.0472,    Adjusted R-squared:  0.02888 \nF-statistic: 2.576 on 1 and 52 DF,  p-value: 0.1146\n\n\nThis could be driven by the outlier, but we will include glcm2_square as a covariate during model selection.\nTop of Tabset\n\n\n\n\nSummary\nGLCM2_square will be considered as a potential covariate during model selection."
  },
  {
    "objectID": "Project_3_Predictive_Modelling/Project_3_R/Code/Project_Submission.html#perform-scaling",
    "href": "Project_3_Predictive_Modelling/Project_3_R/Code/Project_Submission.html#perform-scaling",
    "title": "Predictive Modelling",
    "section": "Perform Scaling",
    "text": "Perform Scaling\nWe will perform z norm scaling using caret, which will appropriately scale the test set of each fold using the mean and standard deviation of each respective training set."
  },
  {
    "objectID": "Project_3_Predictive_Modelling/Project_3_R/Code/Project_Submission.html#cost-function",
    "href": "Project_3_Predictive_Modelling/Project_3_R/Code/Project_Submission.html#cost-function",
    "title": "Predictive Modelling",
    "section": "Cost Function",
    "text": "Cost Function\nThe cost function in a linear regression is Root Mean Square Error (RMSE):\n\\[ \\text{RMSE} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2} \\]\nWhere y is the actual change in outcome variable and y-hat is the predicted change, and n is the number of observations.\n\nMapping Function\nIn the case of linear regression, the mapping function is essentially each beta in the model.\n\n\nGoal\nThe goal is to select a mapping function that minimizes the cost function and thereby produces the best predictions for the outcome variable.\nWe will be using the caret package to perform model training using 5-fold cross validation and Ordinary Least Squares (OLS). Another option is to perform gradient descent."
  },
  {
    "objectID": "Project_3_Predictive_Modelling/Project_3_R/Code/Project_Submission.html#1A",
    "href": "Project_3_Predictive_Modelling/Project_3_R/Code/Project_Submission.html#1A",
    "title": "Predictive Modelling",
    "section": "Model 1A: Baseline Kidney Volume",
    "text": "Model 1A: Baseline Kidney Volume\n\nAnalysisVisualizationSummary\n\n\nTo answer the researcher’s first question for Task 1, we will perform a predictive model that uses the baseline height-corrected total kidney volume to predict percent change in kidney volume.\n\\[ Kidney Volume Change = 𝛽_0 + 𝛽_1*Log Kidney Volume Baseline + e \\]\n\nset.seed(123)\n\n# Set up a 5-fold cross validation\ntc &lt;- trainControl(method = \"cv\", number = 5)\n\n# Perform the linear regression using 5-fold CV\nmodel1a &lt;- train(kidvol_change ~ log(kidvol_base), \n                 data = data, \n                 method = \"lm\",\n                 trControl = tc)\n# Get RMSE\nmodel1a\n\nLinear Regression \n\n71 samples\n 1 predictor\n\nNo pre-processing\nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 56, 56, 58, 58, 56 \nResampling results:\n\n  RMSE      Rsquared    MAE     \n  8.096326  0.05257111  6.192124\n\nTuning parameter 'intercept' was held constant at a value of TRUE\n\n# Get model coefficients\nsummary(model1a)\n\n\nCall:\nlm(formula = .outcome ~ ., data = dat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-16.166  -5.741  -1.240   3.758  27.000 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept)          26.582     11.698   2.272   0.0262 *\n`log(kidvol_base)`   -2.987      2.028  -1.473   0.1453  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.176 on 69 degrees of freedom\nMultiple R-squared:  0.03049,   Adjusted R-squared:  0.01644 \nF-statistic:  2.17 on 1 and 69 DF,  p-value: 0.1453\n\n# Examine RSME and R Squared of each fold\npretty_print(model1a$resample)\n\n\n\n\nRMSE\nRsquared\nMAE\nResample\n\n\n\n\n9.164071\n0.1296555\n6.966941\nFold1\n\n\n8.938726\n0.1208880\n6.144274\nFold2\n\n\n9.002214\n0.0078560\n7.279776\nFold3\n\n\n6.777802\n0.0009846\n5.651400\nFold4\n\n\n6.598816\n0.0034716\n4.918228\nFold5\n\n\n\n\n\n\n\nThe model has an RMSE of 8.10.\nTop of Tabset\n\n\n\nActual vs Predicted Values\n\n# Save predicted values to data set\ndata$predictions1a &lt;- predict(model1a, newdata = data)\n\n# Visualize predicted vs actual values\nggplot(data, aes(x = kidvol_change, y = predictions1a)) +\n  geom_point() +\n  geom_abline(intercept = 0, slope = 1, color = \"blue\", linetype = \"dashed\") +\n  theme_minimal() +\n  labs(title = \"Predicted vs Actual Values for Baseline Kidney Volume\",\n       x = \"Actual Values\",\n       y = \"Predicted Values\")\n\n\n\n\n\n\n\n\nIn a well performing model, the dots are close to a straight line (the blue dashed line), which would indicate perfect overlap between predicted and actual values.\n\n\nResiduals\n\n# Calculate the residuals\ndata$residuals1a &lt;- data$kidvol_change - data$predictions1a\n\n# Plot\nggplot(data, aes(x = predictions1a, y = residuals1a)) +\n  geom_point() +\n  geom_hline(yintercept = 0, color = \"blue\", linetype = \"dashed\") +\n  theme_minimal() +\n  labs(title = \"Residuals for Baseline Kidney Volume\",\n       x = \"Predicted Values\",\n       y = \"Residuals\")\n\n\n\n\n\n\n\n\nThere’s some pretty large differences in the predictions here, with some patients having predicted values that are 15% off or greater!\n\n\nQQ Plot\n\n# Plot QQ Plot\nggplot(data, aes(sample = residuals1a)) +\n  geom_qq() +\n  geom_qq_line() +\n  theme_minimal() +\n  labs(title = \"QQ Plot of Model 1A\")\n\n\n\n\n\n\n\n\nThe QQ plot is almost normal, but not quite.\n\n\nHistogram of Residuals\n\n# Create histogram of residuals\nggplot(data, aes(x = residuals1a)) + \n  geom_histogram(bins = 20) +\n  labs(title = \"Histogram of Residuals for Model 1A\", \n       x = \"Residuals\", \n       y = \"Count\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThe residuals are almost normally distributed, but not quite.\nTop of Tabset\n\n\n\nthe RMSE of the model including kidvol_base alone is 8.11\nThe model does not perform too well, when looking at the plots of the predicted vs actual values.\nThe assumption of normality is almost, but not quite, met.\nTop of Tabset"
  },
  {
    "objectID": "Project_3_Predictive_Modelling/Project_3_R/Code/Project_Submission.html#1B",
    "href": "Project_3_Predictive_Modelling/Project_3_R/Code/Project_Submission.html#1B",
    "title": "Predictive Modelling",
    "section": "Model 1B: MRI Features",
    "text": "Model 1B: MRI Features\nTo answer the researcher’s second question for Task 1, we will run a predictive model that uses only MRI image features to predict percent change in kidney volume at year 3.\n\\[ Kidney Volume Change = 𝛽_0 + 𝛽_1*Txti2 + e \\]\n\nModel Selection\nModel selection will be performed on Fold 1 to avoid data snooping on the test set.\nThe potential scaled covariates for our final model after feature engineering and interactive variable selection are:\n\ngeom_avg\ngabor3\ntxti2\nlbp5_log\nglcm2_square\n\nWe will perform model selection using backwards elimination and BIC.\n\nBackwards Elimination IAnalysis IVisualization ILeverage and InfluenceBackwards Elimination IIAnalysis IIVisualization IISummary\n\n\n\n# Create variables\ndata1_train &lt;- data1_train |&gt; \n  mutate(geom_avg_scale = scale((geom1_yeo+geom2_yeo)/2),\n         gabor3_scale = scale(gabor3),\n         txti2_scale = scale(txti2),\n         lbp5_log_scale = scale(log(lbp5)),\n         glcm2_square_scale = scale(glcm2**2))\n\n# First build the model with all variables\nmodel1b &lt;- lm(kidvol_change ~ geom_avg_scale + gabor3_scale + txti2_scale + lbp5_log_scale + glcm2_square_scale, data = data1_train)\nsummary(model1b)\n\n\nCall:\nlm(formula = kidvol_change ~ geom_avg_scale + gabor3_scale + \n    txti2_scale + lbp5_log_scale + glcm2_square_scale, data = data1_train)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-17.458  -4.477  -1.072   2.927  18.913 \n\nCoefficients:\n                   Estimate Std. Error t value         Pr(&gt;|t|)    \n(Intercept)          9.3544     0.9681   9.662 0.00000000000077 ***\ngeom_avg_scale      -0.5476     1.0569  -0.518           0.6067    \ngabor3_scale        -1.8407     1.0054  -1.831           0.0733 .  \ntxti2_scale         -2.0236     1.0275  -1.970           0.0547 .  \nlbp5_log_scale      -1.0410     1.0918  -0.953           0.3451    \nglcm2_square_scale   1.9041     1.0320   1.845           0.0712 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.114 on 48 degrees of freedom\nMultiple R-squared:  0.2765,    Adjusted R-squared:  0.2011 \nF-statistic: 3.668 on 5 and 48 DF,  p-value: 0.006844\n\n# Perform backward elimination \nols_step_backward_sbc(model1b)\n\n\n                               Stepwise Summary                                \n-----------------------------------------------------------------------------\nStep    Variable            AIC        SBC       SBIC        R2       Adj. R2 \n-----------------------------------------------------------------------------\n 0      Full Model        372.792    386.715    221.016    0.27647    0.20110 \n 1      geom_avg_scale    371.093    383.027    218.848    0.27242    0.21303 \n 2      lbp5_log_scale    370.716    380.661    218.098    0.25023    0.20524 \n-----------------------------------------------------------------------------\n\nFinal Model Output \n------------------\n\n                         Model Summary                           \n----------------------------------------------------------------\nR                        0.500       RMSE                 6.828 \nR-Squared                0.250       MSE                 46.621 \nAdj. R-Squared           0.205       Coef. Var           75.855 \nPred R-Squared          -0.235       AIC                370.716 \nMAE                      5.016       SBC                380.661 \n----------------------------------------------------------------\n RMSE: Root Mean Square Error \n MSE: Mean Square Error \n MAE: Mean Absolute Error \n AIC: Akaike Information Criteria \n SBC: Schwarz Bayesian Criteria \n\n                               ANOVA                                \n-------------------------------------------------------------------\n                Sum of                                             \n               Squares        DF    Mean Square      F        Sig. \n-------------------------------------------------------------------\nRegression     840.194         3        280.065    5.562    0.0023 \nResidual      2517.532        50         50.351                    \nTotal         3357.727        53                                   \n-------------------------------------------------------------------\n\n                                      Parameter Estimates                                       \n-----------------------------------------------------------------------------------------------\n             model      Beta    Std. Error    Std. Beta      t        Sig      lower     upper \n-----------------------------------------------------------------------------------------------\n       (Intercept)     9.354         0.966                  9.688    0.000     7.415    11.294 \n      gabor3_scale    -2.114         0.981       -0.266    -2.155    0.036    -4.084    -0.143 \n       txti2_scale    -2.200         1.016       -0.276    -2.165    0.035    -4.240    -0.159 \nglcm2_square_scale     2.076         1.018        0.261     2.040    0.047     0.032     4.121 \n-----------------------------------------------------------------------------------------------\n\n\ngeom_avg and lbp5_log are removed based on BIC. AIC prefers the model with lbp5_log\nHowever, we must note that we suspect patients 37 and 51 to be outliers on total kidney volume change, and we saw previously that the correlations for gabor3 and glcm2_square may be being driven by these outlier patients.\nTop of Tabset\n\n\n\nPerform 5-Fold Cross Validation\nThe final model selected via backwards elimination includes gabor3, txti2, and glcm2_square\n\nset.seed(123)\n# Create variables for original data set\ndata &lt;- data |&gt; \n  mutate(gabor3_scale = scale(gabor3),\n         txti2_scale = scale(txti2),\n         glcm2_square = glcm2**2,\n         glcm2_square_scale = scale(glcm2**2),\n         lbp5_log_scale = scale(log(lbp5)),\n         geom_avg_scale = scale(geom1+geom2)/2)\n\n# Perform the linear regression using 5-fold CV\nmodel1b &lt;- train(kidvol_change ~ gabor3 + txti2 + glcm2_square, \n                 data = data, \n                 method = \"lm\",\n                 trControl = tc,\n                 preProc = \"scale\")\n# Get RMSE\nmodel1b\n\nLinear Regression \n\n71 samples\n 3 predictor\n\nPre-processing: scaled (3) \nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 56, 56, 58, 58, 56 \nResampling results:\n\n  RMSE      Rsquared   MAE     \n  9.032055  0.1690167  6.528863\n\nTuning parameter 'intercept' was held constant at a value of TRUE\n\n# Get model coefficients\nsummary(model1b)\n\n\nCall:\nlm(formula = .outcome ~ ., data = dat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-16.630  -4.574  -1.414   3.490  28.925 \n\nCoefficients:\n             Estimate Std. Error t value          Pr(&gt;|t|)    \n(Intercept)    9.0641     1.0337   8.768 0.000000000000996 ***\ngabor3         0.1500     0.9800   0.153            0.8788    \ntxti2         -2.2213     0.9844  -2.256            0.0273 *  \nglcm2_square   0.8494     0.9690   0.877            0.3838    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.049 on 67 degrees of freedom\nMultiple R-squared:  0.08767,   Adjusted R-squared:  0.04681 \nF-statistic: 2.146 on 3 and 67 DF,  p-value: 0.1026\n\n\nThe RMSE is 9.03, worse than the model with kidvol_base alone.\nTop of Tabset\n\n\n\n\nActual vs Predicted Values\n\n# Save predicted valuest to data set\ndata$predictions1b &lt;- predict(model1b, newdata = data)\n\n# Visualize predicted vs actual values\nggplot(data, aes(x = kidvol_change, y = predictions1b)) +\n  geom_point() +\n  geom_abline(intercept = 0, slope = 1, color = \"blue\", linetype = \"dashed\") +\n  theme_minimal() +\n  labs(title = \"Predicted vs Actual Values for MRI Features\",\n       x = \"Actual Values\",\n       y = \"Predicted Values\")\n\n\n\n\n\n\n\n\n\n\nResiduals\n\n# Calculate the residuals\ndata$residuals1b &lt;- data$kidvol_change - data$predictions1b\n\n# Plot\nggplot(data, aes(x = predictions1a, y = residuals1b)) +\n  geom_point() +\n  geom_hline(yintercept = 0, color = \"blue\", linetype = \"dashed\") +\n  theme_minimal() +\n  labs(title = \"Residuals for MRI Features\",\n       x = \"Predicted Values\",\n       y = \"Residuals\")\n\n\n\n\n\n\n\n\nOur residuals are comparable in this model compared to modela 1A using kidvol_base alone. Some It appears that more residuals are closer to 0, but some predictions are as large as 30% off!\n\n\nQQ Plot\n\n# Plot QQ Plot\nggplot(data, aes(sample = residuals1b)) +\n  geom_qq() +\n  geom_qq_line() +\n  theme_minimal() +\n  labs(title = \"QQ Plot of Model 1B\")\n\n\n\n\n\n\n\n\n\n\nHistogram of Residuals\n\n# Create histogram of residuals\nggplot(data, aes(x = residuals1b)) + \n  geom_histogram(bins = 20) +\n  labs(title = \"Histogram of Residuals for Model 1B\", \n       x = \"Residuals\", \n       y = \"Count\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nTop of Tabset\n\n\n\nThe cutoff for leverage (hat-value) is 2(p+1)/n, where p is the number of variables in the model, or 2(3+1)/71 = 0.112.\nThe cut off for Cook’s D &gt; 1.0.\n\n# Extract model\nmodel1b_ext &lt;- model1b$finalModel\n\n# Influence plot\ninfluence_measures &lt;- influencePlot(model1b_ext, main = \"Influence Plot\",\n              sub = \"Circle size is proportional to Cook's Distance\")\n\n\n\n\n\n\n\n# Get row numbers of outliers\ninfluence_measures\n\n      StudRes       Hat     CookD\nX2   3.205536 0.3120826 1.0236809\nX8   4.481013 0.1736179 0.8208826\nX9   3.131995 0.3212338 1.0257341\nX35 -1.208289 0.4497401 0.2962813\n\n\nWe have four clear outliers that are points of high leverage and influence.\nLet’s identify them\n\n# Identify outlier patients\noutliers &lt;- data[c(2, 8, 9, 35),]\n\n# Pretty print\npretty_print(head(outliers))\n\n\n\n\nSubject_ID\ngeom1\ngeom2\ngabor1\ngabor2\ngabor3\ngabor4\ngabor5\nglcm1\nglcm2\ntxti1\ntxti2\ntxti3\ntxti4\ntxti5\nlbp1\nlbp2\nlbp3\nlbp4\nlbp5\nkidvol_base\nkidvol_visit2\nprogression\nkidvol_change\nfold_number\npredictions1a\nresiduals1a\ngabor3_scale\ntxti2_scale\nglcm2_square\nglcm2_square_scale\nlbp5_log_scale\ngeom_avg_scale\npredictions1b\nresiduals1b\n\n\n\n\n51\n-43.393038\n1882.95573\n0.0047377\n0.0544573\n0.0002580\n0.0000224\n0.0029656\n2454.8065\n6026075.1\n-14.67151\n-9.355575\n137.26039\n215.2532\n87.526780\n0.1577583\n0.1676594\n0.0264497\n0.0248877\n0.0281097\n185\n387\nFast\n36.28\nFold2\n10.989456\n25.290543\n0.01648347\n-1.3536710\n36313581159049\n4.4785466\n-0.4385074\n-0.06753985\n16.224288\n20.055712\n\n\n37\n-3.860771\n14.90555\n0.2621932\n-0.1983384\n-0.0520030\n0.0687453\n0.0393381\n871.6238\n759728.0\n-14.72259\n-1.969424\n28.99502\n216.7546\n3.878631\n-0.0557547\n0.0722141\n-0.0040263\n0.0031086\n0.0052149\n173\n372\nFast\n38.19\nFold3\n11.189771\n27.000229\n-3.32238849\n-0.2849587\n577186627450\n-0.3302081\n-1.1025159\n-0.22901595\n9.264656\n28.925344\n\n\n52\n77.054904\n5937.45816\n0.4598489\n0.1556082\n0.0715563\n0.2114611\n0.0242139\n-638.4451\n407612.2\n20.26785\n9.686888\n196.33234\n410.7855\n93.835797\n9.7525713\n1.1625327\n11.3376835\n95.1126464\n1.3514824\n358\n640\nFast\n26.18\nFold4\n9.017566\n17.162434\n4.57161897\n1.4016092\n166147673143\n-0.3855183\n1.0880277\n0.30114927\n6.655437\n19.524563\n\n\n48\n-124.266792\n15442.23561\n-0.1630575\n0.0017660\n-0.0002880\n0.0265878\n0.0000031\n2545.1960\n6478022.6\n6.41584\n7.649503\n49.07799\n41.1630\n58.514902\n-0.1337633\n3.0589420\n-0.4091741\n0.0178926\n9.3571262\n290\n327\nSlow\n4.21\nFold5\n9.646765\n-5.436765\n-0.01839687\n1.1068172\n41964776184221\n5.2389818\n1.8507097\n1.12273578\n11.399413\n-7.189413\n\n\n\n\n\n\n\nAs predicted, patients 37 and 52 are points of high leverage and influence.\nAlso as identified before, patients 48 and 52 are points of high leverage and influence, which can be denoted by their absurdly high values for lbp1 and lbp2.\nSince we previously identified that certain correlations such as with gabor3 and glcm2_square were being driven by these outlier patients, we will re-run model selection again using backwards elimination, with these patients removed.\nTop of Tabset\n\n\n\nRemove Outliers and Re-Run Variable Selection\nWe will perform backwards elimination again with outlier patients removed.\nWe can expect gabor3 and glcm2_square to be removed since their correlations were driven by these outliers, as identified in earlier plots.\n\n# Remove outlier patients from data set\ndata1_train &lt;- data1_train |&gt; \n  filter(!Subject_ID %in% c(37, 48, 51, 52))\n\n# Create model\nmodel1b2 &lt;- lm(kidvol_change ~ geom_avg_scale + gabor3_scale + txti2_scale + lbp5_log_scale + glcm2_square_scale, data = data1_train)\n\n# Examine Model Summary\nsummary(model1b2)\n\n\nCall:\nlm(formula = kidvol_change ~ geom_avg_scale + gabor3_scale + \n    txti2_scale + lbp5_log_scale + glcm2_square_scale, data = data1_train)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-9.3899 -3.2977 -0.9786  2.4046 15.9736 \n\nCoefficients:\n                   Estimate Std. Error t value           Pr(&gt;|t|)    \n(Intercept)          8.1922     0.7809  10.491 0.0000000000000867 ***\ngeom_avg_scale      -0.1844     0.8252  -0.223            0.82416    \ngabor3_scale         0.9018     0.9673   0.932            0.35609    \ntxti2_scale         -2.1787     0.8004  -2.722            0.00913 ** \nlbp5_log_scale      -0.9964     0.8565  -1.163            0.25073    \nglcm2_square_scale  -1.2907     1.1787  -1.095            0.27918    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.538 on 46 degrees of freedom\nMultiple R-squared:  0.1898,    Adjusted R-squared:  0.1017 \nF-statistic: 2.155 on 5 and 46 DF,  p-value: 0.07558\n\n# Perform backward elimination \nols_step_forward_sbc(model1b2)\n\n\n                              Stepwise Summary                              \n--------------------------------------------------------------------------\nStep    Variable         AIC        SBC       SBIC        R2       Adj. R2 \n--------------------------------------------------------------------------\n 0      Base Model     334.154    338.056    186.402    0.00000    0.00000 \n 1      txti2_scale    328.649    334.503    181.305    0.13439    0.11708 \n--------------------------------------------------------------------------\n\nFinal Model Output \n------------------\n\n                         Model Summary                          \n---------------------------------------------------------------\nR                       0.367       RMSE                 5.384 \nR-Squared               0.134       MSE                 28.989 \nAdj. R-Squared          0.117       Coef. Var           66.297 \nPred R-Squared          0.067       AIC                328.649 \nMAE                     4.093       SBC                334.503 \n---------------------------------------------------------------\n RMSE: Root Mean Square Error \n MSE: Mean Square Error \n MAE: Mean Absolute Error \n AIC: Akaike Information Criteria \n SBC: Schwarz Bayesian Criteria \n\n                               ANOVA                                \n-------------------------------------------------------------------\n                Sum of                                             \n               Squares        DF    Mean Square      F        Sig. \n-------------------------------------------------------------------\nRegression     234.037         1        234.037    7.763    0.0075 \nResidual      1507.420        50         30.148                    \nTotal         1741.457        51                                   \n-------------------------------------------------------------------\n\n                                  Parameter Estimates                                    \n----------------------------------------------------------------------------------------\n      model      Beta    Std. Error    Std. Beta      t        Sig      lower     upper \n----------------------------------------------------------------------------------------\n(Intercept)     8.344         0.762                 10.954    0.000     6.814     9.874 \ntxti2_scale    -2.136         0.767       -0.367    -2.786    0.008    -3.677    -0.596 \n----------------------------------------------------------------------------------------\n\n\nAs predicted, when excluding these outlier patients, gabor3 and glcm2_square are no longer significant.\nThe only selected now using BIC is txti2!\nLet’s perform 5-fold CV again using this model.\nTop of Tabset\n\n\n\n\nRemove Outliers\n\n# Remove outlier patients from data set\ndata &lt;- data |&gt; \n  filter(!Subject_ID %in% c(37, 48, 51, 52))\n\n\n\nPerform Regression with 5-Fold Cross Validation\n\nset.seed(123)\n\n# Set up a 5-fold cross validation\ntc &lt;- trainControl(method = \"cv\", number = 5)\n\n# Perform the linear regression using 5-fold CV\nmodel1b2 &lt;- train(kidvol_change ~ txti2, \n                 data = data, \n                 method = \"lm\",\n                 trControl = tc,\n                 preProc = \"scale\")\n\n# Get RMSE\nmodel1b2\n\nLinear Regression \n\n67 samples\n 1 predictor\n\nPre-processing: scaled (1) \nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 54, 55, 53, 55, 51 \nResampling results:\n\n  RMSE      Rsquared  MAE    \n  6.153724  0.171239  4.68434\n\nTuning parameter 'intercept' was held constant at a value of TRUE\n\n# Get model coefficients\nsummary(model1b2)\n\n\nCall:\nlm(formula = .outcome ~ ., data = dat)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.6726  -3.5604  -0.5467   3.0722  18.8813 \n\nCoefficients:\n            Estimate Std. Error t value             Pr(&gt;|t|)    \n(Intercept)   8.3805     0.7661  10.939 0.000000000000000222 ***\ntxti2        -2.0443     0.7718  -2.649               0.0101 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.27 on 65 degrees of freedom\nMultiple R-squared:  0.09742,   Adjusted R-squared:  0.08353 \nF-statistic: 7.016 on 1 and 65 DF,  p-value: 0.01013\n\n\nThe RMSE is now 6.15, drastically lower than with kidvol_base_log alone (but we need to rerun that model without outliers).\nThis model predicts 17.0% of the variance in kidvol_change.\nTop of Tabset\n\n\n\n\nActual vs Predicted Values\n\n# Save predicted valuest to data set\ndata$predictions1b2 &lt;- predict(model1b2, newdata = data)\n\n# Visualize predicted vs actual values\nggplot(data, aes(x = kidvol_change, y = predictions1b2)) +\n  geom_point() +\n  geom_abline(intercept = 0, slope = 1, color = \"blue\", linetype = \"dashed\") +\n  theme_minimal() +\n  labs(title = \"Predicted vs Actual Values for TXTI2\",\n       x = \"Actual Values\",\n       y = \"Predicted Values\")\n\n\n\n\n\n\n\n\nThe predicted models are still not that close to the actual values.\n\n\nResiduals\n\n# Calculate the residuals\ndata$residuals1b2 &lt;- data$kidvol_change - data$predictions1b2\n\n# Plot residuals\nggplot(data, aes(x = predictions1b2, y = residuals1b2)) +\n  geom_point() +\n  geom_hline(yintercept = 0, color = \"blue\", linetype = \"dashed\") +\n  theme_minimal() +\n  labs(title = \"Residuals for TXTI2\",\n       x = \"Predicted Values\",\n       y = \"Residuals\")\n\n\n\n\n\n\n\n\nThe residuals still look pretty large.\n\n\nQQ Plot\n\n# Plot QQ Plot\nggplot(data, aes(sample = residuals1b2)) +\n  geom_qq() +\n  geom_qq_line() +\n  theme_minimal() +\n  labs(title = \"QQ Plot of TXTI2\")\n\n\n\n\n\n\n\n\n\n\nHistogram of Residuals\n\n# Create histogram of residuals\nggplot(data, aes(x = residuals1b2)) + \n  geom_histogram(bins = 20) +\n  labs(title = \"Histogram of Residuals for TXTI2\", \n       x = \"Residuals\", \n       y = \"Count\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThe residuals are now approximately normally distributed, and would be more so with a larger sample size.\n\n\nLeverage and Influence\n\n# Extract model\nmodel1b2_ext &lt;- model1b2$finalModel\n\n# Influence plot\ninfluence_measures &lt;- influencePlot(model1b2_ext, main = \"Influence Plot\",\n              sub = \"Circle size is proportional to Cook's Distance\")\n\n\n\n\n\n\n\n# Get row numbers of outliers\ninfluence_measures\n\n       StudRes        Hat       CookD\nX5   2.7790166 0.01496267 0.053157442\nX27 -0.5734689 0.10123968 0.018715616\nX35 -1.7314354 0.05744408 0.088628443\nX53  0.2718257 0.13375256 0.005786868\nX60  3.2804634 0.03089234 0.149126512\n\n\nWe no longer have outliers in this model as assessed by cutoffs using leverage and influence.\nTop of Tabset\n\n\n\nIn this step we discovered that patiens 37, 48, 51, and 52 were outliers with high leverage and influence in our model.\nAfter removing them, only txti2 was selected as an MRI image feature predicting change in kidney volume.\nThe RMSE for model 1B after removing outliers was 6.15"
  },
  {
    "objectID": "Project_3_Predictive_Modelling/Project_3_R/Code/Project_Submission.html#1C",
    "href": "Project_3_Predictive_Modelling/Project_3_R/Code/Project_Submission.html#1C",
    "title": "Predictive Modelling",
    "section": "Model 1C: Baseline Kidney Volume and TXTI2",
    "text": "Model 1C: Baseline Kidney Volume and TXTI2\n\nAnalysisVisualizationSummary\n\n\nTo answer the researcher’s third question, we will run a predictive model that uses the baseline height-corrected total kidney volume AND MRI image features to predict percent change in kidney volume.\n\\[ Kidney Volume Change = 𝛽_0 + 𝛽_{1}*Baseline Kidney Volume + 𝛽_{2}*Txti2 +  e \\]\n\nset.seed(123)\n\n# Perform the linear regression using 5-fold CV\nmodel1c &lt;- train(kidvol_change ~ log(kidvol_base) + txti2, \n                 data = data, \n                 method = \"lm\",\n                 trControl = tc,\n                 preProc = \"scale\",\n                 metric = \"RMSE\")\n# Get RMSE\nmodel1c\n\nLinear Regression \n\n67 samples\n 2 predictor\n\nPre-processing: scaled (2) \nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 54, 55, 53, 55, 51 \nResampling results:\n\n  RMSE      Rsquared   MAE     \n  6.140875  0.2383243  4.643702\n\nTuning parameter 'intercept' was held constant at a value of TRUE\n\n# Get model coefficients\nsummary(model1c)\n\n\nCall:\nlm(formula = .outcome ~ ., data = dat)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-16.3316  -3.5730  -0.3356   3.2914  17.0199 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)         21.2149     9.3480   2.269  0.02662 * \n`log(kidvol_base)`  -1.0820     0.7855  -1.378  0.17315   \ntxti2               -2.2803     0.7855  -2.903  0.00506 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.228 on 64 degrees of freedom\nMultiple R-squared:  0.1234,    Adjusted R-squared:  0.09601 \nF-statistic: 4.505 on 2 and 64 DF,  p-value: 0.01477\n\n\nThe RMSE for the combined model is 6.14, about the same as the model using just txti2 alone.\nTop of Tabset\n\n\n\nActual Vs Predicted\n\n# Save predicted valuest to data set\ndata$predictions1c &lt;- predict(model1c, newdata = data)\n\n# Visualize predicted vs actual values\nggplot(data, aes(x = kidvol_change, y = predictions1c)) +\n  geom_point() +\n  geom_abline(intercept = 0, slope = 1, color = \"blue\", linetype = \"dashed\") +\n  theme_minimal() +\n  labs(title = \"Predicted vs Actual Values for Baseline Kidney Volumne and TXTI2\",\n       x = \"Actual Values\",\n       y = \"Predicted Values\")\n\n\n\n\n\n\n\n\nThe model still does not have the best prediction.\n\n\nResiduals\n\n# Calculate the residuals\ndata$residuals1c &lt;- data$kidvol_change - data$predictions1c\n\n# Plot residuals\nggplot(data, aes(x = predictions1c, y = residuals1c)) +\n  geom_point() +\n  geom_hline(yintercept = 0, color = \"blue\", linetype = \"dashed\") +\n  theme_minimal() +\n  labs(title = \"Residuals for Baseline Kidney Volume and TXTI2\",\n       x = \"Predicted Values\",\n       y = \"Residuals\")\n\n\n\n\n\n\n\n\nThe residuals are slightly better than the model with kidvol_base alone.\n\n# Plot QQ Plot\nggplot(data, aes(sample = residuals1c)) +\n  geom_qq() +\n  geom_qq_line() +\n  theme_minimal() +\n  labs(title = \"QQ Plot of Baseline Kidney Volume and  TXTI2\")\n\n\n\n\n\n\n\n\nThis is the best QQ plot we have had so far.\n\n\nHistogram of Residuals\n\n# Create histogram of residuals\nggplot(data, aes(x = residuals1c)) + \n  geom_histogram(bins = 20) +\n  labs(title = \"Histogram of Residuals for Baseline Kidney Volume and TXTI2\", \n       x = \"Residuals\", \n       y = \"Count\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nLeverage and Influence\n\n# Extract model\nmodel1c_ext &lt;- model1c$finalModel\n\n# Influence plot\ninfluence_measures &lt;- influencePlot(model1c_ext, main = \"Influence Plot\",\n              sub = \"Circle size is proportional to Cook's Distance\")\n\n\n\n\n\n\n\n# Get row numbers of outliers\ninfluence_measures\n\n      StudRes        Hat       CookD\nX2  0.7595324 0.12618822 0.027954583\nX5  2.8250641 0.01514097 0.036876777\nX29 2.3152733 0.04010315 0.069889503\nX53 0.2032994 0.13606032 0.002202691\nX60 3.0216117 0.07797505 0.228366413\n\n\nThere are no outliers (some edge cases but we will retain them)\n\n\nPlot Final Model\n\n# Plot final model\nggplot(data, aes(x = log(kidvol_base), y = kidvol_change)) +\n  geom_point() +\n  geom_smooth(method = \"lm\") +\n  labs(title = \"Model 1C: Baseline Kidney Volume and Txti2\",\n       x = \"Baseline Kidney Volume (log)\",\n       y = \"Change in Kidney Volume (%)\") +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nThis plot shows that generally, the larger your kidney volume at baseline, the less of an increase there was in size over 3 years.\nTop of Tabset\n\n\n\nThe RMSE for model 1C is 6.14, this is similar performance to model 1B.\nModel 1C meets the assumptions of a linear regression (normality) the best.\nTop of Tabset"
  },
  {
    "objectID": "Project_3_Predictive_Modelling/Project_3_R/Code/Project_Submission.html#model-comparison",
    "href": "Project_3_Predictive_Modelling/Project_3_R/Code/Project_Submission.html#model-comparison",
    "title": "Predictive Modelling",
    "section": "Model Comparison",
    "text": "Model Comparison\nHere we will compare how models 1A, 1B, and 1C performed.\nNote: We removed outliers over the course of our model inspection. Thus we will re-run each analysis to get the true RMSE for each model.\n\nModel 1A\n\nset.seed(123)\n##### Model 1A\n# Perform the linear regression using 5-fold CV\nmodel1a &lt;- train(kidvol_change ~ log(kidvol_base), \n                 data = data, \n                 method = \"lm\",\n                 trControl = tc)\n# Get RMSE\nmodel1a\n\nLinear Regression \n\n67 samples\n 1 predictor\n\nNo pre-processing\nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 54, 55, 53, 55, 51 \nResampling results:\n\n  RMSE      Rsquared    MAE     \n  6.519605  0.04277987  5.194627\n\nTuning parameter 'intercept' was held constant at a value of TRUE\n\n\n\n\nModel 1B\n\nset.seed(123)\n##### Model 1B\n# Perform the linear regression using 5-fold CV\nmodel1b2 &lt;- train(kidvol_change ~ txti2, \n                 data = data, \n                 method = \"lm\",\n                 trControl = tc,\n                 preProc = \"scale\")\n# Get RMSE\nmodel1b2\n\nLinear Regression \n\n67 samples\n 1 predictor\n\nPre-processing: scaled (1) \nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 54, 55, 53, 55, 51 \nResampling results:\n\n  RMSE      Rsquared  MAE    \n  6.153724  0.171239  4.68434\n\nTuning parameter 'intercept' was held constant at a value of TRUE\n\n\n\n\nModel 1C\n\nset.seed(123)\n# Perform the linear regression using 5-fold CV\nmodel1c &lt;- train(kidvol_change ~ log(kidvol_base) + txti2_scale, \n                 data = data, \n                 method = \"lm\",\n                 trControl = tc)\n# Get RMSE\nmodel1c\n\nLinear Regression \n\n67 samples\n 2 predictor\n\nNo pre-processing\nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 54, 55, 53, 55, 51 \nResampling results:\n\n  RMSE      Rsquared   MAE     \n  6.140875  0.2383243  4.643702\n\nTuning parameter 'intercept' was held constant at a value of TRUE\n\n\n\n\nFinal Average RMSE\n\n\n\nModel\nRMSE\n\n\n\n\n1A: Baseline Kidney Volume\n6.52\n\n\n1B: Txti2\n6.15\n\n\n1C Baseline Kidney Volume and Txti2\n6.14\n\n\n\n\n\nCompare Model Performance\nWe can also use the caret package to compare perfomance between these three models.\n\nUpper Diagonal Values: These are the differences in the metric values between models. Positive values mean the first model has a higher metric value, while negative values mean the first model has a lower metric value.\nLower Diagonal Values: These are the p-values from hypothesis tests comparing the metric values between models. Small p-values (typically &lt; 0.05) indicate significant differences between the models for that metric.\n\n\n# Create list of models to compare\nmodel_list &lt;- list(`Kidney Volume at Baseline` = model1a, `Txti2` = model1b2, `Kidney Volume at Baseline and Txti2` = model1c)\n\n# Compare models\nresults &lt;- resamples(model_list)\nsummary(results)\n\n\nCall:\nsummary.resamples(object = results)\n\nModels: Kidney Volume at Baseline, Txti2, Kidney Volume at Baseline and Txti2 \nNumber of resamples: 5 \n\nMAE \n                                        Min.  1st Qu.   Median     Mean\nKidney Volume at Baseline           4.296977 5.268043 5.312707 5.194627\nTxti2                               3.442303 4.553295 4.891441 4.684340\nKidney Volume at Baseline and Txti2 3.282857 4.494957 4.952172 4.643702\n                                     3rd Qu.     Max. NA's\nKidney Volume at Baseline           5.438730 5.656678    0\nTxti2                               5.203527 5.331133    0\nKidney Volume at Baseline and Txti2 5.112327 5.376196    0\n\nRMSE \n                                        Min.  1st Qu.   Median     Mean\nKidney Volume at Baseline           5.426090 6.382835 6.398772 6.519605\nTxti2                               4.084323 6.119866 6.382093 6.153724\nKidney Volume at Baseline and Txti2 4.117591 5.917561 6.331809 6.140875\n                                     3rd Qu.     Max. NA's\nKidney Volume at Baseline           6.857043 7.533286    0\nTxti2                               6.536207 7.646131    0\nKidney Volume at Baseline and Txti2 6.550354 7.787060    0\n\nRsquared \n                                            Min.    1st Qu.     Median\nKidney Volume at Baseline           0.0003237652 0.02525581 0.03695319\nTxti2                               0.0177893816 0.04359540 0.10855879\nKidney Volume at Baseline and Txti2 0.0163224093 0.05880114 0.19970450\n                                          Mean    3rd Qu.      Max. NA's\nKidney Volume at Baseline           0.04277987 0.05047119 0.1008954    0\nTxti2                               0.17123905 0.26687362 0.4193780    0\nKidney Volume at Baseline and Txti2 0.23832433 0.43143200 0.4853616    0\n\n# Compare the RMSEs \ndiffs &lt;- diff(results) \nsummary(diffs)\n\n\nCall:\nsummary.diff.resamples(object = diffs)\n\np-value adjustment: bonferroni \nUpper diagonal: estimates of the difference\nLower diagonal: p-value for H0: difference = 0\n\nMAE \n                                    Kidney Volume at Baseline Txti2  \nKidney Volume at Baseline                                     0.51029\nTxti2                               0.05817                          \nKidney Volume at Baseline and Txti2 0.06738                   1.00000\n                                    Kidney Volume at Baseline and Txti2\nKidney Volume at Baseline           0.55092                            \nTxti2                               0.04064                            \nKidney Volume at Baseline and Txti2                                    \n\nRMSE \n                                    Kidney Volume at Baseline Txti2  \nKidney Volume at Baseline                                     0.36588\nTxti2                               0.6808                           \nKidney Volume at Baseline and Txti2 0.7453                    1.0000 \n                                    Kidney Volume at Baseline and Txti2\nKidney Volume at Baseline           0.37873                            \nTxti2                               0.01285                            \nKidney Volume at Baseline and Txti2                                    \n\nRsquared \n                                    Kidney Volume at Baseline Txti2   \nKidney Volume at Baseline                                     -0.12846\nTxti2                               0.3766                            \nKidney Volume at Baseline and Txti2 0.2353                    0.2573  \n                                    Kidney Volume at Baseline and Txti2\nKidney Volume at Baseline           -0.19554                           \nTxti2                               -0.06709                           \nKidney Volume at Baseline and Txti2                                    \n\n# Plot RMSEs of each model\ndotplot(results, metric = \"RMSE\", main = \"Comparison of RMSE Across Models\")\n\n\n\n\n\n\n\n\nModel A had an RMSE that was 0.37 points higher than model B (p = 0.68), and 0.74 points higher than model C (p = 0.75)"
  },
  {
    "objectID": "Project_3_Predictive_Modelling/Project_3_R/Code/Project_Submission.html#results",
    "href": "Project_3_Predictive_Modelling/Project_3_R/Code/Project_Submission.html#results",
    "title": "Predictive Modelling",
    "section": "Results",
    "text": "Results\nTo evaluate whether differences in model performance were statistically significant, RMSE for each model were compared using pairwise t-tests with Bonferroni p-value correction. The differences in model performance were not statistically significant. Model A had a 0.37 higher RMSE compared to model B (p = 0.68), and a 0.38 higher RMSE compared to model C (p = 0.75).\n\n\n\nModel\nRMSE\nP-Adjusted\n\n\n\n\n\n1A: Baseline Kidney Volume\n6.52\n–\n\n\n\n1B: Txti2\n6.15\n0.68\n—\n\n\n1C: Baseline Kidney Volume and Txti2\n6.14\n0.75\n1.0000"
  },
  {
    "objectID": "Project_3_Predictive_Modelling/Project_3_R/Code/Project_Submission.html#conclusion",
    "href": "Project_3_Predictive_Modelling/Project_3_R/Code/Project_Submission.html#conclusion",
    "title": "Predictive Modelling",
    "section": "Conclusion",
    "text": "Conclusion\nThe differences in model performance for task one were not statistically significant. Model A had an RMSE that was 0.37 points higher than model B (p = 0.68), and 0.74 points higher than model C (p = 0.75). This may be due to small sample size however.\nThus, including MRI image features into the predictive model did not increase predictive capability above and beyond that of just using kidney volume measurements at baseline.\nOn the other hand, if it is true that a model with baseline kidney volume, a model with txti2 alone, and a model with both included all perform similarly at predicting percent change in kidney volume after 3 years, then this could provide support for predicting change in kidney volume percent by EITHER MRI image features or baseline kidney volume.\nThat is, utilizing MRI image features alone may offer similar predictive capabilities to using kidney volume measurements taken by a practiced physician. Therefore, if one is easier or cheaper than the other to acquire, the easier alternative could be used in place of the harder alternative and still achieve similar predictive power. For example, if there are MRI records but perhaps kidney volume measurements were not taken, then the MRI images can be used instead for making predictions."
  },
  {
    "objectID": "Project_3_Predictive_Modelling/Project_3_R/Code/Project_Submission.html#2A",
    "href": "Project_3_Predictive_Modelling/Project_3_R/Code/Project_Submission.html#2A",
    "title": "Predictive Modelling",
    "section": "Model 2A",
    "text": "Model 2A\n\nBaseline Kidney Volume\nTo answer the researcher’s first question for this task, we will run a predictive model that uses the baseline height-corrected total kidney volume to predict whether a patient had fast or slow disease progression.\n\\[ logit(Progression) = 𝛽_0 + 𝛽_{Baseline Kidney Volume} + e \\]\n\nAnalysisVisualizationSummary\n\n\n\nlibrary(plotROC)\n# Convert progression into a factor\ndata &lt;- data |&gt;\n  mutate(progression_num = as.numeric(progression)-1)\ndata$progression\n\n [1] Slow Fast Slow Slow Fast Fast Slow Fast Slow Slow Fast Slow Fast Fast Slow\n[16] Slow Slow Fast Slow Fast Fast Fast Fast Fast Slow Fast Fast Slow Fast Slow\n[31] Slow Slow Slow Fast Slow Slow Slow Slow Slow Fast Fast Fast Slow Slow Fast\n[46] Slow Fast Slow Fast Fast Fast Slow Slow Slow Fast Fast Fast Fast Slow Fast\n[61] Slow Slow Fast Fast Slow Fast Fast\nLevels: Slow Fast\n\nset.seed(123)\n# Create a training control object with 5-fold cross-validation and class probabilities \ntc &lt;- trainControl(method = \"cv\", number = 5, classProbs = TRUE, summaryFunction = twoClassSummary)\n\n# Perform the linear regression using 5-fold CV\nmodel2a &lt;- train(progression~ log(kidvol_base), \n                 data = data, \n                 method = \"glm\",\n                 family = \"binomial\",\n                 trControl = tc,\n                 metric = \"ROC\",\n                 preProc = \"scale\")\n\n# Examine model\nmodel2a\n\nGeneralized Linear Model \n\n67 samples\n 1 predictor\n 2 classes: 'Slow', 'Fast' \n\nPre-processing: scaled (1) \nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 54, 53, 53, 55, 53 \nResampling results:\n\n  ROC        Sens  Spec     \n  0.4746032  0.3   0.7142857\n\nsummary(model2a)\n\n\nCall:\nNULL\n\nCoefficients:\n                   Estimate Std. Error z value Pr(&gt;|z|)\n(Intercept)         0.96887    2.93675   0.330    0.741\n`log(kidvol_base)` -0.07914    0.24665  -0.321    0.748\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 92.867  on 66  degrees of freedom\nResidual deviance: 92.764  on 65  degrees of freedom\nAIC: 96.764\n\nNumber of Fisher Scoring iterations: 3\n\n\nkidvol_base_log is not a significant predictor of change in kidney volume (p = 0.75).\nThe AUC for this model is 0.47, which is near guessing. The sensitivity is 0.3 and the specificity is 0.71\nTop of Tabset\n\n\n\nConfusion Matrix\n\n# Save predictions\ndata$predictions2a &lt;- predict(model2a, newdata = data)\ndata$predictions2a_prob &lt;- predict(model2a, newdata = data, type = \"prob\")[,2]\ndata$predictions2a_num &lt;- as.numeric(data$predictions2a_prob)-1\n\n# Create confusion matrix\ncm &lt;- table(Predicted = data$predictions2a, Actual = data$progression)\n\n# Examine confusion matrix and performance\ncm\n\n         Actual\nPredicted Slow Fast\n     Slow   11   11\n     Fast   22   23\n\n\nWe can see the poor performance clearly in the confusion matrix.\n\n\nROC Curve\n\n# Plot ROC Curve\nroc2a &lt;- ggplot(data, aes(m = predictions2a_num, d = progression_num)) +\n  geom_roc(n.cuts = 10, labels = F, labelround = 4) +\n  theme_minimal() +\n  labs(title = \"ROC Curve Model 2A: Baseline Kidney Volume\",\n       x = \"False Positive Rate\",\n       y = \"True Positive Rate\") +\n  geom_abline(intercept = 0, slope = 1, linetype = \"dashed\", color = \"blue\")\n\n# Visualize Plot\nroc2a\n\n\n\n\n\n\n\n\nWe can see that the ROC curve is very poor, near guessing.\n\n\n\nThe model using kidvol_base_log alone performed poorly.\nAUC: 0.47\nSensitivity: 0.3\nSpecificity: 0.71\nTop of Tabset"
  },
  {
    "objectID": "Project_3_Predictive_Modelling/Project_3_R/Code/Project_Submission.html#2B",
    "href": "Project_3_Predictive_Modelling/Project_3_R/Code/Project_Submission.html#2B",
    "title": "Predictive Modelling",
    "section": "Model 2B: MRI Image Features",
    "text": "Model 2B: MRI Image Features\nTo answer the researcher’s second question for this task, we will run a predictive model that uses the MRI features to predict whether a patient had fast or slow disease progression.\n\\[ logit(Progression) = 𝛽_0 + 𝛽_{MRI Feature 1} + 𝛽_{MRI Feature 1} + ... + e \\]\nAs the correlation matrices revealed similar relationship between the MRI image features and progression as they did with kidvol_change, we will select the same candidate covariates for Task 2 as we did for Task 1.\nThese are:\n\ngeom_avg\ngabor3\ntxti2\nlbp5_log\nglcm2_square\n\n\nModel SelectionAnalysisVisualizationMessing around with saturated modelSummary\n\n\n\n# Create variable\ndata1_train &lt;- data1_train |&gt; \n  mutate(progression_num = as.numeric(progression)-1)\n\n# Create model\nmodel2b &lt;- glm(progression_num ~ geom_avg_scale + gabor3_scale + txti2_scale + lbp5_log_scale + glcm2_square_scale, data = data1_train, family = \"binomial\")\n\n# Examine Model Summary\nsummary(model2b)\n\n\nCall:\nglm(formula = progression_num ~ geom_avg_scale + gabor3_scale + \n    txti2_scale + lbp5_log_scale + glcm2_square_scale, family = \"binomial\", \n    data = data1_train)\n\nCoefficients:\n                   Estimate Std. Error z value Pr(&gt;|z|)   \n(Intercept)         -0.1231     0.3313  -0.372  0.71018   \ngeom_avg_scale      -0.1339     0.3567  -0.375  0.70737   \ngabor3_scale         0.8120     0.4575   1.775  0.07593 . \ntxti2_scale         -1.2378     0.4499  -2.751  0.00594 **\nlbp5_log_scale      -0.7129     0.3976  -1.793  0.07295 . \nglcm2_square_scale  -0.6465     0.4903  -1.318  0.18738   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 72.010  on 51  degrees of freedom\nResidual deviance: 55.253  on 46  degrees of freedom\nAIC: 67.253\n\nNumber of Fisher Scoring iterations: 5\n\n# Perform backward elimination \nols_step_forward_sbc(model2b)\n\n\n                              Stepwise Summary                               \n---------------------------------------------------------------------------\nStep    Variable        AIC       SBC         SBIC         R2       Adj. R2 \n---------------------------------------------------------------------------\n 0      Base Model     79.405    83.308    -66879.305    0.00000    0.00000 \n 1      txti2_scale    71.935    77.789    -96068.990    0.16650    0.14983 \n---------------------------------------------------------------------------\n\nFinal Model Output \n------------------\n\n                        Model Summary                          \n--------------------------------------------------------------\nR                       0.408       RMSE                0.456 \nR-Squared               0.167       MSE                 0.208 \nAdj. R-Squared          0.150       Coef. Var          96.757 \nPred R-Squared          0.113       AIC                71.935 \nMAE                     0.419       SBC                77.789 \n--------------------------------------------------------------\n RMSE: Root Mean Square Error \n MSE: Mean Square Error \n MAE: Mean Absolute Error \n AIC: Akaike Information Criteria \n SBC: Schwarz Bayesian Criteria \n\n                              ANOVA                                \n------------------------------------------------------------------\n               Sum of                                             \n              Squares        DF    Mean Square      F        Sig. \n------------------------------------------------------------------\nRegression      2.161         1          2.161    9.988    0.0027 \nResidual       10.819        50          0.216                    \nTotal          12.981        51                                   \n------------------------------------------------------------------\n\n                                  Parameter Estimates                                    \n----------------------------------------------------------------------------------------\n      model      Beta    Std. Error    Std. Beta      t        Sig      lower     upper \n----------------------------------------------------------------------------------------\n(Intercept)     0.487         0.065                  7.542    0.000     0.357     0.616 \ntxti2_scale    -0.205         0.065       -0.408    -3.160    0.003    -0.336    -0.075 \n----------------------------------------------------------------------------------------\n\n\nThe model with the smallest BIC is with txti2 alone, thus that will be our final model.\nTop of Tabset\n\n\n\nset.seed(123)\n\n# Perform the linear regression using 5-fold CV\nmodel2b &lt;- train(progression ~ txti2, \n                 data = data, \n                 method = \"glm\",\n                 family = \"binomial\",\n                 trControl = tc,\n                 metric = \"ROC\",\n                 preProc = \"scale\")\n\n# Examine model\nmodel2b\n\nGeneralized Linear Model \n\n67 samples\n 1 predictor\n 2 classes: 'Slow', 'Fast' \n\nPre-processing: scaled (1) \nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 54, 53, 53, 55, 53 \nResampling results:\n\n  ROC        Sens       Spec     \n  0.7376417  0.5666667  0.6428571\n\nsummary(model2b)\n\n\nCall:\nNULL\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)   \n(Intercept)  0.01522    0.26188   0.058  0.95364   \ntxti2       -0.82651    0.30174  -2.739  0.00616 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 92.867  on 66  degrees of freedom\nResidual deviance: 83.741  on 65  degrees of freedom\nAIC: 87.741\n\nNumber of Fisher Scoring iterations: 4\n\n\ntxti2 is a significant predictor of kidvol_change (p = 0.0062).\nThe AUC is 0.74, the senstivity is 0.57, and the specificity is 0.64\nTop of Tabset\n\n\n\nConfusion Matrix\n\n# Save predictions\ndata$predictions2b &lt;- predict(model2b, newdata = data)\ndata$predictions2b_prob &lt;- predict(model2b, newdata = data, type = \"prob\")[,2]\ndata$predictions2b_num &lt;- as.numeric(data$predictions2b_prob)-1\n\n# Create confusion matrix\ncm &lt;- table(Predicted = data$predictions2b, Actual = data$progression)\n\n# Examine confusion matrix and performance\ncm\n\n         Actual\nPredicted Slow Fast\n     Slow   20   12\n     Fast   13   22\n\n\nWe can see that we are getting more correct hits in the confusion matrix compared to using baseline kidney volume alone.\nTop of Tabset\n\n\nROC Curve\n\n# Plot ROC Curve\nroc2b &lt;- ggplot(data, aes(m = predictions2b_num, d = progression_num)) +\n  geom_roc(n.cuts = 10, labels = F, labelround = 4) +\n  theme_minimal() +\n  labs(title = \"ROC Curve Model 2B: Txti2\",\n       x = \"False Positive Rate\",\n       y = \"True Positive Rate\") +\n  geom_abline(intercept = 0, slope = 1, linetype = \"dashed\", color = \"blue\")\n\n# Visualize Plot\nroc2b\n\n\n\n\n\n\n\n\nThat’s not a bad curve!\nTop of Tabset\n\n\n\n\nset.seed(123)\n\n# Create variable\ndata &lt;- data |&gt; \n  mutate(geom_avg = (geom1+geom2/2))\n\n# Perform the linear regression using 5-fold CV\nmodel2b_sat &lt;- train(progression ~ txti2 + geom_avg + log(lbp5) + gabor3, \n                 data = data, \n                 method = \"glm\",\n                 family = \"binomial\",\n                 trControl = tc,\n                 metric = \"ROC\",\n                 preProc = \"scale\")\n\n# Examine model\nmodel2b_sat\n\nGeneralized Linear Model \n\n67 samples\n 4 predictor\n 2 classes: 'Slow', 'Fast' \n\nPre-processing: scaled (4) \nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 54, 53, 53, 55, 53 \nResampling results:\n\n  ROC        Sens       Spec     \n  0.6654195  0.6047619  0.6190476\n\nsummary(model2b_sat)\n\n\nCall:\nNULL\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)   \n(Intercept) -0.28058    0.42210  -0.665  0.50622   \ntxti2       -0.81344    0.31123  -2.614  0.00896 **\ngeom_avg    -0.01038    0.26837  -0.039  0.96915   \n`log(lbp5)` -0.30809    0.29114  -1.058  0.28996   \ngabor3       0.10950    0.26274   0.417  0.67686   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 92.867  on 66  degrees of freedom\nResidual deviance: 82.263  on 62  degrees of freedom\nAIC: 92.263\n\nNumber of Fisher Scoring iterations: 4\n\n# Save predictions\ndata$predictions2b_sat &lt;- predict(model2b_sat, newdata = data)\ndata$predictions2b_prob_sat &lt;- predict(model2b_sat, newdata = data, type = \"prob\")[,2]\ndata$predictions2b_num_sat &lt;- as.numeric(data$predictions2b_prob_sat)-1\n\n# Create confusion matrix\ncm &lt;- table(Predicted = data$predictions2b, Actual = data$progression)\n\n# Examine confusion matrix and performance\ncm\n\n         Actual\nPredicted Slow Fast\n     Slow   20   12\n     Fast   13   22\n\n# Plot ROC Curve\nroc2b &lt;- ggplot(data, aes(m = predictions2b_num, d = progression_num)) +\n  geom_roc(n.cuts = 10, labels = F, labelround = 4) +\n  theme_minimal() +\n  labs(title = \"ROC Curve Model 2B: Txti2\",\n       x = \"False Positive Rate\",\n       y = \"True Positive Rate\") +\n  geom_abline(intercept = 0, slope = 1, linetype = \"dashed\", color = \"blue\")\n\n# Visualize Plot\nroc2b\n\n\n\n\n\n\n\n# Calculate area under the curve\ncalc_auc(roc2b)$AUC\n\n[1] 0.6934046\n\n\nNot that different from just txti2. I just wanted to check because I was getting the BIC backwards selection choosing this model.\n\n\nModel 2B performs much better than model 2A. We can especially see this when we look at the ROC curve.\nAUC: 0.74\nSensitivity: 0.57\nSpecificity: 0.64\nTop of Tabset"
  },
  {
    "objectID": "Project_3_Predictive_Modelling/Project_3_R/Code/Project_Submission.html#2C",
    "href": "Project_3_Predictive_Modelling/Project_3_R/Code/Project_Submission.html#2C",
    "title": "Predictive Modelling",
    "section": "Model 2C: Baseline Kidney Volume and MRI Image Features",
    "text": "Model 2C: Baseline Kidney Volume and MRI Image Features\nTo answer the researcher’s third question, we will run a predictive model that uses the baseline height-corrected total kidney volume AND MRI image features to predict slow vs fast disease progression.\n\\[ logit(Progression) = 𝛽_0 + 𝛽_{Baseline Kidney Volume} + 𝛽_{Txti2} + ... + e \\]\n\nAnalysisVisualizationSummary\n\n\n\nset.seed(123)\n\n# Perform the linear regression using 5-fold CV\nmodel2c &lt;- train(progression ~ log(kidvol_base) + txti2, \n                 data = data, \n                 method = \"glm\",\n                 family = \"binomial\",\n                 trControl = tc,\n                 metric = \"ROC\",\n                 preProc = \"scale\")\n\n# Examine model\nmodel2c\n\nGeneralized Linear Model \n\n67 samples\n 2 predictor\n 2 classes: 'Slow', 'Fast' \n\nPre-processing: scaled (2) \nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 54, 53, 53, 55, 53 \nResampling results:\n\n  ROC        Sens       Spec     \n  0.7064626  0.5666667  0.5857143\n\nsummary(model2c)\n\n\nCall:\nNULL\n\nCoefficients:\n                   Estimate Std. Error z value Pr(&gt;|z|)   \n(Intercept)          3.2899     3.2933   0.999  0.31781   \n`log(kidvol_base)`  -0.2757     0.2768  -0.996  0.31912   \ntxti2               -0.8901     0.3106  -2.866  0.00416 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 92.867  on 66  degrees of freedom\nResidual deviance: 82.711  on 64  degrees of freedom\nAIC: 88.711\n\nNumber of Fisher Scoring iterations: 4\n\n\nThe model performs similarly to using txti2 alone.\nThe AUC is 0.71, the sensitivity is 0.57, and the specificity is 0.59\nTop of Tabset\n\n\n\nConfusion Matrix\n\n# Save predictions\ndata$predictions2c &lt;- predict(model2c, newdata = data)\ndata$predictions2c_prob &lt;- predict(model2c, newdata = data, type = \"prob\")[,2]\ndata$predictions2c_num &lt;- as.numeric(data$predictions2c_prob)-1\n\n# Create confusion matrix\ncm &lt;- table(Predicted = data$predictions2c, Actual = data$progression)\n\n# Examine confusion matrix and performance\ncm\n\n         Actual\nPredicted Slow Fast\n     Slow   22   13\n     Fast   11   21\n\n\nNot much different than model 2.\n\n\nROC Curve\n\n# Plot ROC Curve\nroc2c &lt;- ggplot(data, aes(m = predictions2c_num, d = progression_num)) +\n  geom_roc(n.cuts = 10, labels = F, labelround = 4) +\n  theme_minimal() +\n  labs(title = \"ROC Curve Model 2C: Baseline Kidney Volume and Txti2\",\n       x = \"False Positive Rate\",\n       y = \"True Positive Rate\") +\n  geom_abline(intercept = 0, slope = 1, linetype = \"dashed\", color = \"blue\")\n\n# Visualize Plot\nroc2c\n\n\n\n\n\n\n\n\nNot much different than just using txti2 alone.\nTop of Tabset\n\n\n\nModel C does not appear to perform much differently than model B.\nAUC: 0.71\nSensitivity: 0.57\nSpecificity is 0.59\nTop of Tabset"
  },
  {
    "objectID": "Project_3_Predictive_Modelling/Project_3_R/Code/Project_Submission.html#model-comparison-1",
    "href": "Project_3_Predictive_Modelling/Project_3_R/Code/Project_Submission.html#model-comparison-1",
    "title": "Predictive Modelling",
    "section": "Model Comparison",
    "text": "Model Comparison\n\n# Create list of models to compare\nmodel_list &lt;- list(`Kidney Volume at Baseline` = model2a, Txti2 = model2b, `Kidney Volume at Baseline and Txti2` = model2c)\n\n# Compare models\nresults &lt;- resamples(model_list)\nsummary(results)\n\n\nCall:\nsummary.resamples(object = results)\n\nModels: Kidney Volume at Baseline, Txti2, Kidney Volume at Baseline and Txti2 \nNumber of resamples: 5 \n\nROC \n                                         Min.   1st Qu.    Median      Mean\nKidney Volume at Baseline           0.3469388 0.4081633 0.4761905 0.4746032\nTxti2                               0.5555556 0.6428571 0.6938776 0.7376417\nKidney Volume at Baseline and Txti2 0.5833333 0.6428571 0.6530612 0.7064626\n                                      3rd Qu.      Max. NA's\nKidney Volume at Baseline           0.5306122 0.6111111    0\nTxti2                               0.7959184 1.0000000    0\nKidney Volume at Baseline and Txti2 0.6734694 0.9795918    0\n\nSens \n                                         Min.   1st Qu.    Median      Mean\nKidney Volume at Baseline           0.0000000 0.2857143 0.2857143 0.3000000\nTxti2                               0.1428571 0.1666667 0.6666667 0.5666667\nKidney Volume at Baseline and Txti2 0.1666667 0.2857143 0.6666667 0.5666667\n                                      3rd Qu. Max. NA's\nKidney Volume at Baseline           0.4285714  0.5    0\nTxti2                               0.8571429  1.0    0\nKidney Volume at Baseline and Txti2 0.7142857  1.0    0\n\nSpec \n                                         Min.   1st Qu.    Median      Mean\nKidney Volume at Baseline           0.5714286 0.5714286 0.7142857 0.7142857\nTxti2                               0.4285714 0.5000000 0.7142857 0.6428571\nKidney Volume at Baseline and Txti2 0.4285714 0.4285714 0.5000000 0.5857143\n                                      3rd Qu.      Max. NA's\nKidney Volume at Baseline           0.7142857 1.0000000    0\nTxti2                               0.7142857 0.8571429    0\nKidney Volume at Baseline and Txti2 0.7142857 0.8571429    0\n\nresults$models\n\n[1] \"Kidney Volume at Baseline\"           \"Txti2\"                              \n[3] \"Kidney Volume at Baseline and Txti2\"\n\n# Compare the AUCs \ndiffs &lt;- diff(results) \nsummary(diffs)\n\n\nCall:\nsummary.diff.resamples(object = diffs)\n\np-value adjustment: bonferroni \nUpper diagonal: estimates of the difference\nLower diagonal: p-value for H0: difference = 0\n\nROC \n                                    Kidney Volume at Baseline Txti2   \nKidney Volume at Baseline                                     -0.26304\nTxti2                               0.2809                            \nKidney Volume at Baseline and Txti2 0.3134                    0.8648  \n                                    Kidney Volume at Baseline and Txti2\nKidney Volume at Baseline           -0.23186                           \nTxti2                                0.03118                           \nKidney Volume at Baseline and Txti2                                    \n\nSens \n                                    Kidney Volume at Baseline\nKidney Volume at Baseline                                    \nTxti2                               0.4126                   \nKidney Volume at Baseline and Txti2 0.2756                   \n                                    Txti2                 \nKidney Volume at Baseline           -0.2666666666666666630\nTxti2                                                     \nKidney Volume at Baseline and Txti2 1.0000                \n                                    Kidney Volume at Baseline and Txti2\nKidney Volume at Baseline           -0.2666666666666666630             \nTxti2                               -0.0000000000000000111             \nKidney Volume at Baseline and Txti2                                    \n\nSpec \n                                    Kidney Volume at Baseline Txti2  \nKidney Volume at Baseline                                     0.07143\nTxti2                               1                                \nKidney Volume at Baseline and Txti2 1                         1      \n                                    Kidney Volume at Baseline and Txti2\nKidney Volume at Baseline           0.12857                            \nTxti2                               0.05714                            \nKidney Volume at Baseline and Txti2                                    \n\n# Plot RMSEs of each model\ndotplot(results, metric = \"ROC\",\n        main = \"Comparison of AUC Across Models\")\n\n\n\n\n\n\n\ndotplot(results, metric = \"Sens\",\n        main = \"Comparison of Sensitivity Across Models\")\n\n\n\n\n\n\n\ndotplot(results, metric = \"Spec\",\n        main = \"Comparison of Specificity Across Models\")\n\n\n\n\n\n\n\n\nThe difference in performance between models was not statistically significant (report AUC’s and 95% CI’s here)"
  },
  {
    "objectID": "Project_3_Predictive_Modelling/Project_3_R/Code/Project_Submission.html#results-1",
    "href": "Project_3_Predictive_Modelling/Project_3_R/Code/Project_Submission.html#results-1",
    "title": "Predictive Modelling",
    "section": "Results",
    "text": "Results\nTable 2. Results of Pairwise T-Tests Comparing Model Performance Predicting Disease Progression using AUC\n\n\n\n\n\n\n\n\n\n\nModel\nAUC\nSensitivity\nSpecificity\n  P-Adjusted\n\n\nBaseline Kidney Volume\n0.47\n0.30\n0.71\n—\n\n\nTxti2\n0.74\n0.57\n0.64\n0.28\n\n\nBaseline Kidney Volume and Txti2\n0.71\n0.57\n0.59\n0.31"
  },
  {
    "objectID": "Project_3_Predictive_Modelling/Project_3_R/Code/Project_Submission.html#discussion",
    "href": "Project_3_Predictive_Modelling/Project_3_R/Code/Project_Submission.html#discussion",
    "title": "Predictive Modelling",
    "section": "Discussion",
    "text": "Discussion\nThe model using txti2 alone performed better at predicting disease progression than the model using kidvol_base alone (AUC = , 95% CI: ,). However, this difference was not statistically significant (p = ___).\nAdditionally, the model incorporating both txti2 and kidvol_base did not perform better than the model using just txti (AUC = , 95% CI: )\nHowever, examining the ROC curves and the 95% CIs for all of the models, it does appear that there is a trend towards these difference being significant. It is possible that these may have not been statistically significant due to a small sample size (N = 67)."
  },
  {
    "objectID": "Project_3_Predictive_Modelling/Project_3_SAS/Code/images/Megan_Watson_Project-02.html",
    "href": "Project_3_Predictive_Modelling/Project_3_SAS/Code/images/Megan_Watson_Project-02.html",
    "title": "Data Checking - Statistical Consulting",
    "section": "",
    "text": "This is the Quarto markdown for the live consultation in BIOS 6621: Statistical Consulting.\nThis is a research project encompassing the entire data analysis pipeline, from initial consultation to scope of work writing, execution of statistical analysis, and completion of project deliverables."
  },
  {
    "objectID": "Project_3_Predictive_Modelling/Project_3_SAS/Code/images/Megan_Watson_Project-02.html#general-information",
    "href": "Project_3_Predictive_Modelling/Project_3_SAS/Code/images/Megan_Watson_Project-02.html#general-information",
    "title": "Data Checking - Statistical Consulting",
    "section": "General Information",
    "text": "General Information\n\n\n\n\n\n\n\n\n\nInvestigator\nMegan Watson\nDate\nSeptember 30, 2024\n\n\nProject Title\nQuality Improvement into Standard Practice: Persistent Practice Changes and Decreased Length of Stay for 3 years following a Medical ICU Physical Therapy Quality Improvement Project"
  },
  {
    "objectID": "Project_3_Predictive_Modelling/Project_3_SAS/Code/images/Megan_Watson_Project-02.html#project-description",
    "href": "Project_3_Predictive_Modelling/Project_3_SAS/Code/images/Megan_Watson_Project-02.html#project-description",
    "title": "Data Checking - Statistical Consulting",
    "section": "Project Description",
    "text": "Project Description\n\nBackground\n             The objective of this study is to assess the impact of a MICU physical therapy (PT) quality improvement (QI) project on physical therapist practice, and quantify the changes in length of stay (LOS) and mechanical ventilation (MV) time. The goals of the QI Initiative were to increase percentage of MICU admissions receiving PT, decrease time from MICU admission to first PT, and increase the frequency of PT visits. The main question the researcher came to us with was: how to correctly model the longitudinal data in the study?\n\n\nStudy Design\n              This was a retrospective cohort study, comparing 3 main time periods: Pre-QI Initiative (before April 2015), During QI Initiative (April 2015-Dec 2015), and After-QI Initiative (Jan 2016 and later). The QI initiative was deployed during the 9-month period between April 2015 and December 2015, during which time there were education and staffing changes. Patients were stratified as either having mechanical ventilation or not having mechanical ventilation. The primary outcomes of interest were MICU LOS and time on MV. Secondary outcomes of interest were the non-ICU LOS, total hospital LOS, and discharge location.\n\n\nDescription of the Data\n              Data was received as a de-identified .csv. Data points consist of: Admit and Discharge Time, PT Consult Time, Admit to Consult Time, First Therapy Time, Consult to Therapy Time, Admit to Therapy Time, Total Therapy Visits, Days with Therapy Visits, Average, min, and Max Therapy Length, Admit and Discharge Date/Time, Total MV Days, Hospital LOS, Patient Age, Sex, “Problem List”, and Codes (presumably ICD9/10).\n\n\nAnticipated Sample Size / Study Population\n              The sample size is N = ~3000-4000, consisting of patients admitted to the MICU with a LOS between 2 and 30 days (I.e. exclude &lt; 2 days or &gt; 30 days). This was a single site study performed here at Colorado University.\n\n\nHypothesis\n              The hypothesis for the study was that LOS and time on MV would decrease when comparing the pre- and intra-initiative time periods, and when comparing the pre- and post-initiative time periods. Clinical significance for this outcome is considered as a 1-2 day change in LOS. The intended end product of this project is a publication in a PT journal.\n\n\nAnalysis Plan (Sketch)\n              Data should first be examined for missingness, as this has not been performed. Similarly, repeat MICU admissions should be explored and controlled for to ensure independence of observations. Death during MICU stay should also be examined and adjusted for if needed. The analysis plan discussed was to utilize splines to capture the trajectory of LOS or time on MV over time by fitting smooth curves to the data. Linear contrasts should be performed to assess if there is a significant difference in LOS or time on MV when comparing two timepoints (i.e. pre- and intra-, pre- and post- , and post- and intra-QI Initiative. Supplementary analyses should be performed to assess secondary outcomes of interest of non-ICU LOS and total hospital LOS."
  },
  {
    "objectID": "Project_3_Predictive_Modelling/Project_3_SAS/Code/images/Megan_Watson_Project-02.html#project-deliverables",
    "href": "Project_3_Predictive_Modelling/Project_3_SAS/Code/images/Megan_Watson_Project-02.html#project-deliverables",
    "title": "Data Checking - Statistical Consulting",
    "section": "Project Deliverables",
    "text": "Project Deliverables\n             At this stage, Megan has only asked for insight into what method to use to account for variance in time for her analysis. There are currently no other deliverables.\nTimeline / Deadlines\nAt this time, Megan has not asked for data analysis efforts from us.\nShould this change, the anticipated project start date will be 1 week after our next meeting with her, where revisions to the Comprehensive Analysis Report are also expected to be completed.  The assigned analysts will reach out to the investigator to confirm the project timeframe and the project start date may be altered if necessary. 2 weeks will be allowed for initial data analysis by the assigned analyst.\nA follow-up meeting is to occur no later than 1 month after the next meeting with Megan. During the follow-up meeting, the project team will discuss and preliminary results of the analysis, mockup the desired tables and figures (1-2 descriptive tables, 1-2 analysis results tables and up to 4 graphs), and discuss next steps. During the follow-up meeting, the team will establish a meeting schedule for the remainder of the project. \nEstimated Cost\nThis represents our best estimate of the effort needed to complete the project. Should the scope of work change substantially, the analyst(s) will discuss changes with the investigator and issue a new or amended project agreement.\n\n\n\n\nEffort\n\n\n\n\nMonths\nPhD Faculty\nMaster’s Faculty\nStudent\n\n\n\n\n\n\n\n\n1\n20%\n0%\n80%\n\n\n\n\nTotal Costs\n’Bout Tree Fiddy"
  },
  {
    "objectID": "Project_3_Predictive_Modelling/Project_3_SAS/Code/images/Megan_Watson_Project-02.html#Outcome_Variables",
    "href": "Project_3_Predictive_Modelling/Project_3_SAS/Code/images/Megan_Watson_Project-02.html#Outcome_Variables",
    "title": "Data Checking - Statistical Consulting",
    "section": "Outcome Variables",
    "text": "Outcome Variables\n\nMechanical VentilationMICU Length of StayHospital Length of StayNon-ICU Length of StayDischarge Location\n\n\nFirst we will begin with mechanical ventilation time (MV)\n\n# Create histogram and qqplot\ndistr_plots(data_ex, \"MV_Total\")\n\nWarning: `aes_string()` was deprecated in ggplot2 3.0.0.\nℹ Please use tidy evaluation idioms with `aes()`.\nℹ See also `vignette(\"ggplot2-in-packages\")` for more information.\n\n\nWarning: Removed 3166 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\nWarning: Removed 3166 rows containing non-finite outside the scale range\n(`stat_qq()`).\n\n\nWarning: Removed 3166 rows containing non-finite outside the scale range\n(`stat_qq_line()`).\n\n\n\n\n\n\n\n\n\nThat’s some severe right skewness! Is that logarithmic? Let’s try and transform and see what happens.\n\n# Perform a log transform of mechanical ventilation time and plot\ndata_ex$MV_Total_log &lt;- log(data_ex$MV_Total)\n\n# Create histogram and qqplot\ndistr_plots(data_ex, \"MV_Total_log\", 10)\n\nWarning: Removed 3166 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\nWarning: Removed 3166 rows containing non-finite outside the scale range\n(`stat_qq()`).\n\n\nWarning: Removed 3166 rows containing non-finite outside the scale range\n(`stat_qq_line()`).\n\n\n\n\n\n\n\n\n\nThat’s better. Maybe try a square root transformation?\n\n# Perform a sqrt transformation and plot\ndata_ex$MV_Total_sqrt &lt;- sqrt(data_ex$MV_Total)\n\n# Create histogram and qqplot\ndistr_plots(data_ex, \"MV_Total_sqrt\", 10)\n\nWarning: Removed 3166 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\nWarning: Removed 3166 rows containing non-finite outside the scale range\n(`stat_qq()`).\n\n\nWarning: Removed 3166 rows containing non-finite outside the scale range\n(`stat_qq_line()`).\n\n\n\n\n\n\n\n\n\nThat looks worse. Out of curiosity I’m going to try using the bestNormalize package I’ve been learning to see if it can recommend the best tranformation.\n\nBNobject &lt;- bestNormalize(data_ex$MV_Total)\n\nWarning: `progress_estimated()` was deprecated in dplyr 1.0.0.\nℹ The deprecated feature was likely used in the bestNormalize package.\n  Please report the issue to the authors.\n\nBNobject\n\nBest Normalizing transformation with 3264 Observations\n Estimated Normality Statistics (Pearson P / df, lower =&gt; more normal):\n - arcsinh(x): 24.36\n - Box-Cox: 24.4203\n - Center+scale: 23.7916\n - Double Reversed Log_b(x+a): 26.1832\n - Exp(x): 343.5131\n - Log_b(x+a): 24.338\n - orderNorm (ORQ): 24.1263\n - sqrt(x + a): 24.0999\n - Yeo-Johnson: 24.5655\nEstimation method: Out-of-sample via CV with 10 folds and 5 repeats\n \nBased off these, bestNormalize chose:\ncenter_scale(x) Transformation with 3264 nonmissing obs.\n Estimated statistics:\n - mean (before standardization) = 6.102022 \n - sd (before standardization) = 5.111536 \n\n\nThe short explanation of how these values work is that the value closest to 1 is the best transformation, but these are all VERY far from one. So that didn’t help.\n\nBoxplot\n\n#Create boxplot to assess for outliers\nggplot(data_ex, aes(y = MV_Total_log)) +\n  geom_boxplot() +\n  labs(title = \"Boxplot of MV Total Log\",\n       y = \"MV Total Log\")\n\nWarning: Removed 3166 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\n\n\n\n\nThere are also no outliers for MV_Total_Log!\n\n\nSummary\nIt looks like the best option we have is a log transformation of MV TOTAL.\nAlternatively, we will likely end up running a Poisson or Negative Binomial regression to handle this data.\nTop of Tabset\n\n\n\n\n# Create histogram and qqplot\ndistr_plots(data_ex, \"Unit_LOS\", 10)\n\n\n\n\n\n\n\n\nThat’s super skewed as well! Let’s try a log transform.\n\n# Perform log transform of MICU LOS\ndata_ex$Unit_LOS_log &lt;- log(data_ex$Unit_LOS)\n\n# Create histogram and qqplot\ndistr_plots(data_ex, \"Unit_LOS_log\", 10)\n\n\n\n\n\n\n\n\nWe’ve got more of an S shape going there. I wonder if a BoxCox transformation is more appropriate?\n\nlibrary(bestNormalize)\n# Use bestNormalize to try to find best transformation\nBNobject &lt;- bestNormalize(data_ex$Unit_LOS)\nBNobject\n\nBest Normalizing transformation with 6430 Observations\n Estimated Normality Statistics (Pearson P / df, lower =&gt; more normal):\n - arcsinh(x): 9.2214\n - Box-Cox: 5.6531\n - Center+scale: 34.5953\n - Double Reversed Log_b(x+a): 57.0874\n - Log_b(x+a): 9.2214\n - orderNorm (ORQ): 1.2696\n - sqrt(x + a): 19.9924\n - Yeo-Johnson: 5.648\nEstimation method: Out-of-sample via CV with 10 folds and 5 repeats\n \nBased off these, bestNormalize chose:\norderNorm Transformation with 6430 nonmissing obs and ties\n - 515 unique values \n - Original quantiles:\n    0%    25%    50%    75%   100% \n 48.00  66.00  95.00 166.75 720.00 \n\n# Perform boxcox transformation\ndata_ex$Unit_LOS_boxcox &lt;- boxcox(data_ex$Unit_LOS)$x.t\n\n# Create histogram and qqplot\ndistr_plots(data_ex, \"Unit_LOS_boxcox\", 25)\n\n\n\n\n\n\n\n\n\n# Create boxplot to assess for outliers\nggplot(data_ex, aes(y = Unit_LOS_boxcox)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\nThere are no outliers for boxcox Unit LOS.\nThat histogram looks better at least, and we don’t have any outliers. The bestNormalize package offers ways to back transform after you run your analysis, but I haven’t gotten that far in learning it yet. It looks like a boxcox transformation is a candidate however.\n\nSummary\nWill come back to this. I think there is a specific link function we use in this situation when we have an s-shaped qq plot like that.\nTop of Tabset\n\n\n\n\n# Create histogram and qqplot\ndistr_plots(data_ex, \"`HOSPITAL LOS`\", 25)\n\n\n\n\n\n\n\n\nHospital LOS also looks logarithmic.\nLet’s transform and assess.\n\n# Perform log transform \ndata_ex$HOSPITAL_LOS_log &lt;- log(data_ex$`HOSPITAL LOS`)\n\n# Create histogram and qqplot\ndistr_plots(data_ex, \"HOSPITAL_LOS_log\", 25)\n\n\n\n\n\n\n\nggplot(data_ex, aes(y = HOSPITAL_LOS_log)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\nLooks good! There are a handful of outliers in there, but the data looks normal and those values will likely be included.\n\nSummary\nHOSPITAL LOS is normal after a log transform. We will either perform that or a Poisson or Negative Binomial Top of Tabset\n\n\n\nWe must compute NON_ICU_LOS by subtracting Unit_LOS from HOSPITAL LOS.\n\n# Compute variable for non icu LOS.\ndata_ex &lt;- data_ex |&gt; \n  mutate(NON_ICU_LOS = `HOSPITAL LOS` - Unit_LOS)\n\n# Create histogram and qqplot\ndistr_plots(data_ex, \"NON_ICU_LOS\", 25)\n\n\n\n\n\n\n\n\nAs expected, that looks exponential.\n\n# Perform log transform\ndata_ex$NON_ICU_LOS_log &lt;- log(data_ex$NON_ICU_LOS)\n\n# Create histogram and qqplot\ndistr_plots(data_ex, \"NON_ICU_LOS_log\", 25)\n\nWarning: Removed 106 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\nWarning: Removed 106 rows containing non-finite outside the scale range\n(`stat_qq()`).\n\n\nWarning: Removed 106 rows containing non-finite outside the scale range\n(`stat_qq_line()`).\n\n\n\n\n\n\n\n\n\nThat’s bimodal. Odd.\nLet’s try a square root transform\n\n# Perform log transform\ndata_ex$NON_ICU_LOS_sqrt &lt;- sqrt(data_ex$NON_ICU_LOS)\n\n# Create histogram and qqplot\ndistr_plots(data_ex, \"NON_ICU_LOS_sqrt\", 25)\n\n\n\n\n\n\n\n\nLooks crummy.\nLet’s try bestNormalize\n\nBNobject &lt;- bestNormalize(data_ex$NON_ICU_LOS)\nBNobject\n\nBest Normalizing transformation with 6430 Observations\n Estimated Normality Statistics (Pearson P / df, lower =&gt; more normal):\n - arcsinh(x): 7.2819\n - Center+scale: 50.0424\n - Double Reversed Log_b(x+a): 58.7953\n - Log_b(x+a): 8.71\n - orderNorm (ORQ): 1.7327\n - sqrt(x + a): 10.2385\n - Yeo-Johnson: 5.4621\nEstimation method: Out-of-sample via CV with 10 folds and 5 repeats\n \nBased off these, bestNormalize chose:\norderNorm Transformation with 6430 nonmissing obs and ties\n - 1045 unique values \n - Original quantiles:\n    0%    25%    50%    75%   100% \n   0.0   32.0  110.5  284.0 9206.0 \n\n# Perform boxcox transformation\ndata_ex$NON_ICU_LOS_yeo &lt;- yeojohnson(data_ex$NON_ICU_LOS)$x.t\n\n# Create histogram and qqplot\ndistr_plots(data_ex, \"NON_ICU_LOS_yeo\", 25)\n\n\n\n\n\n\n\n\nLooks better! Still bimodal but much more normal.\n\nSummary\nLooks like Yeo-Johnson transformation may be the way to go.\nTop of Tabset\n\n\n\n\n# Examine discharge locations\npretty_print(table(data_ex$DISCH_DISP))\n\n\n\n\nVar1\nFreq\n\n\n\n\nAcute IP to RPCU\n5\n\n\nAdministrative Discharge\n1\n\n\nAdmitted as an Inpatient\n3\n\n\nAgainst Clinical Advice\n22\n\n\nAnother Health Care Institution Not Defined w/Planned Readmission\n1\n\n\nCourt/Law Enforcement\n22\n\n\nDischarge Lounge\n6\n\n\nDischarged to Other Facility\n23\n\n\nDischarged/transferred to a Designated Cancer Center or Children’s Hospital\n41\n\n\nDrug/Alch Detox D/C\n4\n\n\nExpired\n1137\n\n\nExpired in Medical Facility\n8\n\n\nExpired Readmit as Organ Donor\n7\n\n\nFederal Hospital\n43\n\n\nHome-Health Care Svc\n759\n\n\nHome or Self Care\n2417\n\n\nHome or Self Care w/Planned Readmission\n1\n\n\nHospice/Home\n213\n\n\nHospice/Medical Facility\n211\n\n\nIntermediate Care Facility\n14\n\n\nIV Therapy Provider\n1\n\n\nLeft Against Medical Advice\n49\n\n\nLong Term Care\n451\n\n\nMental Health Facility\n2\n\n\nNursing Facility\n37\n\n\nPsychiatric Hospital\n34\n\n\nRehab Facility\n249\n\n\nShort Term Hospital\n71\n\n\nSkilled Nursing Facility\n596\n\n\nSwing Bed\n1\n\n\nTRANSFER TO CANCER OR CHILDRENS\n1\n\n\n\n\n\n\n\nThere are a few options we have for to analyze discharge location.\n\nWe can spit on expired vs not expired\nWe can split on hom vs other\nWe can just look at all the groups with the largest N, and collapse the rest into a single variable of ‘other’. This would be\n\nExpired (1137), Home (3177), Hospice (424), long term care (451), rehab (249), skilled Nursing facility (596)\n\n\nWe will have to ask the researcher how she wants to group up these discharge locations! Keeping in mind that since we are looking at three different time periods, each group will essentially by a third of what is shown here."
  },
  {
    "objectID": "Project_3_Predictive_Modelling/Project_3_SAS/Code/images/Megan_Watson_Project-02.html#average-micu-los",
    "href": "Project_3_Predictive_Modelling/Project_3_SAS/Code/images/Megan_Watson_Project-02.html#average-micu-los",
    "title": "Data Checking - Statistical Consulting",
    "section": "Average MICU LOS",
    "text": "Average MICU LOS\n\n# Summarize and plot the data\ndata_ex |&gt; \n  group_by(Admit_MonthYear) |&gt; \n  summarise(avg_Unit_LOS = mean(Unit_LOS, na.rm = T)) |&gt; \n  ggplot(aes(x = Admit_MonthYear, y = avg_Unit_LOS)) +\n  geom_line() +\n  theme_minimal() +\n  geom_rect(data = highlight_periods, aes(xmin = start, xmax = end, ymin = -Inf, ymax = Inf), \n            inherit.aes = FALSE, fill = \"lightblue\", alpha = 0.2) +\n  labs(title = \"Average MICU LOS over Time\",\n       x = \"Admit Date\",\n       y = \"Avg MICU LOS\")\n\n\n\n\n\n\n\n\nIt does not appear that MICU_LOS has decreased across any of the time periods.\nThis is including patients that did not receive PT, so of course there’s no difference.\n\nMICU LOS over Time by Mechanical Ventilation\n\n# Dummy code MV status \ndata_ex$MV_YN &lt;- ifelse(is.na(data_ex$MV_Total), 0, 1)\n\n# Make it a factor for better plotting\ndata_ex &lt;- data_ex |&gt; \n  mutate(MV_YN = as.factor(MV_YN))\n\n# Summarize and plot the data\ndata_ex |&gt; \n  group_by(Admit_MonthYear, MV_YN) |&gt; \n  summarise(avg_Unit_LOS = mean(Unit_LOS, na.rm = T)) |&gt; \n  ggplot(aes(x = Admit_MonthYear, y = avg_Unit_LOS, color = MV_YN, group = MV_YN)) +\n  geom_line() +\n  theme_minimal() +\n  geom_rect(data = highlight_periods, aes(xmin = start, xmax = end, ymin = -Inf, ymax = Inf), \n            inherit.aes = FALSE, fill = \"lightblue\", alpha = 0.2) +\n  labs(title = \"Average MICU LOS over Time by Mechanical Ventilation\",\n       x = \"Admit Date\",\n       y = \"Avg MICU LOS\")\n\n`summarise()` has grouped output by 'Admit_MonthYear'. You can override using\nthe `.groups` argument.\n\n\n\n\n\n\n\n\n\nThis shows that those who were mechanically ventilated had a higher MICU length of stay. This does not appear to have differed in relation to the QI iniative.\nI am also including those that did not receive PT here, so that would explain the null finding.\n\ndata_ex &lt;- data_ex |&gt; \n  mutate(PT_YESNO = as.factor(PT_YESNO))\n\n# Summarize and plot the data\ndata_ex |&gt; \n  group_by(Admit_MonthYear, MV_YN, PT_YESNO) |&gt; \n  summarise(avg_Unit_LOS = mean(Unit_LOS, na.rm = T)) |&gt; \n  ggplot(aes(x = Admit_MonthYear, y = avg_Unit_LOS, color = PT_YESNO)) +\n  geom_point() +\n  geom_smooth(method = \"loess\", se = F) +\n  theme_minimal() +\n  geom_rect(data = highlight_periods, aes(xmin = start, xmax = end, ymin = -Inf, ymax = Inf), \n            inherit.aes = FALSE, fill = \"lightblue\", alpha = 0.2) +\n  labs(title = \"Average MICU LOS over Time by Mechanical Ventilation\",\n       x = \"Admit Date\",\n       y = \"Avg MICU LOS\") +\n  facet_wrap(~MV_YN)\n\n`summarise()` has grouped output by 'Admit_MonthYear', 'MV_YN'. You can\noverride using the `.groups` argument.\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nI think this might show an interaction. For those on mechanical ventilation, they seemed to decrease in MICU LOS more if they received PT.\nFor those not on MV, the PT did not seem to change their average LOS.\n\n\nAverage MICU LOS over Time by Physical Therapy\n\n# Make it a factor for better plotting\ndata_ex &lt;- data_ex |&gt; \n  mutate(PT_YESNO = as.factor(PT_YESNO))\n\n# Summarize and plot the data\ndata_ex |&gt; \n  group_by(Admit_MonthYear, PT_YESNO) |&gt; \n  summarise(avg_Unit_LOS = mean(Unit_LOS, na.rm = T)) |&gt; \n  filter(Admit_MonthYear &gt; as.Date(\"2012-01-01\")) |&gt; \n  ggplot(aes(x = Admit_MonthYear, y = avg_Unit_LOS, color = PT_YESNO, group = PT_YESNO)) +\n  geom_line() +\n  theme_minimal() +\n  geom_rect(data = highlight_periods, aes(xmin = start, xmax = end, ymin = -Inf, ymax = Inf), \n            inherit.aes = FALSE, fill = \"lightblue\", alpha = 0.2) +\n  labs(title = \"Average MICU LOS over Time by Physical Therapy\",\n       x = \"Admit Date\",\n       y = \"Avg MICU LOS\")\n\n`summarise()` has grouped output by 'Admit_MonthYear'. You can override using\nthe `.groups` argument.\n\n\n\n\n\n\n\n\n\n\n# Summarize and plot the data\ndata_ex |&gt; \n  group_by(Admit_MonthYear, PT_YESNO) |&gt; \n  summarise(avg_Unit_LOS = mean(Unit_LOS, na.rm = T)) |&gt; \n  filter(Admit_MonthYear &gt; as.Date(\"2012-01-01\")) |&gt; \n  ggplot(aes(x = Admit_MonthYear, y = avg_Unit_LOS, color = PT_YESNO, group = PT_YESNO)) +\n  geom_point() +\n  geom_smooth(method = \"loess\", se = F) +\n  theme_minimal() +\n  geom_rect(data = highlight_periods, aes(xmin = start, xmax = end, ymin = -Inf, ymax = Inf), \n            inherit.aes = FALSE, fill = \"lightblue\", alpha = 0.2) +\n  labs(title = \"Average MICU LOS over Time by Physical Therapy\",\n       x = \"Admit Date\",\n       y = \"Avg MICU LOS\")\n\n`summarise()` has grouped output by 'Admit_MonthYear'. You can override using\nthe `.groups` argument.\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nInteresting! It does look like average MICU length of stay decreased slighly immediately after the QI initiative. It also looks like it may be increasing slightly from then on.\nThat does appear that those received PT had a decrease before and after the iniative\nBut overall it looks like the QI may have successfuly decreased MICU LOS.\n\n\nFitting with Splines\n\nlibrary(splines)\n# Summarize and plot the data\ndata_ex |&gt; \n  group_by(Admit_MonthYear, PT_YESNO) |&gt; \n  summarise(avg_Unit_LOS = mean(Unit_LOS, na.rm = T)) |&gt; \n  filter(Admit_MonthYear &gt; as.Date(\"2012-01-01\")) |&gt; \n  ggplot(aes(x = Admit_MonthYear, y = avg_Unit_LOS, color = PT_YESNO, group = PT_YESNO)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", formula = y ~ ns(x, df = 4), se = F) +\n  theme_minimal() +\n  geom_rect(data = highlight_periods, aes(xmin = start, xmax = end, ymin = -Inf, ymax = Inf), \n            inherit.aes = FALSE, fill = \"lightblue\", alpha = 0.2) +\n  labs(title = \"Average MICU LOS over Time by Physical Therapy\",\n       x = \"Admit Date\",\n       y = \"Avg MICU LOS\")\n\n`summarise()` has grouped output by 'Admit_MonthYear'. You can override using\nthe `.groups` argument.\n\n\n\n\n\n\n\n\n\nLooking just at those who received PT\n\n# Make it a factor for better plotting\ndata_ex &lt;- data_ex |&gt; \n  mutate(PT_YESNO = as.factor(PT_YESNO))\n\n# Summarize and plot the data\ndata_ex |&gt; \n  group_by(Admit_MonthYear, PT_YESNO) |&gt; \n  summarise(avg_Unit_LOS = mean(Unit_LOS, na.rm = T)) |&gt; \n  filter(Admit_MonthYear &gt; as.Date(\"2012-01-01\")) |&gt; \n  filter(PT_YESNO == 1) |&gt; \n  ggplot(aes(x = Admit_MonthYear, y = avg_Unit_LOS, group = PT_YESNO)) +\n  geom_point() +\n  geom_smooth(method = \"loess\", se = F) +\n  theme_minimal() +\n  geom_rect(data = highlight_periods, aes(xmin = start, xmax = end, ymin = -Inf, ymax = Inf), \n            inherit.aes = FALSE, fill = \"lightblue\", alpha = 0.2) +\n  labs(title = \"Average MICU LOS over Time by Physical Therapy\",\n       x = \"Admit Date\",\n       y = \"Avg MICU LOS\")\n\n`summarise()` has grouped output by 'Admit_MonthYear'. You can override using\nthe `.groups` argument.\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nFitting a loess curve, it does appear that for those that receive PT, they had a decrease in average MICU LOS during the intervention compared to before. It also appears that they are increasing in average MICU following the interention, but this may still be lower compared to the pre-intervention period.\n:::"
  },
  {
    "objectID": "Project_3_Predictive_Modelling/Project_3_SAS/Code/images/Megan_Watson_Project-02.html#increase-frequency-of-pt-visits",
    "href": "Project_3_Predictive_Modelling/Project_3_SAS/Code/images/Megan_Watson_Project-02.html#increase-frequency-of-pt-visits",
    "title": "Data Checking - Statistical Consulting",
    "section": "Increase Frequency of PT Visits",
    "text": "Increase Frequency of PT Visits\n\n# Summarize and plot the data\ndata_ex |&gt; \n  group_by(Admit_MonthYear) |&gt; \n  summarise(avg_pct_therapy = mean(`PERCENT OF DAYS WITH THERAPY VISITS`, na.rm = T)) |&gt; \n  ggplot(aes(x = Admit_MonthYear, y = avg_pct_therapy)) +\n  geom_line() +\n  theme_minimal() +\n  geom_rect(data = highlight_periods, aes(xmin = start, xmax = end, ymin = -Inf, ymax = Inf), \n            inherit.aes = FALSE, fill = \"lightblue\", alpha = 0.2) +\n  labs(title = \"Average Percentage of Days with Physical Therapy Over Time\",\n       x = \"Admit Date\",\n       y = \"Average Days with Physical Therapy (%)\")\n\n\n\n\n\n\n\n\nInteresting! They successfully and drastically increased the percentage of days that patients had physical therapy (roughly from 10-20% pre-iniative to 50-60% post iniative). This percentage did seem to drop off over time during the post- initiative period however, though it is still higher than it was pre-initiative (now roughly 30-40%).\n\nFit with Loess\n\n# Summarize and plot the data\ndata_ex |&gt; \n  group_by(Admit_MonthYear) |&gt; \n  summarise(avg_pct_therapy = mean(`PERCENT OF DAYS WITH THERAPY VISITS`, na.rm = T)) |&gt; \n  ggplot(aes(x = Admit_MonthYear, y = avg_pct_therapy)) +\n  geom_point() +\n  geom_smooth(method = \"loess\", se = F) +\n  theme_minimal() +\n  geom_rect(data = highlight_periods, aes(xmin = start, xmax = end, ymin = -Inf, ymax = Inf), \n            inherit.aes = FALSE, fill = \"lightblue\", alpha = 0.2) +\n  labs(title = \"Average Percentage of Days with Physical Therapy Over Time\",\n       x = \"Admit Date\",\n       y = \"Average Days with Physical Therapy (%)\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\nFit with Splines\n\n# Summarize and plot the data\ndata_ex |&gt; \n  group_by(Admit_MonthYear) |&gt; \n  summarise(avg_pct_therapy = mean(`PERCENT OF DAYS WITH THERAPY VISITS`, na.rm = T)) |&gt; \n  ggplot(aes(x = Admit_MonthYear, y = avg_pct_therapy)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", formula = y ~ ns(x, df = 5), se = F) +\n  theme_minimal() +\n  geom_rect(data = highlight_periods, aes(xmin = start, xmax = end, ymin = -Inf, ymax = Inf), \n            inherit.aes = FALSE, fill = \"lightblue\", alpha = 0.2) +\n  labs(title = \"Average Percentage of Days with Physical Therapy Over Time\",\n       x = \"Admit Date\",\n       y = \"Average Days with Physical Therapy (%)\")"
  },
  {
    "objectID": "Project_3_Predictive_Modelling/Project_3_SAS/Code/images/Megan_Watson_Project-02.html#decrease-time-from-admit-to-therapy",
    "href": "Project_3_Predictive_Modelling/Project_3_SAS/Code/images/Megan_Watson_Project-02.html#decrease-time-from-admit-to-therapy",
    "title": "Data Checking - Statistical Consulting",
    "section": "Decrease Time from Admit to Therapy",
    "text": "Decrease Time from Admit to Therapy\n\n# Summarize and plot the data\ndata_ex |&gt; \n  group_by(Admit_MonthYear) |&gt; \n  summarise(avg_ADMIT_TO_TREAT = mean(`ADMIT TO TREAT`, na.rm = T)) |&gt; \n  ggplot(aes(x = Admit_MonthYear, y = avg_ADMIT_TO_TREAT)) +\n  geom_line() +\n  theme_minimal() +\n  geom_rect(data = highlight_periods, aes(xmin = start, xmax = end, ymin = -Inf, ymax = Inf), \n            inherit.aes = FALSE, fill = \"lightblue\", alpha = 0.2) +\n  annotate(\"text\", x = as.Date(\"2015-09-01\"), y = Inf, \n           label = \"QI-Initiative\", color = \"skyblue2\", size = 4, vjust = 1.5,) +\n  labs(title = \"Time from Admit to Therapy over Time\",\n       x = \"Admit Date\",\n       y = \"Time from Admit to Therapy\")\n\n\n\n\n\n\n\n\nThey successfully decreased the time from a patient’s first admit to their first physical therapy session.\nThis means that patients were receiving PT sooner.\n\nPlot with Loess\n\n# Summarize and plot the data\ndata_ex |&gt; \n  group_by(Admit_MonthYear) |&gt; \n  summarise(avg_ADMIT_TO_TREAT = mean(`ADMIT TO TREAT`, na.rm = T)) |&gt; \n  ggplot(aes(x = Admit_MonthYear, y = avg_ADMIT_TO_TREAT)) +\n  geom_point() +\n  geom_smooth(method = \"loess\", se = F) +\n  theme_minimal() +\n  geom_rect(data = highlight_periods, aes(xmin = start, xmax = end, ymin = -Inf, ymax = Inf), \n            inherit.aes = FALSE, fill = \"lightblue\", alpha = 0.2) +\n  annotate(\"text\", x = as.Date(\"2015-09-01\"), y = Inf, \n           label = \"QI-Initiative\", color = \"skyblue2\", size = 4, vjust = 1.5,) +\n  labs(title = \"Time from Admit to Therapy over Time\",\n       x = \"Admit Date\",\n       y = \"Time from Admit to Therapy\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nYeah looks crummier with loess, will likely delete."
  },
  {
    "objectID": "Project_3_Predictive_Modelling/Project_3_SAS/Code/images/Megan_Watson_Project-02.html#increase-percentage-of-admissions-receiving-physical-therapy",
    "href": "Project_3_Predictive_Modelling/Project_3_SAS/Code/images/Megan_Watson_Project-02.html#increase-percentage-of-admissions-receiving-physical-therapy",
    "title": "Data Checking - Statistical Consulting",
    "section": "Increase Percentage of Admissions Receiving Physical Therapy",
    "text": "Increase Percentage of Admissions Receiving Physical Therapy\n\n# Summarize and calculate percentages\nsummary_data &lt;- data_ex |&gt; \n  group_by(Admit_MonthYear, PT_YESNO) |&gt; \n  summarise(count = n()) |&gt; \n  mutate(total_count = sum(count)) |&gt; \n  ungroup() |&gt; \n  mutate(percentage = (count / total_count) * 100) |&gt; \n  filter(PT_YESNO == 1)\n\n`summarise()` has grouped output by 'Admit_MonthYear'. You can override using\nthe `.groups` argument.\n\n# Plot the data\nggplot(summary_data, aes(x = Admit_MonthYear, y = percentage)) +\n  geom_line() +\n  theme_minimal() +\n  geom_rect(data = highlight_periods, aes(xmin = start, xmax = end, ymin = -Inf, ymax = Inf), \n            inherit.aes = FALSE, fill = \"lightblue\", alpha = 0.2) +\n  annotate(\"text\", x = as.Date(\"2015-09-01\"), y = Inf, \n           label = \"QI-Initiative\", color = \"skyblue2\", size = 4, vjust = 1.5) +\n  labs(title = \"Percentage of Admissions Receiving Physical Therapy\",\n       x = \"Admit Date\",\n       y = \"Percentage\")\n\n\n\n\n\n\n\n\nThey also successfully increased the percentage of patients receiving PT from ~40% pre-initiative to ~80% during the initiative. This percentage then decreases to ~70% post-initiative.\n::::"
  },
  {
    "objectID": "Project_3_Predictive_Modelling/Project_3_SAS/Code/images/Megan_Watson_Project-02.html#mechanical-ventilation-1",
    "href": "Project_3_Predictive_Modelling/Project_3_SAS/Code/images/Megan_Watson_Project-02.html#mechanical-ventilation-1",
    "title": "Data Checking - Statistical Consulting",
    "section": "Mechanical Ventilation",
    "text": "Mechanical Ventilation\nThe researcher expressed that patients would be stratified on Mechanical ventilation v. NO mechanical ventilation. However, I do not see this variable. Let’s go ahead and create it, assuming that patients with NA for MV_TOTAL were simply never on mechanical ventilation.\n\n# Dummy code MV status \ndata_ex$MV_YN &lt;- ifelse(is.na(data_ex$MV_Total), 0, 1)\n\nLet’s check that worked as intended.\n\n# Filter down to variables of interest to verify dummy coding worked\ncheck &lt;- data_ex %&gt;%\n  select(id, MV_Total, MV_YN)\n\n# Pretty print\nkable(head(check), format = \"html\") %&gt;%\n  kable_styling(bootstrap_options = c(\"condensed\", \"stripe\", \"hover\"))\n\n\n\n\nid\nMV_Total\nMV_YN\n\n\n\n\n2\nNA\n0\n\n\n3\n13\n1\n\n\n4\nNA\n0\n\n\n5\n16\n1\n\n\n8\nNA\n0\n\n\n10\n2\n1\n\n\n\n\n\n\n\nLooks good."
  },
  {
    "objectID": "Project_3_Predictive_Modelling/Project_3_SAS/Code/images/Megan_Watson_Project-02.html#qi-initiative-time-period",
    "href": "Project_3_Predictive_Modelling/Project_3_SAS/Code/images/Megan_Watson_Project-02.html#qi-initiative-time-period",
    "title": "Data Checking - Statistical Consulting",
    "section": "QI-Initiative Time Period",
    "text": "QI-Initiative Time Period\nLet’s group participants into the time period that they were present during the initiative for.\n\n# Create new variable based on QI initiative time period.\ndata_ex &lt;- data_ex |&gt; \n  mutate(initiative = ifelse(Admit_MonthYear &lt; as.Date(\"2015-04-01\"), \"Pre\",\n                             ifelse(Admit_MonthYear &gt; as.Date(\"2015-12-01\"), \"Post\", \"During\")))\n\nLet’s double check we did that right.\n\n# Check counts\ntable(data_ex$initiative)\n\n\nDuring   Post    Pre \n   656   2860   2889 \n\n# Double check assigning\ndata_ex |&gt; \n  select(id, Admit_MonthYear, initiative) |&gt; \n  filter(Admit_MonthYear &lt; as.Date(\"2015-04-01\")) |&gt; \n  arrange(desc(Admit_MonthYear)) |&gt; \n  head() |&gt; \n  pretty_print()\n\n\n\n\nid\nAdmit_MonthYear\ninitiative\n\n\n\n\n5759\n2015-03-01\nPre\n\n\n5760\n2015-03-01\nPre\n\n\n5761\n2015-03-01\nPre\n\n\n5763\n2015-03-01\nPre\n\n\n5765\n2015-03-01\nPre\n\n\n5767\n2015-03-01\nPre\n\n\n\n\n\n\n# Double check assigning\ndata_ex |&gt; \n  select(id, Admit_MonthYear, initiative) |&gt; \n  filter(Admit_MonthYear &gt; as.Date(\"2015-04-01\")) |&gt; \n  arrange(Admit_MonthYear) |&gt; \n  head() |&gt; \n  pretty_print()\n\n\n\n\nid\nAdmit_MonthYear\ninitiative\n\n\n\n\n6094\n2015-05-01\nDuring\n\n\n6096\n2015-05-01\nDuring\n\n\n6099\n2015-05-01\nDuring\n\n\n6100\n2015-05-01\nDuring\n\n\n6105\n2015-05-01\nDuring\n\n\n6106\n2015-05-01\nDuring\n\n\n\n\n\n\n# Double check assigning\ndata_ex |&gt; \n  select(id, Admit_MonthYear, initiative) |&gt; \n  filter(Admit_MonthYear &gt;= as.Date(\"2015-12-01\")) |&gt; \n  arrange(Admit_MonthYear) |&gt; \n  head() |&gt; \n  pretty_print()\n\n\n\n\nid\nAdmit_MonthYear\ninitiative\n\n\n\n\n7167\n2015-12-01\nDuring\n\n\n7169\n2015-12-01\nDuring\n\n\n7171\n2015-12-01\nDuring\n\n\n7172\n2015-12-01\nDuring\n\n\n7174\n2015-12-01\nDuring\n\n\n7176\n2015-12-01\nDuring\n\n\n\n\n\n\n# Double check assigning\ndata_ex |&gt; \n  select(id, Admit_MonthYear, initiative) |&gt; \n  filter(Admit_MonthYear &gt; as.Date(\"2015-12-01\")) |&gt; \n  arrange(Admit_MonthYear) |&gt; \n  head() |&gt; \n  pretty_print()\n\n\n\n\nid\nAdmit_MonthYear\ninitiative\n\n\n\n\n7324\n2016-01-01\nPost\n\n\n7331\n2016-01-01\nPost\n\n\n7336\n2016-01-01\nPost\n\n\n7339\n2016-01-01\nPost\n\n\n7343\n2016-01-01\nPost\n\n\n7345\n2016-01-01\nPost\n\n\n\n\n\n\n\nLooks good.\n\nSome Graphing\n\ndata_ex &lt;- data_ex |&gt; \n  mutate(initiative = as.factor(initiative)) |&gt; \n  mutate(initiative = relevel(initiative, ref = \"Pre\"))\n\nggplot(data_ex, aes(x = PT_YESNO, y = Unit_LOS_log, fill = initiative)) +\n  geom_boxplot() +\n  theme_minimal() +\n  scale_fill_brewer(palette = \"Pastel2\")\n\n\n\n\n\n\n\n\nThis shows that MICU LOS decreased in the during and post QI initiative periods compared to the pre-iniatitive period.\n\ndata_ex |&gt; \n  filter(PT_YESNO == 1) |&gt; \n  ggplot(aes(x = initiative, y = Unit_LOS_log, fill = initiative)) +\n  geom_boxplot() +\n  theme_minimal() +\n  scale_fill_brewer(palette = \"Pastel2\")\n\n\n\n\n\n\n\n\n\ndata_ex |&gt; \n  filter(PT_YESNO == 1) |&gt; \n  ggplot(aes(x = initiative, y = Unit_LOS_log, fill = initiative)) +\n  geom_boxplot() +\n  theme_minimal() +\n  scale_fill_brewer(palette = \"Pastel2\")\n\n\n\n\n\n\n\n\nThere is an interaction here between PT and unit LOS and initiative.\nIn other words, you HAVE to filter based on PT\n\n\nMV Total\n\nggplot(data_ex, aes(x = PT_YESNO, y = MV_Total_log, fill = initiative)) +\n  geom_boxplot() +\n  theme_minimal() +\n  scale_fill_brewer(palette = \"Pastel2\")\n\nWarning: Removed 3149 rows containing non-finite outside the scale range\n(`stat_boxplot()`)."
  },
  {
    "objectID": "Project_3_Predictive_Modelling/Project_3_SAS/Code/images/Megan_Watson_Project-02.html#check-for-repeat-admissions",
    "href": "Project_3_Predictive_Modelling/Project_3_SAS/Code/images/Megan_Watson_Project-02.html#check-for-repeat-admissions",
    "title": "Data Checking - Statistical Consulting",
    "section": "Check for Repeat Admissions",
    "text": "Check for Repeat Admissions\n\n# Check for repeate admissions\nlength(unique(data_ex$id))\n\n[1] 6405\n\ndim(data_ex)\n\n[1] 6405   40\n\n\nThe number of unique patient id’s is the same number as the number of rows in our data set.\nSo we do not have repeate admissions in this data set and do not need to account for repeated measures! (We meet the assumption of independent observations)\n:::"
  },
  {
    "objectID": "Project_3_Predictive_Modelling/Project_3_SAS/Code/Project_3_Predictive_Modelling_SAS.html",
    "href": "Project_3_Predictive_Modelling/Project_3_SAS/Code/Project_3_Predictive_Modelling_SAS.html",
    "title": "Predictive Modelling (SAS)",
    "section": "",
    "text": "The aim of the current study is to investigate whether MRI image features extracted from baseline kidney images can enhance the prediction of disease progression in young Autosomal Dominant Polycystic Kidney Disease (ADPKD) patients. This study is important because recent literature has demonstrated the inclusion of MRI image features improves the prognostic accuracy of models in adults, but this has not yet been demonstrated in children. Improving prediction models could greatly assist in diagnosing and predicting kidney disease outcomes in children, which is more challenging than in adults and could lead to improved treatment.\nNote: There is a lot of extraneous work I did when I originally performed this in R when exploring the data set, and we will not be retreading that in this SAS analysis.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe two clinical hypotheses for this study are that including MRI image features will improve model performance compared to using baseline kidney volume alone in models predicting\n\nPercentage change of total kidney volume growth\nClassification of a patient as having fast or slow progression of the disease."
  },
  {
    "objectID": "Project_3_Predictive_Modelling/Project_3_SAS/Code/Project_3_Predictive_Modelling_SAS.html#gabor-transform",
    "href": "Project_3_Predictive_Modelling/Project_3_SAS/Code/Project_3_Predictive_Modelling_SAS.html#gabor-transform",
    "title": "Predictive Modelling (SAS)",
    "section": "Gabor Transform",
    "text": "Gabor Transform\n\nWhat is it\nThe Gabor Transform, named after Dennis Gabor, is a powerful technique for analyzing the texture of MRI images. By convolving the image with a Gabor filter, it becomes possible to discriminate between features based on intensity differences. This allows the target region and background to be differentiated if they possess distinct features, as they will exhibit different intensity levels. Moreover, the Gabor Transform has tunable parameters, such as the frequency of the sinusoidal wave, which can be adjusted to extract specific textures from the images. Higher frequencies are ideal for capturing fine textures, while lower frequencies are better suited for coarse textures.\n\n\nHow it Works\nThe Gabor Filter first applies a Gaussian Envelope to focus on a small region of the image. It then applies a sinusoidal wave that oscillates at a specific frequency and captures the variation in intensities at that frequency and orientation within that region.\n\n\nExample of Gabor Transform\n\n\n\nExample of applying a Gaussian, then Gabor, and Laplacian Filter (GGL) for differentiation between two different textures (+’s vs L’s) (2).\n\n\n\n\nImage Features Provided by the Gabor Transform\nIn general, Gabor functions can easily extract features of:\n\nSpatial Frequency (e.g. how often pixel intensity changes in a given area)\nDensity (e.g. concentration of features within a certain area)\nOrientation (e.g. Textures or edges at 0°, 45°, 90°, 135°)\nPhase (e.g. alignment/distance of features)\nEnergy (e.g. overall intensity)\n\nSources: 1, 2"
  },
  {
    "objectID": "Project_3_Predictive_Modelling/Project_3_SAS/Code/Project_3_Predictive_Modelling_SAS.html#gray-level-co-occurrence-matrix",
    "href": "Project_3_Predictive_Modelling/Project_3_SAS/Code/Project_3_Predictive_Modelling_SAS.html#gray-level-co-occurrence-matrix",
    "title": "Predictive Modelling (SAS)",
    "section": "Gray Level Co-Occurrence Matrix",
    "text": "Gray Level Co-Occurrence Matrix\n\nWhat is it\nThe Gray Level Co-Occurrence Matrix (GLCM) is another powerful tool for extracting texture features from an MRI image. GLCM works under the assumption that the textural information in an image is contained in the “average” spatial relationship that the gray tones in the image have to each other. For example, when a small patch of the picture has little variation of features, the dominant property is tone; When a small patch of a picture has high variation, the dominant property is texture (3).\n\n\nHow it Works\nGLCM examines the spatial relationship between pairs of pixels in an image and calculates how often pairs of pixel values occur at a specified distance and orientation. It does this by computing a set of gray-tone spacial-dependence matrices for various angular relationships and distances between neighboring resolution cell pairs on the image (3).\n\n\nExample of GLCM\n\n\n\nExample of textural features extracted from two different land-use category images (3).\n\n\n\n\nImage Features Provided by GLCM\nIn general, GLCM provides information on the following features:\n\nHomogeneity\nLinear Structure\nContrast\nNumber and Nature of Boundaries\nComplexity of the Image"
  },
  {
    "objectID": "Project_3_Predictive_Modelling/Project_3_SAS/Code/Project_3_Predictive_Modelling_SAS.html#local-binary-pattern",
    "href": "Project_3_Predictive_Modelling/Project_3_SAS/Code/Project_3_Predictive_Modelling_SAS.html#local-binary-pattern",
    "title": "Predictive Modelling (SAS)",
    "section": "Local Binary Pattern",
    "text": "Local Binary Pattern\n\nWhat is it\nThe Local Binary Pattern (LBP) is a third powerful method for extracting texture features from an image. An important and fundamental property of texture is how uniform the patterns are, and LBP captures this by detecting these uniform patterns in circular neighborhoods at any rotation and spatial resolution. LBP is rotation invariant, meaning it does not matter what rotation the image is at; it will always extract very similar features) (4).\n\n\nHow it Works\nLBP works by comparing the intensity of a central pixel with its neighboring pixels and encoding this relationship into a binary pattern. For each neighbor, if it’s intensity is greater or equal to the central pixel, it gets assigned a 1 (otherwise a 0). The binary values of all neighbors are then concatenated to form a binary number, and this number is converted into a decimal that represent the LBP for the central pixel (4).\n\n\nExample of LBP\n\n\n\nExample of the 36 unique comparisons that can be made between neighboring pixels (4).\n\n\n\n\nCode Information\n\n\n\n\n\n\n\nImage Features Provided by the LBP\nIn general, the LBP provides image features on:\n\nUniformity\nLocal Contrast\nTexture Description\nSpatial Patterns\nGray Level Distribution"
  },
  {
    "objectID": "Project_3_Predictive_Modelling/Project_3_SAS/Code/Project_3_Predictive_Modelling_SAS.html#study-design",
    "href": "Project_3_Predictive_Modelling/Project_3_SAS/Code/Project_3_Predictive_Modelling_SAS.html#study-design",
    "title": "Predictive Modelling (SAS)",
    "section": "Study Design",
    "text": "Study Design\nThe investigators recruited 71 young patients with ADPKD and collected MRI data at baseline and after 3 years. Additionally, the height corrected kidney volume for each patient was collected at baseline and 3 years by a physician, and the percentage change calculated. Patients were classified as having slow or fast progression of the disease based on this percentage change. Image features were extracted from the baseline MRI images including 2 image features on kidney geometric information, 5 features based on Gabor transform, 2 features based on gray level co-occurrence matrix, 5 features based on image textures, and 5 features based on local binary pattern.\n\nStatistical Hypotheses\n\nA linear regression model predicting percentage change of total kidney volume growth including MRI image features and baseline kidney volume will have better performance than a model with baseline kidney volume alone, as determined by specificity, sensitivity, PPV,  NPV,  accuracy, and AUC.\nA logistic regression model predicting classification of disease progression as slow or fast including MRI image features and baseline kidney volume will have better performance than a model with baseline kidney volume alone, as determined by specificity, sensitivity, PPV,  NPV,  accuracy, and AUC."
  },
  {
    "objectID": "Project_3_Predictive_Modelling/Project_3_SAS/Code/Project_3_Predictive_Modelling_SAS.html#connect-to-sas",
    "href": "Project_3_Predictive_Modelling/Project_3_SAS/Code/Project_3_Predictive_Modelling_SAS.html#connect-to-sas",
    "title": "Predictive Modelling (SAS)",
    "section": "Connect to SAS",
    "text": "Connect to SAS\n\n# This code connects to SAS On Demand\nlibrary(configSAS)\nconfigSAS::set_sas_engine()\n\nUsing SAS Config named: oda\nSAS Connection established. Subprocess id is 12208\n\n#  This code allows you to run ```{sas}``` chunks\nsas = knitr::opts_chunk$get(\"sas\")"
  },
  {
    "objectID": "Project_3_Predictive_Modelling/Project_3_SAS/Code/Project_3_Predictive_Modelling_SAS.html#create-library",
    "href": "Project_3_Predictive_Modelling/Project_3_SAS/Code/Project_3_Predictive_Modelling_SAS.html#create-library",
    "title": "Predictive Modelling (SAS)",
    "section": "Create Library",
    "text": "Create Library\n\n* Create library;\n%LET CourseRoot = /home/u63376223/sasuser.v94/Advanced Data Analysis;\nLIBNAME Proj3 \"&CourseRoot/Project 3 Predictive Model\";"
  },
  {
    "objectID": "Project_3_Predictive_Modelling/Project_3_SAS/Code/Project_3_Predictive_Modelling_SAS.html#read-in-data",
    "href": "Project_3_Predictive_Modelling/Project_3_SAS/Code/Project_3_Predictive_Modelling_SAS.html#read-in-data",
    "title": "Predictive Modelling (SAS)",
    "section": "Read In Data",
    "text": "Read In Data\n\n* Import the dataset;\nPROC IMPORT\n    DATAFILE = \"&CourseRoot/Project 3 Predictive Model/Project3_data.csv\"\n    OUT = Proj3.data\n    REPLACE;\n    GETNAMES = YES;\n    RUN;"
  },
  {
    "objectID": "Project_3_Predictive_Modelling/Project_3_SAS/Code/Project_3_Predictive_Modelling_SAS.html#examine-data",
    "href": "Project_3_Predictive_Modelling/Project_3_SAS/Code/Project_3_Predictive_Modelling_SAS.html#examine-data",
    "title": "Predictive Modelling (SAS)",
    "section": "Examine Data",
    "text": "Examine Data\n\n* Examine data;\nPROC PRINT DATA = PROJ3.Data (OBS=6);\n    RUN;\n\nlstlog\n\n\n&lt;!DOCTYPE html&gt;\n\n\n\n\n\nSAS Output\n\n\n\n\n\n\n\nThe SAS System\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObs\n\n\nSubject ID\n\n\ngeom1\n\n\ngeom2\n\n\ngabor1\n\n\ngabor2\n\n\ngabor3\n\n\ngabor4\n\n\ngabor5\n\n\nglcm1\n\n\nglcm2\n\n\ntxti1\n\n\ntxti2\n\n\ntxti3\n\n\ntxti4\n\n\ntxti5\n\n\nlbp1\n\n\nlbp2\n\n\nlbp3\n\n\nlbp4\n\n\nlbp5\n\n\ntkvht_base\n\n\ntkvht_visit2\n\n\nprogression\n\n\ntkvht_change\n\n\n\n\n\n\n1\n\n\n1\n\n\n1.071922859\n\n\n1.149018615\n\n\n-0.029205293\n\n\n-0.079807149\n\n\n0.002330791\n\n\n0.000852949\n\n\n0.006369181\n\n\n405.5714233\n\n\n164488.1794\n\n\n-4.401442194\n\n\n-12.08514995\n\n\n53.19208893\n\n\n19.37269339\n\n\n146.0508494\n\n\n-0.521181744\n\n\n0.081225947\n\n\n-0.042333481\n\n\n0.27163041\n\n\n0.006597654\n\n\n435\n\n\n585\n\n\n1\n\n\n11.44\n\n\n\n\n2\n\n\n2\n\n\n54.03147612\n\n\n2919.400411\n\n\n-0.049985252\n\n\n-0.126631032\n\n\n0.006329684\n\n\n0.002498525\n\n\n0.016035418\n\n\n-580.9386534\n\n\n337489.7191\n\n\n3.595325019\n\n\n-1.991768234\n\n\n-7.161054166\n\n\n12.926362\n\n\n3.967140699\n\n\n-0.55724044\n\n\n-0.466561063\n\n\n0.259986692\n\n\n0.310516908\n\n\n0.217679225\n\n\n555\n\n\n595\n\n\n0\n\n\n2.42\n\n\n\n\n3\n\n\n3\n\n\n-32.02438892\n\n\n1025.561486\n\n\n-0.015271484\n\n\n0.0424398\n\n\n-0.000648119\n\n\n0.000233218\n\n\n0.001801137\n\n\n-1037.25839\n\n\n1075904.968\n\n\n25.36733564\n\n\n-5.308749186\n\n\n-134.6688224\n\n\n643.5017174\n\n\n28.18281792\n\n\n-0.251547115\n\n\n0.677893451\n\n\n-0.170522142\n\n\n0.063275951\n\n\n0.459539531\n\n\n191\n\n\n249\n\n\n1\n\n\n10.08\n\n\n\n\n4\n\n\n4\n\n\n-42.8324855\n\n\n1834.621814\n\n\n-0.015394838\n\n\n0.043545875\n\n\n-0.000670382\n\n\n0.000237001\n\n\n0.001896243\n\n\n-1018.078916\n\n\n1036484.679\n\n\n25.51853114\n\n\n-5.561043005\n\n\n-141.9096491\n\n\n651.1954318\n\n\n30.9251993\n\n\n-0.246369599\n\n\n0.66845853\n\n\n-0.16468786\n\n\n0.060697979\n\n\n0.446836807\n\n\n334\n\n\n498\n\n\n1\n\n\n16.47\n\n\n\n\n5\n\n\n5\n\n\n-10.63734442\n\n\n113.1530962\n\n\n0.163043661\n\n\n0.079753327\n\n\n0.013003274\n\n\n0.026583235\n\n\n0.006360593\n\n\n64.64082706\n\n\n4178.436523\n\n\n2.446874615\n\n\n-6.297954169\n\n\n-15.41030418\n\n\n5.98719538\n\n\n39.66422672\n\n\n-0.566313835\n\n\n0.267699416\n\n\n-0.151601883\n\n\n0.32071136\n\n\n0.071662977\n\n\n263\n\n\n311\n\n\n0\n\n\n6.14\n\n\n\n\n6\n\n\n6\n\n\n71.07947699\n\n\n5052.292049\n\n\n-0.082449171\n\n\n-0.110154232\n\n\n0.009082125\n\n\n0.006797866\n\n\n0.012133955\n\n\n-1459.904295\n\n\n2131320.55\n\n\n2.432427468\n\n\n-6.506258344\n\n\n-15.82600151\n\n\n5.916703385\n\n\n42.33139764\n\n\n-1.199913635\n\n\n-0.110897204\n\n\n0.133067067\n\n\n1.439792732\n\n\n0.01229819\n\n\n609\n\n\n793\n\n\n1\n\n\n10.09\n\n\n\n\n\n\n\n\n\n\n\u00149                                                          The SAS System                    Saturday, December 28, 2024 08:59:00 PM\n\n139        ods listing close;ods html5 (id=saspy_internal) file=_tomods1 options(bitmap_mode='inline') device=svg style=HTMLBlue;\n139      ! ods graphics on / outputfmt=png;\n140        \n141        * Examine data;\n142        PROC PRINT DATA = PROJ3.Data (OBS=6);\n143         RUN;\n144        \n145        \n146        ods html5 (id=saspy_internal) close;ods listing;\n147        \n\u001410                                                         The SAS System                    Saturday, December 28, 2024 08:59:00 PM\n\n148"
  },
  {
    "objectID": "Project_3_Predictive_Modelling/Project_3_SAS/Code/Project_3_Predictive_Modelling_SAS.html#examine-formats",
    "href": "Project_3_Predictive_Modelling/Project_3_SAS/Code/Project_3_Predictive_Modelling_SAS.html#examine-formats",
    "title": "Predictive Modelling (SAS)",
    "section": "Examine Formats",
    "text": "Examine Formats\n\n* Examine formats;  \nPROC CONTENTS DATA = PROJ3.DATA;\n    RUN;\n\nlstlog\n\n\n&lt;!DOCTYPE html&gt;\n\n\n\n\n\nSAS Output\n\n\n\n\n\n\n\nThe SAS System\n\n\n\n\nThe CONTENTS Procedure\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Set Name\n\n\nPROJ3.DATA\n\n\nObservations\n\n\n71\n\n\n\n\nMember Type\n\n\nDATA\n\n\nVariables\n\n\n24\n\n\n\n\nEngine\n\n\nV9\n\n\nIndexes\n\n\n0\n\n\n\n\nCreated\n\n\n12/28/2024 20:59:38\n\n\nObservation Length\n\n\n192\n\n\n\n\nLast Modified\n\n\n12/28/2024 20:59:38\n\n\nDeleted Observations\n\n\n0\n\n\n\n\nProtection\n\n\n \n\n\nCompressed\n\n\nNO\n\n\n\n\nData Set Type\n\n\n \n\n\nSorted\n\n\nNO\n\n\n\n\nLabel\n\n\n \n\n\n \n\n\n \n\n\n\n\nData Representation\n\n\nSOLARIS_X86_64, LINUX_X86_64, ALPHA_TRU64, LINUX_IA64\n\n\n \n\n\n \n\n\n\n\nEncoding\n\n\nutf-8 Unicode (UTF-8)\n\n\n \n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEngine/Host Dependent Information\n\n\n\n\n\n\nData Set Page Size\n\n\n131072\n\n\n\n\nNumber of Data Set Pages\n\n\n1\n\n\n\n\nFirst Data Page\n\n\n1\n\n\n\n\nMax Obs per Page\n\n\n682\n\n\n\n\nObs in First Data Page\n\n\n71\n\n\n\n\nNumber of Data Set Repairs\n\n\n0\n\n\n\n\nFilename\n\n\n/home/u63376223/sasuser.v94/Advanced Data Analysis/Project 3 Predictive Model/data.sas7bdat\n\n\n\n\nRelease Created\n\n\n9.0401M7\n\n\n\n\nHost Created\n\n\nLinux\n\n\n\n\nInode Number\n\n\n19396045502\n\n\n\n\nAccess Permission\n\n\nrw-r–r–\n\n\n\n\nOwner Name\n\n\nu63376223\n\n\n\n\nFile Size\n\n\n256KB\n\n\n\n\nFile Size (bytes)\n\n\n262144\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAlphabetic List of Variables and Attributes\n\n\n\n\n#\n\n\nVariable\n\n\nType\n\n\nLen\n\n\nFormat\n\n\nInformat\n\n\n\n\n\n\n1\n\n\nSubject ID\n\n\nNum\n\n\n8\n\n\nBEST12.\n\n\nBEST32.\n\n\n\n\n4\n\n\ngabor1\n\n\nNum\n\n\n8\n\n\nBEST12.\n\n\nBEST32.\n\n\n\n\n5\n\n\ngabor2\n\n\nNum\n\n\n8\n\n\nBEST12.\n\n\nBEST32.\n\n\n\n\n6\n\n\ngabor3\n\n\nNum\n\n\n8\n\n\nBEST12.\n\n\nBEST32.\n\n\n\n\n7\n\n\ngabor4\n\n\nNum\n\n\n8\n\n\nBEST12.\n\n\nBEST32.\n\n\n\n\n8\n\n\ngabor5\n\n\nNum\n\n\n8\n\n\nBEST12.\n\n\nBEST32.\n\n\n\n\n2\n\n\ngeom1\n\n\nNum\n\n\n8\n\n\nBEST12.\n\n\nBEST32.\n\n\n\n\n3\n\n\ngeom2\n\n\nNum\n\n\n8\n\n\nBEST12.\n\n\nBEST32.\n\n\n\n\n9\n\n\nglcm1\n\n\nNum\n\n\n8\n\n\nBEST12.\n\n\nBEST32.\n\n\n\n\n10\n\n\nglcm2\n\n\nNum\n\n\n8\n\n\nBEST12.\n\n\nBEST32.\n\n\n\n\n16\n\n\nlbp1\n\n\nNum\n\n\n8\n\n\nBEST12.\n\n\nBEST32.\n\n\n\n\n17\n\n\nlbp2\n\n\nNum\n\n\n8\n\n\nBEST12.\n\n\nBEST32.\n\n\n\n\n18\n\n\nlbp3\n\n\nNum\n\n\n8\n\n\nBEST12.\n\n\nBEST32.\n\n\n\n\n19\n\n\nlbp4\n\n\nNum\n\n\n8\n\n\nBEST12.\n\n\nBEST32.\n\n\n\n\n20\n\n\nlbp5\n\n\nNum\n\n\n8\n\n\nBEST12.\n\n\nBEST32.\n\n\n\n\n23\n\n\nprogression\n\n\nNum\n\n\n8\n\n\nBEST12.\n\n\nBEST32.\n\n\n\n\n21\n\n\ntkvht_base\n\n\nNum\n\n\n8\n\n\nBEST12.\n\n\nBEST32.\n\n\n\n\n24\n\n\ntkvht_change\n\n\nNum\n\n\n8\n\n\nBEST12.\n\n\nBEST32.\n\n\n\n\n22\n\n\ntkvht_visit2\n\n\nNum\n\n\n8\n\n\nBEST12.\n\n\nBEST32.\n\n\n\n\n11\n\n\ntxti1\n\n\nNum\n\n\n8\n\n\nBEST12.\n\n\nBEST32.\n\n\n\n\n12\n\n\ntxti2\n\n\nNum\n\n\n8\n\n\nBEST12.\n\n\nBEST32.\n\n\n\n\n13\n\n\ntxti3\n\n\nNum\n\n\n8\n\n\nBEST12.\n\n\nBEST32.\n\n\n\n\n14\n\n\ntxti4\n\n\nNum\n\n\n8\n\n\nBEST12.\n\n\nBEST32.\n\n\n\n\n15\n\n\ntxti5\n\n\nNum\n\n\n8\n\n\nBEST12.\n\n\nBEST32.\n\n\n\n\n\n\n\n\n\n\n\n\u001411                                                         The SAS System                    Saturday, December 28, 2024 08:59:00 PM\n\n151        ods listing close;ods html5 (id=saspy_internal) file=_tomods1 options(bitmap_mode='inline') device=svg style=HTMLBlue;\n151      ! ods graphics on / outputfmt=png;\n152        \n153        * Examine formats;   \n154        PROC CONTENTS DATA = PROJ3.DATA;\n155         RUN;\n156        \n157        \n158        ods html5 (id=saspy_internal) close;ods listing;\n159        \n\u001412                                                         The SAS System                    Saturday, December 28, 2024 08:59:00 PM\n\n160        \n\n\n\n\nThe data is completely clean and everything is numeric as it should be! We also have no missing variables in this data set."
  },
  {
    "objectID": "Project_3_Predictive_Modelling/Project_3_SAS/Code/Project_3_Predictive_Modelling_SAS.html#create-factors",
    "href": "Project_3_Predictive_Modelling/Project_3_SAS/Code/Project_3_Predictive_Modelling_SAS.html#create-factors",
    "title": "Predictive Modelling (SAS)",
    "section": "Create Factors",
    "text": "Create Factors\nLet’s just convert progression into a factor that contains the levels for “slow” or “fast’ and we’re good to go.\n\n* Create format;\nPROC FORMAT;\n    VALUE ProgressionCd\n        0 = \"Slow\"\n        1 = \"Fast\";\n    RUN;\n    \n* Apply Format;\nDATA Proj3.Data;\n    SET Proj3.Data;\n    FORMAT progression ProgressionCd.;\n    RUN;\n\n\nSummary\nOur data set consists of:\n\n71 patients\n19 MRI image features\n4 kidney volume variables"
  },
  {
    "objectID": "Project_3_Predictive_Modelling/Project_3_SAS/Code/Project_3_Predictive_Modelling_SAS.html#correlation-matrix-i",
    "href": "Project_3_Predictive_Modelling/Project_3_SAS/Code/Project_3_Predictive_Modelling_SAS.html#correlation-matrix-i",
    "title": "Predictive Modelling (SAS)",
    "section": "Correlation Matrix I",
    "text": "Correlation Matrix I\nTo assess for the best engineering of features and select the most promising covariates for this predictive model, I will create the following correlation matrices.\n\nWith unaltered features & engineered features\nWith averages and interactions to aggregate features from the same class\nWith squared features\n\nThis process should uncover hidden relationships between features that are not immediately obvious (such as a squared feature being a significant predictor, but not in its original form).\nThis first matrix will contain the unaltered features in their original form, as well as the engineered features as determined by examination of distributions.\n\nCreate Correlation Matrix\n\n* Print correlation matrix;\nPROC CORR DATA = Proj3.Data OUTP = corr_matr;\n    VAR tkvht_base tkvht_change progression  geom1 geom2 gabor1 gabor2  gabor3  gabor4  gabor5  glcm1   glcm2   txti1   txti2   txti3   txti4   txti5   lbp1    lbp2    lbp3    lbp4    lbp5; * List variables here;\n    RUN;\n\n* Create a dataset with only the correlation coefficients;\nDATA corr_only;\n    SET corr_matr;\n    IF _TYPE_ = 'CORR'; /* Select only the rows with correlation coefficients */\nRUN;\n\n* Reshape the correlation matrix for the heat map;\nDATA heatmap_data;\n    SET corr_only;\n    ARRAY vars[*] tkvht_base tkvht_change  progression  geom1 geom2 gabor1  gabor2  gabor3  gabor4  gabor5  glcm1   glcm2   txti1   txti2   txti3   txti4   txti5   lbp1    lbp2    lbp3    lbp4    lbp5; * List variables here;\n    DO i = 1 to DIM(vars);\n        ROW = _NAME_;\n        COL = VNAME(vars[i]);\n        corr_value = vars[i];\n        IF NOT missing(corr_value) THEN DO;\n            LABEL = PUT(corr_value, 8.2); * Format the text label;\n            OUTPUT;\n        END;\n    END;\n    KEEP row col corr_value label;\nRUN;\n\n\n\nPlot the Correlation Matrix as a Heat Map\n\n* Set the dimensions of the output graphic;\nODS GRAPHICS / WIDTH=1000px HEIGHT=800px;\n\n* Create the Heat Map;\nPROC SGPLOT DATA=heatmap_data;\n    HEATMAPPARM X=col Y=row COLORRESPONSE = corr_value / COLORMODEL = (blue white red);\n    TEXT X=col Y=row TEXT=label / TEXTATTRS = (SIZE=8) POSITION = center;\n    XAXIS DISPLAY = (nolabel) DISCRETEORDER = data;\n    YAXIS DISPLAY=(nolabel) DISCRETEORDER = data;\n    TITLE \"Correlation Matrix Heat Map\";\nRUN;\n\nlstlog\n\n\n&lt;!DOCTYPE html&gt;\n\n\n\n\n\nSAS Output\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001417                                                         The SAS System                    Saturday, December 28, 2024 08:59:00 PM\n\n219        ods listing close;ods html5 (id=saspy_internal) file=_tomods1 options(bitmap_mode='inline') device=svg style=HTMLBlue;\n219      ! ods graphics on / outputfmt=png;\n220        \n221        * Set the dimensions of the output graphic;\n222        ODS GRAPHICS / WIDTH=1000px HEIGHT=800px;\n223        \n224        * Create the Heat Map;\n225        PROC SGPLOT DATA=heatmap_data;\n226            HEATMAPPARM X=col Y=row COLORRESPONSE = corr_value / COLORMODEL = (blue white red);\n227            TEXT X=col Y=row TEXT=label / TEXTATTRS = (SIZE=8) POSITION = center;\n228            XAXIS DISPLAY = (nolabel) DISCRETEORDER = data;\n229            YAXIS DISPLAY=(nolabel) DISCRETEORDER = data;\n230            TITLE \"Correlation Matrix Heat Map\";\n231        RUN;\n232        \n233        \n234        ods html5 (id=saspy_internal) close;ods listing;\n235        \n\u001418                                                         The SAS System                    Saturday, December 28, 2024 08:59:00 PM\n\n236"
  },
  {
    "objectID": "Project_3_Predictive_Modelling/Project_3_SAS/Code/Project_3_Predictive_Modelling_SAS.html#summary-1",
    "href": "Project_3_Predictive_Modelling/Project_3_SAS/Code/Project_3_Predictive_Modelling_SAS.html#summary-1",
    "title": "Predictive Modelling (SAS)",
    "section": "Summary",
    "text": "Summary\nThe variables with the strongest correlations to tkvht_change are:\n\ngabor1 (r = 0.20)\ngabor4 (r = 0.22)\ntxti2 (r = 0.28)\ntxti3 (r= 0.19)\nlbp1 (r = 0.21)\nlbp3 (r = 0.25)\nlbp4 (r = 0.24)"
  },
  {
    "objectID": "Project_3_Predictive_Modelling/Project_3_SAS/Code/Project_3_Predictive_Modelling_SAS.html#correlation-matrix-ii",
    "href": "Project_3_Predictive_Modelling/Project_3_SAS/Code/Project_3_Predictive_Modelling_SAS.html#correlation-matrix-ii",
    "title": "Predictive Modelling (SAS)",
    "section": "Correlation Matrix II",
    "text": "Correlation Matrix II\n\nCreate Correlation Matrix\n\n* Print correlation matrix;\nPROC CORR DATA = Proj3.Data OUTP = corr_matr;\n    VAR tkvht_base tkvht_change progression gabor_avg gabor_int txti_avg txti_int lbp_avg lbp_int;\n    RUN;\n    \n* Create a dataset with only the correlation coefficients;\nDATA corr_only;\n    SET corr_matr;\n    IF _TYPE_ = 'CORR'; /* Select only the rows with correlation coefficients */\nRUN;\n\n* Reshape the correlation matrix for the heat map;\nDATA heatmap_data;\n    SET corr_only;\n    ARRAY vars[*] tkvht_base tkvht_change progression gabor_avg gabor_int txti_avg txti_int lbp_avg lbp_int;\n    DO i = 1 to DIM(vars);\n        ROW = _NAME_;\n        COL = VNAME(vars[i]);\n        corr_value = vars[i];\n        IF NOT missing(corr_value) THEN DO;\n            LABEL = PUT(corr_value, 8.2); * Format the text label;\n            OUTPUT;\n        END;\n    END;\n    KEEP row col corr_value label;\nRUN;\n\n\n\nPlot the Correlation Matrix as a Heatmap\n\n* Set the dimensions of the output graphic;\nODS GRAPHICS / WIDTH=1000px HEIGHT=800px;\n\n* Create the Heat Map;\nPROC SGPLOT DATA=heatmap_data;\n    HEATMAPPARM X=col Y=row COLORRESPONSE = corr_value / COLORMODEL = (blue white red);\n    TEXT X=col Y=row TEXT=label / TEXTATTRS = (SIZE=8) POSITION = center;\n    XAXIS DISPLAY = (nolabel) DISCRETEORDER = data;\n    YAXIS DISPLAY=(nolabel) DISCRETEORDER = data;\n    TITLE \"Correlation Matrix Heat Map\";\nRUN;\n\nlstlog\n\n\n&lt;!DOCTYPE html&gt;\n\n\n\n\n\nSAS Output\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001423                                                         The SAS System                    Saturday, December 28, 2024 08:59:00 PM\n\n293        ods listing close;ods html5 (id=saspy_internal) file=_tomods1 options(bitmap_mode='inline') device=svg style=HTMLBlue;\n293      ! ods graphics on / outputfmt=png;\n294        \n295        * Set the dimensions of the output graphic;\n296        ODS GRAPHICS / WIDTH=1000px HEIGHT=800px;\n297        \n298        * Create the Heat Map;\n299        PROC SGPLOT DATA=heatmap_data;\n300            HEATMAPPARM X=col Y=row COLORRESPONSE = corr_value / COLORMODEL = (blue white red);\n301            TEXT X=col Y=row TEXT=label / TEXTATTRS = (SIZE=8) POSITION = center;\n302            XAXIS DISPLAY = (nolabel) DISCRETEORDER = data;\n303            YAXIS DISPLAY=(nolabel) DISCRETEORDER = data;\n304            TITLE \"Correlation Matrix Heat Map\";\n305        RUN;\n306        \n307        \n308        ods html5 (id=saspy_internal) close;ods listing;\n309        \n\u001424                                                         The SAS System                    Saturday, December 28, 2024 08:59:00 PM\n\n310"
  },
  {
    "objectID": "Project_3_Predictive_Modelling/Project_3_SAS/Code/Project_3_Predictive_Modelling_SAS.html#summary-2",
    "href": "Project_3_Predictive_Modelling/Project_3_SAS/Code/Project_3_Predictive_Modelling_SAS.html#summary-2",
    "title": "Predictive Modelling (SAS)",
    "section": "Summary",
    "text": "Summary\nThe strongest correlations for this round are:\n\ngabor_int (r = 0.26)\nlbp_int (r = 0.24)\n\nThese are slightly better than any one individual term, and will be chosen if one of the square terms does not have a stronger correlation.\nThe txti average and interaction terms did not perform better than txti2 alone."
  },
  {
    "objectID": "Project_3_Predictive_Modelling/Project_3_SAS/Code/Project_3_Predictive_Modelling_SAS.html#correlation-matrix-ii-1",
    "href": "Project_3_Predictive_Modelling/Project_3_SAS/Code/Project_3_Predictive_Modelling_SAS.html#correlation-matrix-ii-1",
    "title": "Predictive Modelling (SAS)",
    "section": "Correlation Matrix II",
    "text": "Correlation Matrix II\n\nCreate the Correlation Matrix\n\n* Print correlation matrix;\nPROC CORR DATA = Proj3.Data OUTP = corr_matr;\n    VAR tkvht_base tkvht_change progression geom1_sq geom2_sq gabor1_sq gabor2_sq gabor3_sq gabor4_sq gabor5_sq glcm1_sq glcm2_sq txti1_sq txti2_sq txti3_sq txti4_sq txti5_sq lbp1_sq lbp2_sq lbp3_sq lbp4_sq lbp5_sq;\n    RUN;\n    \n* Create a dataset with only the correlation coefficients;\nDATA corr_only;\n    SET corr_matr;\n    IF _TYPE_ = 'CORR'; /* Select only the rows with correlation coefficients */\nRUN;\n\n* Reshape the correlation matrix for the heat map;\nDATA heatmap_data;\n    SET corr_only;\n    ARRAY vars[*] tkvht_base tkvht_change progression geom1_sq geom2_sq gabor1_sq gabor2_sq gabor3_sq gabor4_sq gabor5_sq glcm1_sq glcm2_sq txti1_sq txti2_sq txti3_sq txti4_sq txti5_sq lbp1_sq lbp2_sq lbp3_sq lbp4_sq lbp5_sq;\n    DO i = 1 to DIM(vars);\n        ROW = _NAME_;\n        COL = VNAME(vars[i]);\n        corr_value = vars[i];\n        IF NOT missing(corr_value) THEN DO;\n            LABEL = PUT(corr_value, 8.2); * Format the text label;\n            OUTPUT;\n        END;\n    END;\n    KEEP row col corr_value label;\nRUN;\n\n\n\nPlot the Correlation Matrix as a Heatmap\n\n* Set the dimensions of the output graphic;\nODS GRAPHICS / WIDTH=1000px HEIGHT=800px;\n\n* Create the Heat Map;\nPROC SGPLOT DATA=heatmap_data;\n    HEATMAPPARM X=col Y=row COLORRESPONSE = corr_value / COLORMODEL = (blue white red);\n    TEXT X=col Y=row TEXT=label / TEXTATTRS = (SIZE=8) POSITION = center;\n    XAXIS DISPLAY = (nolabel) DISCRETEORDER = data;\n    YAXIS DISPLAY=(nolabel) DISCRETEORDER = data;\n    TITLE \"Correlation Matrix Heat Map\";\nRUN;\n\nlstlog\n\n\n&lt;!DOCTYPE html&gt;\n\n\n\n\n\nSAS Output\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001429                                                         The SAS System                    Saturday, December 28, 2024 08:59:00 PM\n\n380        ods listing close;ods html5 (id=saspy_internal) file=_tomods1 options(bitmap_mode='inline') device=svg style=HTMLBlue;\n380      ! ods graphics on / outputfmt=png;\n381        \n382        * Set the dimensions of the output graphic;\n383        ODS GRAPHICS / WIDTH=1000px HEIGHT=800px;\n384        \n385        * Create the Heat Map;\n386        PROC SGPLOT DATA=heatmap_data;\n387            HEATMAPPARM X=col Y=row COLORRESPONSE = corr_value / COLORMODEL = (blue white red);\n388            TEXT X=col Y=row TEXT=label / TEXTATTRS = (SIZE=8) POSITION = center;\n389            XAXIS DISPLAY = (nolabel) DISCRETEORDER = data;\n390            YAXIS DISPLAY=(nolabel) DISCRETEORDER = data;\n391            TITLE \"Correlation Matrix Heat Map\";\n392        RUN;\n393        \n394        \n395        ods html5 (id=saspy_internal) close;ods listing;\n396        \n\u001430                                                         The SAS System                    Saturday, December 28, 2024 08:59:00 PM\n\n397"
  },
  {
    "objectID": "Project_3_Predictive_Modelling/Project_3_SAS/Code/Project_3_Predictive_Modelling_SAS.html#summary-3",
    "href": "Project_3_Predictive_Modelling/Project_3_SAS/Code/Project_3_Predictive_Modelling_SAS.html#summary-3",
    "title": "Predictive Modelling (SAS)",
    "section": "Summary",
    "text": "Summary\nThe only quadratic term that performs better than previous terms is gabor3_sq (r = 0.35), which will be chosen instead of gabor_int."
  },
  {
    "objectID": "Project_3_Predictive_Modelling/Project_3_SAS/Code/Project_3_Predictive_Modelling_SAS.html#feature-engineering-summary",
    "href": "Project_3_Predictive_Modelling/Project_3_SAS/Code/Project_3_Predictive_Modelling_SAS.html#feature-engineering-summary",
    "title": "Predictive Modelling (SAS)",
    "section": "Feature Engineering Summary",
    "text": "Feature Engineering Summary\nThe final variables MRI image features chosen are:\n\nlbp_int\ntxti2\ngabor3_sq"
  },
  {
    "objectID": "Project_3_Predictive_Modelling/Project_3_SAS/Code/Project_3_Predictive_Modelling_SAS.html#perform-scaling",
    "href": "Project_3_Predictive_Modelling/Project_3_SAS/Code/Project_3_Predictive_Modelling_SAS.html#perform-scaling",
    "title": "Predictive Modelling (SAS)",
    "section": "Perform Scaling",
    "text": "Perform Scaling\n\nStandardize All Features\n\n* Standardize the variables;\nPROC STANDARD DATA = Proj3.Data OUT = data_scale MEAN = 0 STD = 1;\n    VAR lbp_int txti2 gabor3_sq;\n    RUN;"
  },
  {
    "objectID": "Project_3_Predictive_Modelling/Project_3_SAS/Code/Project_3_Predictive_Modelling_SAS.html#cost-function",
    "href": "Project_3_Predictive_Modelling/Project_3_SAS/Code/Project_3_Predictive_Modelling_SAS.html#cost-function",
    "title": "Predictive Modelling (SAS)",
    "section": "Cost Function",
    "text": "Cost Function\nThe cost function in a linear regression is Root Mean Square Error (RMSE):\n\\[ \\text{RMSE} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2} \\]\nWhere y is the actual change in outcome variable and y-hat is the predicted change, and n is the number of observations.\n\nMapping Function\nIn the case of linear regression, the mapping function is essentially each beta in the model.\n\n\nGoal\nThe goal is to select a mapping function that minimizes the cost function and thereby produces the best predictions for the outcome variable.\nWe will be using PROC GLMSELECT to perform 5-fold cross validation and all of our analyses and Ordinary Least Squares (OLS). Another option is to perform gradient descent."
  },
  {
    "objectID": "Project_3_Predictive_Modelling/Project_3_SAS/Code/Project_3_Predictive_Modelling_SAS.html#1A",
    "href": "Project_3_Predictive_Modelling/Project_3_SAS/Code/Project_3_Predictive_Modelling_SAS.html#1A",
    "title": "Predictive Modelling (SAS)",
    "section": "Model 1A: Baseline Kidney Volume",
    "text": "Model 1A: Baseline Kidney Volume\n\nAnalysisVisualizationSummary\n\n\nTo answer the researcher’s first question for Task 1, we will perform a predictive model that uses the baseline height-corrected total kidney volume to predict percent change in kidney volume.\n\\[ Kidney Volume Change = 𝛽_0 + 𝛽_1*Log Kidney Volume Baseline + e \\]\n\n*** Model 1A, Baseline Kidney Volume;\n* Perform 5-Fold Cross Validation and Linear Regression;\nPROC GLMSELECT DATA = Proj3.Data_scaled PLOTS = ALL SEED = 123;\n    PARTITION FRACTION(VALIDATE = 0.2); * Sets up the 5-Fold CV;\n    MODEL tkvht_change = tkvht_base / CVMETHOD =  SPLIT(5) SELECTION=none;\n    OUTPUT OUT = Model1A PRED = Predicted RESID = Residuals;\n    RUN;\n\nlstlog\n\n\n&lt;!DOCTYPE html&gt;\n\n\n\n\n\nSAS Output\n\n\n\n\n\n\n\nCorrelation Matrix Heat Map\n\n\n\n\nThe GLMSELECT Procedure\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Set\n\n\nPROJ3.DATA_SCALED\n\n\n\n\nDependent Variable\n\n\ntkvht_change\n\n\n\n\nSelection Method\n\n\nNone\n\n\n\n\nRandom Number Seed\n\n\n123\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNumber of Observations Read\n\n\n71\n\n\n\n\nNumber of Observations Used\n\n\n71\n\n\n\n\nNumber of Observations Used for Training\n\n\n59\n\n\n\n\nNumber of Observations Used for Validation\n\n\n12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDimensions\n\n\n\n\n\n\nNumber of Effects\n\n\n2\n\n\n\n\nNumber of Parameters\n\n\n2\n\n\n\n\n\n\n\n\nCorrelation Matrix Heat Map\n\n\n\n\nThe GLMSELECT Procedure\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLeast Squares Summary\n\n\n\n\nStep\n\n\nEffectEntered\n\n\nNumberEffects In\n\n\nSBC\n\n\nASE\n\n\nValidationASE\n\n\n\n\n\n\n* Optimal Value of Criterion\n\n\n\n\n\n\n0\n\n\nIntercept\n\n\n1\n\n\n244.0296*\n\n\n58.3806\n\n\n111.0698\n\n\n\n\n1\n\n\ntkvht_base\n\n\n2\n\n\n247.2271\n\n\n57.5163\n\n\n107.9482*\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCorrelation Matrix Heat Map\n\n\n\n\nThe GLMSELECT Procedure\n\n\nLeast Squares Model (No Selection)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnalysis of Variance\n\n\n\n\nSource\n\n\nDF\n\n\nSum ofSquares\n\n\nMeanSquare\n\n\nF Value\n\n\nPr &gt; F\n\n\n\n\n\n\nModel\n\n\n1\n\n\n50.99565\n\n\n50.99565\n\n\n0.86\n\n\n0.3586\n\n\n\n\nError\n\n\n57\n\n\n3393.46096\n\n\n59.53440\n\n\n \n\n\n \n\n\n\n\nCorrected Total\n\n\n58\n\n\n3444.45662\n\n\n \n\n\n \n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRoot MSE\n\n\n7.71585\n\n\n\n\nDependent Mean\n\n\n8.88119\n\n\n\n\nR-Square\n\n\n0.0148\n\n\n\n\nAdj R-Sq\n\n\n-.0025\n\n\n\n\nAIC\n\n\n304.07202\n\n\n\n\nAICC\n\n\n304.50839\n\n\n\n\nSBC\n\n\n247.22710\n\n\n\n\nASE (Train)\n\n\n57.51629\n\n\n\n\nASE (Validate)\n\n\n107.94821\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nParameter Estimates\n\n\n\n\nParameter\n\n\nDF\n\n\nEstimate\n\n\nStandardError\n\n\nt Value\n\n\nPr &gt; |t|\n\n\n\n\n\n\nIntercept\n\n\n1\n\n\n10.523955\n\n\n2.039514\n\n\n5.16\n\n\n&lt;.0001\n\n\n\n\ntkvht_base\n\n\n1\n\n\n-0.004515\n\n\n0.004878\n\n\n-0.93\n\n\n0.3586\n\n\n\n\n\n\n\n\n\n\n\n\u001433                                                         The SAS System                    Saturday, December 28, 2024 08:59:00 PM\n\n413        ods listing close;ods html5 (id=saspy_internal) file=_tomods1 options(bitmap_mode='inline') device=svg style=HTMLBlue;\n413      ! ods graphics on / outputfmt=png;\n414        \n415        *** Model 1A, Baseline Kidney Volume;\n416        * Perform 5-Fold Cross Validation and Linear Regression;\n417        PROC GLMSELECT DATA = Proj3.Data_scaled PLOTS = ALL SEED = 123;\n418         PARTITION FRACTION(VALIDATE = 0.2); * Sets up the 5-Fold CV;\n419         MODEL tkvht_change = tkvht_base / CVMETHOD =  SPLIT(5) SELECTION=none;\n420         OUTPUT OUT = Model1A PRED = Predicted RESID = Residuals;\n421         RUN;\n422        \n423        \n424        ods html5 (id=saspy_internal) close;ods listing;\n425        \n\u001434                                                         The SAS System                    Saturday, December 28, 2024 08:59:00 PM\n\n426        \n\n\n\n\nTop of Tabset\n\n\n\n* Plot residuals vs. predicted values;\nPROC SGPLOT DATA = Model1A;\n    SCATTER X=Predicted Y=Residuals;\n    REFLINE 0 / AXIS=y LINEATTRS=(pattern=shortdash);\n    XAXIS LABEL='Predicted Values';\n    YAXIS LABEL='Residuals';\n    TITLE 'Residuals vs. Predicted Values';\n    RUN;\n\nlstlog\n\n\n&lt;!DOCTYPE html&gt;\n\n\n\n\n\nSAS Output\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001435                                                         The SAS System                    Saturday, December 28, 2024 08:59:00 PM\n\n429        ods listing close;ods html5 (id=saspy_internal) file=_tomods1 options(bitmap_mode='inline') device=svg style=HTMLBlue;\n429      ! ods graphics on / outputfmt=png;\n430        \n431        * Plot residuals vs. predicted values;\n432        PROC SGPLOT DATA = Model1A;\n433            SCATTER X=Predicted Y=Residuals;\n434            REFLINE 0 / AXIS=y LINEATTRS=(pattern=shortdash);\n435            XAXIS LABEL='Predicted Values';\n436            YAXIS LABEL='Residuals';\n437            TITLE 'Residuals vs. Predicted Values';\n438         RUN;\n439        \n440        \n441        ods html5 (id=saspy_internal) close;ods listing;\n442        \n\u001436                                                         The SAS System                    Saturday, December 28, 2024 08:59:00 PM\n\n443        \n\n\n\n\nTop of Tabset\n\n\nthe RMSE of the model including kidvol_base alone is 7.72.\nThe model does not perform too well, when looking at the plots of the predicted vs actual values.\nThe assumption of normality is almost, but not quite, met.\nTop of Tabset"
  },
  {
    "objectID": "Project_3_Predictive_Modelling/Project_3_SAS/Code/Project_3_Predictive_Modelling_SAS.html#1B",
    "href": "Project_3_Predictive_Modelling/Project_3_SAS/Code/Project_3_Predictive_Modelling_SAS.html#1B",
    "title": "Predictive Modelling (SAS)",
    "section": "Model 1B: MRI Features",
    "text": "Model 1B: MRI Features\nTo answer the researcher’s second question for Task 1, we will run a predictive model that uses only MRI image features to predict percent change in kidney volume at year 3.\n\\[ Kidney Volume Change = 𝛽_0 + 𝛽_1*Txti2 + 𝛽_{2}*LbpInt + e \\]\n\nModel Selection\nThe potential scaled covariates for our final model after feature engineering and interactive variable selection are:\n\nlbp_int\ntxti2\ngabor3_sq\n\nWe will perform model selection using backwards elimination and BIC.\n\nModel SelectionAnalysisVisualizationSummary\n\n\n\n** Model 1B: MRI Image Features;\n* Perform 5-Fold Cross Validation and Linear Regression;\nPROC GLMSELECT DATA = Proj3.Data_scaled PLOTS = ALL SEED = 123;\n    PARTITION FRACTION(VALIDATE = 0.2); * Sets up the 5-Fold CV;\n    MODEL tkvht_change = lbp_int txti2 gabor3_sq / CVMETHOD =  SPLIT(5) SELECTION = NONE;\n    RUN;\n\nlstlog\n\n\n&lt;!DOCTYPE html&gt;\n\n\n\n\n\nSAS Output\n\n\n\n\n\n\n\nResiduals vs. Predicted Values\n\n\n\n\nThe GLMSELECT Procedure\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Set\n\n\nPROJ3.DATA_SCALED\n\n\n\n\nDependent Variable\n\n\ntkvht_change\n\n\n\n\nSelection Method\n\n\nNone\n\n\n\n\nRandom Number Seed\n\n\n123\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNumber of Observations Read\n\n\n71\n\n\n\n\nNumber of Observations Used\n\n\n71\n\n\n\n\nNumber of Observations Used for Training\n\n\n59\n\n\n\n\nNumber of Observations Used for Validation\n\n\n12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDimensions\n\n\n\n\n\n\nNumber of Effects\n\n\n4\n\n\n\n\nNumber of Parameters\n\n\n4\n\n\n\n\n\n\n\n\nResiduals vs. Predicted Values\n\n\n\n\nThe GLMSELECT Procedure\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLeast Squares Summary\n\n\n\n\nStep\n\n\nEffectEntered\n\n\nNumberEffects In\n\n\nSBC\n\n\nASE\n\n\nValidationASE\n\n\n\n\n\n\n* Optimal Value of Criterion\n\n\n\n\n\n\n0\n\n\nIntercept\n\n\n1\n\n\n244.0296\n\n\n58.3806\n\n\n111.0698\n\n\n\n\n1\n\n\nlbp_int\n\n\n2\n\n\n242.6501\n\n\n53.2231\n\n\n113.0121\n\n\n\n\n2\n\n\ntxti2\n\n\n3\n\n\n239.2228*\n\n\n46.8660\n\n\n102.0524*\n\n\n\n\n3\n\n\ngabor3_sq\n\n\n4\n\n\n243.2656\n\n\n46.8383\n\n\n106.1342\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nResiduals vs. Predicted Values\n\n\n\n\nThe GLMSELECT Procedure\n\n\nLeast Squares Model (No Selection)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnalysis of Variance\n\n\n\n\nSource\n\n\nDF\n\n\nSum ofSquares\n\n\nMeanSquare\n\n\nF Value\n\n\nPr &gt; F\n\n\n\n\n\n\nModel\n\n\n3\n\n\n680.99444\n\n\n226.99815\n\n\n4.52\n\n\n0.0067\n\n\n\n\nError\n\n\n55\n\n\n2763.46218\n\n\n50.24477\n\n\n \n\n\n \n\n\n\n\nCorrected Total\n\n\n58\n\n\n3444.45662\n\n\n \n\n\n \n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRoot MSE\n\n\n7.08835\n\n\n\n\nDependent Mean\n\n\n8.88119\n\n\n\n\nR-Square\n\n\n0.1977\n\n\n\n\nAdj R-Sq\n\n\n0.1539\n\n\n\n\nAIC\n\n\n295.95543\n\n\n\n\nAICC\n\n\n297.08750\n\n\n\n\nSBC\n\n\n243.26558\n\n\n\n\nASE (Train)\n\n\n46.83834\n\n\n\n\nASE (Validate)\n\n\n106.13419\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nParameter Estimates\n\n\n\n\nParameter\n\n\nDF\n\n\nEstimate\n\n\nStandardError\n\n\nt Value\n\n\nPr &gt; |t|\n\n\n\n\n\n\nIntercept\n\n\n1\n\n\n8.980217\n\n\n0.925709\n\n\n9.70\n\n\n&lt;.0001\n\n\n\n\nlbp_int\n\n\n1\n\n\n2.706174\n\n\n1.368868\n\n\n1.98\n\n\n0.0531\n\n\n\n\ntxti2\n\n\n1\n\n\n-2.665708\n\n\n0.976472\n\n\n-2.73\n\n\n0.0085\n\n\n\n\ngabor3_sq\n\n\n1\n\n\n-0.261434\n\n\n1.451159\n\n\n-0.18\n\n\n0.8577\n\n\n\n\n\n\n\n\n\n\n\n\u001437                                                         The SAS System                    Saturday, December 28, 2024 08:59:00 PM\n\n446        ods listing close;ods html5 (id=saspy_internal) file=_tomods1 options(bitmap_mode='inline') device=svg style=HTMLBlue;\n446      ! ods graphics on / outputfmt=png;\n447        \n448        ** Model 1B: MRI Image Features;\n449        * Perform 5-Fold Cross Validation and Linear Regression;\n450        PROC GLMSELECT DATA = Proj3.Data_scaled PLOTS = ALL SEED = 123;\n451         PARTITION FRACTION(VALIDATE = 0.2); * Sets up the 5-Fold CV;\n452         MODEL tkvht_change = lbp_int txti2 gabor3_sq / CVMETHOD =  SPLIT(5) SELECTION = NONE;\n453         RUN;\n454        \n455        \n456        ods html5 (id=saspy_internal) close;ods listing;\n457        \n\u001438                                                         The SAS System                    Saturday, December 28, 2024 08:59:00 PM\n\n458        \n\n\n\n\nWe can see that gabor3_sq decreases the performance on all metrics, and itself is not a significant predictor of tkvht_change. It will be removed from the final model.\nTop of Tabset\n\n\nThe final model selected via backwards elimination includes lbp_int andtxti2`.\n\n** Model 1B: MRI Image Features;\n* Perform 5-Fold Cross Validation and Linear Regression;\nPROC GLMSELECT DATA = Proj3.Data_scaled PLOTS = ALL SEED = 123;\n    PARTITION FRACTION(VALIDATE = 0.2); * Sets up the 5-Fold CV;\n    MODEL tkvht_change = lbp_int txti2 / CVMETHOD =  SPLIT(5) SELECTION = NONE;\n    OUTPUT OUT = Model1B PRED = Predicted RESID = Residuals;\n    RUN;\n\nlstlog\n\n\n&lt;!DOCTYPE html&gt;\n\n\n\n\n\nSAS Output\n\n\n\n\n\n\n\nResiduals vs. Predicted Values\n\n\n\n\nThe GLMSELECT Procedure\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Set\n\n\nPROJ3.DATA_SCALED\n\n\n\n\nDependent Variable\n\n\ntkvht_change\n\n\n\n\nSelection Method\n\n\nNone\n\n\n\n\nRandom Number Seed\n\n\n123\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNumber of Observations Read\n\n\n71\n\n\n\n\nNumber of Observations Used\n\n\n71\n\n\n\n\nNumber of Observations Used for Training\n\n\n59\n\n\n\n\nNumber of Observations Used for Validation\n\n\n12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDimensions\n\n\n\n\n\n\nNumber of Effects\n\n\n3\n\n\n\n\nNumber of Parameters\n\n\n3\n\n\n\n\n\n\n\n\nResiduals vs. Predicted Values\n\n\n\n\nThe GLMSELECT Procedure\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLeast Squares Summary\n\n\n\n\nStep\n\n\nEffectEntered\n\n\nNumberEffects In\n\n\nSBC\n\n\nASE\n\n\nValidationASE\n\n\n\n\n\n\n* Optimal Value of Criterion\n\n\n\n\n\n\n0\n\n\nIntercept\n\n\n1\n\n\n244.0296\n\n\n58.3806\n\n\n111.0698\n\n\n\n\n1\n\n\nlbp_int\n\n\n2\n\n\n242.6501\n\n\n53.2231\n\n\n113.0121\n\n\n\n\n2\n\n\ntxti2\n\n\n3\n\n\n239.2228*\n\n\n46.8660\n\n\n102.0524*\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nResiduals vs. Predicted Values\n\n\n\n\nThe GLMSELECT Procedure\n\n\nLeast Squares Model (No Selection)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnalysis of Variance\n\n\n\n\nSource\n\n\nDF\n\n\nSum ofSquares\n\n\nMeanSquare\n\n\nF Value\n\n\nPr &gt; F\n\n\n\n\n\n\nModel\n\n\n2\n\n\n679.36370\n\n\n339.68185\n\n\n6.88\n\n\n0.0021\n\n\n\n\nError\n\n\n56\n\n\n2765.09292\n\n\n49.37666\n\n\n \n\n\n \n\n\n\n\nCorrected Total\n\n\n58\n\n\n3444.45662\n\n\n \n\n\n \n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRoot MSE\n\n\n7.02685\n\n\n\n\nDependent Mean\n\n\n8.88119\n\n\n\n\nR-Square\n\n\n0.1972\n\n\n\n\nAdj R-Sq\n\n\n0.1686\n\n\n\n\nAIC\n\n\n293.99023\n\n\n\n\nAICC\n\n\n294.73097\n\n\n\n\nSBC\n\n\n239.22284\n\n\n\n\nASE (Train)\n\n\n46.86598\n\n\n\n\nASE (Validate)\n\n\n102.05238\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nParameter Estimates\n\n\n\n\nParameter\n\n\nDF\n\n\nEstimate\n\n\nStandardError\n\n\nt Value\n\n\nPr &gt; |t|\n\n\n\n\n\n\nIntercept\n\n\n1\n\n\n8.987483\n\n\n0.916805\n\n\n9.80\n\n\n&lt;.0001\n\n\n\n\nlbp_int\n\n\n1\n\n\n2.514704\n\n\n0.855219\n\n\n2.94\n\n\n0.0048\n\n\n\n\ntxti2\n\n\n1\n\n\n-2.640614\n\n\n0.958101\n\n\n-2.76\n\n\n0.0079\n\n\n\n\n\n\n\n\n\n\n\n\u001439                                                         The SAS System                    Saturday, December 28, 2024 08:59:00 PM\n\n461        ods listing close;ods html5 (id=saspy_internal) file=_tomods1 options(bitmap_mode='inline') device=svg style=HTMLBlue;\n461      ! ods graphics on / outputfmt=png;\n462        \n463        ** Model 1B: MRI Image Features;\n464        * Perform 5-Fold Cross Validation and Linear Regression;\n465        PROC GLMSELECT DATA = Proj3.Data_scaled PLOTS = ALL SEED = 123;\n466         PARTITION FRACTION(VALIDATE = 0.2); * Sets up the 5-Fold CV;\n467         MODEL tkvht_change = lbp_int txti2 / CVMETHOD =  SPLIT(5) SELECTION = NONE;\n468         OUTPUT OUT = Model1B PRED = Predicted RESID = Residuals;\n469         RUN;\n470        \n471        \n472        ods html5 (id=saspy_internal) close;ods listing;\n473        \n\u001440                                                         The SAS System                    Saturday, December 28, 2024 08:59:00 PM\n\n474        \n\n\n\n\nTop of Tabset\n\n\n\nActual vs Predicted Values\n\n* Plot residuals vs. predicted values;\nPROC SGPLOT DATA = Model1B;\n    SCATTER X=Predicted Y=Residuals;\n    REFLINE 0 / AXIS=y LINEATTRS=(pattern=shortdash);\n    XAXIS LABEL='Predicted Values';\n    YAXIS LABEL='Residuals';\n    TITLE 'Residuals vs. Predicted Values';\n    RUN;\n\nlstlog\n\n\n&lt;!DOCTYPE html&gt;\n\n\n\n\n\nSAS Output\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001441                                                         The SAS System                    Saturday, December 28, 2024 08:59:00 PM\n\n477        ods listing close;ods html5 (id=saspy_internal) file=_tomods1 options(bitmap_mode='inline') device=svg style=HTMLBlue;\n477      ! ods graphics on / outputfmt=png;\n478        \n479        * Plot residuals vs. predicted values;\n480        PROC SGPLOT DATA = Model1B;\n481            SCATTER X=Predicted Y=Residuals;\n482            REFLINE 0 / AXIS=y LINEATTRS=(pattern=shortdash);\n483            XAXIS LABEL='Predicted Values';\n484            YAXIS LABEL='Residuals';\n485            TITLE 'Residuals vs. Predicted Values';\n486         RUN;\n487        \n488        \n489        ods html5 (id=saspy_internal) close;ods listing;\n490        \n\u001442                                                         The SAS System                    Saturday, December 28, 2024 08:59:00 PM\n\n491        \n\n\n\n\nTop of Tabset\n\n\n\nThe RMSE for the model including only MRI Image features was 7.02, with both lbp_int and txti2 being highly significant predictors of tkvht_change.\nThis model with MRI images features alone performed better than that using only baseline kidney volume (RMSE = 7.70).\nTop of Tabset"
  },
  {
    "objectID": "Project_3_Predictive_Modelling/Project_3_SAS/Code/Project_3_Predictive_Modelling_SAS.html#1C",
    "href": "Project_3_Predictive_Modelling/Project_3_SAS/Code/Project_3_Predictive_Modelling_SAS.html#1C",
    "title": "Predictive Modelling (SAS)",
    "section": "Model 1C: Baseline Kidney Volume and MRI Image Features",
    "text": "Model 1C: Baseline Kidney Volume and MRI Image Features\n\nAnalysisVisualizationSummary\n\n\nTo answer the researcher’s third question, we will run a predictive model that uses the baseline height-corrected total kidney volume AND MRI image features to predict percent change in kidney volume.\n\\[ Kidney Volume Change = 𝛽_0 + 𝛽_{1}*Baseline Kidney Volume + 𝛽_{2}*Txti2 + 𝛽_{3}*LbpInt + e \\]\n\n** Model 1C: Baseline + MRI Image Features;\n* Perform 5-Fold Cross Validation and Linear Regression;\nPROC GLMSELECT DATA = Proj3.Data_scaled PLOTS = ALL SEED = 123;\n    PARTITION FRACTION(VALIDATE = 0.2); * Sets up the 5-Fold CV;\n    MODEL tkvht_change = tkvht_base lbp_int txti2 / CVMETHOD =  SPLIT(5) SELECTION = NONE;\n    OUTPUT OUT = Model1C PRED = Predicted RESID = Residuals;\n    RUN;\n\nlstlog\n\n\n&lt;!DOCTYPE html&gt;\n\n\n\n\n\nSAS Output\n\n\n\n\n\n\n\nResiduals vs. Predicted Values\n\n\n\n\nThe GLMSELECT Procedure\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Set\n\n\nPROJ3.DATA_SCALED\n\n\n\n\nDependent Variable\n\n\ntkvht_change\n\n\n\n\nSelection Method\n\n\nNone\n\n\n\n\nRandom Number Seed\n\n\n123\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNumber of Observations Read\n\n\n71\n\n\n\n\nNumber of Observations Used\n\n\n71\n\n\n\n\nNumber of Observations Used for Training\n\n\n59\n\n\n\n\nNumber of Observations Used for Validation\n\n\n12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDimensions\n\n\n\n\n\n\nNumber of Effects\n\n\n4\n\n\n\n\nNumber of Parameters\n\n\n4\n\n\n\n\n\n\n\n\nResiduals vs. Predicted Values\n\n\n\n\nThe GLMSELECT Procedure\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLeast Squares Summary\n\n\n\n\nStep\n\n\nEffectEntered\n\n\nNumberEffects In\n\n\nSBC\n\n\nASE\n\n\nValidationASE\n\n\n\n\n\n\n* Optimal Value of Criterion\n\n\n\n\n\n\n0\n\n\nIntercept\n\n\n1\n\n\n244.0296\n\n\n58.3806\n\n\n111.0698\n\n\n\n\n1\n\n\ntkvht_base\n\n\n2\n\n\n247.2271\n\n\n57.5163\n\n\n107.9482\n\n\n\n\n2\n\n\nlbp_int\n\n\n3\n\n\n245.7787\n\n\n52.3739\n\n\n109.7687\n\n\n\n\n3\n\n\ntxti2\n\n\n4\n\n\n241.7845*\n\n\n45.6772\n\n\n97.1320*\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nResiduals vs. Predicted Values\n\n\n\n\nThe GLMSELECT Procedure\n\n\nLeast Squares Model (No Selection)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnalysis of Variance\n\n\n\n\nSource\n\n\nDF\n\n\nSum ofSquares\n\n\nMeanSquare\n\n\nF Value\n\n\nPr &gt; F\n\n\n\n\n\n\nModel\n\n\n3\n\n\n749.50019\n\n\n249.83340\n\n\n5.10\n\n\n0.0035\n\n\n\n\nError\n\n\n55\n\n\n2694.95643\n\n\n48.99921\n\n\n \n\n\n \n\n\n\n\nCorrected Total\n\n\n58\n\n\n3444.45662\n\n\n \n\n\n \n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRoot MSE\n\n\n6.99994\n\n\n\n\nDependent Mean\n\n\n8.88119\n\n\n\n\nR-Square\n\n\n0.2176\n\n\n\n\nAdj R-Sq\n\n\n0.1749\n\n\n\n\nAIC\n\n\n294.47439\n\n\n\n\nAICC\n\n\n295.60647\n\n\n\n\nSBC\n\n\n241.78454\n\n\n\n\nASE (Train)\n\n\n45.67723\n\n\n\n\nASE (Validate)\n\n\n97.13202\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nParameter Estimates\n\n\n\n\nParameter\n\n\nDF\n\n\nEstimate\n\n\nStandardError\n\n\nt Value\n\n\nPr &gt; |t|\n\n\n\n\n\n\nIntercept\n\n\n1\n\n\n10.922833\n\n\n1.857650\n\n\n5.88\n\n\n&lt;.0001\n\n\n\n\ntkvht_base\n\n\n1\n\n\n-0.005306\n\n\n0.004435\n\n\n-1.20\n\n\n0.2367\n\n\n\n\nlbp_int\n\n\n1\n\n\n2.523276\n\n\n0.851974\n\n\n2.96\n\n\n0.0045\n\n\n\n\ntxti2\n\n\n1\n\n\n-2.716157\n\n\n0.956518\n\n\n-2.84\n\n\n0.0063\n\n\n\n\n\n\n\n\n\n\n\n\u001443                                                         The SAS System                    Saturday, December 28, 2024 08:59:00 PM\n\n494        ods listing close;ods html5 (id=saspy_internal) file=_tomods1 options(bitmap_mode='inline') device=svg style=HTMLBlue;\n494      ! ods graphics on / outputfmt=png;\n495        \n496        ** Model 1C: Baseline + MRI Image Features;\n497        * Perform 5-Fold Cross Validation and Linear Regression;\n498        PROC GLMSELECT DATA = Proj3.Data_scaled PLOTS = ALL SEED = 123;\n499         PARTITION FRACTION(VALIDATE = 0.2); * Sets up the 5-Fold CV;\n500         MODEL tkvht_change = tkvht_base lbp_int txti2 / CVMETHOD =  SPLIT(5) SELECTION = NONE;\n501         OUTPUT OUT = Model1C PRED = Predicted RESID = Residuals;\n502         RUN;\n503        \n504        \n505        ods html5 (id=saspy_internal) close;ods listing;\n506        \n\u001444                                                         The SAS System                    Saturday, December 28, 2024 08:59:00 PM\n\n507        \n\n\n\n\nTop of Tabset\n\n\n\n* Plot residuals vs. predicted values;\nPROC SGPLOT DATA = Model1C;\n    SCATTER X=Predicted Y=Residuals;\n    REFLINE 0 / AXIS=y LINEATTRS=(pattern=shortdash);\n    XAXIS LABEL='Predicted Values';\n    YAXIS LABEL='Residuals';\n    TITLE 'Residuals vs. Predicted Values';\n    RUN;\n\nlstlog\n\n\n&lt;!DOCTYPE html&gt;\n\n\n\n\n\nSAS Output\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001445                                                         The SAS System                    Saturday, December 28, 2024 08:59:00 PM\n\n510        ods listing close;ods html5 (id=saspy_internal) file=_tomods1 options(bitmap_mode='inline') device=svg style=HTMLBlue;\n510      ! ods graphics on / outputfmt=png;\n511        \n512        * Plot residuals vs. predicted values;\n513        PROC SGPLOT DATA = Model1C;\n514            SCATTER X=Predicted Y=Residuals;\n515            REFLINE 0 / AXIS=y LINEATTRS=(pattern=shortdash);\n516            XAXIS LABEL='Predicted Values';\n517            YAXIS LABEL='Residuals';\n518            TITLE 'Residuals vs. Predicted Values';\n519         RUN;\n520        \n521        \n522        ods html5 (id=saspy_internal) close;ods listing;\n523        \n\u001446                                                         The SAS System                    Saturday, December 28, 2024 08:59:00 PM\n\n524        \n\n\n\n\nTop of Tabset\n\n\nModel 1C with baseline kidney volume and MRI image features (MRSE = 7.00) did not perform better than model 1B using MRI image features alone (MRSE = 7.02).\nTop of Tabset"
  },
  {
    "objectID": "Project_3_Predictive_Modelling/Project_3_SAS/Code/Project_3_Predictive_Modelling_SAS.html#model-comparison",
    "href": "Project_3_Predictive_Modelling/Project_3_SAS/Code/Project_3_Predictive_Modelling_SAS.html#model-comparison",
    "title": "Predictive Modelling (SAS)",
    "section": "Model Comparison",
    "text": "Model Comparison\nHere we will compare how models 1A, 1B, and 1C performed.\n\nFinal Average RMSE\n\n\n\nModel\nRMSE\n\n\n\n\n1A: Baseline Kidney Volume\n7.72\n\n\n1B: MRI Image Features\n7.02\n\n\n1C Baseline Kidney Volume and MRI\n7.00"
  },
  {
    "objectID": "Project_3_Predictive_Modelling/Project_3_SAS/Code/Project_3_Predictive_Modelling_SAS.html#conclusion",
    "href": "Project_3_Predictive_Modelling/Project_3_SAS/Code/Project_3_Predictive_Modelling_SAS.html#conclusion",
    "title": "Predictive Modelling (SAS)",
    "section": "Conclusion",
    "text": "Conclusion\nThe differences in model performance for task one were not statistically significant. Model A had an RMSE that was 0.70 points higher than model B, and 0.72 points higher than model C.\nThus, including MRI image features into the predictive model slightly increased predictive capability above and beyond that of just using kidney volume measurements at baseline."
  },
  {
    "objectID": "Project_3_Predictive_Modelling/Project_3_SAS/Code/Project_3_Predictive_Modelling_SAS.html#2A",
    "href": "Project_3_Predictive_Modelling/Project_3_SAS/Code/Project_3_Predictive_Modelling_SAS.html#2A",
    "title": "Predictive Modelling (SAS)",
    "section": "Model 2A",
    "text": "Model 2A\n\nBaseline Kidney Volume\nTo answer the researcher’s first question for this task, we will run a predictive model that uses the baseline height-corrected total kidney volume to predict whether a patient had fast or slow disease progression.\n\\[ logit(Progression) = 𝛽_0 + 𝛽_1*{Baseline Kidney Volume} + e \\]\n\nAnalysisSummary\n\n\n\nTrain the Model\n\n* Run Model;\n%cross_validate(data=Proj3.Data_scaled, target=progression, predictors=tkvht_base);\n\n\n\nAcquire AUC and ROC Curve\n\n*ODS TRACE ON;\nODS EXCLUDE ModelInfo NOBS ResponseProfile ConvergenceStatus ROCcurve;\n/* Calculate ROC curve and AUC the each model */\nPROC LOGISTIC DATA = all_results;\n    MODEL progression(EVENT = 'Fast') = tkvht_base;\n    OUTPUT OUT = Model2A PREDICTED = Model2Apred;\n    ROC;\nRUN;\n*ODS TRACE OFF;\n\nlstlog\n\n\n&lt;!DOCTYPE html&gt;\n\n\n\n\n\nSAS Output\n\n\n\n\n\n\n\nResiduals vs. Predicted Values\n\n\n\n\nThe LOGISTIC Procedure\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel Fit Statistics\n\n\n\n\nCriterion\n\n\nIntercept Only\n\n\nIntercept and Covariates\n\n\n\n\n\n\nAIC\n\n\n100.300\n\n\n102.109\n\n\n\n\nSC\n\n\n102.563\n\n\n106.634\n\n\n\n\n-2 Log L\n\n\n98.300\n\n\n98.109\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTesting Global Null Hypothesis: BETA=0\n\n\n\n\nTest\n\n\nChi-Square\n\n\nDF\n\n\nPr &gt; ChiSq\n\n\n\n\n\n\nLikelihood Ratio\n\n\n0.1913\n\n\n1\n\n\n0.6619\n\n\n\n\nScore\n\n\n0.1912\n\n\n1\n\n\n0.6619\n\n\n\n\nWald\n\n\n0.1901\n\n\n1\n\n\n0.6628\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnalysis of Maximum Likelihood Estimates\n\n\n\n\nParameter\n\n\nDF\n\n\nEstimate\n\n\nStandardError\n\n\nWaldChi-Square\n\n\nPr &gt; ChiSq\n\n\n\n\n\n\nIntercept\n\n\n1\n\n\n0.2730\n\n\n0.4932\n\n\n0.3065\n\n\n0.5798\n\n\n\n\ntkvht_base\n\n\n1\n\n\n-0.00053\n\n\n0.00122\n\n\n0.1901\n\n\n0.6628\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOdds Ratio Estimates\n\n\n\n\nEffect\n\n\nPoint Estimate\n\n\n95% WaldConfidence Limits\n\n\n\n\n\n\ntkvht_base\n\n\n0.999\n\n\n0.997\n\n\n1.002\n\n\n\n\n\n\n\n\nROC Model: ROC1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n-2 Log L\n\n\n=\n\n\n98.300\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnalysis of Maximum Likelihood Estimates\n\n\n\n\nParameter\n\n\nDF\n\n\nEstimate\n\n\nStandardError\n\n\nWaldChi-Square\n\n\nPr &gt; ChiSq\n\n\n\n\n\n\nIntercept\n\n\n1\n\n\n0.0846\n\n\n0.2376\n\n\n0.1267\n\n\n0.7219\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nROC Association Statistics\n\n\n\n\nROC Model\n\n\nMann-Whitney\n\n\nSomers' D\n\n\nGamma\n\n\nTau-a\n\n\n\n\nArea\n\n\nStandardError\n\n\n95% WaldConfidence Limits\n\n\n\n\n\n\nModel\n\n\n0.5246\n\n\n0.0703\n\n\n0.3868\n\n\n0.6625\n\n\n0.0493\n\n\n0.0494\n\n\n0.0249\n\n\n\n\nROC1\n\n\n0.5000\n\n\n0\n\n\n0.5000\n\n\n0.5000\n\n\n0\n\n\n.\n\n\n0\n\n\n\n\n\n\n\n\n\n\n\n\u001453                                                         The SAS System                    Saturday, December 28, 2024 08:59:00 PM\n\n595        ods listing close;ods html5 (id=saspy_internal) file=_tomods1 options(bitmap_mode='inline') device=svg style=HTMLBlue;\n595      ! ods graphics on / outputfmt=png;\n596        \n597        *ODS TRACE ON;\n598        ODS EXCLUDE ModelInfo NOBS ResponseProfile ConvergenceStatus ROCcurve;\n599        /* Calculate ROC curve and AUC the each model */\n600        PROC LOGISTIC DATA = all_results;\n601            MODEL progression(EVENT = 'Fast') = tkvht_base;\n602            OUTPUT OUT = Model2A PREDICTED = Model2Apred;\n603            ROC;\n604        RUN;\n605        *ODS TRACE OFF;\n606        \n607        \n608        ods html5 (id=saspy_internal) close;ods listing;\n609        \n\u001454                                                         The SAS System                    Saturday, December 28, 2024 08:59:00 PM\n\n610        \n\n\n\n\nTop of Tabset\n\n\n\nThe AUC for the model with baseline kidney volume alone was 0.52, essentially just guessing.\nTop of Tabset"
  },
  {
    "objectID": "Project_3_Predictive_Modelling/Project_3_SAS/Code/Project_3_Predictive_Modelling_SAS.html#2B",
    "href": "Project_3_Predictive_Modelling/Project_3_SAS/Code/Project_3_Predictive_Modelling_SAS.html#2B",
    "title": "Predictive Modelling (SAS)",
    "section": "Model 2B: MRI Image Features",
    "text": "Model 2B: MRI Image Features\nTo answer the researcher’s second question for this task, we will run a predictive model that uses the MRI features to predict whether a patient had fast or slow disease progression.\n\\[ logit(Progression) = 𝛽_0 + 𝛽_1*Txti2 + 𝛽_2*LbpInt + e \\]\nAs the correlation matrices revealed similar relationship between the MRI image features and progression as they did with kidvol_change, we will use the same final variables as in task model 1B.\nThese are:\n\nlbp_int\ntxti2\n\n\nAnalysisSummary\n\n\n\nTrain the Model\n\n* Train Model;\n%cross_validate(data=Proj3.Data_scaled, target=progression, predictors=lbp_int txti2);\n\n\n\nAcquire AUC and ROC Curve\n\n**** Model 2B;\nODS EXCLUDE ModelInfo NOBS ResponseProfile ConvergenceStatus ROCcurve;\n/* Calculate ROC curve and AUC for the model */\nPROC LOGISTIC DATA = all_results;\n    MODEL progression(EVENT = 'Fast') = lbp_int txti2;\n    OUTPUT OUT = Model2B PREDICTED = Model2Bpred;\n    ROC;\nRUN;\n\nlstlog\n\n\n&lt;!DOCTYPE html&gt;\n\n\n\n\n\nSAS Output\n\n\n\n\n\n\n\nResiduals vs. Predicted Values\n\n\n\n\nThe LOGISTIC Procedure\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel Fit Statistics\n\n\n\n\nCriterion\n\n\nIntercept Only\n\n\nIntercept and Covariates\n\n\n\n\n\n\nAIC\n\n\n100.300\n\n\n92.067\n\n\n\n\nSC\n\n\n102.563\n\n\n98.855\n\n\n\n\n-2 Log L\n\n\n98.300\n\n\n86.067\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTesting Global Null Hypothesis: BETA=0\n\n\n\n\nTest\n\n\nChi-Square\n\n\nDF\n\n\nPr &gt; ChiSq\n\n\n\n\n\n\nLikelihood Ratio\n\n\n12.2331\n\n\n2\n\n\n0.0022\n\n\n\n\nScore\n\n\n11.0650\n\n\n2\n\n\n0.0040\n\n\n\n\nWald\n\n\n8.8090\n\n\n2\n\n\n0.0122\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnalysis of Maximum Likelihood Estimates\n\n\n\n\nParameter\n\n\nDF\n\n\nEstimate\n\n\nStandardError\n\n\nWaldChi-Square\n\n\nPr &gt; ChiSq\n\n\n\n\n\n\nIntercept\n\n\n1\n\n\n0.1701\n\n\n0.8343\n\n\n0.0415\n\n\n0.8385\n\n\n\n\nlbp_int\n\n\n1\n\n\n1.1025\n\n\n6.6925\n\n\n0.0271\n\n\n0.8692\n\n\n\n\ntxti2\n\n\n1\n\n\n-0.9018\n\n\n0.3042\n\n\n8.7881\n\n\n0.0030\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOdds Ratio Estimates\n\n\n\n\nEffect\n\n\nPoint Estimate\n\n\n95% WaldConfidence Limits\n\n\n\n\n\n\nlbp_int\n\n\n3.012\n\n\n&lt;0.001\n\n\n&gt;999.999\n\n\n\n\ntxti2\n\n\n0.406\n\n\n0.224\n\n\n0.737\n\n\n\n\n\n\n\n\nROC Model: ROC1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n-2 Log L\n\n\n=\n\n\n98.300\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnalysis of Maximum Likelihood Estimates\n\n\n\n\nParameter\n\n\nDF\n\n\nEstimate\n\n\nStandardError\n\n\nWaldChi-Square\n\n\nPr &gt; ChiSq\n\n\n\n\n\n\nIntercept\n\n\n1\n\n\n0.0846\n\n\n0.2376\n\n\n0.1267\n\n\n0.7219\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nROC Association Statistics\n\n\n\n\nROC Model\n\n\nMann-Whitney\n\n\nSomers' D\n\n\nGamma\n\n\nTau-a\n\n\n\n\nArea\n\n\nStandardError\n\n\n95% WaldConfidence Limits\n\n\n\n\n\n\nModel\n\n\n0.7162\n\n\n0.0609\n\n\n0.5969\n\n\n0.8355\n\n\n0.4324\n\n\n0.4324\n\n\n0.2189\n\n\n\n\nROC1\n\n\n0.5000\n\n\n0\n\n\n0.5000\n\n\n0.5000\n\n\n0\n\n\n.\n\n\n0\n\n\n\n\n\n\n\n\n\n\n\n\u001457                                                         The SAS System                    Saturday, December 28, 2024 08:59:00 PM\n\n624        ods listing close;ods html5 (id=saspy_internal) file=_tomods1 options(bitmap_mode='inline') device=svg style=HTMLBlue;\n624      ! ods graphics on / outputfmt=png;\n625        \n626        **** Model 2B;\n627        ODS EXCLUDE ModelInfo NOBS ResponseProfile ConvergenceStatus ROCcurve;\n628        /* Calculate ROC curve and AUC for the model */\n629        PROC LOGISTIC DATA = all_results;\n630            MODEL progression(EVENT = 'Fast') = lbp_int txti2;\n631            OUTPUT OUT = Model2B PREDICTED = Model2Bpred;\n632            ROC;\n633        RUN;\n634        \n635        \n636        ods html5 (id=saspy_internal) close;ods listing;\n637        \n\u001458                                                         The SAS System                    Saturday, December 28, 2024 08:59:00 PM\n\n638        \n\n\n\n\nTop of Tabset\n\n\n\nThe AUC for the model with only MRI image features was 0.72, MUCH better performance than the model with just baseline kidney volume.\nTop of Tabset"
  },
  {
    "objectID": "Project_3_Predictive_Modelling/Project_3_SAS/Code/Project_3_Predictive_Modelling_SAS.html#2C",
    "href": "Project_3_Predictive_Modelling/Project_3_SAS/Code/Project_3_Predictive_Modelling_SAS.html#2C",
    "title": "Predictive Modelling (SAS)",
    "section": "Model 2C: Baseline Kidney Volume and MRI Image Features",
    "text": "Model 2C: Baseline Kidney Volume and MRI Image Features\nTo answer the researcher’s third question, we will run a predictive model that uses the baseline height-corrected total kidney volume AND MRI image features to predict slow vs fast disease progression.\n\\[ logit(Progression) = 𝛽_0 + 𝛽_1*Baseline Kidney Volume + 𝛽_2*Txti2 + 𝛽_3*LbpInt ... + e \\]\n\nAnalysisSummary\n\n\n\nTrain the Model\n\n* Train Model;\n%cross_validate(data=Proj3.Data_scaled, target=progression, predictors=tkvht_base lbp_int txti2);\n\n\n\nAcquire AUC and ROC Curve\n\n**** Model 2C;\nODS EXCLUDE ModelInfo NOBS ResponseProfile ConvergenceStatus ROCcurve;\n/* Calculate ROC curve and AUC for the model */\nPROC LOGISTIC DATA = all_results;\n    MODEL progression(EVENT = 'Fast') = tkvht_base lbp_int txti2;\n    OUTPUT OUT = Model2C PREDICTED = Model2Cpred;\n    ROC;\nRUN;\n\nlstlog\n\n\n&lt;!DOCTYPE html&gt;\n\n\n\n\n\nSAS Output\n\n\n\n\n\n\n\nResiduals vs. Predicted Values\n\n\n\n\nThe LOGISTIC Procedure\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel Fit Statistics\n\n\n\n\nCriterion\n\n\nIntercept Only\n\n\nIntercept and Covariates\n\n\n\n\n\n\nAIC\n\n\n100.300\n\n\n93.520\n\n\n\n\nSC\n\n\n102.563\n\n\n102.571\n\n\n\n\n-2 Log L\n\n\n98.300\n\n\n85.520\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTesting Global Null Hypothesis: BETA=0\n\n\n\n\nTest\n\n\nChi-Square\n\n\nDF\n\n\nPr &gt; ChiSq\n\n\n\n\n\n\nLikelihood Ratio\n\n\n12.7797\n\n\n3\n\n\n0.0051\n\n\n\n\nScore\n\n\n11.6079\n\n\n3\n\n\n0.0089\n\n\n\n\nWald\n\n\n9.3021\n\n\n3\n\n\n0.0255\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnalysis of Maximum Likelihood Estimates\n\n\n\n\nParameter\n\n\nDF\n\n\nEstimate\n\n\nStandardError\n\n\nWaldChi-Square\n\n\nPr &gt; ChiSq\n\n\n\n\n\n\nIntercept\n\n\n1\n\n\n0.5218\n\n\n1.0109\n\n\n0.2665\n\n\n0.6057\n\n\n\n\ntkvht_base\n\n\n1\n\n\n-0.00098\n\n\n0.00135\n\n\n0.5218\n\n\n0.4701\n\n\n\n\nlbp_int\n\n\n1\n\n\n1.1211\n\n\n7.1631\n\n\n0.0245\n\n\n0.8756\n\n\n\n\ntxti2\n\n\n1\n\n\n-0.9170\n\n\n0.3041\n\n\n9.0948\n\n\n0.0026\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOdds Ratio Estimates\n\n\n\n\nEffect\n\n\nPoint Estimate\n\n\n95% WaldConfidence Limits\n\n\n\n\n\n\ntkvht_base\n\n\n0.999\n\n\n0.996\n\n\n1.002\n\n\n\n\nlbp_int\n\n\n3.068\n\n\n&lt;0.001\n\n\n&gt;999.999\n\n\n\n\ntxti2\n\n\n0.400\n\n\n0.220\n\n\n0.725\n\n\n\n\n\n\n\n\nROC Model: ROC1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n-2 Log L\n\n\n=\n\n\n98.300\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnalysis of Maximum Likelihood Estimates\n\n\n\n\nParameter\n\n\nDF\n\n\nEstimate\n\n\nStandardError\n\n\nWaldChi-Square\n\n\nPr &gt; ChiSq\n\n\n\n\n\n\nIntercept\n\n\n1\n\n\n0.0846\n\n\n0.2376\n\n\n0.1267\n\n\n0.7219\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nROC Association Statistics\n\n\n\n\nROC Model\n\n\nMann-Whitney\n\n\nSomers' D\n\n\nGamma\n\n\nTau-a\n\n\n\n\nArea\n\n\nStandardError\n\n\n95% WaldConfidence Limits\n\n\n\n\n\n\nModel\n\n\n0.7202\n\n\n0.0608\n\n\n0.6010\n\n\n0.8394\n\n\n0.4404\n\n\n0.4404\n\n\n0.2229\n\n\n\n\nROC1\n\n\n0.5000\n\n\n0\n\n\n0.5000\n\n\n0.5000\n\n\n0\n\n\n.\n\n\n0\n\n\n\n\n\n\n\n\n\n\n\n\u001461                                                         The SAS System                    Saturday, December 28, 2024 08:59:00 PM\n\n652        ods listing close;ods html5 (id=saspy_internal) file=_tomods1 options(bitmap_mode='inline') device=svg style=HTMLBlue;\n652      ! ods graphics on / outputfmt=png;\n653        \n654        **** Model 2C;\n655        ODS EXCLUDE ModelInfo NOBS ResponseProfile ConvergenceStatus ROCcurve;\n656        /* Calculate ROC curve and AUC for the model */\n657        PROC LOGISTIC DATA = all_results;\n658            MODEL progression(EVENT = 'Fast') = tkvht_base lbp_int txti2;\n659            OUTPUT OUT = Model2C PREDICTED = Model2Cpred;\n660            ROC;\n661        RUN;\n662        \n663        \n664        ods html5 (id=saspy_internal) close;ods listing;\n665        \n\u001462                                                         The SAS System                    Saturday, December 28, 2024 08:59:00 PM\n\n666        \n\n\n\n\nTop of Tabset\n\n\n\nThe AUC for the model including both baseline kidney volume and MRI image features is 0.72.\nAdding baseline kidney volume did not improve the predictive power above and beyond that of using MRI image features alone.\nTop of Tabset"
  },
  {
    "objectID": "Project_3_Predictive_Modelling/Project_3_SAS/Code/Project_3_Predictive_Modelling_SAS.html#model-comparison-1",
    "href": "Project_3_Predictive_Modelling/Project_3_SAS/Code/Project_3_Predictive_Modelling_SAS.html#model-comparison-1",
    "title": "Predictive Modelling (SAS)",
    "section": "Model Comparison",
    "text": "Model Comparison\nTable 2. Results of Model Performance Predicting Disease Progression using AUC\n\n\n\nModel\nAUC\n\n\n\n\nBaseline Kidney Volume\n0.52\n\n\nMRI Image Features\n0.72\n\n\nBaseline Kidney Volume and MRI\n0.72"
  },
  {
    "objectID": "Project_3_Predictive_Modelling/Project_3_SAS/Code/Project_3_Predictive_Modelling_SAS.html#conclusion-1",
    "href": "Project_3_Predictive_Modelling/Project_3_SAS/Code/Project_3_Predictive_Modelling_SAS.html#conclusion-1",
    "title": "Predictive Modelling (SAS)",
    "section": "Conclusion",
    "text": "Conclusion\nThe model including MRI images features increased the AUC by 0.20, when compared to the model using baseline kidney volume alone. Including baseline kidney volume did not increase predictive power above that of just using the MRI image features alone."
  },
  {
    "objectID": "Project_3_Predictive_Modelling/Project_3_SAS/Code/Project_3_Predictive_Modelling_SAS.html#roc-curves",
    "href": "Project_3_Predictive_Modelling/Project_3_SAS/Code/Project_3_Predictive_Modelling_SAS.html#roc-curves",
    "title": "Predictive Modelling (SAS)",
    "section": "ROC Curves",
    "text": "ROC Curves\nCode adapted from here\n\n\nCode\n*** Final ROC Curves PLOT;\n* Merge Data sets;\nDATA ROC;\n   MERGE Model2A Model2B Model2C;\nRUN;\n\n\n* overlay two or more ROC curves by using variables of predicted values;\nPROC LOGISTIC DATA = ROC;\n   MODEL progression(EVENT='Fast') = Model2APred Model2BPred Model2CPred / NOFIT;\n   ROC 'Model2A' PRED=Model2APred;\n   ROC 'Model2B'   PRED=Model2BPred;\n   ROC 'Model2C'    PRED=Model2CPred;\n   ODS SELECT ROCOverlay;\n   /* optional: for a statistical comparison, use ROCCONTRAST stmt and remove the ODS SELECT stmt */\n   *roccontrast reference('Expert Model') / estimate e;\nRUN;\n\n\nlstlog\n\n\n&lt;!DOCTYPE html&gt;\n\n\n\n\n\nSAS Output\n\n\n\n\n\n\n\nResiduals for Model 1C Predicting Change in Kidney Volume Size\n\n\n\n\nThe LOGISTIC Procedure\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u001465                                                         The SAS System                    Saturday, December 28, 2024 08:59:00 PM\n\n686        ods listing close;ods html5 (id=saspy_internal) file=_tomods1 options(bitmap_mode='inline') device=svg style=HTMLBlue;\n686      ! ods graphics on / outputfmt=png;\n687        \n688        *** Final ROC Curves PLOT;\n689        * Merge Data sets;\n690        DATA ROC;\n691           MERGE Model2A Model2B Model2C;\n692        RUN;\n693        \n694        \n695        * overlay two or more ROC curves by using variables of predicted values;\n696        PROC LOGISTIC DATA = ROC;\n697           MODEL progression(EVENT='Fast') = Model2APred Model2BPred Model2CPred / NOFIT;\n698           ROC 'Model2A' PRED=Model2APred;\n699           ROC 'Model2B'   PRED=Model2BPred;\n700           ROC 'Model2C' PRED=Model2CPred;\n701           ODS SELECT ROCOverlay;\n702           /* optional: for a statistical comparison, use ROCCONTRAST stmt and remove the ODS SELECT stmt */\n703           *roccontrast reference('Expert Model') / estimate e;\n704        RUN;\n705        \n706        \n707        ods html5 (id=saspy_internal) close;ods listing;\n708        \n\u001466                                                         The SAS System                    Saturday, December 28, 2024 08:59:00 PM\n\n709"
  },
  {
    "objectID": "SetUpSAS.html",
    "href": "SetUpSAS.html",
    "title": "saspy test",
    "section": "",
    "text": "This is a test to get SAS up and running in Quarto\nI need to create two files:\n\nsascfg_personal.py contains details on how to connect to a SAS instance\n_authinfo contains authentication details\n\nprint(\"Hello World\")\nimport saspy\nsas_session = saspy.SASsession(cfgfile=\"C:\\\\Users\\\\sviea\\\\anaconda3\\\\Lib\\\\site-packages\\\\saspy\\\\sascfg_personal.py\")\nsas_session\ninstall.packages(\"configSAS\")\n\n\nCode\n#| capture: log\nproc candisc data=sashelp.iris out=outcan distance anova;\n   class Species;\n   var SepalLength SepalWidth PetalLength PetalWidth;\nrun;\n\n\n\u00145                                                          The SAS System                    Saturday, December 28, 2024 09:00:00 PM\n\n24         ods listing close;ods html5 (id=saspy_internal) file=_tomods1 options(bitmap_mode='inline') device=svg style=HTMLBlue;\n24       ! ods graphics on / outputfmt=png;\n25         \n26         #| capture: log\n           _\n           180\nERROR 180-322: Statement is not valid or it is used out of proper order.\n\n27         proc candisc data=sashelp.iris out=outcan distance anova;\n28            class Species;\n              _____\n              180\nERROR 180-322: Statement is not valid or it is used out of proper order.\n\n29            var SepalLength SepalWidth PetalLength PetalWidth;\n              ___\n              180\nERROR 180-322: Statement is not valid or it is used out of proper order.\n\n30         run;\n31         \n32         \n33         ods html5 (id=saspy_internal) close;ods listing;\n34         \n\u00146                                                          The SAS System                    Saturday, December 28, 2024 09:00:00 PM\n\n35         \n\n\nERROR 180-322: Statement is not valid or it is used out of proper order. None\n\n\nresults = sas_session.submit(\"\"\"\ndata BPressure;\n    length PatientID $2;\n    input PatientID $ Systolic Diastolic @@;\n    datalines;\nCK 120 50  SS 96  60 FR 100 70\nCP 120 75  BL 140 90 ES 120 70\nCP 165 110 JI 110 40 MC 119 66\nFC 125 76  RW 133 60 KD 108 54\nDS 110 50  JW 130 80 BH 120 65\nJW 134 80  SB 118 76 NS 122 78\nGS 122 70  AB 122 78 EC 112 62\nHH 122 82\n;\n\ntitle 'Systolic and Diastolic Blood Pressure';\nods select BasicMeasures Quantiles;\nproc univariate data=BPressure;\n   var Systolic Diastolic;\nrun;\n\"\"\")\nfrom IPython.display import display, HTML\ndef show(x):\n    display(HTML(\"&lt;h6&gt;Log&lt;/h6&gt;\"))\n    print(x['LOG'])\n    display(HTML(\"&lt;h6&gt;Output&lt;/h6&gt;\"))\n    display(HTML(x.get('LST', '')))\nshow(results)\nimport sas_kernel\nll = sas_session.submit(\"\"\"\nlibname work list;\n\nproc sql;\n   select type, count(*) as 'number of models'n, avg(MPG_city) as 'Avg(MPG_City)'n\n   from sashelp.cars\n   group by type\n   order by 3 desc;\nquit; \n\"\"\")\nll.keys()\nsas_session.HTML(ll['LST'])\nll = sas_session.submitLST(\"\"\"\nlibname work list;\n\nproc sql;\n   select type, count(*) as 'number of models'n, avg(MPG_city) as 'Avg(MPG_City)'n\n   from sashelp.cars\n   group by type\n   order by 3 desc;\nquit; \n\"\"\")\nresult = sas_session.submit(\"proc print data=sashelp.class; run;\")\nprint(result['LOG'])\ndata_sas = sas_session.sasdata('cars', 'sashelp')\ndata_sas.means()\ndata_sas.bar('EngineSize')\nfrom IPython.display import HTML, display\nimport saspy\n\n# Start SAS session\nsas = saspy.SASsession(cfgname='oda')\n\n# Submit SAS code\nresult = sas.submit(\"proc print data=sashelp.class; run;\")\n\n# Display the output in the Quarto document\ndisplay(HTML(result['LST']))\n\n\nCode\nproc candisc data=sashelp.iris out=outcan distance anova;\n   class Species;\n   var SepalLength SepalWidth PetalLength PetalWidth;\nrun;\n\n\n\n\n\n\n\nSAS Output\n\n\n\n\n\nThe SAS System \n\n\nThe CANDISC Procedure\n\n\n\n\n\n\nTotal Sample Size\n150\nDF Total\n149\n\n\nVariables\n4\nDF Within Classes\n147\n\n\nClasses\n3\nDF Between Classes\n2\n\n\n\n\n\n\n\n\n\n\nNumber of Observations Read\n150\n\n\nNumber of Observations Used\n150\n\n\n\n\n\n\n\n\n\n\nClass Level Information\n\n\nSpecies\nVariable\nName\nFrequency\nWeight\nProportion\n\n\n\n\nSetosa\nSetosa\n50\n50.0000\n0.333333\n\n\nVersicolor\nVersicolor\n50\n50.0000\n0.333333\n\n\nVirginica\nVirginica\n50\n50.0000\n0.333333\n\n\n\n\n\n\n\nThe SAS System \n\n\nThe CANDISC Procedure\n\n\n\n\n\n\n\nSquared Distance to Species\n\n\nFrom Species\nSetosa\nVersicolor\nVirginica\n\n\n\n\nSetosa\n0\n89.86419\n179.38471\n\n\nVersicolor\n89.86419\n0\n17.20107\n\n\nVirginica\n179.38471\n17.20107\n0\n\n\n\n\n\n\n\n\n\n\nF Statistics, NDF=4, DDF=144 for Squared Distance to Species\n\n\nFrom Species\nSetosa\nVersicolor\nVirginica\n\n\n\n\nSetosa\n0\n550.18889\n1098\n\n\nVersicolor\n550.18889\n0\n105.31265\n\n\nVirginica\n1098\n105.31265\n0\n\n\n\n\n\n\n\n\n\n\nProb &gt; Mahalanobis Distance for Squared Distance to Species\n\n\nFrom Species\nSetosa\nVersicolor\nVirginica\n\n\n\n\nSetosa\n1.0000\n&lt;.0001\n&lt;.0001\n\n\nVersicolor\n&lt;.0001\n1.0000\n&lt;.0001\n\n\nVirginica\n&lt;.0001\n&lt;.0001\n1.0000\n\n\n\n\n\n\n\n\nThe SAS System \n\n\nThe CANDISC Procedure\n\n\n\n\n\n\nUnivariate Test Statistics\n\n\nF Statistics, Num DF=2, Den DF=147\n\n\nVariable\nLabel\nTotal\nStandard\nDeviation\nPooled\nStandard\nDeviation\nBetween\nStandard\nDeviation\nR-Square\nR-Square\n/ (1-RSq)\nF Value\nPr &gt; F\n\n\n\n\nSepalLength\nSepal Length (mm)\n8.2807\n5.1479\n7.9506\n0.6187\n1.6226\n119.26\n&lt;.0001\n\n\nSepalWidth\nSepal Width (mm)\n4.3587\n3.3969\n3.3682\n0.4008\n0.6688\n49.16\n&lt;.0001\n\n\nPetalLength\nPetal Length (mm)\n17.6530\n4.3033\n20.9070\n0.9414\n16.0566\n1180.16\n&lt;.0001\n\n\nPetalWidth\nPetal Width (mm)\n7.6224\n2.0465\n8.9673\n0.9289\n13.0613\n960.01\n&lt;.0001\n\n\n\n\n\n\n\n\n\n\nAverage R-Square\n\n\n\n\nUnweighted\n0.7224358\n\n\nWeighted by Variance\n0.8689444\n\n\n\n\n\n\n\n\n\n\nMultivariate Statistics and F Approximations\n\n\nS=2 M=0.5 N=71\n\n\nStatistic\nValue\nF Value\nNum DF\nDen DF\nPr &gt; F\n\n\n\n\nWilks' Lambda\n0.02343863\n199.15\n8\n288\n&lt;.0001\n\n\nPillai's Trace\n1.19189883\n53.47\n8\n290\n&lt;.0001\n\n\nHotelling-Lawley Trace\n32.47732024\n582.20\n8\n203.4\n&lt;.0001\n\n\nRoy's Greatest Root\n32.19192920\n1166.96\n4\n145\n&lt;.0001\n\n\n\nNOTE: F Statistic for Roy's Greatest Root is an upper bound.\n\n\nNOTE: F Statistic for Wilks' Lambda is exact.\n\n\n\n\n\n\n\n\nThe SAS System \n\n\nThe CANDISC Procedure\n\n\n\n\n\n\n\n \nCanonical\nCorrelation\nAdjusted\nCanonical\nCorrelation\nApproximate\nStandard\nError\nSquared\nCanonical\nCorrelation\nEigenvalues of Inv(E)*H\n= CanRsq/(1-CanRsq)\nTest of H0: The canonical correlations in the current row and all that follow are zero\n\n\n \nEigenvalue\nDifference\nProportion\nCumulative\nLikelihood\nRatio\nApproximate\nF Value\nNum DF\nDen DF\nPr &gt; F\n\n\n\n\n1\n0.984821\n0.984508\n0.002468\n0.969872\n32.1919\n31.9065\n0.9912\n0.9912\n0.02343863\n199.15\n8\n288\n&lt;.0001\n\n\n2\n0.471197\n0.461445\n0.063734\n0.222027\n0.2854\n \n0.0088\n1.0000\n0.77797337\n13.79\n3\n145\n&lt;.0001\n\n\n\n\n\n\n\nThe SAS System \n\n\nThe CANDISC Procedure\n\n\n\n\n\n\n\nTotal Canonical Structure\n\n\nVariable\nLabel\nCan1\nCan2\n\n\n\n\nSepalLength\nSepal Length (mm)\n0.791888\n0.217593\n\n\nSepalWidth\nSepal Width (mm)\n-0.530759\n0.757989\n\n\nPetalLength\nPetal Length (mm)\n0.984951\n0.046037\n\n\nPetalWidth\nPetal Width (mm)\n0.972812\n0.222902\n\n\n\n\n\n\n\n\n\n\nBetween Canonical Structure\n\n\nVariable\nLabel\nCan1\nCan2\n\n\n\n\nSepalLength\nSepal Length (mm)\n0.991468\n0.130348\n\n\nSepalWidth\nSepal Width (mm)\n-0.825658\n0.564171\n\n\nPetalLength\nPetal Length (mm)\n0.999750\n0.022358\n\n\nPetalWidth\nPetal Width (mm)\n0.994044\n0.108977\n\n\n\n\n\n\n\n\n\n\nPooled Within Canonical Structure\n\n\nVariable\nLabel\nCan1\nCan2\n\n\n\n\nSepalLength\nSepal Length (mm)\n0.222596\n0.310812\n\n\nSepalWidth\nSepal Width (mm)\n-0.119012\n0.863681\n\n\nPetalLength\nPetal Length (mm)\n0.706065\n0.167701\n\n\nPetalWidth\nPetal Width (mm)\n0.633178\n0.737242\n\n\n\n\n\n\n\n\nThe SAS System \n\n\nThe CANDISC Procedure\n\n\n\n\n\n\n\nTotal-Sample Standardized Canonical Coefficients\n\n\nVariable\nLabel\nCan1\nCan2\n\n\n\n\nSepalLength\nSepal Length (mm)\n-0.686779533\n0.019958173\n\n\nSepalWidth\nSepal Width (mm)\n-0.668825075\n0.943441829\n\n\nPetalLength\nPetal Length (mm)\n3.885795047\n-1.645118866\n\n\nPetalWidth\nPetal Width (mm)\n2.142238715\n2.164135931\n\n\n\n\n\n\n\n\n\n\nPooled Within-Class Standardized Canonical Coefficients\n\n\nVariable\nLabel\nCan1\nCan2\n\n\n\n\nSepalLength\nSepal Length (mm)\n-.4269548486\n0.0124075316\n\n\nSepalWidth\nSepal Width (mm)\n-.5212416758\n0.7352613085\n\n\nPetalLength\nPetal Length (mm)\n0.9472572487\n-.4010378190\n\n\nPetalWidth\nPetal Width (mm)\n0.5751607719\n0.5810398645\n\n\n\n\n\n\n\n\n\n\nRaw Canonical Coefficients\n\n\nVariable\nLabel\nCan1\nCan2\n\n\n\n\nSepalLength\nSepal Length (mm)\n-.0829377642\n0.0024102149\n\n\nSepalWidth\nSepal Width (mm)\n-.1534473068\n0.2164521235\n\n\nPetalLength\nPetal Length (mm)\n0.2201211656\n-.0931921210\n\n\nPetalWidth\nPetal Width (mm)\n0.2810460309\n0.2839187853\n\n\n\n\n\n\n\n\n\n\n\nClass Means on Canonical Variables\n\n\nSpecies\nCan1\nCan2\n\n\n\n\nSetosa\n-7.607599927\n0.215133017\n\n\nVersicolor\n1.825049490\n-0.727899622\n\n\nVirginica\n5.782550437\n0.512766605"
  },
  {
    "objectID": "Statistical_Consulting/Vieau_Project_01/Code/Megan_Watson_Project.html",
    "href": "Statistical_Consulting/Vieau_Project_01/Code/Megan_Watson_Project.html",
    "title": "Assessing the Impact of a Physical Therapy Quality-Improvement Initiative at a Colorado University Hospital",
    "section": "",
    "text": "This is the Quarto markdown for the live consultation project in BIOS 6621: Statistical Consulting.\nThis is a research project encompassing the entire data analysis pipeline, from initial consultation to scope of work writing, execution of statistical analysis, and completion of project deliverables.\nFor this study we run a series of negative binomial regressions to assess the success of a physical therapy quality improvement initiative at Anschutz Hospital, finding significant results and drastic improvements the form of: decreased MICU length of stay, decreased mechanical ventilation times, and decreased overall hospital length of stay!\nWe also discover an interesting interaction, where the QI-initiative improved outcomes for mechanically ventilated patients in particular."
  },
  {
    "objectID": "Statistical_Consulting/Vieau_Project_01/Code/Megan_Watson_Project.html#general-information",
    "href": "Statistical_Consulting/Vieau_Project_01/Code/Megan_Watson_Project.html#general-information",
    "title": "Assessing the Impact of a Physical Therapy Quality-Improvement Initiative at a Colorado University Hospital",
    "section": "General Information",
    "text": "General Information\n\n\n\n\n\n\n\n\n\nInvestigator\nMegan Watson\nDate\nSeptember 30, 2024\n\n\nProject Title\nQuality Improvement into Standard Practice: Persistent Practice Changes and Decreased Length of Stay for 3 years following a Medical ICU Physical Therapy Quality Improvement Project"
  },
  {
    "objectID": "Statistical_Consulting/Vieau_Project_01/Code/Megan_Watson_Project.html#project-description",
    "href": "Statistical_Consulting/Vieau_Project_01/Code/Megan_Watson_Project.html#project-description",
    "title": "Assessing the Impact of a Physical Therapy Quality-Improvement Initiative at a Colorado University Hospital",
    "section": "Project Description",
    "text": "Project Description\n\nBackground\n             The objective of this study is to assess the impact of a MICU physical therapy (PT) quality improvement (QI) project on physical therapist practice, and quantify the changes in length of stay (LOS) and mechanical ventilation (MV) time. The goals of the QI Initiative were to increase percentage of MICU admissions receiving PT, decrease time from MICU admission to first PT, and increase the frequency of PT visits. The main question the researcher came to us with was: how to correctly model the longitudinal data in the study?\n\n\nStudy Design\n              This was a retrospective cohort study, comparing 3 main time periods: Pre-QI Initiative (before April 2015), During QI Initiative (April 2015-Dec 2015), and After-QI Initiative (Jan 2016 and later). The QI initiative was deployed during the 9-month period between April 2015 and December 2015, during which time there were education and staffing changes. Patients were stratified as either having mechanical ventilation or not having mechanical ventilation. The primary outcomes of interest were MICU LOS and time on MV. Secondary outcomes of interest were the non-ICU LOS, total hospital LOS, and discharge location.\n\n\nDescription of the Data\n              Data was received as a de-identified .csv. Data points consist of: Admit and Discharge Time, PT Consult Time, Admit to Consult Time, First Therapy Time, Consult to Therapy Time, Admit to Therapy Time, Total Therapy Visits, Days with Therapy Visits, Average, min, and Max Therapy Length, Admit and Discharge Date/Time, Total MV Days, Hospital LOS, Patient Age, Sex, “Problem List”, and Codes (presumably ICD9/10).\n\n\nAnticipated Sample Size / Study Population\n              The sample size is N = ~3000-4000, consisting of patients admitted to the MICU with a LOS between 2 and 30 days (I.e. exclude &lt; 2 days or &gt; 30 days). This was a single site study performed here at Colorado University.\n\n\nHypothesis\n              The hypothesis for the study was that LOS and time on MV would decrease when comparing the pre- and intra-initiative time periods, and when comparing the pre- and post-initiative time periods. Clinical significance for this outcome is considered as a 1-2 day change in LOS. The intended end product of this project is a publication in a PT journal.\n\n\nAnalysis Plan (Sketch)\n              The next steps are to perform rigorous analyses on each outcome variable to determine if the observed differences are statistically significant. We will perform a Poisson or Negative Binomial Regression with QI-initiative time period as the independent variable, and each respective outcome variable as the dependent variable, resulting in 7 individual models:\n\nLog( E(% of admissions receiving PTi | QI-Initiativei ) ) = β0 + β1 * QI-Initiativei + β2 * Covariatesi + …\nLog( E(Time before First PTi | QI-Initiativei) ) = β0 + β1 * QI-Initiativei + β2 * Covariates + …\nLog( E(Frequency of PT visitsi | QI-Initiativei) ) = β0 + β1 * QI-Initiativei + β2 * Corvariatesi + …\nLog( E(Unit LOSi | QI-Initiativei ) ) = β0 + β1 * QI-Initiativei + β2 * Covariatesi + …\nLog( E(MV Totali | QI-Initiativei ) ) = β0 + β1 * QI-Initiativei + β2 * Covariatesi + …\nLog( E(Non-ICU LOSi | QI-Initiativei ) ) = β0 + β1 * QI-Initiativei + β2 * Covariatesi + …\nLog( E(Hospital LOSi | QI-Initiativei ) ) = β0+ β1 * QI-Initiativei + β2 * Covariatesi + …\n\nPerforming these analyses as a Poisson or Negative Binomial Regression allows us to account for the right-skewness of these count variables. A Negative Binomial Regression will be performed in the case of overdispersion, where the variance is greater than the mean"
  },
  {
    "objectID": "Statistical_Consulting/Vieau_Project_01/Code/Megan_Watson_Project.html#project-deliverables",
    "href": "Statistical_Consulting/Vieau_Project_01/Code/Megan_Watson_Project.html#project-deliverables",
    "title": "Assessing the Impact of a Physical Therapy Quality-Improvement Initiative at a Colorado University Hospital",
    "section": "Project Deliverables",
    "text": "Project Deliverables\n             At this stage, Megan has only asked for insight into what method to use to account for variance in time for her analysis. There are currently no other deliverables.\nTimeline / Deadlines\nAt this time, Megan has not asked for data analysis efforts from us.\nShould this change, the anticipated project start date will be 1 week after our next meeting with her, where revisions to the Comprehensive Analysis Report are also expected to be completed.  The assigned analysts will reach out to the investigator to confirm the project timeframe and the project start date may be altered if necessary. 2 weeks will be allowed for initial data analysis by the assigned analyst.\nA follow-up meeting is to occur no later than 1 month after the next meeting with Megan. During the follow-up meeting, the project team will discuss and preliminary results of the analysis, mockup the desired tables and figures (1-2 descriptive tables, 1-2 analysis results tables and up to 4 graphs), and discuss next steps. During the follow-up meeting, the team will establish a meeting schedule for the remainder of the project. \nEstimated Cost\nThis represents our best estimate of the effort needed to complete the project. Should the scope of work change substantially, the analyst(s) will discuss changes with the investigator and issue a new or amended project agreement.\n\n\n\n\nEffort\n\n\n\n\nMonths\nPhD Faculty\nMaster’s Faculty\nStudent\n\n\n\n\n\n\n\n\n1\n20%\n0%\n80%\n\n\n\n\nTotal Costs\n’Bout Tree Fiddy"
  },
  {
    "objectID": "Statistical_Consulting/Vieau_Project_01/Code/Megan_Watson_Project.html#missingness",
    "href": "Statistical_Consulting/Vieau_Project_01/Code/Megan_Watson_Project.html#missingness",
    "title": "Assessing the Impact of a Physical Therapy Quality-Improvement Initiative at a Colorado University Hospital",
    "section": "Missingness",
    "text": "Missingness\n\ngg_miss_var(data)\n\n\n\n\n\n\n\n\nWe are missing the most data in MV_Total , followed by CONSULT TO TREAT, ADMIT TO TREAT, and PROBLEM LIST DX CODES. Missingness does not seem to be an issue for the rest of the variables.\n\nvis_miss(data)\n\n\n\n\n\n\n\n\nImmediately concerning is the 65% missing data on MV_TOTAL, the variable for total time on mechanical ventilation, which happens to me a main outcome of interest.\nCONSULT TO TREAT has 56% missing and ADMIT TO TREAT has 55% missing. This could be because, in emergency situations where immediate critical care is required, patients might be directly admitted to the MICU without a formal consult.\nPROBLEM LIST DX CODES and PROBLEM LIST DX NAME have 25% and 20% missing, respectively. Odd that their missingness doesn’t align exactly. This isn’t actually too bad, and our sample size is large enough that we may still be able to include these variables in our analysis if we wanted to. We’d just have to do some processing to place patients into large enough overarching diagnosis categories that we’d have enough patients in each group. Not the researcher’s primary research question however so not looking any further into it. We’d also have to examine missingness and see if there are and patterns or it’s MCAR.\n\nSummary\nBased on the initial missingness checks, we will exclude variables with excessive missingness.\nThese variables will be retained for reporting in Table 1 for the researcher, but will be excluded from consideration in the analysis.\n\nConsult to Treat and Admit to Treat are missing &gt;= 55% of values, and will thus be excluded from analysis.\nProblem List DX Codes is missing 25% and Problem List DX Names is missing 20% of values. Additionally, it would be challenging to include these variables in the analysis in a meaningful way due to sample size limitations, so these will be excluded from consideration in the final analysis.\nAdmit to Consult is missing 23% of values.\n\nAll other variables were missing &lt; 1% and thus missingness does not need to be addressed for these variables."
  },
  {
    "objectID": "Statistical_Consulting/Vieau_Project_01/Code/Megan_Watson_Project.html#data-quality-check",
    "href": "Statistical_Consulting/Vieau_Project_01/Code/Megan_Watson_Project.html#data-quality-check",
    "title": "Assessing the Impact of a Physical Therapy Quality-Improvement Initiative at a Colorado University Hospital",
    "section": "Data Quality Check",
    "text": "Data Quality Check\nLet’s perform some simple min/max checks to assess if data has been entered correctly.\n\n# This function summarizes the mins and maxes of numeric variables\nsummarize_column &lt;- function(column) {\n  if (is.numeric(column)) {\n    return(data.frame(\n      Type = \"Numeric\",\n      Min = min(column, na.rm = TRUE),\n      Max = max(column, na.rm = TRUE)\n    ))\n  }\n}\n\n# Apply the function to each column and bind the results into a single data frame\nsummary_df &lt;- map_dfr(data, summarize_column, .id = \"Column\") %&gt;%\n  mutate(across(everything(), ~ format(., scientific = FALSE))) # Eliminates scientific notation\n\n# Pretty print the mins and maxes of longform data_2\nkable(summary_df, format = \"html\") %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\"))\n\n\n\n\nColumn\nType\nMin\nMax\n\n\n\n\nid\nNumeric\n1\n13109\n\n\nMV_Total\nNumeric\n1\n75\n\n\nHOSPITAL LOS\nNumeric\n4\n9270\n\n\nADMIT TO CONSULT\nNumeric\n0\n1172\n\n\nCONSULT TO TREAT\nNumeric\n-405\n3046\n\n\nADMIT TO TREAT\nNumeric\n-1\n983\n\n\nTOTAL THERAPY VISITS\nNumeric\n0\n69\n\n\nUnit_LOS\nNumeric\n1\n1811\n\n\nDAYS WITH THERAPY VISITS\nNumeric\n0\n33\n\n\nPERCENT OF DAYS WITH THERAPY VISITS\nNumeric\n0\n100\n\n\nAVG THERAPY LENGTH\nNumeric\n-1970\n2144\n\n\nMAX THERAPY LENGTH\nNumeric\n-85\n4744\n\n\nMIN THERAPY LENGTH\nNumeric\n-6841\n1253\n\n\nACTUAL DAYS POST MICU\nNumeric\n0\n383\n\n\nPT or OT VISIT DAYS POST MICU\nNumeric\n0\n68\n\n\nPERCENT POST ICU DAYS THERAPY\nNumeric\n0\n100\n\n\n\n\n\n\n\nHOSPITAL LOS has a range of 4 to 9270. Presumably this is in hours, so a range of 0.01 to 386.25 days in the hospital. I believe the variable we are actually interested in for MICU LOS is UNIT LOS however, so a range of 1 to1811 hours, or 0.41 to 75.46 days. (Just realized none of us asked for a data dictionary even though it was a room of ~20 stats master’s students 😅).\nThe researcher outlined the exclusion criteria of &lt; 2 days or &gt; 30 days in the MICU for her investigation. It looks like we will have to filter the data set to match these criteria.\nCONSULT TO TREAT has a range of -405 to 3056. ADMIT TO TREAT has a range of -1 to 983. I think these variables are the time from consultation or admission until the time treatment for that patient began. Again, without a data dictionary I am unsure what to make of those negative values.\nImportantly, AVG, MAX, and MIN THERAPY LENGTH all have negative values for the minimums. This doesn’t make any sense at face value, will have to investigate and circle back to the researchers for how this data was coded.\nSome variables like MV_Total are in hours, and some like DAYS WITH THERAPY VISITS are in days. I don’t think we need to run any conversions, but this will be important to at least keep track of for interpretations.\nAside from that everything else looks coded properly."
  },
  {
    "objectID": "Statistical_Consulting/Vieau_Project_01/Code/Megan_Watson_Project.html#Filtering",
    "href": "Statistical_Consulting/Vieau_Project_01/Code/Megan_Watson_Project.html#Filtering",
    "title": "Assessing the Impact of a Physical Therapy Quality-Improvement Initiative at a Colorado University Hospital",
    "section": "Filtering",
    "text": "Filtering\nLet’s go ahead and filter the data set according to the experimenter’s exclusion criteria of &lt; 2 days or &gt; 30 days in the MICU so we are not unnecessarily investigating superfluous data in our continued examinations.\n\n# Filter data set based on researcher's exclusion criteria\ndata_ex &lt;- data[data$Unit_LOS &gt;= 48 & data$Unit_LOS &lt;= 720,]\n\n# Check dimensions\ndim(data_ex)\n\n[1] 6430   25\n\n\nWe now have a dataset of 6430 patients!"
  },
  {
    "objectID": "Statistical_Consulting/Vieau_Project_01/Code/Megan_Watson_Project.html#Clean",
    "href": "Statistical_Consulting/Vieau_Project_01/Code/Megan_Watson_Project.html#Clean",
    "title": "Assessing the Impact of a Physical Therapy Quality-Improvement Initiative at a Colorado University Hospital",
    "section": "Data Cleaning",
    "text": "Data Cleaning\nHere we will go one by one through our potential covariates and make sure they are all properly coded and there are no erroenous values.\n\nICD10 CodeDepartment Before MICUDepartment after MICUAdmit to ConsultConsult to TreatAdmit to TreatTotal Therapy VisitsDays with Therapy VisitsTherapy LengthDays Post MICUPost MICU PT or OT VisitsPost ICU Days with TherapyAgeAge QuartileAdmit Date\n\n\nNote: PROBLEM LIST DX CODES and PROBLEM LIST DX CODES have 100’s of levels comprising sundry diagnoses.\nWithout a direction for the specific use of these variables we can’t do much with them, so they will likely be excluded from the final analysis.\nTop of Tabset\n\n\n\n# Get counts\npretty_print(table(data_ex$`DEPT BEFORE Unit`))\n\n\n\n\nVar1\nFreq\n\n\n\n\nAMC ACE UNIT\n79\n\n\nAMC ADMISSION TRN UNIT\n1\n\n\nAMC AIP OR\n60\n\n\nAMC AIP PACU\n59\n\n\nAMC AIP PACU HOLD INPT\n7\n\n\nAMC AIP PERIOP SVC\n5\n\n\nAMC AIP PRE-OP\n5\n\n\nAMC AOP OR\n1\n\n\nAMC AOP PACU\n1\n\n\nAMC BMT/ONC UNIT\n232\n\n\nAMC BRONCH LAB\n5\n\n\nAMC BURN ICU\n82\n\n\nAMC CARD CATH LAB\n15\n\n\nAMC CARD EP LAB\n4\n\n\nAMC CARD MED UNIT\n9\n\n\nAMC CARD POSTPROC UNIT\n5\n\n\nAMC CARD PRE/POST PROC\n22\n\n\nAMC CARDIOTHORACIC ICU\n56\n\n\nAMC CC SURGE UNIT\n20\n\n\nAMC CICU\n39\n\n\nAMC CRITICAL CARE ANNX\n1\n\n\nAMC CTRC UNIT\n14\n\n\nAMC DIALYSIS UNIT\n10\n\n\nAMC EMERGENCY\n2962\n\n\nAMC GI PRE/POST\n9\n\n\nAMC INTRNL MED UNIT\n30\n\n\nAMC IR\n95\n\n\nAMC LABOR AND DELIVERY\n10\n\n\nAMC M/S PROG CARE UNIT\n887\n\n\nAMC MED HLTH SVC UNIT\n39\n\n\nAMC MED SPEC UNIT\n284\n\n\nAMC MED SURG UNIT\n7\n\n\nAMC MED/GYN/ONC UNIT\n131\n\n\nAMC MEDICAL ICU\n1\n\n\nAMC MOTHER BABY UNIT\n2\n\n\nAMC NEONATAL ICU\n2\n\n\nAMC NEURO ICU\n96\n\n\nAMC NEUROSCIENCES UNIT\n68\n\n\nAMC ORTHO UNIT\n34\n\n\nAMC PULM UNIT\n179\n\n\nAMC SURG SPEC UNIT\n2\n\n\nAMC SURG TRAUMA ICU\n52\n\n\nAMC SURGICAL ICU\n57\n\n\nAMC SURGICAL UNIT\n12\n\n\nAMC THRU UNIT\n15\n\n\nAMC TRANSPLANT HEP\n10\n\n\nAMC TRANSPLANT UNIT\n55\n\n\nCARDIOLOGY UNIT\n67\n\n\nEXPRESS ADMIT UNIT\n18\n\n\nGREENVAL ED\n1\n\n\nHEM/ONC UNIT\n27\n\n\nINTERNAL MEDICINE UNIT\n26\n\n\nMED SURGE HOLDING UNIT\n6\n\n\nMISSIPPI ED\n1\n\n\nOLD AMC CARD ICU\n124\n\n\nPRE-ADMISSION\n389\n\n\n\n\n\n\n\nI lack the clinical knowledge to make use of this variable.\nIt will likely not be used in the final analysis.\nTop of Tabset\n\n\n\n# Get counts\npretty_print(table(data_ex$`DEPT AFTER Unit`))\n\n\n\n\nVar1\nFreq\n\n\n\n\nAMC ACE UNIT\n323\n\n\nAMC AIP OR\n4\n\n\nAMC AIP SUITES UNIT\n1\n\n\nAMC BMT/ONC UNIT\n253\n\n\nAMC BURN ICU\n17\n\n\nAMC CARD MED UNIT\n57\n\n\nAMC CARDIOTHORACIC ICU\n23\n\n\nAMC CC SURGE UNIT\n9\n\n\nAMC CICU\n23\n\n\nAMC CRITICAL CARE ANNX\n6\n\n\nAMC DIALYSIS PRE-DISCH\n6\n\n\nAMC DIALYSIS UNIT\n1\n\n\nAMC INTRNL MED UNIT\n134\n\n\nAMC IR\n1\n\n\nAMC LABOR AND DELIVERY\n13\n\n\nAMC M/S PROG CARE UNIT\n991\n\n\nAMC MED HLTH SVC UNIT\n203\n\n\nAMC MED SPEC UNIT\n906\n\n\nAMC MED SURG UNIT\n27\n\n\nAMC MED/GYN/ONC UNIT\n264\n\n\nAMC MEDICAL ICU\n4\n\n\nAMC NEURO ICU\n26\n\n\nAMC NEUROSCIENCES UNIT\n133\n\n\nAMC ORTHO UNIT\n51\n\n\nAMC PULM UNIT\n522\n\n\nAMC SURG SPEC UNIT\n3\n\n\nAMC SURG TRAUMA ICU\n30\n\n\nAMC SURGICAL ICU\n40\n\n\nAMC SURGICAL UNIT\n36\n\n\nAMC SURGICALSURGE UNIT\n1\n\n\nAMC THRU UNIT\n38\n\n\nAMC TRANSPLANT HEP\n19\n\n\nAMC TRANSPLANT UNIT\n108\n\n\nCARDIOLOGY UNIT\n134\n\n\nEXPRESS ADMIT UNIT\n3\n\n\nINTERNAL MEDICINE UNIT\n113\n\n\nOLD AMC CARD ICU\n27\n\n\nPOST-DISCHARGE\n1880\n\n\n\n\n\n\n\nSimilarly, I lack the clinical knowledge to make use of this variable without a specific direction from the researcher. It will likely be excluded in this analysis.\nTop of Tabset\n\n\n\n# Create histogram and qqplot\ndistr_plots(data_ex, \"`ADMIT TO CONSULT`\", 25)\n\nWarning: `aes_string()` was deprecated in ggplot2 3.0.0.\nℹ Please use tidy evaluation idioms with `aes()`.\nℹ See also `vignette(\"ggplot2-in-packages\")` for more information.\n\n\nWarning: Removed 862 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\nWarning: Removed 862 rows containing non-finite outside the scale range\n(`stat_qq()`).\n\n\nWarning: Removed 862 rows containing non-finite outside the scale range\n(`stat_qq_line()`).\n\n\n\n\n\n\n\n\n\nTry a log transform.\n\n# Perform log transform\ndata_ex$ADMIT_TO_CONSULT_log &lt;- log(data_ex$`ADMIT TO CONSULT`)\n\n# Create histogram and qqplot\ndistr_plots(data_ex, \"ADMIT_TO_CONSULT_log\", 25)\n\nWarning: Removed 4021 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\nWarning: Removed 4021 rows containing non-finite outside the scale range\n(`stat_qq()`).\n\n\nWarning: Removed 4021 rows containing non-finite outside the scale range\n(`stat_qq_line()`).\n\n\n\n\n\n\n\n\n\nThis looks incredibly messy to use. And has 23% missing values. We will likely exclude.\nCould we convert it to a binary variable of whether the patient received a consult or not?\nTop of Tabset\n\n\nCONSULT TO TREAT contained negative values. Let’s explore\n\n# Create histogram for time from consult to treatment\nhist(data_ex$`CONSULT TO TREAT`)\n\n\n\n\n\n\n\n\nLooks like we have a lot of values extending below 0, so it’s not just some mis-entered data.\n\n# Sort by time from consult to treatment\nsorted_data &lt;- data_ex |&gt; \n  arrange(`CONSULT TO TREAT`) |&gt; \n  dplyr::select(id, `CONSULT TO TREAT`)\n\n# Pretty print table\npretty_print(head(sorted_data))\n\n\n\n\nid\nCONSULT TO TREAT\n\n\n\n\n10440\n-405\n\n\n11514\n-385\n\n\n7006\n-189\n\n\n9262\n-166\n\n\n11502\n-143\n\n\n11474\n-118\n\n\n\n\n\n\n\nCONSULT TO TREAT looks coded correctly. I believe it just has an event at timepoint zero, most likely admit.\n\nSummary\nCONSULT TO TREAT is coded correctly. However it has 56% missing values and will thus not be used.\nTop of Tabset\n\n\n\nThe -1 value we saw as the min for ADMIT TO TREAT makes me suspicious that’s how missing data was coded for this variable.\n\n# Create histogram for time from admit to treatment\nhist(data_ex$`ADMIT TO TREAT`)\n\n\n\n\n\n\n\n\nThe histogram shows there were very few patients with a value of -1.\n\n# Sort by time from admit to treatment\nsorted_data &lt;- data_ex %&gt;%\n  arrange(`ADMIT TO TREAT`) %&gt;%\n  dplyr::select(id, `ADMIT TO TREAT`)\n\n# Pretty print table\nkable(head(sorted_data), format = \"html\") %&gt;%\n  kable_styling(bootstrap_options = c(\"condensed\", \"hover\", \"striped\"))\n\n\n\n\nid\nADMIT TO TREAT\n\n\n\n\n8941\n-1\n\n\n9949\n-1\n\n\n11126\n0\n\n\n37\n1\n\n\n1275\n2\n\n\n3579\n2\n\n\n\n\n\n\n\nThere were only 2 patients with an admit to treatment time of -1, and 1 patient with a value of 0. Will have to double check with the experimenters but this looks believable.\n\nSummary\nThere are two patients with a -1 time to admit.\nHowever, ADMIT TO TREAT has 55% missing values and will thus not be included in the final analysis.\nTop of Tabset\n\n\n\nTOTAL THERAPY VISITS is a promising covariate.\nThose who had more therapy days could have a quicker recovery if physical therapy improves patient condition.\nOr alternatively, more therapy days could be indicative of worse physical health, and could potentially increase the outcome variables of total MV time and MICU LOS.\nEither way it is prudent to account for it in the model.\n\n# Create histogram and qqplot\ndistr_plots(data_ex, \"`TOTAL THERAPY VISITS`\", 25)\n\n\n\n\n\n\n\n\nLet’s try a log transform.\n\n# Perform log transform of average therapy length\ndata_ex$TOTAL_THERAPY_VISITS_log &lt;- log(data_ex$`TOTAL THERAPY VISITS`)\n\n# Create histogram and qqplot\ndistr_plots(data_ex, \"TOTAL_THERAPY_VISITS_log\", 8)\n\nWarning: Removed 2324 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\nWarning: Removed 2324 rows containing non-finite outside the scale range\n(`stat_qq()`).\n\n\nWarning: Removed 2324 rows containing non-finite outside the scale range\n(`stat_qq_line()`).\n\n\n\n\n\n\n\n\n\nTop of Tabset\n\nSummary\nTOTAL THERAPY VISITS appears to have a logarithmic distribution. Poisson regression should take care of this\n\n\n\n\n# Create histogram and qqplot\ndistr_plots(data_ex, \"`DAYS WITH THERAPY VISITS`\", 8)\n\n\n\n\n\n\n\n\nDAYS WITH THERAPY VISITS appears to be coded correctly. Poisson regression will handle the distribution.\nTop of Tabset\n\n\n\n# Create histogram and qqplot\ndistr_plots(data_ex, \"`PERCENT OF DAYS WITH THERAPY VISITS`\", 8)\n\n\n\n\n\n\n\n\nInteresting, there was a lot of patients with either 0 or 100% of their days including physical therapy.\nThis could be a good covariate to include in the final model.\nTop of Tabset\n\n\n\nNegative Therapy LengthAverage Therapy LengthMax Therapy LengthMin Therapy Length\n\n\nWe saw earlier that average, min, and max therapy length had negative values for their minimum, which does not make sense. Let’s investigate.\n\n# Create histogram and qqplot\ndistr_plots(data_ex, \"`AVG THERAPY LENGTH`\", 8)\n\n\n\n\n\n\n\n\nAh, we can see that we have a one incredibly low and negative values, and some incredibly high values.\n\n# Create a table to checkout our frequencies for low and high values\ntable(data_ex$`AVG THERAPY LENGTH`)\n\n\n-1424     0     4     5     6     7     8     9    10    11    12    13    14 \n    1  2374     1     9     4     4    12    13    53    17    38    29    31 \n   15    16    17    18    19    20    21    22    23    24    25    26    27 \n  117    48    60    43    59   168    89    78   107    88   213   127   117 \n   28    29    30    31    32    33    34    35    36    37    38    39    40 \n  113   114   174   127   122   102   111   129    92   110    85    82   141 \n   41    42    43    44    45    46    47    48    49    50    51    52    53 \n   78    79    71    77    80    60    56    42    58    48    23    28    37 \n   54    55    56    57    58    59    60    61    62    63    64    65    66 \n   34    35    21    15    18    16    23    12    12     5    13    10     5 \n   67    68    69    70    71    72    73    74    75    76    77    81    83 \n    4     6     5     8    10     4     3     5     5     2     1     1     1 \n   85    86    90    93    97   362   617   644  1001  1043  1506  2093  2144 \n    2     3     2     1     1     1     1     1     1     1     1     1     1 \n\n\nWe have 1 patient with a -1424 average length in therapy that is most likely an erroneous entry, and some patients with very high times in therapy.\nI wonder who that patient is that is so advanced they’ve spent negative time in therapy.\n\n# Investigate patient with negative therapy lengths\nnegative &lt;- data_ex[data_ex$`AVG THERAPY LENGTH` == -1424,] %&gt;% \n  dplyr::select(id, `AVG THERAPY LENGTH`, `MIN THERAPY LENGTH`, `MAX THERAPY LENGTH`)\n\n# Pretty Print\nkable(negative, format = \"html\") %&gt;%\n  kable_styling(bootstrap_options = c(\"condensed\", \"hover\", \"striped\"))\n\n\n\n\nid\nAVG THERAPY LENGTH\nMIN THERAPY LENGTH\nMAX THERAPY LENGTH\n\n\n\n\n10462\n-1424\n-2726\n-85\n\n\n\n\n\n\n\nAha! This patient has negative values for all therapy lengths.\nLet’s see if they are the only one that has negative values.\n\ntable(data_ex$`MIN THERAPY LENGTH`)\n\n\n-2726     0     3     4     5     6     7     8     9    10    11    12    13 \n    1  2376     6     4    41    20    26    78    57   234    67    83    67 \n   14    15    16    17    18    19    20    21    22    23    24    25    26 \n   94   275    81    60    94    70   230    75    39   178   144   271   115 \n   27    28    29    30    31    32    33    34    35    36    37    38    39 \n   90    84    86   206    77    75    70    63    70    46    43    61    53 \n   40    41    42    43    44    45    46    47    48    49    50    51    52 \n  100    38    34    36    34    37    22    26    20    22    29    17     8 \n   53    54    55    56    57    58    59    60    61    62    63    64    65 \n   17    18    29     9    11     9    13    12     5     8     4     5     8 \n   66    67    68    69    70    71    72    73    74    75    76    77    81 \n    1     5     2     2     6     6     3     2     4     4     2     1     2 \n   83    85    86    90    93    97  1204  1253 \n    1     1     2     1     1     1     1     1 \n\ntable(data_ex$`MAX THERAPY LENGTH`)\n\n\n -85    0    4    5    6    7    8    9   10   11   12   13   14   15   16   17 \n   1 2374    1    8    4    5   10   13   50   14   19   15   22  124   34   20 \n  18   19   20   21   22   23   24   25   26   27   28   29   30   31   32   33 \n  27   38  153   39   18   73   58  197   64   67   72   62  210   74   68   59 \n  34   35   36   37   38   39   40   41   42   43   44   45   46   47   48   49 \n  61  106   63   60   98   92  158   79   82   62   61   98   58   58   53   46 \n  50   51   52   53   54   55   56   57   58   59   60   61   62   63   64   65 \n  81   36   31   70   79   91   57   48   47   44   66   46   28   31   23   25 \n  66   67   68   69   70   71   72   73   74   75   76   77   78   79   80   81 \n  16   15   35   25   42   17   19   14   22   16   16   10    9    8    9    5 \n  82   83   84   85   86   87   88   89   90   91   92   93   94   95   96   97 \n   4    7   10   13   10    5    3    6    9    4    7    7    2    3    1    3 \n  98   99  100  101  102  103  104  105  107  109  110  117  140 1034 1540 1725 \n   1    2    1    2    2    1    1    3    1    1    2    1    1    1    1    1 \n2963 3035 3067 3119 4248 \n   1    1    1    1    1 \n\n\nThey are! What probably happened here was their minimum is actually 85, and their max is actually 2726, and those got reversed into negatives somehow. Similarly, their average should be 1424.\nWe’ll double check with the investigator, but for now let’s just delete this patient and re run the histogram and qqplot.\n\n# Remove negative patient\ndata_ex &lt;- data_ex %&gt;% filter(data_ex$id != 10462)\n\nTop of Tabset\n\n\n\n# Create histogram and qqplot\ndistr_plots(data_ex, \"`AVG THERAPY LENGTH`\", 20)\n\n\n\n\n\n\n\n\nWe may have some outliers that spent very long in therapy\n\n# Examine potential outliers\ndata_ex |&gt; \n  arrange(desc(`AVG THERAPY LENGTH`)) |&gt; \n  dplyr::select(id, `MIN THERAPY LENGTH`, `MAX THERAPY LENGTH`, `AVG THERAPY LENGTH`) |&gt;\n  head(n=10) |&gt; \n  pretty_print()\n\n\n\n\nid\nMIN THERAPY LENGTH\nMAX THERAPY LENGTH\nAVG THERAPY LENGTH\n\n\n\n\n178\n1253\n3035\n2144\n\n\n12985\n1204\n4248\n2093\n\n\n3230\n50\n2963\n1506\n\n\n2942\n32\n1540\n1043\n\n\n12803\n14\n3119\n1001\n\n\n12640\n25\n3067\n644\n\n\n6661\n8\n1725\n617\n\n\n10917\n26\n1034\n362\n\n\n8071\n97\n97\n97\n\n\n7733\n93\n93\n93\n\n\n\n\n\n\n\nSuccess! That is not a logarithmic relationship, that’s just 8 patients who had an extremely long stay in therapy.\nLet’s set these values to NA.\n\n# Filter out outlier patients\ndata_ex &lt;- data_ex |&gt; \n  mutate(`AVG THERAPY LENGTH` = ifelse(id %in% c(178, 12985, 3230, 2942, 12803, 12640, 6661, 10917), NA, `AVG THERAPY LENGTH`))\n\nand re-plot\n\n# Create histogram and qqplot\ndistr_plots(data_ex, \"`AVG THERAPY LENGTH`\", 20)\n\nWarning: Removed 8 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\nWarning: Removed 8 rows containing non-finite outside the scale range\n(`stat_qq()`).\n\n\nWarning: Removed 8 rows containing non-finite outside the scale range\n(`stat_qq_line()`).\n\n\n\n\n\n\n\n\n\nThat’s a normal distribution with a preponderance of patients who did not have physical therapy.\nTop of Tabset\n\n\n\n# Set outlier values to NA\ndata_ex &lt;- data_ex |&gt; \n  mutate(`MAX THERAPY LENGTH` = ifelse(id %in% c(178, 12985, 3230, 2942, 12803, 12640, 6661, 10917), NA, `MAX THERAPY LENGTH`))\n\n# Create histogram and qqplot\ndistr_plots(data_ex, \"`MAX THERAPY LENGTH`\", 20)\n\nWarning: Removed 8 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\nWarning: Removed 8 rows containing non-finite outside the scale range\n(`stat_qq()`).\n\n\nWarning: Removed 8 rows containing non-finite outside the scale range\n(`stat_qq_line()`).\n\n\n\n\n\n\n\n\n\nMax therapy appears mostly normally distributed, not including the patients that did not receive PT.\nThe highest patient has a max length of 140, and the next highest has a max stay of ~100. Not drastic enough to call that patient an outlier.\n\nSummary\nMAX THERAPY LENGTH appears normally distributed for those that received PT.\nTop of Tabset\n\n\n\n\n# Set outlier values to NA\ndata_ex &lt;- data_ex |&gt; \n  mutate(`MIN THERAPY LENGTH` = ifelse(id %in% c(178, 12985), NA, `MIN THERAPY LENGTH`))\n\n# Create histogram and qqplot\ndistr_plots(data_ex, \"`MIN THERAPY LENGTH`\", 20)\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_qq()`).\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_qq_line()`).\n\n\n\n\n\n\n\n\n\n\nSummary\nMIN THERAPY LENGTH appears mostly normal for thos who received it.\nTop of Tabset\n\n\n\n\n\n\nThis variable represents the number of days a patient has spent in a standard hospital unit after being discharged from the Medical Intensive Care Unit (MICU).\n\n# Create histogram and qqplot\ndistr_plots(data_ex, \"`ACTUAL DAYS POST MICU`\", 20)\n\n\n\n\n\n\n\n\nIt appears logarithmic.\n\n# Log transform variable\ndata_ex &lt;- data_ex |&gt; \n  mutate(ACTUAL_DAYS_POST_MICU_log = log(`ACTUAL DAYS POST MICU`))\n\n# Create histogram and qqplot\ndistr_plots(data_ex, \"ACTUAL_DAYS_POST_MICU_log\", 15)\n\nWarning: Removed 1988 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\nWarning: Removed 1988 rows containing non-finite outside the scale range\n(`stat_qq()`).\n\n\nWarning: Removed 1988 rows containing non-finite outside the scale range\n(`stat_qq_line()`).\n\n\n\n\n\n\n\n\n\nThat appears normally distributed now!\n\nSummary\nDays post MICU is normal after log transforming.\nTop of Tabset\n\n\n\nThis variable represents the number of days that patients have been visited by physical therapists (PT) or occupational therapists (OT) after being discharged from the Medical Intensive Care Unit (MICU). It captures the extent of post-ICU rehabilitation services provided to patients and can be important for understanding recovery trajectories.\n\n# Create histogram and qqplot\ndistr_plots(data_ex, \"`PT or OT VISIT DAYS POST MICU`\", 15)\n\n\n\n\n\n\n\n\nThis variable is important for our analysis as it will not effect our outcome variables.\nTop of Tabset\n\n\nThis variable is the percentage of days after a patient is discharged from the Intensive Care Unit (ICU) during which they received therapy, such as physical therapy (PT) or occupational therapy (OT).\nThis variable is likely not important as it will not effect our outcome variables.\n\n# Create histogram and qqplot\ndistr_plots(data_ex, \"`PERCENT POST ICU DAYS THERAPY`\", 15)\n\n\n\n\n\n\n\n\nTop of Tabset\n\n\n\n# Examine counts\npretty_print(table(data_ex$Age_Range))\n\n\n\n\nVar1\nFreq\n\n\n\n\n18-39\n1121\n\n\n40-49\n899\n\n\n50-59\n1492\n\n\n60-69\n1574\n\n\n70+\n1327\n\n\nNegative Age\n16\n\n\n\n\n\n\n\nThat’s curious, what does “Negative Age” mean?\n\n# Examine negative age\ndata_ex |&gt; \n  filter(Age_Range == \"Negative Age\") |&gt; \n  pretty_print()\n\n\n\n\nid\nSEX\nMV_Total\nHOSPITAL LOS\nPROBLEM LIST DX CODES\nPROBLEM LIST DX NAMES\nDISCH_DISP\nDEPT BEFORE Unit\nDEPT AFTER Unit\nADMIT TO CONSULT\nCONSULT TO TREAT\nADMIT TO TREAT\nTOTAL THERAPY VISITS\nUnit_LOS\nDAYS WITH THERAPY VISITS\nPERCENT OF DAYS WITH THERAPY VISITS\nAVG THERAPY LENGTH\nMAX THERAPY LENGTH\nMIN THERAPY LENGTH\nACTUAL DAYS POST MICU\nPT or OT VISIT DAYS POST MICU\nPERCENT POST ICU DAYS THERAPY\nAge_Range\nAdmit_MonthYear\nAge_Quartile\nADMIT_TO_CONSULT_log\nTOTAL_THERAPY_VISITS_log\nACTUAL_DAYS_POST_MICU_log\n\n\n\n\n1394\nM\nNA\n139\n292.81, 296.80, 314.0\nADHD (attention deficit hypera\nHome or Self Care\nAMC EMERGENCY\nAMC MED SPEC UNIT\nNA\nNA\nNA\n0\n69\n0\n0.00\n0\n0\n0\n2\n0\n0.00\nNegative Age\n2012-08\n14-42\nNA\n-Inf\n0.6931472\n\n\n2815\nM\nNA\n415\n289.81, 289.84, 453.0\nAntiphospholipid antibody synd\nHome or Self Care\nAMC IR\nAMC MED SPEC UNIT\nNA\nNA\nNA\n0\n93\n0\n0.00\n0\n0\n0\n13\n0\n0.00\nNegative Age\n2013-07\n14-42\nNA\n-Inf\n2.5649494\n\n\n4090\nM\nNA\n150\n490, 949.0\nBronchitis, Burn\nHome or Self Care\nAMC BURN ICU\nAMC BURN ICU\n0\nNA\nNA\n0\n64\n0\n0.00\n0\n0\n0\n2\n0\n0.00\nNegative Age\n2014-03\n14-42\n-Inf\n-Inf\n0.6931472\n\n\n4282\nM\nNA\n195\nNA\nNA\nHome or Self Care\nAMC IR\nPOST-DISCHARGE\nNA\nNA\nNA\n0\n190\n0\n0.00\n0\n0\n0\n0\n0\n0.00\nNegative Age\n2014-05\n14-42\nNA\n-Inf\n-Inf\n\n\n4800\nF\nNA\n365\nNA\nNA\nHome-Health Care Svc\nAMC CARDIOTHORACIC ICU\nAMC M/S PROG CARE UNIT\nNA\nNA\nNA\n0\n50\n0\n0.00\n0\n0\n0\n6\n0\n0.00\nNegative Age\n2014-08\n14-42\nNA\n-Inf\n1.7917595\n\n\n6342\nM\nNA\n55\n452\nPortal vein thrombosis\nHome or Self Care\nAMC IR\nPOST-DISCHARGE\nNA\nNA\nNA\n0\n49\n0\n0.00\n0\n0\n0\n0\n0\n0.00\nNegative Age\n2015-06\n14-42\nNA\n-Inf\n-Inf\n\n\n6412\nF\nNA\n66\n453.40\nDVT (deep venous thrombosis\nDischarged/transferred to a Designated Cancer Center or Children’s Hospital\nAMC IR\nPOST-DISCHARGE\nNA\nNA\nNA\n0\n48\n0\n0.00\n0\n0\n0\n0\n0\n0.00\nNegative Age\n2015-06\n14-42\nNA\n-Inf\n-Inf\n\n\n6447\nF\nNA\n78\n453.40\nDVT (deep venous thrombosis\nTRANSFER TO CANCER OR CHILDRENS\nAMC IR\nPOST-DISCHARGE\n1\n67\n68\n1\n73\n1\n33.33\n31\n31\n31\n0\n0\n0.00\nNegative Age\n2015-07\n14-42\n0.00000\n0.0000000\n-Inf\n\n\n6479\nF\nNA\n55\nNA\nNA\nDischarged/transferred to a Designated Cancer Center or Children’s Hospital\nAMC IR\nPOST-DISCHARGE\n1\n43\n44\n1\n49\n1\n50.00\n24\n24\n24\n0\n0\n0.00\nNegative Age\n2015-07\n14-42\n0.00000\n0.0000000\n-Inf\n\n\n7512\nF\nNA\n80\n244.9, 278.00, 415.19\nAcquired hypothyroidism, Con\nHome or Self Care\nAMC IR\nAMC PULM UNIT\n0\nNA\nNA\n0\n56\n0\n0.00\n0\n0\n0\n1\n1\n100.00\nNegative Age\n2016-02\n14-42\n-Inf\n-Inf\n0.0000000\n\n\n8292\nF\nNA\n53\n345.90, 625.8, 780.02\nAdnexal mass, Anticholinergic\nHome or Self Care\nPRE-ADMISSION\nPOST-DISCHARGE\n1\nNA\nNA\n0\n53\n0\n0.00\n0\n0\n0\n0\n0\n0.00\nNegative Age\n2016-07\n14-42\n0.00000\n-Inf\n-Inf\n\n\n8779\nF\n5\n191\nNA\nSuicide attempt by drug inges\nHome or Self Care\nAMC EMERGENCY\nAMC MED SPEC UNIT\n1\n89\n90\n1\n106\n1\n20.00\n45\n45\n45\n3\n1\n33.33\nNegative Age\n2016-10\n14-42\n0.00000\n0.0000000\n1.0986123\n\n\n9152\nF\n5\n430\nNA\nPneumonia of both lungs due t\nHome or Self Care\nAMC CARDIOTHORACIC ICU\nAMC MED SPEC UNIT\n0\n229\n16\n9\n167\n8\n100.00\n54\n70\n40\n2\n2\n100.00\nNegative Age\n2017-01\n14-42\n-Inf\n2.1972246\n0.6931472\n\n\n9398\nM\nNA\n91\n977.9\nIntentional drug overdose (HC\nCourt/Law Enforcement\nAMC EMERGENCY\nPOST-DISCHARGE\n0\n33\n31\n2\n85\n2\n66.67\n23\n32\n15\n0\n0\n0.00\nNegative Age\n2017-02\n14-42\n-Inf\n0.6931472\n-Inf\n\n\n9793\nF\nNA\n74\n453.40\nDVT (deep venous thrombosis\nHome or Self Care\nAMC IR\nAMC MED HLTH SVC UNIT\n1\n41\n42\n1\n50\n1\n50.00\n52\n67\n38\n1\n0\n0.00\nNegative Age\n2017-05\n14-42\n0.00000\n0.0000000\n0.0000000\n\n\n12029\nM\nNA\n61\n453.42\nDVT of lower extremity (deep v\nHome or Self Care\nAMC CARD PRE/POST PROC\nPOST-DISCHARGE\n44\n3\n47\n1\n53\n1\n50.00\n40\n40\n40\n0\n0\n0.00\nNegative Age\n2018-09\n14-42\n3.78419\n0.0000000\n-Inf\n\n\n\n\n\n\n\nI don’t see any pattern.\nLet’s plot the counts.\n\n# Plot counts\nggplot(data_ex, aes(x = Age_Range, fill = Age_Range)) +\n  geom_bar() +\n  theme_minimal() +\n  labs(title = \"Count by Age Range\",\n       x = \"Age Range\",\n       y = \"Count\",\n       fill = \"Age Range\") +\n  scale_fill_brewer(palette = \"Pastel2\")\n\n\n\n\n\n\n\n\nWe have a sufficient size for each group and can use age range as a covariate.\nTop of Tabset\n\n\nThis is just a different variable for age, where they divided the sample up by quartiles.\n\n# Check counts\ntable(data_ex$Age_Quartile)\n\n\n14-42 43-56 57-66   67+ \n 1379  1637  1652  1761 \n\n\nWe can use either age variables, it really shouldn’t matter unless there is some important clinical distinction that is driving up their divisions in Age_Range.\nTop of Tabset\n\n\nAdmit_MonthYear is a character. Let’s go ahead and convert to datetime using lubridate.\n\n# Convert admit month year to datetime\ndata_ex &lt;- data_ex |&gt; \n  mutate(Admit_MonthYear = ym(Admit_MonthYear))\n\nNow we can plot how many patients were admitted each month.\nWe can also highlight the timeframe of the quality improvement iniative to visualize pre-, intra-, and post- periods.\n\n# Define the time periods to highlight\nhighlight_periods &lt;- data.frame(\n  start = as.Date(c(\"2015-04-01\")),\n  end = as.Date(c(\"2016-01-01\"))\n)\n\n# Summarize and plot the data\ndata_ex |&gt; \n  group_by(Admit_MonthYear) |&gt; \n  summarise(count = n()) |&gt; \n  ggplot(aes(x = Admit_MonthYear, y = count)) +\n  geom_line() +\n  theme_minimal() +\n  geom_rect(data = highlight_periods, aes(xmin = start, xmax = end, ymin = -Inf, ymax = Inf), \n            inherit.aes = FALSE, fill = \"lightblue\", alpha = 0.2) +\n  labs(title = \"Admissions Over Time\",\n       x = \"Admit Date\",\n       y = \"Count\")\n\n\n\n\n\n\n\n\nIt does not appear that the number of patients admitted throughout the year really changed that drastically. I don’t think we will have to control for this."
  },
  {
    "objectID": "Statistical_Consulting/Vieau_Project_01/Code/Megan_Watson_Project.html#Creation",
    "href": "Statistical_Consulting/Vieau_Project_01/Code/Megan_Watson_Project.html#Creation",
    "title": "Assessing the Impact of a Physical Therapy Quality-Improvement Initiative at a Colorado University Hospital",
    "section": "Variable Creation",
    "text": "Variable Creation\nHere we will create any variables necessary for our analysis\n\nMechanical VentilationReceived Physical Therapy Variable CreationQI-Initiative Time Period\n\n\nThe researcher expressed that patients would be stratified on Mechanical ventilation v. NO mechanical ventilation. However, I do not see this variable. Let’s go ahead and create it, assuming that patients with NA for MV_TOTAL were simply never on mechanical ventilation.\n\n# Dummy code MV status \ndata_ex$MV_YN &lt;- ifelse(is.na(data_ex$MV_Total), 0, 1)\n\nLet’s check that worked as intended.\n\n# Filter down to variables of interest to verify dummy coding worked\ncheck &lt;- data_ex %&gt;%\n  dplyr::select(id, MV_Total, MV_YN)\n\n# Pretty print\nkable(head(check), format = \"html\") %&gt;%\n  kable_styling(bootstrap_options = c(\"condensed\", \"stripe\", \"hover\"))\n\n\n\n\nid\nMV_Total\nMV_YN\n\n\n\n\n2\nNA\n0\n\n\n3\n13\n1\n\n\n4\nNA\n0\n\n\n5\n16\n1\n\n\n8\nNA\n0\n\n\n10\n2\n1\n\n\n\n\n\n\n\nLooks good.\nTop of Tabset\n\n\nLet’s create a dummy variable for whether patients received PT or not.\n\n# Create variable for reception of PT\ndata_ex &lt;- data_ex |&gt; \n  mutate(PT_YN = ifelse(is.na(`AVG THERAPY LENGTH`), 1,\n                           ifelse(`AVG THERAPY LENGTH` == 0, 0, 1)))\n\n# Double check calculations\ndata_ex |&gt; \n  dplyr::select(`AVG THERAPY LENGTH`, `DAYS WITH THERAPY VISITS`, PT_YN) |&gt; \n  head() |&gt; \n  pretty_print()\n\n\n\n\nAVG THERAPY LENGTH\nDAYS WITH THERAPY VISITS\nPT_YN\n\n\n\n\n0\n0\n0\n\n\n27\n2\n1\n\n\n17\n2\n1\n\n\n25\n4\n1\n\n\n0\n0\n0\n\n\n0\n0\n0\n\n\n\n\n\n\n# Convert to factor for better plotting later on\ndata_ex &lt;- data_ex |&gt; \n  mutate(PT_YN = as.factor(PT_YN))\n\nLooks good, not we can answer one of their research questions of whether PT enrollment increased after the intervention.\nTop of Tabset\n\n\nLet’s group participants into the time period that they were present during the initiative for.\n\n# Create new variable based on QI initiative time period.\ndata_ex &lt;- data_ex |&gt; \n  mutate(initiative = ifelse(Admit_MonthYear &lt; as.Date(\"2015-04-01\"), \"Pre\",\n                             ifelse(Admit_MonthYear &gt; as.Date(\"2015-12-01\"), \"Post\", \"During\")))\n\nLet’s double check we did that right.\n\n# Check counts\ntable(data_ex$initiative)\n\n\nDuring   Post    Pre \n   661   2871   2897 \n\n# Double check assigning\ndata_ex |&gt; \n  dplyr::select(id, Admit_MonthYear, initiative) |&gt; \n  filter(Admit_MonthYear &lt; as.Date(\"2015-04-01\")) |&gt; \n  arrange(desc(Admit_MonthYear)) |&gt; \n  head() |&gt; \n  pretty_print()\n\n\n\n\nid\nAdmit_MonthYear\ninitiative\n\n\n\n\n5759\n2015-03-01\nPre\n\n\n5760\n2015-03-01\nPre\n\n\n5761\n2015-03-01\nPre\n\n\n5763\n2015-03-01\nPre\n\n\n5765\n2015-03-01\nPre\n\n\n5767\n2015-03-01\nPre\n\n\n\n\n\n\n# Double check assigning\ndata_ex |&gt; \n  dplyr::select(id, Admit_MonthYear, initiative) |&gt; \n  filter(Admit_MonthYear &gt; as.Date(\"2015-04-01\")) |&gt; \n  arrange(Admit_MonthYear) |&gt; \n  head() |&gt; \n  pretty_print()\n\n\n\n\nid\nAdmit_MonthYear\ninitiative\n\n\n\n\n6094\n2015-05-01\nDuring\n\n\n6096\n2015-05-01\nDuring\n\n\n6099\n2015-05-01\nDuring\n\n\n6100\n2015-05-01\nDuring\n\n\n6105\n2015-05-01\nDuring\n\n\n6106\n2015-05-01\nDuring\n\n\n\n\n\n\n# Double check assigning\ndata_ex |&gt; \n  dplyr::select(id, Admit_MonthYear, initiative) |&gt; \n  filter(Admit_MonthYear &gt;= as.Date(\"2015-12-01\")) |&gt; \n  arrange(Admit_MonthYear) |&gt; \n  head() |&gt; \n  pretty_print()\n\n\n\n\nid\nAdmit_MonthYear\ninitiative\n\n\n\n\n7167\n2015-12-01\nDuring\n\n\n7169\n2015-12-01\nDuring\n\n\n7171\n2015-12-01\nDuring\n\n\n7172\n2015-12-01\nDuring\n\n\n7174\n2015-12-01\nDuring\n\n\n7176\n2015-12-01\nDuring\n\n\n\n\n\n\n# Double check assigning\ndata_ex |&gt; \n  dplyr::select(id, Admit_MonthYear, initiative) |&gt; \n  filter(Admit_MonthYear &gt; as.Date(\"2015-12-01\")) |&gt; \n  arrange(Admit_MonthYear) |&gt; \n  head() |&gt; \n  pretty_print()\n\n\n\n\nid\nAdmit_MonthYear\ninitiative\n\n\n\n\n7324\n2016-01-01\nPost\n\n\n7331\n2016-01-01\nPost\n\n\n7336\n2016-01-01\nPost\n\n\n7339\n2016-01-01\nPost\n\n\n7343\n2016-01-01\nPost\n\n\n7345\n2016-01-01\nPost\n\n\n\n\n\n\n\nLooks good.\nTop of Tabset"
  },
  {
    "objectID": "Statistical_Consulting/Vieau_Project_01/Code/Megan_Watson_Project.html#Outcome_Variables",
    "href": "Statistical_Consulting/Vieau_Project_01/Code/Megan_Watson_Project.html#Outcome_Variables",
    "title": "Assessing the Impact of a Physical Therapy Quality-Improvement Initiative at a Colorado University Hospital",
    "section": "Outcome Variables",
    "text": "Outcome Variables\n\nMechanical VentilationMICU Length of StayHospital Length of StayNon-ICU Length of StayDischarge Location\n\n\nFirst we will begin with mechanical ventilation time (MV)\n\n# Create histogram and qqplot\ndistr_plots(data_ex, \"MV_Total\")\n\nWarning: Removed 3166 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\nWarning: Removed 3166 rows containing non-finite outside the scale range\n(`stat_qq()`).\n\n\nWarning: Removed 3166 rows containing non-finite outside the scale range\n(`stat_qq_line()`).\n\n\n\n\n\n\n\n\n\nThat’s some severe right skewness! Is that logarithmic? Let’s try and transform and see what happens.\n\n# Perform a log transform of mechanical ventilation time and plot\ndata_ex$MV_Total_log &lt;- log(data_ex$MV_Total)\n\n# Create histogram and qqplot\ndistr_plots(data_ex, \"MV_Total_log\", 10)\n\nWarning: Removed 3166 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\nWarning: Removed 3166 rows containing non-finite outside the scale range\n(`stat_qq()`).\n\n\nWarning: Removed 3166 rows containing non-finite outside the scale range\n(`stat_qq_line()`).\n\n\n\n\n\n\n\n\n\nThat’s better. Maybe try a square root transformation?\n\n# Perform a sqrt transformation and plot\ndata_ex$MV_Total_sqrt &lt;- sqrt(data_ex$MV_Total)\n\n# Create histogram and qqplot\ndistr_plots(data_ex, \"MV_Total_sqrt\", 10)\n\nWarning: Removed 3166 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\nWarning: Removed 3166 rows containing non-finite outside the scale range\n(`stat_qq()`).\n\n\nWarning: Removed 3166 rows containing non-finite outside the scale range\n(`stat_qq_line()`).\n\n\n\n\n\n\n\n\n\nThat looks worse. Out of curiosity I’m going to try using the bestNormalize package I’ve been learning to see if it can recommend the best tranformation.\n\nBNobject &lt;- bestNormalize(data_ex$MV_Total)\n\nWarning: `progress_estimated()` was deprecated in dplyr 1.0.0.\nℹ The deprecated feature was likely used in the bestNormalize package.\n  Please report the issue to the authors.\n\nBNobject\n\nBest Normalizing transformation with 3263 Observations\n Estimated Normality Statistics (Pearson P / df, lower =&gt; more normal):\n - arcsinh(x): 24.5519\n - Box-Cox: 24.6177\n - Center+scale: 23.9754\n - Double Reversed Log_b(x+a): 26.2191\n - Exp(x): 343.5041\n - Log_b(x+a): 24.5127\n - orderNorm (ORQ): 24.2628\n - sqrt(x + a): 24.3697\n - Yeo-Johnson: 24.7852\nEstimation method: Out-of-sample via CV with 10 folds and 5 repeats\n \nBased off these, bestNormalize chose:\ncenter_scale(x) Transformation with 3263 nonmissing obs.\n Estimated statistics:\n - mean (before standardization) = 6.101747 \n - sd (before standardization) = 5.112296 \n\n\nThe short explanation of how these values work is that the value closest to 1 is the best transformation, but these are all VERY far from one. So that didn’t help.\n\nBoxplot\n\n#Create boxplot to assess for outliers\nggplot(data_ex, aes(y = MV_Total_log)) +\n  geom_boxplot() +\n  labs(title = \"Boxplot of MV Total Log\",\n       y = \"MV Total Log\")\n\nWarning: Removed 3166 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\n\n\n\n\nThere are also no outliers for MV_Total_Log!\n\n\nSummary\nIt looks like the best option we have is a log transformation of MV TOTAL.\nAlternatively, we will likely end up running a Poisson or Negative Binomial regression to handle this data.\nTop of Tabset\n\n\n\n\n# Create histogram and qqplot\ndistr_plots(data_ex, \"Unit_LOS\", 10)\n\n\n\n\n\n\n\n\nThat’s super skewed as well! Let’s try a log transform.\n\n# Perform log transform of MICU LOS\ndata_ex$Unit_LOS_log &lt;- log(data_ex$Unit_LOS)\n\n# Create histogram and qqplot\ndistr_plots(data_ex, \"Unit_LOS_log\", 10)\n\n\n\n\n\n\n\n\nWe’ve got more of an S shape going there. I wonder if a BoxCox transformation is more appropriate?\n\nlibrary(bestNormalize)\n# Use bestNormalize to try to find best transformation\nBNobject &lt;- bestNormalize(data_ex$Unit_LOS)\nBNobject\n\nBest Normalizing transformation with 6429 Observations\n Estimated Normality Statistics (Pearson P / df, lower =&gt; more normal):\n - arcsinh(x): 9.3327\n - Box-Cox: 5.6787\n - Center+scale: 34.4724\n - Double Reversed Log_b(x+a): 56.0199\n - Log_b(x+a): 9.3285\n - orderNorm (ORQ): 1.3347\n - sqrt(x + a): 19.5863\n - Yeo-Johnson: 5.6924\nEstimation method: Out-of-sample via CV with 10 folds and 5 repeats\n \nBased off these, bestNormalize chose:\norderNorm Transformation with 6429 nonmissing obs and ties\n - 515 unique values \n - Original quantiles:\n  0%  25%  50%  75% 100% \n  48   66   95  167  720 \n\n# Perform boxcox transformation\ndata_ex$Unit_LOS_boxcox &lt;- boxcox(data_ex$Unit_LOS)$x.t\n\n# Create histogram and qqplot\ndistr_plots(data_ex, \"Unit_LOS_boxcox\", 25)\n\n\n\n\n\n\n\n\n\n# Create boxplot to assess for outliers\nggplot(data_ex, aes(y = Unit_LOS_boxcox)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\nThere are no outliers for boxcox Unit LOS.\nThat histogram looks better at least, and we don’t have any outliers. The bestNormalize package offers ways to back transform after you run your analysis, but I haven’t gotten that far in learning it yet. It looks like a boxcox transformation is a candidate however.\n\nSummary\nThis odd distribution will be handled later with a negative binomial regression.\nTop of Tabset\n\n\n\n\n# Create histogram and qqplot\ndistr_plots(data_ex, \"`HOSPITAL LOS`\", 25)\n\n\n\n\n\n\n\n\nHospital LOS also looks logarithmic.\nLet’s transform and assess.\n\n# Perform log transform \ndata_ex$HOSPITAL_LOS_log &lt;- log(data_ex$`HOSPITAL LOS`)\n\n# Create histogram and qqplot\ndistr_plots(data_ex, \"HOSPITAL_LOS_log\", 25)\n\n\n\n\n\n\n\nggplot(data_ex, aes(y = HOSPITAL_LOS_log)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\nLooks good! There are a handful of outliers in there, but the data looks normal and those values will likely be included.\n\nSummary\nHOSPITAL LOS is normal after a log transform. We will either perform that or a Poisson or Negative Binomial Top of Tabset\n\n\n\nWe must compute NON_ICU_LOS by subtracting Unit_LOS from HOSPITAL LOS.\n\n# Compute variable for non icu LOS.\ndata_ex &lt;- data_ex |&gt; \n  mutate(NON_ICU_LOS = `HOSPITAL LOS` - Unit_LOS)\n\n# Create histogram and qqplot\ndistr_plots(data_ex, \"NON_ICU_LOS\", 25)\n\n\n\n\n\n\n\n\nAs expected, that looks exponential.\n\n# Perform log transform\ndata_ex$NON_ICU_LOS_log &lt;- log(data_ex$NON_ICU_LOS)\n\n# Create histogram and qqplot\ndistr_plots(data_ex, \"NON_ICU_LOS_log\", 25)\n\nWarning: Removed 106 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\nWarning: Removed 106 rows containing non-finite outside the scale range\n(`stat_qq()`).\n\n\nWarning: Removed 106 rows containing non-finite outside the scale range\n(`stat_qq_line()`).\n\n\n\n\n\n\n\n\n\nThat’s bimodal. Odd.\nLet’s try a square root transform\n\n# Perform log transform\ndata_ex$NON_ICU_LOS_sqrt &lt;- sqrt(data_ex$NON_ICU_LOS)\n\n# Create histogram and qqplot\ndistr_plots(data_ex, \"NON_ICU_LOS_sqrt\", 25)\n\n\n\n\n\n\n\n\nLooks crummy.\nLet’s try bestNormalize\n\nBNobject &lt;- bestNormalize(data_ex$NON_ICU_LOS)\nBNobject\n\nBest Normalizing transformation with 6429 Observations\n Estimated Normality Statistics (Pearson P / df, lower =&gt; more normal):\n - arcsinh(x): 7.0928\n - Center+scale: 53.1844\n - Double Reversed Log_b(x+a): 58.7531\n - Log_b(x+a): 8.7735\n - orderNorm (ORQ): 1.6299\n - sqrt(x + a): 10.6332\n - Yeo-Johnson: 5.4241\nEstimation method: Out-of-sample via CV with 10 folds and 5 repeats\n \nBased off these, bestNormalize chose:\norderNorm Transformation with 6429 nonmissing obs and ties\n - 1045 unique values \n - Original quantiles:\n  0%  25%  50%  75% 100% \n   0   32  110  284 9206 \n\n# Perform boxcox transformation\ndata_ex$NON_ICU_LOS_yeo &lt;- yeojohnson(data_ex$NON_ICU_LOS)$x.t\n\n# Create histogram and qqplot\ndistr_plots(data_ex, \"NON_ICU_LOS_yeo\", 25)\n\n\n\n\n\n\n\n\nLooks better! Still bimodal but much more normal.\n\nSummary\nLooks like Yeo-Johnson transformation may be the way to go.\nTop of Tabset\n\n\n\n\n# Examine discharge locations\npretty_print(table(data_ex$DISCH_DISP))\n\n\n\n\nVar1\nFreq\n\n\n\n\nAcute IP to RPCU\n5\n\n\nAdministrative Discharge\n1\n\n\nAdmitted as an Inpatient\n3\n\n\nAgainst Clinical Advice\n22\n\n\nAnother Health Care Institution Not Defined w/Planned Readmission\n1\n\n\nCourt/Law Enforcement\n22\n\n\nDischarge Lounge\n6\n\n\nDischarged to Other Facility\n23\n\n\nDischarged/transferred to a Designated Cancer Center or Children’s Hospital\n41\n\n\nDrug/Alch Detox D/C\n4\n\n\nExpired\n1137\n\n\nExpired in Medical Facility\n8\n\n\nExpired Readmit as Organ Donor\n7\n\n\nFederal Hospital\n43\n\n\nHome-Health Care Svc\n758\n\n\nHome or Self Care\n2417\n\n\nHome or Self Care w/Planned Readmission\n1\n\n\nHospice/Home\n213\n\n\nHospice/Medical Facility\n211\n\n\nIntermediate Care Facility\n14\n\n\nIV Therapy Provider\n1\n\n\nLeft Against Medical Advice\n49\n\n\nLong Term Care\n451\n\n\nMental Health Facility\n2\n\n\nNursing Facility\n37\n\n\nPsychiatric Hospital\n34\n\n\nRehab Facility\n249\n\n\nShort Term Hospital\n71\n\n\nSkilled Nursing Facility\n596\n\n\nSwing Bed\n1\n\n\nTRANSFER TO CANCER OR CHILDRENS\n1\n\n\n\n\n\n\n\nThere are a few options we have for to analyze discharge location.\n\nWe can spit on expired vs not expired\nWe can split on hom vs other\nWe can just look at all the groups with the largest N, and collapse the rest into a single variable of ‘other’. This would be\n\nExpired (1137), Home (3177), Hospice (424), long term care (451), rehab (249), skilled Nursing facility (596)\n\n\nWe will have to ask the researcher how she wants to group up these discharge locations! Keeping in mind that since we are looking at three different time periods, each group will essentially by a third of what is shown here."
  },
  {
    "objectID": "Statistical_Consulting/Vieau_Project_01/Code/Megan_Watson_Project.html#Goal_Two",
    "href": "Statistical_Consulting/Vieau_Project_01/Code/Megan_Watson_Project.html#Goal_Two",
    "title": "Assessing the Impact of a Physical Therapy Quality-Improvement Initiative at a Colorado University Hospital",
    "section": "Goal 2: Decrease Time from Admit to Therapy",
    "text": "Goal 2: Decrease Time from Admit to Therapy\n\nVisualizationAnalysisInterpretationVisualization IISummary\n\n\n\n# Summarize and plot the data\ndata_ex |&gt; \n  group_by(Admit_MonthYear) |&gt; \n  summarise(avg_ADMIT_TO_TREAT = mean(`ADMIT TO TREAT`, na.rm = T)) |&gt; \n  ggplot(aes(x = Admit_MonthYear, y = avg_ADMIT_TO_TREAT)) +\n  geom_line() +\n  theme_minimal() +\n  geom_rect(data = highlight_periods, aes(xmin = start, xmax = end, ymin = -Inf, ymax = Inf), \n            inherit.aes = FALSE, fill = \"lightblue\", alpha = 0.2) +\n  annotate(\"text\", x = as.Date(\"2015-09-01\"), y = Inf, \n           label = \"QI-Initiative\", color = \"skyblue2\", size = 4, vjust = 1.5,) +\n  labs(title = \"Time from Admit to Therapy over Time\",\n       x = \"Admit Date\",\n       y = \"Time from Admit to Therapy\")\n\n\n\n\n\n\n\n\nThey successfully decreased the time from a patient’s first admit to their first physical therapy session.\nThis means that patients were receiving PT sooner.\nTop of Tabset\n\n\n\nCorrect Erroneous Value\nNote: We have a patient with a -1 value for admit to treat time, which is not allowed in a poisson or negative binomial model, and is also not possible, so we will set this to NA.\n\n# Remove patient with -1 value for Admit to Treat time\ndata_ex &lt;- data_ex |&gt; \n  mutate(`ADMIT TO TREAT` = ifelse(`ADMIT TO TREAT` == -1, NA, `ADMIT TO TREAT`))\n\n\n\nCheck for Overdispersion\n\n# Compare mean and variance\npaste0(\"Mean = \", mean(data_ex$`ADMIT TO TREAT`, na.rm = T))\n\n[1] \"Mean = 64.0505548705302\"\n\n# Compare mean and variance\npaste0(\"Variance = \", var(data_ex$`ADMIT TO TREAT`, na.rm = T))\n\n[1] \"Variance = 4357.15605235608\"\n\n\nThe variance is drastically greater than the mean, indicating that a negative binomial regression is more appropriate.\n\n\nPerforming the Negative Binomial Regression\n\n# Change reference level\ndata_ex$initiative &lt;- relevel(data_ex$initiative, ref = \"Pre\")\n\n# Fit a Negative Binomial model\nmodel_goal2_nb &lt;- glm.nb(`ADMIT TO TREAT` ~ initiative + Age_Range + Month, data = data_ex)\n\n# Examine Model\nsummary(model_goal2_nb)\n\n\nCall:\nglm.nb(formula = `ADMIT TO TREAT` ~ initiative + Age_Range + \n    Month, data = data_ex, init.theta = 1.903810295, link = log)\n\nCoefficients:\n                 Estimate Std. Error z value             Pr(&gt;|z|)    \n(Intercept)       4.81099    0.05102  94.287 &lt; 0.0000000000000002 ***\ninitiativeDuring -0.90959    0.03823 -23.793 &lt; 0.0000000000000002 ***\ninitiativePost   -0.68103    0.02620 -25.989 &lt; 0.0000000000000002 ***\nAge_Range40-49   -0.04208    0.04388  -0.959              0.33754    \nAge_Range50-59   -0.09610    0.03854  -2.493              0.01266 *  \nAge_Range60-69   -0.20880    0.03733  -5.593   0.0000000223330684 ***\nAge_Range70+     -0.29504    0.03832  -7.699   0.0000000000000137 ***\nMonth2           -0.10524    0.05622  -1.872              0.06121 .  \nMonth3           -0.23000    0.05558  -4.139   0.0000349491162686 ***\nMonth4           -0.05975    0.05385  -1.110              0.26717    \nMonth5           -0.13652    0.05672  -2.407              0.01609 *  \nMonth6           -0.01583    0.05774  -0.274              0.78394    \nMonth7           -0.05387    0.05838  -0.923              0.35618    \nMonth8           -0.14939    0.05710  -2.616              0.00889 ** \nMonth9           -0.02346    0.05772  -0.406              0.68439    \nMonth10          -0.14485    0.05560  -2.605              0.00919 ** \nMonth11          -0.04564    0.05697  -0.801              0.42313    \nMonth12           0.13637    0.05580   2.444              0.01452 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Negative Binomial(1.9038) family taken to be 1)\n\n    Null deviance: 5448.3  on 4047  degrees of freedom\nResidual deviance: 4355.2  on 4030  degrees of freedom\n  (2381 observations deleted due to missingness)\nAIC: 40513\n\nNumber of Fisher Scoring iterations: 1\n\n              Theta:  1.9038 \n          Std. Err.:  0.0408 \n\n 2 x log-likelihood:  -40474.7490 \n\nplot(model_goal2_nb)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Get 95% CI's\nCIs &lt;- confint(model_goal2_nb)\n\nWaiting for profiling to be done...\n\npretty_print(CIs)\n\n\n\n\n\n2.5 %\n97.5 %\n\n\n\n\n(Intercept)\n4.7137308\n4.9098407\n\n\ninitiativeDuring\n-0.9843501\n-0.8342671\n\n\ninitiativePost\n-0.7324678\n-0.6298252\n\n\nAge_Range40-49\n-0.1279882\n0.0440121\n\n\nAge_Range50-59\n-0.1717565\n-0.0207688\n\n\nAge_Range60-69\n-0.2821640\n-0.1358854\n\n\nAge_Range70+\n-0.3702590\n-0.2201763\n\n\nMonth2\n-0.2153569\n0.0050027\n\n\nMonth3\n-0.3388703\n-0.1210989\n\n\nMonth4\n-0.1658558\n0.0461453\n\n\nMonth5\n-0.2477053\n-0.0252075\n\n\nMonth6\n-0.1288225\n0.0974659\n\n\nMonth7\n-0.1678508\n0.0604720\n\n\nMonth8\n-0.2612686\n-0.0373159\n\n\nMonth9\n-0.1363227\n0.0896816\n\n\nMonth10\n-0.2535057\n-0.0362073\n\n\nMonth11\n-0.1569210\n0.0658237\n\n\nMonth12\n0.0272480\n0.2455280\n\n\n\n\n\n\n\n\n\nChange Reference Category\n\n# Change reference level\ndata_ex$initiative &lt;- relevel(data_ex$initiative, ref = \"During\")\n\n# Fit a Negative Binomial model\nmodel_goal2_nb &lt;- glm.nb(`ADMIT TO TREAT` ~ initiative + Age_Range + Month, data = data_ex)\n\n# Examine Model\nsummary(model_goal2_nb)\n\n\nCall:\nglm.nb(formula = `ADMIT TO TREAT` ~ initiative + Age_Range + \n    Month, data = data_ex, init.theta = 1.903810295, link = log)\n\nCoefficients:\n               Estimate Std. Error z value             Pr(&gt;|z|)    \n(Intercept)     3.90140    0.05834  66.870 &lt; 0.0000000000000002 ***\ninitiativePre   0.90959    0.03823  23.793 &lt; 0.0000000000000002 ***\ninitiativePost  0.22856    0.03581   6.383   0.0000000001736857 ***\nAge_Range40-49 -0.04208    0.04388  -0.959              0.33754    \nAge_Range50-59 -0.09610    0.03854  -2.493              0.01266 *  \nAge_Range60-69 -0.20880    0.03733  -5.593   0.0000000223330684 ***\nAge_Range70+   -0.29504    0.03832  -7.699   0.0000000000000137 ***\nMonth2         -0.10524    0.05622  -1.872              0.06121 .  \nMonth3         -0.23000    0.05558  -4.139   0.0000349491162686 ***\nMonth4         -0.05975    0.05385  -1.110              0.26717    \nMonth5         -0.13652    0.05672  -2.407              0.01609 *  \nMonth6         -0.01583    0.05774  -0.274              0.78394    \nMonth7         -0.05387    0.05838  -0.923              0.35618    \nMonth8         -0.14939    0.05710  -2.616              0.00889 ** \nMonth9         -0.02346    0.05772  -0.406              0.68439    \nMonth10        -0.14485    0.05560  -2.605              0.00919 ** \nMonth11        -0.04564    0.05697  -0.801              0.42313    \nMonth12         0.13637    0.05580   2.444              0.01452 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Negative Binomial(1.9038) family taken to be 1)\n\n    Null deviance: 5448.3  on 4047  degrees of freedom\nResidual deviance: 4355.2  on 4030  degrees of freedom\n  (2381 observations deleted due to missingness)\nAIC: 40513\n\nNumber of Fisher Scoring iterations: 1\n\n              Theta:  1.9038 \n          Std. Err.:  0.0408 \n\n 2 x log-likelihood:  -40474.7490 \n\n# Get 95% CI's\nCIs &lt;- confint(model_goal2_nb)\n\nWaiting for profiling to be done...\n\npretty_print(CIs)\n\n\n\n\n\n2.5 %\n97.5 %\n\n\n\n\n(Intercept)\n3.7876454\n4.0165436\n\n\ninitiativePre\n0.8342671\n0.9843501\n\n\ninitiativePost\n0.1580166\n0.2983406\n\n\nAge_Range40-49\n-0.1279882\n0.0440121\n\n\nAge_Range50-59\n-0.1717565\n-0.0207688\n\n\nAge_Range60-69\n-0.2821640\n-0.1358854\n\n\nAge_Range70+\n-0.3702590\n-0.2201763\n\n\nMonth2\n-0.2153569\n0.0050027\n\n\nMonth3\n-0.3388703\n-0.1210989\n\n\nMonth4\n-0.1658558\n0.0461453\n\n\nMonth5\n-0.2477053\n-0.0252075\n\n\nMonth6\n-0.1288225\n0.0974659\n\n\nMonth7\n-0.1678508\n0.0604720\n\n\nMonth8\n-0.2612686\n-0.0373159\n\n\nMonth9\n-0.1363227\n0.0896816\n\n\nMonth10\n-0.2535057\n-0.0362073\n\n\nMonth11\n-0.1569210\n0.0658237\n\n\nMonth12\n0.0272480\n0.2455280\n\n\n\n\n\n\n\nSince patients that did not receive a consult were coded as NA instead of 0, we do not have to perform a zero inflated regression this time.\n\nModel Comparison\n\n# Perform the Poisson regression\nmodel_goal2 &lt;- glm(`ADMIT TO TREAT` ~ initiative + Age_Range + Month, data = data_ex, family = poisson())\n\n# Compare model performance\nbic_p &lt;- BIC(model_goal2)\nbic_nb &lt;- BIC(model_goal2_nb)\n\naic_p &lt;- AIC(model_goal2)\naic_nb &lt;- AIC(model_goal2_nb)\n\n# Create a data frame to display the values \nmodel_comparison &lt;- data.frame(Model = c(\"Poisson Model\", \"Negative Binomial Model\"), \n                               BIC = c(bic_p, bic_nb), \n                               AIC = c(bic_p, aic_nb))\n\n# Print resulting table\npretty_print(model_comparison)\n\n\n\n\nModel\nBIC\nAIC\n\n\n\n\nPoisson Model\n175314.11\n175314.11\n\n\nNegative Binomial Model\n40632.56\n40512.75\n\n\n\n\n\n\n\nAIC and BIC drastically prefer the negative binomial model, indicating it is a better fit for our data.\n\n# Perform the likelihood ratio test \nlr_test &lt;- lrtest(model_goal2, model_goal2_nb) \nlr_test\n\nLikelihood ratio test\n\nModel 1: `ADMIT TO TREAT` ~ initiative + Age_Range + Month\nModel 2: `ADMIT TO TREAT` ~ initiative + Age_Range + Month\n  #Df LogLik Df  Chisq            Pr(&gt;Chisq)    \n1  18 -87582                                    \n2  19 -20237  1 134690 &lt; 0.00000000000000022 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe likelihood ratio test also confirms the negative binomial model performs better.\nTop of Tabset\n\n\n\n\n\nOverall Model\nThe likelihood ratio test indicates that the Negative Binomial model provides a significantly better fit than the Poisson model (χ² = 134758, df = 1, p &lt; 0.0001), supporting the decision to run the analysis as a Negative Binomial model due to overdispersion.\n\n\nMain Effects\nQI-initiative time period is a significant predictor for time from admit to treatment, while controlling for age and admission month. Compared to patients in the Pre time period, patients in the During time period had a mean admit to treatment time that was 0.40 times that of patients in the Pre period (z = 23.80, 95% CI: [0.37, 0.43], p &lt; 0.0001), and patients in the Post time period had a mean admit to treatment time that was 0.51 times that of patients in the Pre period (z = -26.01, 95% CI: [0.48, 0.53], p &lt; 0.0001). Additionally, compared to patients in the During time period, those in the Post time period had a mean admit to treatment time that was 1.26 times higher than that of patients during the During period (z = 6.37, 95% CI: [1.17, 1.35], p &lt; 0.0001), indicating a slight increase in admit to treatment time after the During period.\n\n\nAge\nAge was a significant predictor for time from admit to treatment, while controlling for QI-initiative time period and admission month. Compared to those who were 18-39, patients who were 50-59 had 0.91 times (z = -2.497, 95% CI: [0.84, 0.98], p = 0.0125), patients who were 60-69 had 0.81 times (z = -5.60, 95% CI: [0.75, 0.87], p &lt; 0.0001), and patients who were 70+ had 0.74 times (z = -7.70, 95% CI: [0.69, 0.80], p &lt; 0.0001) the admit to treatment time. Patients who were 40-49 did not have a statistically significant difference in admit to treatment time compared to those who were 18-39 (z = -0.97, 95% CI: [-.88, 1.04], p = 0.33).\n\n\nSeasonality\nAdmission Month was a significant predictor for admit to treatment time, while controlling for QI-initiative time period and age. For instance, those admitted during March had an admit to treatment time that was 0.80 times that of those admitted during January (z = -4.11, 95% CI: [0.71, 0.89, p &lt; 0.0001]).\nSince possible comparisons between months are myriad (12!), and the goal of this project was not to compare outcome variables between months (merely to control for these differences due to seasonality), further comparisons will not be made.\nTop of Tabset\n\n\n\n\nInitiative\n\n# Change reference level\ndata_ex$initiative &lt;- relevel(data_ex$initiative, ref = \"Pre\")\n\n# Fit a Negative Binomial model\nmodel_goal2_nb &lt;- glm.nb(`ADMIT TO TREAT` ~ initiative + Age_Range + Month, data = data_ex)\n\n# Extract the effect for initiative\neffect_initiative &lt;- effect(\"initiative\", model_goal2_nb)\n\n# Convert the effect object to a data frame for ggplot\neffect_data &lt;- as.data.frame(effect_initiative)\n\n# Plot using ggplot2\nggplot(effect_data, aes(x = initiative, y = fit, group = 1)) +\n  geom_line(size = 1, color = \"aquamarine\") +\n  geom_point(size = 3) +\n  geom_errorbar(aes(ymin = lower, ymax = upper), width = 0.2, alpha = 0.5) +\n  labs(\n    title = \"Effect of Initiative on Admit to Physical Therapy Wait Time\",\n    x = \"Initiative\",\n    y = \"Time from Admit to Physical Therapy\"\n  ) +\n  theme_minimal()\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n\n\n\nAge Range\n\n# Extract the effect for initiative\neffect_age &lt;- effect(\"Age_Range\", model_goal2_nb)\n\n# Convert the effect object to a data frame for ggplot\neffect_data &lt;- as.data.frame(effect_age)\n\n# Plot using ggplot2\nggplot(effect_data, aes(x = Age_Range, y = fit, group = 1)) +\n  geom_line(size = 1, color = \"aquamarine\") +\n  geom_point(size = 3) +\n  geom_errorbar(aes(ymin = lower, ymax = upper), width = 0.2, alpha = 0.5) +\n  labs(\n    title = \"Age Range on Admit to Physical Therapy Wait Time\",\n    x = \"Age\",\n    y = \"Time from Admit to Physical Therapy\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n# Extract the effect for initiative\neffect_month &lt;- effect(\"Month\", model_goal2_nb)\n\n# Convert the effect object to a data frame for ggplot\neffect_data &lt;- as.data.frame(effect_month)\n\n# Plot using ggplot2\nggplot(effect_data, aes(x = Month, y = fit, group = 1)) +\n  geom_line(size = 1, color = \"aquamarine\") +\n  geom_point(size = 3) +\n  geom_errorbar(aes(ymin = lower, ymax = upper), width = 0.2, alpha = 0.5) +\n  labs(\n    title = \"Age Range on Admit to Physical Therapy Wait Time\",\n    x = \"Age\",\n    y = \"Time from Admit to Physical Therapy\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nTop of Tabset\n\n\n\n\nQI-Initiative\nThe QI-initiative successfully decreased the wait time from admit to therapy, with patients admitted in the During period having a mean wait time that was 0.40 times that of patients in the Pre period (p &lt; 0.0001), and patients admitted in the Post period having a wait time that was 0.51 times that of patients in the Pre period (p &lt; 0.0001).\nAdditionally, there was a slight increase in wait time from admit to therapy after the QI-initiative, with those admitted in the Post time period having a mean wait time that was 1.26 times higher than that of patients during the During period (p &lt; 0.0001).\nThe QI-initiative effectively halved the wait time from admission to therapy!\n\n\nAge\nThere was an overall effect where patients that were 50 and older had an average admit to therapy wait time that was faster than patients who were 18-39. This indicates that patients who were 50 and older were being prioritized to receive physical therapy sooner.\n\n\nSeasonality\nThere were significant differences in wait time from admit to therapy based on the month the patient was admitted, likely just due to differences in staffing (such as in December and January), making this important that we included Admission Month as a covariate in our model that we controlled for.\nTop of Tabset"
  },
  {
    "objectID": "Statistical_Consulting/Vieau_Project_01/Code/Megan_Watson_Project.html#Goal_3",
    "href": "Statistical_Consulting/Vieau_Project_01/Code/Megan_Watson_Project.html#Goal_3",
    "title": "Assessing the Impact of a Physical Therapy Quality-Improvement Initiative at a Colorado University Hospital",
    "section": "Goal 3: Increase Percentage of Admissions Receiving Physical Therapy",
    "text": "Goal 3: Increase Percentage of Admissions Receiving Physical Therapy\n\nVisualizationAnalysisInterpretationVisualization IISummary\n\n\n\n# Convert factor to numeric \ndata_ex$PT_YN &lt;- as.numeric(data_ex$PT_YN)\ndata_ex$PT_YN &lt;- ifelse(data_ex$PT_YN == 1, 0, 1)\n\n# Summarize and calculate percentages\nsummary_data &lt;- data_ex |&gt; \n  group_by(Admit_MonthYear, PT_YN) |&gt; \n  summarise(count = n()) |&gt; \n  mutate(total_count = sum(count)) |&gt; \n  ungroup() |&gt; \n  mutate(percentage = (count / total_count) * 100) |&gt; \n  filter(PT_YN == 1)\n\n`summarise()` has grouped output by 'Admit_MonthYear'. You can override using\nthe `.groups` argument.\n\n# Plot the data\nggplot(summary_data, aes(x = Admit_MonthYear, y = percentage)) +\n  geom_line() +\n  theme_minimal() +\n  geom_rect(data = highlight_periods, aes(xmin = start, xmax = end, ymin = -Inf, ymax = Inf), \n            inherit.aes = FALSE, fill = \"lightblue\", alpha = 0.2) +\n  annotate(\"text\", x = as.Date(\"2015-09-01\"), y = Inf, \n           label = \"QI-Initiative\", color = \"skyblue2\", size = 4, vjust = 1.5) +\n  labs(title = \"Percentage of Admissions Receiving Physical Therapy\",\n       x = \"Admit Date\",\n       y = \"Percentage\")\n\n\n\n\n\n\n\n\nThey successfully increased the percentage of patients receiving PT from ~40% pre-initiative to ~80% during the initiative. This percentage then decreases to ~70% post-initiative\nTop of Tabset\n\n\nFor this model, we will perform a logistic regression to see if the likelihood of receiving physical therapy differs by QI-initiative time period.\n\nPerforming the Logistic Regression\n\n# Change reference level\ndata_ex$initiative &lt;- relevel(data_ex$initiative, ref = \"Pre\")\n\n# Fit the logistic regression\nmodel_goal3 &lt;- glm(`PT_YN` ~ initiative + Age_Range + Month, data = data_ex, family = binomial())\n\n# Examine Model\nsummary(model_goal3)\n\n\nCall:\nglm(formula = PT_YN ~ initiative + Age_Range + Month, family = binomial(), \n    data = data_ex)\n\nCoefficients:\n                   Estimate Std. Error z value             Pr(&gt;|z|)    \n(Intercept)      -0.6600359  0.1161250  -5.684    0.000000013170247 ***\ninitiativeDuring  2.2619119  0.1251165  18.078 &lt; 0.0000000000000002 ***\ninitiativePost    1.5726502  0.0597410  26.324 &lt; 0.0000000000000002 ***\nAge_Range40-49    0.1942581  0.0991683   1.959               0.0501 .  \nAge_Range50-59    0.3536368  0.0875596   4.039    0.000053722360965 ***\nAge_Range60-69    0.6444062  0.0882687   7.301    0.000000000000287 ***\nAge_Range70+      0.7645952  0.0924485   8.271 &lt; 0.0000000000000002 ***\nMonth2            0.0289775  0.1356988   0.214               0.8309    \nMonth3           -0.0284030  0.1314728  -0.216               0.8290    \nMonth4            0.0623378  0.1354837   0.460               0.6454    \nMonth5           -0.1220391  0.1395826  -0.874               0.3819    \nMonth6            0.0533113  0.1439204   0.370               0.7111    \nMonth7           -0.1210821  0.1423204  -0.851               0.3949    \nMonth8           -0.1722675  0.1378900  -1.249               0.2116    \nMonth9           -0.1891596  0.1357869  -1.393               0.1636    \nMonth10          -0.0008659  0.1338243  -0.006               0.9948    \nMonth11          -0.0928069  0.1371891  -0.676               0.4987    \nMonth12          -0.0974021  0.1348638  -0.722               0.4702    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 8443.4  on 6412  degrees of freedom\nResidual deviance: 7362.3  on 6395  degrees of freedom\n  (16 observations deleted due to missingness)\nAIC: 7398.3\n\nNumber of Fisher Scoring iterations: 4\n\nplot(model_goal3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Get 95% CI's\nCIs &lt;- confint(model_goal3)\n\nWaiting for profiling to be done...\n\npretty_print(CIs)\n\n\n\n\n\n2.5 %\n97.5 %\n\n\n\n\n(Intercept)\n-0.8876220\n-0.4322546\n\n\ninitiativeDuring\n2.0221592\n2.5131628\n\n\ninitiativePost\n1.4560662\n1.6902706\n\n\nAge_Range40-49\n0.0000494\n0.3888507\n\n\nAge_Range50-59\n0.1821466\n0.5254259\n\n\nAge_Range60-69\n0.4716468\n0.8177081\n\n\nAge_Range70+\n0.5837992\n0.9462512\n\n\nMonth2\n-0.2370064\n0.2950948\n\n\nMonth3\n-0.2863169\n0.2291997\n\n\nMonth4\n-0.2030680\n0.3281957\n\n\nMonth5\n-0.3955353\n0.1518012\n\n\nMonth6\n-0.2283691\n0.3360005\n\n\nMonth7\n-0.3999221\n0.1581532\n\n\nMonth8\n-0.4426274\n0.0980607\n\n\nMonth9\n-0.4555713\n0.0768597\n\n\nMonth10\n-0.2632707\n0.2614679\n\n\nMonth11\n-0.3617920\n0.1761451\n\n\nMonth12\n-0.3618651\n0.1669516\n\n\n\n\n\n\n\n\n\nChange the Reference Category\n\n# Change reference level\ndata_ex$initiative &lt;- relevel(data_ex$initiative, ref = \"During\")\n\n# Fit the logistic regression\nmodel_goal3 &lt;- glm(`PT_YN` ~ initiative + Age_Range + Month, data = data_ex, family = binomial())\n\n# Examine Model\nsummary(model_goal3)\n\n\nCall:\nglm(formula = PT_YN ~ initiative + Age_Range + Month, family = binomial(), \n    data = data_ex)\n\nCoefficients:\n                 Estimate Std. Error z value             Pr(&gt;|z|)    \n(Intercept)     1.6018760  0.1650691   9.704 &lt; 0.0000000000000002 ***\ninitiativePre  -2.2619119  0.1251165 -18.078 &lt; 0.0000000000000002 ***\ninitiativePost -0.6892617  0.1275136  -5.405    0.000000064664060 ***\nAge_Range40-49  0.1942581  0.0991683   1.959               0.0501 .  \nAge_Range50-59  0.3536368  0.0875596   4.039    0.000053722360965 ***\nAge_Range60-69  0.6444062  0.0882687   7.301    0.000000000000287 ***\nAge_Range70+    0.7645952  0.0924485   8.271 &lt; 0.0000000000000002 ***\nMonth2          0.0289775  0.1356988   0.214               0.8309    \nMonth3         -0.0284030  0.1314728  -0.216               0.8290    \nMonth4          0.0623378  0.1354837   0.460               0.6454    \nMonth5         -0.1220391  0.1395826  -0.874               0.3819    \nMonth6          0.0533113  0.1439204   0.370               0.7111    \nMonth7         -0.1210821  0.1423204  -0.851               0.3949    \nMonth8         -0.1722675  0.1378900  -1.249               0.2116    \nMonth9         -0.1891596  0.1357869  -1.393               0.1636    \nMonth10        -0.0008659  0.1338243  -0.006               0.9948    \nMonth11        -0.0928069  0.1371891  -0.676               0.4987    \nMonth12        -0.0974021  0.1348638  -0.722               0.4702    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 8443.4  on 6412  degrees of freedom\nResidual deviance: 7362.3  on 6395  degrees of freedom\n  (16 observations deleted due to missingness)\nAIC: 7398.3\n\nNumber of Fisher Scoring iterations: 4\n\n# Get 95% CI's\nCIs &lt;- confint(model_goal3)\n\nWaiting for profiling to be done...\n\npretty_print(CIs)\n\n\n\n\n\n2.5 %\n97.5 %\n\n\n\n\n(Intercept)\n1.2817567\n1.9291957\n\n\ninitiativePre\n-2.5131628\n-2.0221592\n\n\ninitiativePost\n-0.9448513\n-0.4444350\n\n\nAge_Range40-49\n0.0000494\n0.3888507\n\n\nAge_Range50-59\n0.1821466\n0.5254259\n\n\nAge_Range60-69\n0.4716468\n0.8177081\n\n\nAge_Range70+\n0.5837992\n0.9462512\n\n\nMonth2\n-0.2370064\n0.2950948\n\n\nMonth3\n-0.2863169\n0.2291997\n\n\nMonth4\n-0.2030680\n0.3281957\n\n\nMonth5\n-0.3955353\n0.1518012\n\n\nMonth6\n-0.2283691\n0.3360005\n\n\nMonth7\n-0.3999221\n0.1581532\n\n\nMonth8\n-0.4426274\n0.0980607\n\n\nMonth9\n-0.4555713\n0.0768597\n\n\nMonth10\n-0.2632707\n0.2614679\n\n\nMonth11\n-0.3617920\n0.1761451\n\n\nMonth12\n-0.3618651\n0.1669516\n\n\n\n\n\n\n\nTop of Tabset\n\n\n\n\nMain Effects\nQI-initiative time period is a significant predictor for the likelihood of receiving PT, while controlling for age and admission month. Compared to patients in the Pre time period, the odds of receiving physical therapy in the During period were approximately 9.52 times higher (z = 18.144, 95% CI: [7.51, 12.22], p &lt; 0.0001), and the odds of receiving physical therapy in the Post time period were 4.84 times higher (z = 26.41, 95% CI: [4.31, 5.44], p &lt; 0.0001). The odds of receiving physical therapy in the Post time period were 0.51 the odds of receiving physical therapy in the during period (z = -5.35, 95% CI: [0.39,0.65], p &lt; 0.0001).\n\n\nAge\nAge was a significant predictor for the likelihood of receiving PT, while controlling for QI-initiative time period and admission month. Compared to those who were 18-39, the odds of receiving physical therapy were 1.42 times higher for patients aged 50-59 (z = 4.04, 95% CI: [1.20, 1.69], p &lt; 0.0001), 1.91 times higher for patients aged 60-69 (z = 7.30, 95% CI: [1.60, 2.27], p &lt; 0.0001 ), and 2.15 times higher for patients age 70+ (z = 8.273, 95% CI: [1.79,], p = 2.58). The odds of receiving PT did not differ significantly between patients who were 18-39 and those who were 40-49 (z = 1.96, 95% CI: [1.00, 1.48], p = 0.0502).\n\n\nSeasonality\nAdmission Month was not a significant predictor for the likelihood of receiving PT (p’s for each month &gt; 0.5), but was included in the final model to account for potential seasonality effects.\nTop of Tabset\n\n\n\n\n# Change reference level\ndata_ex$initiative &lt;- relevel(data_ex$initiative, ref = \"Pre\")\n\n# Fit the logistic regression\nmodel_goal3 &lt;- glm(`PT_YN` ~ initiative + Age_Range + Month, data = data_ex, family = binomial())\n\n# Plot effects\nplot(allEffects(model_goal3),\n     ylab = \"Likelihood of Receiving PT\")\n\n\n\n\n\n\n\n\nTop of Tabset\n\n\n\nQI-Initiative\nThe QI-initiative was successful at increasing the likelihood that patients received physical therapy. Compared to patients in the Pre time period, the odds of receiving physical therapy in the During period were approximately 9.52 times higher (p &lt; 0.0001), and the odds of receiving physical therapy in the Post time period were 4.84 times higher (p &lt; 0.0001).\nAdditionally, there was a decrease in the likelihood of receiving PT after the During period, with the odds of receiving physical therapy in the Post time period being 0.51 the odds of receiving physical therapy in the during period (p &lt; 0.0001).\n\n\nAge\nThere was an overall effect, where patients 50 years and older were more likely to receive PT compared to those 18-39 years old, while controlling for QI-initiative time period and admission month.\nThis is again evidence that older patients were receiving priority for PT.\nTop of Tabset"
  },
  {
    "objectID": "Statistical_Consulting/Vieau_Project_01/Code/Megan_Watson_Project.html#MICU_LOS",
    "href": "Statistical_Consulting/Vieau_Project_01/Code/Megan_Watson_Project.html#MICU_LOS",
    "title": "Assessing the Impact of a Physical Therapy Quality-Improvement Initiative at a Colorado University Hospital",
    "section": "MICU Length of Stay",
    "text": "MICU Length of Stay\nThe primary question A of the study was whether the QI-initiative successfully decreased the MICU length of stay.\nTo assess this we will run a Poisson or Negative Binomial Regression model predicting UNIT LOS:\n\\(Log( E(MICU LOS | QI-Initiative_i ) ) = β_0 + β_1 * QI-Initiative_i + β2 *Covariates_i + ...\\)\n\nCovariatesModel SelectionAnalysisInterpretationVisualizationSummary\n\n\n\nCovariate Selection\nThe variables that were found to be correlated with MICU LOS were:\n\nHOSPITAL_LOS\nADMIT.TO.CONSULT\nPT_VISITS\nDAYS.WITH.THERAPY.VISITS\nAVERAGE THERAPY LENGTH\nMAX THERAPY LENGTH\n\nAVERAGE THERAPY LENGTH and MAX THERAPY LENGTH blank are just different ways of measuring the same variable, and thus only AVERAGE THERAPY LENGTH will be considered as a covariate because it is more relevant in the context of the study. AVERAGE THERAPY LENGTH is important to control for, because it gives us a final model demonstrating the impact of the QI-initiative time period, regardless of how long the therapy visits were for each patient.\nSimilarly, PT_VISITS and DAYS.WITH.THERAPY.VISITS are essentially the same variable, and thus only PT_VISITS will be selected because it was used in the previous analyses and is therefore more relevant. PT_VISITS will be imporant to account for because it will provide us with a model that demonstrates significance of the QI-initiative, regardless of how many PT visits each patient received.\nADMIT TO CONSULT will not be selected due to ~20% missinginess, and we do not want to exclude patients in this study that did not receive a consultation.\nHOSPITAL_LOS will not be include because it will be an outcome variable and will be analyzed in a separate model. Furthermore, MICU LOS is directly related to HOSPITAL_LOS, and I want to avoid redundant variables due to issues of collinearity (This would be akin to include BMI in a model predicting Height).\nAge_Range will be included to account for age and because it was a significant predictor in the previous 3 analyses.\nMonth will be included because we want to control for potential seasonality.\n\nInteraction Terms\nPreliminary visualization demonstrates that there may be a initiative*MV_YN*PT_YN interaction. Where the initiative only decreased MICU LOS for patients who were mechanically ventilated and received physical therapy.\n\n# Custom labeller function \ncustom_labels &lt;- as_labeller(c(`0` = \"No Mechanical Ventilation\", `1` = \"Mechanical Ventilation\"))\n\n# Summarize and plot the data\ndata_ex |&gt; \n  group_by(Admit_MonthYear, MV_YN, PT_YN) |&gt; \n  summarise(avg_Unit_LOS = mean(Unit_LOS, na.rm = T)) |&gt; \n  ggplot(aes(x = Admit_MonthYear, y = avg_Unit_LOS, color = as.factor(PT_YN))) +\n  geom_point() +\n  geom_smooth(method = \"loess\", se = F) +\n  theme_minimal() +\n  geom_rect(data = highlight_periods, aes(xmin = start, xmax = end, ymin = -Inf, ymax = Inf), \n            inherit.aes = FALSE, fill = \"lightblue\", alpha = 0.2) +\n  labs(title = \"Average MICU LOS over Time by Mechanical Ventilation\",\n       x = \"Admit Date\",\n       y = \"Avg MICU LOS\",\n       color = \"Received Physical Therapy\") +\n  facet_wrap(~MV_YN, labeller = custom_labels)\n\n`summarise()` has grouped output by 'Admit_MonthYear', 'MV_YN'. You can\noverride using the `.groups` argument.\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nThus, interaction terms for both initiative*MV_YN and initiative*PT_YN will be included as well.\n\n\n\nFinal Covariates\nThe final model will include the following potential covariates:\n\nPT_VISITS\nAVERAGE THERAPY LENGTH\nMV_YN\ninitiative*MV_YN\nPT_YN\ninitiative*PT_YN\n\nInclusion of variables into the final model will be decided via BIC using backwards elimination.\nTop of Tabset\n\n\n\n\nCheck for Zero Inflation\nFirst we will consider if zero inflation is necessary for UNIT_LOS.\n\n# Plot histogram of MICU LOS\nhist(data_ex$Unit_LOS)\n\n\n\n\n\n\n\n\nMICU LOS` is not zero inflated and thus we do not need to consider ZIF for our model.\n\n\nCheck for Overdispersion\n\n# Compare mean and variance\npaste0(\"Mean = \", mean(data_ex$Unit_LOS, na.rm = T))\n\n[1] \"Mean = 138.781303468658\"\n\n# Compare mean and variance\npaste0(\"Variance = \", var(data_ex$Unit_LOS, na.rm = T))\n\n[1] \"Variance = 12778.5408389354\"\n\n\nOur variance is dramatically higher than the mean, and thus we have overdispersion and the negative binomial model is more appropriate than the Poisson regression.\n\n\nFull Model\nWe will first begin with a saturated model of all potential covariates.\n\n# Change reference level\ndata_ex$initiative &lt;- relevel(data_ex$initiative, ref = \"Pre\")\n\n# Fit a Negative Binomial model\nmodel_full &lt;- glm.nb(Unit_LOS ~ initiative + Age_Range + Month + PT_VISITS + `AVG THERAPY LENGTH` + MV_YN + initiative*MV_YN + PT_YN + initiative*PT_YN, data = data_ex)\n\n# Examine Model\nsummary(model_full)\n\n\nCall:\nglm.nb(formula = Unit_LOS ~ initiative + Age_Range + Month + \n    PT_VISITS + `AVG THERAPY LENGTH` + MV_YN + initiative * MV_YN + \n    PT_YN + initiative * PT_YN, data = data_ex, init.theta = 4.289646599, \n    link = log)\n\nCoefficients:\n                         Estimate Std. Error z value             Pr(&gt;|z|)    \n(Intercept)             4.3823672  0.0282469 155.145 &lt; 0.0000000000000002 ***\ninitiativeDuring       -0.1453146  0.0604645  -2.403              0.01625 *  \ninitiativePost         -0.0822917  0.0266455  -3.088              0.00201 ** \nAge_Range40-49          0.0317567  0.0221104   1.436              0.15092    \nAge_Range50-59          0.0108876  0.0195151   0.558              0.57691    \nAge_Range60-69         -0.0147466  0.0193828  -0.761              0.44677    \nAge_Range70+           -0.0427559  0.0201383  -2.123              0.03374 *  \nMonth2                  0.0022386  0.0298377   0.075              0.94020    \nMonth3                 -0.0489545  0.0291545  -1.679              0.09312 .  \nMonth4                  0.0068517  0.0291669   0.235              0.81427    \nMonth5                 -0.0367546  0.0303158  -1.212              0.22536    \nMonth6                 -0.0108190  0.0311224  -0.348              0.72812    \nMonth7                  0.0071728  0.0310207   0.231              0.81714    \nMonth8                  0.0341580  0.0302281   1.130              0.25847    \nMonth9                  0.0508336  0.0300095   1.694              0.09028 .  \nMonth10                -0.0231288  0.0293877  -0.787              0.43127    \nMonth11                 0.0319420  0.0300705   1.062              0.28813    \nMonth12                 0.0478080  0.0295638   1.617              0.10585    \nPT_VISITS               0.1383907  0.0030719  45.050 &lt; 0.0000000000000002 ***\n`AVG THERAPY LENGTH`   -0.0013767  0.0006085  -2.262              0.02367 *  \nMV_YN                   0.5787710  0.0184760  31.326 &lt; 0.0000000000000002 ***\nPT_YN                   0.2319438  0.0262562   8.834 &lt; 0.0000000000000002 ***\ninitiativeDuring:MV_YN -0.2381179  0.0431943  -5.513         0.0000000353 ***\ninitiativePost:MV_YN   -0.1415499  0.0263292  -5.376         0.0000000761 ***\ninitiativeDuring:PT_YN -0.2696489  0.0610438  -4.417         0.0000099941 ***\ninitiativePost:PT_YN   -0.2621947  0.0292363  -8.968 &lt; 0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Negative Binomial(4.2896) family taken to be 1)\n\n    Null deviance: 12428.2  on 6404  degrees of freedom\nResidual deviance:  6594.7  on 6379  degrees of freedom\n  (24 observations deleted due to missingness)\nAIC: 69828\n\nNumber of Fisher Scoring iterations: 1\n\n              Theta:  4.2896 \n          Std. Err.:  0.0755 \n\n 2 x log-likelihood:  -69773.6730 \n\n# Examine multicollinearity\nvif(model_full, type = 'predictor')\n\nWarning in vif.lm(model_full, type = \"predictor\"): type = 'predictor' is available only for unweighted linear models;\n  type = 'terms' will be used\n\n\n                          GVIF Df GVIF^(1/(2*Df))\ninitiative           36.811604  2        2.463180\nAge_Range             1.039062  4        1.004801\nMonth                 1.084741 11        1.003704\nPT_VISITS             1.601575  1        1.265534\n`AVG THERAPY LENGTH`  3.519915  1        1.876144\nMV_YN                 2.258908  1        1.502966\nPT_YN                 4.241298  1        2.059441\ninitiative:MV_YN      7.674538  2        1.664420\ninitiative:PT_YN     37.486092  2        2.474387\n\n\nThis provides a highly significant model, where the impact of the QI-initiative depends on both whether a patient was mechanically ventilated AND whether they received PT or not.\nHowever, there are issues of multicollinearity for the initiative*PT_YN interaction term (GVIF = 36.93). Thus, the initiative*PT_YN interaction term will be dropped from further analysis to account for multicollinearity (and to make the model simpler to interpret/more parsimonious).\n\n\nBackwards Elimination\nWe will be forcing in the inclusion of Age_Range, since we saw in our previous models that it is a significant predictor of the outcome variables (and also because it makes sense that older people will stay in the MICU longer, and this just seems like something people would ask if we controlled for).\nSimilarly, Month will be included to account for the seasonality we have seen in previous models and in our plots.\n\n# Import the vif function from the MASS package \nlibrary(MASS)\n\n# Get cleaned data set with missing values omitted because this is necessary with the MASS package\ndata_ex_clean &lt;- data_ex |&gt; \n  dplyr::select(Unit_LOS, initiative, Age_Range, Month, PT_VISITS, `AVG THERAPY LENGTH`, MV_YN, PT_YN)\n\n# Remove rows with missing values\ndata_ex_clean &lt;- na.omit(data_ex_clean)\n\n# Fit a Negative Binomial model\nmodel_MICU1 &lt;- glm.nb(Unit_LOS ~ initiative + Age_Range + Month + PT_VISITS + `AVG THERAPY LENGTH` + MV_YN + initiative*MV_YN + PT_YN, data = data_ex_clean)\n\n# Set n to the number of observations in the dataset\nn &lt;- nrow(data_ex_clean)\n\n# Define the scope to force the inclusion of specific variables\nscope &lt;- list(lower = ~ initiative + Month + Age_Range, upper = formula(model_MICU1))\n\n# Perform backwards elimination using BIC\nbackwards_model_MICU_bic &lt;- stepAIC(model_MICU1, direction = \"backward\", k = log(n), scope = scope)\n\nStart:  AIC=70072.06\nUnit_LOS ~ initiative + Age_Range + Month + PT_VISITS + `AVG THERAPY LENGTH` + \n    MV_YN + initiative * MV_YN + PT_YN\n\n                       Df   AIC\n- `AVG THERAPY LENGTH`  1 70070\n&lt;none&gt;                    70072\n- PT_YN                 1 70093\n- initiative:MV_YN      2 70101\n- PT_VISITS             1 71847\n\nStep:  AIC=70070.11\nUnit_LOS ~ initiative + Age_Range + Month + PT_VISITS + MV_YN + \n    PT_YN + initiative:MV_YN\n\n                   Df   AIC\n&lt;none&gt;                70070\n- PT_YN             1 70090\n- initiative:MV_YN  2 70097\n- PT_VISITS         1 71904\n\n# Summary of the final model\nsummary(backwards_model_MICU_bic)\n\n\nCall:\nglm.nb(formula = Unit_LOS ~ initiative + Age_Range + Month + \n    PT_VISITS + MV_YN + PT_YN + initiative:MV_YN, data = data_ex_clean, \n    init.theta = 4.229383529, link = log)\n\nCoefficients:\n                        Estimate Std. Error z value             Pr(&gt;|z|)    \n(Intercept)             4.432551   0.028026 158.161 &lt; 0.0000000000000002 ***\ninitiativeDuring       -0.329237   0.031607 -10.417 &lt; 0.0000000000000002 ***\ninitiativePost         -0.242224   0.019575 -12.374 &lt; 0.0000000000000002 ***\nAge_Range40-49          0.028116   0.022252   1.264               0.2064    \nAge_Range50-59          0.005061   0.019641   0.258               0.7967    \nAge_Range60-69         -0.020969   0.019499  -1.075               0.2822    \nAge_Range70+           -0.046988   0.020263  -2.319               0.0204 *  \nMonth2                  0.006697   0.030036   0.223               0.8236    \nMonth3                 -0.044043   0.029333  -1.501               0.1332    \nMonth4                  0.007337   0.029360   0.250               0.8027    \nMonth5                 -0.035539   0.030517  -1.165               0.2442    \nMonth6                 -0.009427   0.031329  -0.301               0.7635    \nMonth7                  0.007389   0.031227   0.237               0.8130    \nMonth8                  0.038125   0.030426   1.253               0.2102    \nMonth9                  0.054237   0.030201   1.796               0.0725 .  \nMonth10                -0.014733   0.029567  -0.498               0.6183    \nMonth11                 0.038252   0.030263   1.264               0.2062    \nMonth12                 0.055339   0.029741   1.861               0.0628 .  \nPT_VISITS               0.132363   0.003005  44.047 &lt; 0.0000000000000002 ***\nMV_YN                   0.583938   0.018565  31.454 &lt; 0.0000000000000002 ***\nPT_YN                   0.084725   0.015732   5.386         0.0000000722 ***\ninitiativeDuring:MV_YN -0.226734   0.043356  -5.230         0.0000001699 ***\ninitiativePost:MV_YN   -0.146180   0.026496  -5.517         0.0000000345 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Negative Binomial(4.2294) family taken to be 1)\n\n    Null deviance: 12259.1  on 6404  degrees of freedom\nResidual deviance:  6599.3  on 6382  degrees of freedom\nAIC: 69917\n\nNumber of Fisher Scoring iterations: 1\n\n              Theta:  4.2294 \n          Std. Err.:  0.0744 \n\n 2 x log-likelihood:  -69868.5160 \n\n# Get VIFS\nvif(backwards_model_MICU_bic)\n\nthere are higher-order terms (interactions) in this model\nconsider setting type = 'predictor'; see ?vif\n\n\n                     GVIF Df GVIF^(1/(2*Df))\ninitiative       4.884285  2        1.486621\nAge_Range        1.035810  4        1.004408\nMonth            1.078177 11        1.003427\nPT_VISITS        1.511405  1        1.229392\nMV_YN            2.249919  1        1.499973\nPT_YN            1.502044  1        1.225579\ninitiative:MV_YN 7.621794  2        1.661553\n\n\nThe resulting model omits only AVG THERAPY LENGTH. There are no issues with multicollinearity.\nThis will be selected as the final model.\nTop of Tabset\n\n\n\n\n# Change MV_YN to a factor so I can change levels\ndata_ex$MV_YN &lt;- factor(data_ex$MV_YN)\n\n# Change reference level\ndata_ex$initiative &lt;- relevel(data_ex$initiative, ref = \"Pre\")\n\n# Change reference level\ndata_ex$MV_YN &lt;- relevel(data_ex$MV_YN, ref = \"0\")\n\n# Fit a Negative Binomial model\nmodel_MICU &lt;- glm.nb(Unit_LOS ~ initiative + Age_Range + Month + PT_VISITS + MV_YN + initiative*MV_YN + PT_YN, data = data_ex)\n\n# Examine Model\nsummary(model_MICU)\n\n\nCall:\nglm.nb(formula = Unit_LOS ~ initiative + Age_Range + Month + \n    PT_VISITS + MV_YN + initiative * MV_YN + PT_YN, data = data_ex, \n    init.theta = 4.232773893, link = log)\n\nCoefficients:\n                         Estimate Std. Error z value             Pr(&gt;|z|)    \n(Intercept)              4.432198   0.027994 158.327 &lt; 0.0000000000000002 ***\ninitiativeDuring        -0.329529   0.031585 -10.433 &lt; 0.0000000000000002 ***\ninitiativePost          -0.242447   0.019556 -12.398 &lt; 0.0000000000000002 ***\nAge_Range40-49           0.028817   0.022237   1.296               0.1950    \nAge_Range50-59           0.004942   0.019628   0.252               0.8012    \nAge_Range60-69          -0.021275   0.019482  -1.092               0.2748    \nAge_Range70+            -0.047183   0.020252  -2.330               0.0198 *  \nMonth2                   0.007514   0.029983   0.251               0.8021    \nMonth3                  -0.043533   0.029296  -1.486               0.1373    \nMonth4                   0.007931   0.029309   0.271               0.7867    \nMonth5                  -0.034947   0.030479  -1.147               0.2516    \nMonth6                  -0.008850   0.031292  -0.283               0.7773    \nMonth7                   0.007945   0.031190   0.255               0.7989    \nMonth8                   0.037995   0.030361   1.251               0.2108    \nMonth9                   0.054427   0.030149   1.805               0.0710 .  \nMonth10                 -0.013038   0.029518  -0.442               0.6587    \nMonth11                  0.038812   0.030225   1.284               0.1991    \nMonth12                  0.055942   0.029703   1.883               0.0596 .  \nPT_VISITS                0.132325   0.002995  44.186 &lt; 0.0000000000000002 ***\nMV_YN1                   0.583684   0.018550  31.466 &lt; 0.0000000000000002 ***\nPT_YN                    0.084957   0.015716   5.406         0.0000000645 ***\ninitiativeDuring:MV_YN1 -0.227170   0.043319  -5.244         0.0000001570 ***\ninitiativePost:MV_YN1   -0.146015   0.026476  -5.515         0.0000000349 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Negative Binomial(4.2328) family taken to be 1)\n\n    Null deviance: 12297.6  on 6412  degrees of freedom\nResidual deviance:  6607.5  on 6390  degrees of freedom\n  (16 observations deleted due to missingness)\nAIC: 70010\n\nNumber of Fisher Scoring iterations: 1\n\n              Theta:  4.2328 \n          Std. Err.:  0.0744 \n\n 2 x log-likelihood:  -69962.1400 \n\n# Examine multicollinearity\nvif(model_MICU, type = 'predictor')\n\nWarning in vif.lm(model_MICU, type = \"predictor\"): type = 'predictor' is available only for unweighted linear models;\n  type = 'terms' will be used\n\n\n                     GVIF Df GVIF^(1/(2*Df))\ninitiative       4.890998  2        1.487131\nAge_Range        1.036016  4        1.004433\nMonth            1.078731 11        1.003451\nPT_VISITS        1.512968  1        1.230028\nMV_YN            2.250853  1        1.500284\nPT_YN            1.501266  1        1.225262\ninitiative:MV_YN 7.645003  2        1.662817\n\n# Get 95% CIs\npretty_print(confint(model_MICU))\n\nWaiting for profiling to be done...\n\n\n\n\n\n\n2.5 %\n97.5 %\n\n\n\n\n(Intercept)\n4.3773779\n4.4874118\n\n\ninitiativeDuring\n-0.3909240\n-0.2674550\n\n\ninitiativePost\n-0.2804583\n-0.2043883\n\n\nAge_Range40-49\n-0.0147374\n0.0724346\n\n\nAge_Range50-59\n-0.0335755\n0.0433957\n\n\nAge_Range60-69\n-0.0594529\n0.0168284\n\n\nAge_Range70+\n-0.0868476\n-0.0075588\n\n\nMonth2\n-0.0512439\n0.0663064\n\n\nMonth3\n-0.1009748\n0.0138892\n\n\nMonth4\n-0.0495681\n0.0654062\n\n\nMonth5\n-0.0946755\n0.0248351\n\n\nMonth6\n-0.0701164\n0.0525371\n\n\nMonth7\n-0.0531437\n0.0691373\n\n\nMonth8\n-0.0215312\n0.0975662\n\n\nMonth9\n-0.0046407\n0.1135281\n\n\nMonth10\n-0.0708740\n0.0447881\n\n\nMonth11\n-0.0204323\n0.0980925\n\n\nMonth12\n-0.0022597\n0.1141461\n\n\nPT_VISITS\n0.1259558\n0.1387635\n\n\nMV_YN1\n0.5473376\n0.6200724\n\n\nPT_YN\n0.0537847\n0.1161101\n\n\ninitiativeDuring:MV_YN1\n-0.3120612\n-0.1423300\n\n\ninitiativePost:MV_YN1\n-0.1979045\n-0.0941728\n\n\n\n\n\n\n\n\nChange Reference Level to During and Non-Mechanical Ventilation\n\n# Change MV_YN to a factor so I can change levels\ndata_ex$MV_YN &lt;- factor(data_ex$MV_YN)\n\n# Change reference level\ndata_ex$initiative &lt;- relevel(data_ex$initiative, ref = \"During\")\n\n# Change reference level\ndata_ex$MV_YN &lt;- relevel(data_ex$MV_YN, ref = \"0\")\n\n# Fit a Negative Binomial model\nmodel_MICU &lt;- glm.nb(Unit_LOS ~ initiative + Age_Range + Month + PT_VISITS + MV_YN + initiative*MV_YN + PT_YN, data = data_ex)\n\n# Examine Model\nsummary(model_MICU)\n\n\nCall:\nglm.nb(formula = Unit_LOS ~ initiative + Age_Range + Month + \n    PT_VISITS + MV_YN + initiative * MV_YN + PT_YN, data = data_ex, \n    init.theta = 4.232773893, link = log)\n\nCoefficients:\n                       Estimate Std. Error z value             Pr(&gt;|z|)    \n(Intercept)            4.102669   0.040206 102.041 &lt; 0.0000000000000002 ***\ninitiativePre          0.329529   0.031585  10.433 &lt; 0.0000000000000002 ***\ninitiativePost         0.087082   0.031707   2.746              0.00602 ** \nAge_Range40-49         0.028817   0.022237   1.296              0.19501    \nAge_Range50-59         0.004942   0.019628   0.252              0.80119    \nAge_Range60-69        -0.021275   0.019482  -1.092              0.27482    \nAge_Range70+          -0.047183   0.020252  -2.330              0.01982 *  \nMonth2                 0.007514   0.029983   0.251              0.80211    \nMonth3                -0.043533   0.029296  -1.486              0.13728    \nMonth4                 0.007931   0.029309   0.271              0.78671    \nMonth5                -0.034947   0.030479  -1.147              0.25155    \nMonth6                -0.008850   0.031292  -0.283              0.77732    \nMonth7                 0.007945   0.031190   0.255              0.79894    \nMonth8                 0.037995   0.030361   1.251              0.21078    \nMonth9                 0.054427   0.030149   1.805              0.07104 .  \nMonth10               -0.013038   0.029518  -0.442              0.65872    \nMonth11                0.038812   0.030225   1.284              0.19910    \nMonth12                0.055942   0.029703   1.883              0.05965 .  \nPT_VISITS              0.132325   0.002995  44.186 &lt; 0.0000000000000002 ***\nMV_YN1                 0.356515   0.039214   9.092 &lt; 0.0000000000000002 ***\nPT_YN                  0.084957   0.015716   5.406         0.0000000645 ***\ninitiativePre:MV_YN1   0.227170   0.043319   5.244         0.0000001570 ***\ninitiativePost:MV_YN1  0.081155   0.043130   1.882              0.05989 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Negative Binomial(4.2328) family taken to be 1)\n\n    Null deviance: 12297.6  on 6412  degrees of freedom\nResidual deviance:  6607.5  on 6390  degrees of freedom\n  (16 observations deleted due to missingness)\nAIC: 70010\n\nNumber of Fisher Scoring iterations: 1\n\n              Theta:  4.2328 \n          Std. Err.:  0.0744 \n\n 2 x log-likelihood:  -69962.1400 \n\n# Examine multicollinearity\nvif(model_MICU, type = 'predictor')\n\nWarning in vif.lm(model_MICU, type = \"predictor\"): type = 'predictor' is available only for unweighted linear models;\n  type = 'terms' will be used\n\n\n                      GVIF Df GVIF^(1/(2*Df))\ninitiative        4.890998  2        1.487131\nAge_Range         1.036016  4        1.004433\nMonth             1.078731 11        1.003451\nPT_VISITS         1.512968  1        1.230028\nMV_YN            10.059119  1        3.171611\nPT_YN             1.501266  1        1.225262\ninitiative:MV_YN 22.969128  2        2.189203\n\n# Get 95% CIs\npretty_print(confint(model_MICU))\n\nWaiting for profiling to be done...\n\n\n\n\n\n\n2.5 %\n97.5 %\n\n\n\n\n(Intercept)\n4.0241777\n4.1817738\n\n\ninitiativePre\n0.2674550\n0.3909240\n\n\ninitiativePost\n0.0245821\n0.1489290\n\n\nAge_Range40-49\n-0.0147374\n0.0724346\n\n\nAge_Range50-59\n-0.0335755\n0.0433957\n\n\nAge_Range60-69\n-0.0594529\n0.0168284\n\n\nAge_Range70+\n-0.0868476\n-0.0075588\n\n\nMonth2\n-0.0512439\n0.0663064\n\n\nMonth3\n-0.1009748\n0.0138892\n\n\nMonth4\n-0.0495681\n0.0654062\n\n\nMonth5\n-0.0946755\n0.0248351\n\n\nMonth6\n-0.0701164\n0.0525371\n\n\nMonth7\n-0.0531437\n0.0691373\n\n\nMonth8\n-0.0215312\n0.0975662\n\n\nMonth9\n-0.0046407\n0.1135281\n\n\nMonth10\n-0.0708740\n0.0447881\n\n\nMonth11\n-0.0204323\n0.0980925\n\n\nMonth12\n-0.0022597\n0.1141461\n\n\nPT_VISITS\n0.1259558\n0.1387635\n\n\nMV_YN1\n0.2797462\n0.4332292\n\n\nPT_YN\n0.0537847\n0.1161101\n\n\ninitiativePre:MV_YN1\n0.1423300\n0.3120612\n\n\ninitiativePost:MV_YN1\n-0.0033873\n0.1657326\n\n\n\n\n\n\n\n\n\nChange Reference Level to Pre and Mechanical Ventilation\n\n# Change reference level\ndata_ex$initiative &lt;- relevel(data_ex$initiative, ref = \"Pre\")\n\n# Change reference level\ndata_ex$MV_YN &lt;- relevel(data_ex$MV_YN, ref = \"1\")\n\n# Fit a Negative Binomial model\nmodel_MICU &lt;- glm.nb(Unit_LOS ~ initiative + Age_Range + Month + PT_VISITS + MV_YN + initiative*MV_YN + PT_YN, data = data_ex)\n\n# Examine Model\nsummary(model_MICU)\n\n\nCall:\nglm.nb(formula = Unit_LOS ~ initiative + Age_Range + Month + \n    PT_VISITS + MV_YN + initiative * MV_YN + PT_YN, data = data_ex, \n    init.theta = 4.232773893, link = log)\n\nCoefficients:\n                         Estimate Std. Error z value             Pr(&gt;|z|)    \n(Intercept)              5.015882   0.027992 179.187 &lt; 0.0000000000000002 ***\ninitiativeDuring        -0.556698   0.031731 -17.544 &lt; 0.0000000000000002 ***\ninitiativePost          -0.388461   0.019312 -20.115 &lt; 0.0000000000000002 ***\nAge_Range40-49           0.028817   0.022237   1.296               0.1950    \nAge_Range50-59           0.004942   0.019628   0.252               0.8012    \nAge_Range60-69          -0.021275   0.019482  -1.092               0.2748    \nAge_Range70+            -0.047183   0.020252  -2.330               0.0198 *  \nMonth2                   0.007514   0.029983   0.251               0.8021    \nMonth3                  -0.043533   0.029296  -1.486               0.1373    \nMonth4                   0.007931   0.029309   0.271               0.7867    \nMonth5                  -0.034947   0.030479  -1.147               0.2516    \nMonth6                  -0.008850   0.031292  -0.283               0.7773    \nMonth7                   0.007945   0.031190   0.255               0.7989    \nMonth8                   0.037995   0.030361   1.251               0.2108    \nMonth9                   0.054427   0.030149   1.805               0.0710 .  \nMonth10                 -0.013038   0.029518  -0.442               0.6587    \nMonth11                  0.038812   0.030225   1.284               0.1991    \nMonth12                  0.055942   0.029703   1.883               0.0596 .  \nPT_VISITS                0.132325   0.002995  44.186 &lt; 0.0000000000000002 ***\nMV_YN0                  -0.583684   0.018550 -31.466 &lt; 0.0000000000000002 ***\nPT_YN                    0.084957   0.015716   5.406         0.0000000645 ***\ninitiativeDuring:MV_YN0  0.227170   0.043319   5.244         0.0000001570 ***\ninitiativePost:MV_YN0    0.146015   0.026476   5.515         0.0000000349 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Negative Binomial(4.2328) family taken to be 1)\n\n    Null deviance: 12297.6  on 6412  degrees of freedom\nResidual deviance:  6607.5  on 6390  degrees of freedom\n  (16 observations deleted due to missingness)\nAIC: 70010\n\nNumber of Fisher Scoring iterations: 1\n\n              Theta:  4.2328 \n          Std. Err.:  0.0744 \n\n 2 x log-likelihood:  -69962.1400 \n\n# Examine multicollinearity\nvif(model_MICU, type = 'predictor')\n\nWarning in vif.lm(model_MICU, type = \"predictor\"): type = 'predictor' is available only for unweighted linear models;\n  type = 'terms' will be used\n\n\n                     GVIF Df GVIF^(1/(2*Df))\ninitiative       4.493568  2        1.455955\nAge_Range        1.036016  4        1.004433\nMonth            1.078731 11        1.003451\nPT_VISITS        1.512968  1        1.230028\nMV_YN            2.250853  1        1.500284\nPT_YN            1.501266  1        1.225262\ninitiative:MV_YN 5.785111  2        1.550879\n\n# Get 95% CIs\npretty_print(confint(model_MICU))\n\nWaiting for profiling to be done...\n\n\n\n\n\n\n2.5 %\n97.5 %\n\n\n\n\n(Intercept)\n4.9610409\n5.0711372\n\n\ninitiativeDuring\n-0.6181693\n-0.4946384\n\n\ninitiativePost\n-0.4259046\n-0.3510561\n\n\nAge_Range40-49\n-0.0147374\n0.0724346\n\n\nAge_Range50-59\n-0.0335755\n0.0433957\n\n\nAge_Range60-69\n-0.0594529\n0.0168284\n\n\nAge_Range70+\n-0.0868476\n-0.0075588\n\n\nMonth2\n-0.0512439\n0.0663064\n\n\nMonth3\n-0.1009748\n0.0138892\n\n\nMonth4\n-0.0495681\n0.0654062\n\n\nMonth5\n-0.0946755\n0.0248351\n\n\nMonth6\n-0.0701164\n0.0525371\n\n\nMonth7\n-0.0531437\n0.0691373\n\n\nMonth8\n-0.0215312\n0.0975662\n\n\nMonth9\n-0.0046407\n0.1135281\n\n\nMonth10\n-0.0708740\n0.0447881\n\n\nMonth11\n-0.0204323\n0.0980925\n\n\nMonth12\n-0.0022597\n0.1141461\n\n\nPT_VISITS\n0.1259558\n0.1387635\n\n\nMV_YN0\n-0.6200724\n-0.5473376\n\n\nPT_YN\n0.0537847\n0.1161101\n\n\ninitiativeDuring:MV_YN0\n0.1423300\n0.3120612\n\n\ninitiativePost:MV_YN0\n0.0941728\n0.1979045\n\n\n\n\n\n\n\n\n\nChange Reference Level to During and Mechanical Ventilation\n\n# Change reference level\ndata_ex$initiative &lt;- relevel(data_ex$initiative, ref = \"During\")\n\n# Change reference level\ndata_ex$MV_YN &lt;- relevel(data_ex$MV_YN, ref = \"1\")\n\n# Fit a Negative Binomial model\nmodel_MICU &lt;- glm.nb(Unit_LOS ~ initiative + Age_Range + Month + PT_VISITS + MV_YN + initiative*MV_YN + PT_YN, data = data_ex)\n\n# Examine Model\nsummary(model_MICU)\n\n\nCall:\nglm.nb(formula = Unit_LOS ~ initiative + Age_Range + Month + \n    PT_VISITS + MV_YN + initiative * MV_YN + PT_YN, data = data_ex, \n    init.theta = 4.232773893, link = log)\n\nCoefficients:\n                       Estimate Std. Error z value             Pr(&gt;|z|)    \n(Intercept)            4.459184   0.039367 113.271 &lt; 0.0000000000000002 ***\ninitiativePre          0.556698   0.031731  17.544 &lt; 0.0000000000000002 ***\ninitiativePost         0.168237   0.030021   5.604         0.0000000209 ***\nAge_Range40-49         0.028817   0.022237   1.296               0.1950    \nAge_Range50-59         0.004942   0.019628   0.252               0.8012    \nAge_Range60-69        -0.021275   0.019482  -1.092               0.2748    \nAge_Range70+          -0.047183   0.020252  -2.330               0.0198 *  \nMonth2                 0.007514   0.029983   0.251               0.8021    \nMonth3                -0.043533   0.029296  -1.486               0.1373    \nMonth4                 0.007931   0.029309   0.271               0.7867    \nMonth5                -0.034947   0.030479  -1.147               0.2516    \nMonth6                -0.008850   0.031292  -0.283               0.7773    \nMonth7                 0.007945   0.031190   0.255               0.7989    \nMonth8                 0.037995   0.030361   1.251               0.2108    \nMonth9                 0.054427   0.030149   1.805               0.0710 .  \nMonth10               -0.013038   0.029518  -0.442               0.6587    \nMonth11                0.038812   0.030225   1.284               0.1991    \nMonth12                0.055942   0.029703   1.883               0.0596 .  \nPT_VISITS              0.132325   0.002995  44.186 &lt; 0.0000000000000002 ***\nMV_YN0                -0.356515   0.039214  -9.092 &lt; 0.0000000000000002 ***\nPT_YN                  0.084957   0.015716   5.406         0.0000000645 ***\ninitiativePre:MV_YN0  -0.227170   0.043319  -5.244         0.0000001570 ***\ninitiativePost:MV_YN0 -0.081155   0.043130  -1.882               0.0599 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Negative Binomial(4.2328) family taken to be 1)\n\n    Null deviance: 12297.6  on 6412  degrees of freedom\nResidual deviance:  6607.5  on 6390  degrees of freedom\n  (16 observations deleted due to missingness)\nAIC: 70010\n\nNumber of Fisher Scoring iterations: 1\n\n              Theta:  4.2328 \n          Std. Err.:  0.0744 \n\n 2 x log-likelihood:  -69962.1400 \n\n# Examine multicollinearity\nvif(model_MICU, type = 'predictor')\n\nWarning in vif.lm(model_MICU, type = \"predictor\"): type = 'predictor' is available only for unweighted linear models;\n  type = 'terms' will be used\n\n\n                      GVIF Df GVIF^(1/(2*Df))\ninitiative        4.493568  2        1.455955\nAge_Range         1.036016  4        1.004433\nMonth             1.078731 11        1.003451\nPT_VISITS         1.512968  1        1.230028\nMV_YN            10.059119  1        3.171611\nPT_YN             1.501266  1        1.225262\ninitiative:MV_YN 21.561910  2        2.154874\n\n# Get 95% CIs\npretty_print(confint(model_MICU))\n\nWaiting for profiling to be done...\n\n\n\n\n\n\n2.5 %\n97.5 %\n\n\n\n\n(Intercept)\n4.3825603\n4.5364016\n\n\ninitiativePre\n0.4946384\n0.6181693\n\n\ninitiativePost\n0.1090984\n0.2267173\n\n\nAge_Range40-49\n-0.0147374\n0.0724346\n\n\nAge_Range50-59\n-0.0335755\n0.0433957\n\n\nAge_Range60-69\n-0.0594529\n0.0168284\n\n\nAge_Range70+\n-0.0868476\n-0.0075588\n\n\nMonth2\n-0.0512439\n0.0663064\n\n\nMonth3\n-0.1009748\n0.0138892\n\n\nMonth4\n-0.0495681\n0.0654062\n\n\nMonth5\n-0.0946755\n0.0248351\n\n\nMonth6\n-0.0701164\n0.0525371\n\n\nMonth7\n-0.0531437\n0.0691373\n\n\nMonth8\n-0.0215312\n0.0975662\n\n\nMonth9\n-0.0046407\n0.1135281\n\n\nMonth10\n-0.0708740\n0.0447881\n\n\nMonth11\n-0.0204323\n0.0980925\n\n\nMonth12\n-0.0022597\n0.1141461\n\n\nPT_VISITS\n0.1259558\n0.1387635\n\n\nMV_YN0\n-0.4332292\n-0.2797462\n\n\nPT_YN\n0.0537847\n0.1161101\n\n\ninitiativePre:MV_YN0\n-0.3120612\n-0.1423300\n\n\ninitiativePost:MV_YN0\n-0.1657326\n0.0033873\n\n\n\n\n\n\n\n\n\nChange Reference Level to Post and Non Mechanical Ventilation\n\n# Change reference level\ndata_ex$initiative &lt;- relevel(data_ex$initiative, ref = \"Post\")\n\n# Change reference level\ndata_ex$MV_YN &lt;- relevel(data_ex$MV_YN, ref = \"0\")\n\n# Fit a Negative Binomial model\nmodel_MICU &lt;- glm.nb(Unit_LOS ~ initiative + Age_Range + Month + PT_VISITS + MV_YN + initiative*MV_YN + PT_YN, data = data_ex)\n\n# Examine Model\nsummary(model_MICU)\n\n\nCall:\nglm.nb(formula = Unit_LOS ~ initiative + Age_Range + Month + \n    PT_VISITS + MV_YN + initiative * MV_YN + PT_YN, data = data_ex, \n    init.theta = 4.232773893, link = log)\n\nCoefficients:\n                         Estimate Std. Error z value             Pr(&gt;|z|)    \n(Intercept)              4.189751   0.029335 142.824 &lt; 0.0000000000000002 ***\ninitiativeDuring        -0.087082   0.031707  -2.746              0.00602 ** \ninitiativePre            0.242447   0.019556  12.398 &lt; 0.0000000000000002 ***\nAge_Range40-49           0.028817   0.022237   1.296              0.19501    \nAge_Range50-59           0.004942   0.019628   0.252              0.80119    \nAge_Range60-69          -0.021275   0.019482  -1.092              0.27482    \nAge_Range70+            -0.047183   0.020252  -2.330              0.01982 *  \nMonth2                   0.007514   0.029983   0.251              0.80211    \nMonth3                  -0.043533   0.029296  -1.486              0.13728    \nMonth4                   0.007931   0.029309   0.271              0.78671    \nMonth5                  -0.034947   0.030479  -1.147              0.25155    \nMonth6                  -0.008850   0.031292  -0.283              0.77732    \nMonth7                   0.007945   0.031190   0.255              0.79894    \nMonth8                   0.037995   0.030361   1.251              0.21078    \nMonth9                   0.054427   0.030149   1.805              0.07104 .  \nMonth10                 -0.013038   0.029518  -0.442              0.65872    \nMonth11                  0.038812   0.030225   1.284              0.19910    \nMonth12                  0.055942   0.029703   1.883              0.05965 .  \nPT_VISITS                0.132325   0.002995  44.186 &lt; 0.0000000000000002 ***\nMV_YN1                   0.437670   0.019001  23.034 &lt; 0.0000000000000002 ***\nPT_YN                    0.084957   0.015716   5.406         0.0000000645 ***\ninitiativeDuring:MV_YN1 -0.081155   0.043130  -1.882              0.05989 .  \ninitiativePre:MV_YN1     0.146015   0.026476   5.515         0.0000000349 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Negative Binomial(4.2328) family taken to be 1)\n\n    Null deviance: 12297.6  on 6412  degrees of freedom\nResidual deviance:  6607.5  on 6390  degrees of freedom\n  (16 observations deleted due to missingness)\nAIC: 70010\n\nNumber of Fisher Scoring iterations: 1\n\n              Theta:  4.2328 \n          Std. Err.:  0.0744 \n\n 2 x log-likelihood:  -69962.1400 \n\n# Examine multicollinearity\nvif(model_MICU, type = 'predictor')\n\nWarning in vif.lm(model_MICU, type = \"predictor\"): type = 'predictor' is available only for unweighted linear models;\n  type = 'terms' will be used\n\n\n                     GVIF Df GVIF^(1/(2*Df))\ninitiative       4.890998  2        1.487131\nAge_Range        1.036016  4        1.004433\nMonth            1.078731 11        1.003451\nPT_VISITS        1.512968  1        1.230028\nMV_YN            2.361731  1        1.536792\nPT_YN            1.501266  1        1.225262\ninitiative:MV_YN 6.582019  2        1.601732\n\n# Get 95% CIs\npretty_print(confint(model_MICU))\n\nWaiting for profiling to be done...\n\n\n\n\n\n\n2.5 %\n97.5 %\n\n\n\n\n(Intercept)\n4.1323562\n4.2475341\n\n\ninitiativeDuring\n-0.1489290\n-0.0245821\n\n\ninitiativePre\n0.2043883\n0.2804583\n\n\nAge_Range40-49\n-0.0147374\n0.0724346\n\n\nAge_Range50-59\n-0.0335755\n0.0433957\n\n\nAge_Range60-69\n-0.0594529\n0.0168284\n\n\nAge_Range70+\n-0.0868476\n-0.0075588\n\n\nMonth2\n-0.0512439\n0.0663064\n\n\nMonth3\n-0.1009748\n0.0138892\n\n\nMonth4\n-0.0495681\n0.0654062\n\n\nMonth5\n-0.0946755\n0.0248351\n\n\nMonth6\n-0.0701164\n0.0525371\n\n\nMonth7\n-0.0531437\n0.0691373\n\n\nMonth8\n-0.0215312\n0.0975662\n\n\nMonth9\n-0.0046407\n0.1135281\n\n\nMonth10\n-0.0708740\n0.0447881\n\n\nMonth11\n-0.0204323\n0.0980925\n\n\nMonth12\n-0.0022597\n0.1141461\n\n\nPT_VISITS\n0.1259558\n0.1387635\n\n\nMV_YN1\n0.4004717\n0.4748143\n\n\nPT_YN\n0.0537847\n0.1161101\n\n\ninitiativeDuring:MV_YN1\n-0.1657326\n0.0033873\n\n\ninitiativePre:MV_YN1\n0.0941728\n0.1979045\n\n\n\n\n\n\n\nTop of Tabset\n\n\n\n\nQI-Initiative\nQI-initiative time period was a significant predictor of MICU length of stay, while controlling for age, admission month, reception of physical therapy, and the number of physical therapy visits. The relationship between QI-initiative and MICU length of stay depended on mechanical ventilation status.\n\nQI-Initiative for Non Mechanical Ventilation\nFor those who were not mechanically ventilated, patients admitted in the During period had a mean MICU length of stay that was 0.72 times that of patients admitted in the Pre period (z = -10.54, 95% CI: [0.68,0.76], p &lt; 0.0001), and patients admitted in the Post period had a mean MICU length stay that was 0.78 times that of patients admitted in the Pre time period (z = -12.49, 95% CI: [0.75, 0.81], p &lt; 0.0001). Those in the Post period had a mean MICU length of stay that was 1.09 times higher than patients during the During period (z = 2.78, 95% CI: [1.03,1.16], p = 0.00551).\n\n\nQI-Initiative for Mechanical Ventilation\nFor those who were mechanically ventilated, patients admitted in the During period had a mean MICU length of stay that was 0.57 times that of patients admitted in the Pre period (z = -17.55, 95% CI: [0.54,0.61], p &lt; 0.0001), and patients admitted in the Post period had a mean MICU length stay that was 0.68 times that of patients admitted in the Pre time period (z = -20.14, 95% CI: [0.65, 0.35], p &lt; 0.0001). Those in the Post period had a mean MICU length of stay that was 1.18 times higher than patients during the During period (z = 5.60, 95% CI: [1.12,1.25], p &lt; 0.0001 ).\n\n\n\nMechanical Ventilation\nMechanical ventilation was a significant predictor of MICU length of stay, while controlling for age, admission month, reception of physical therapy, and the number of physical therapy visits. The relationship between Mechanical Ventilation and MICU length of stay differed based on the QI-initiative time period.\n\nMechanical Ventilation in the Pre-Initiative Period\nFor those in the Pre period, patients who were mechanically ventilated had a mean MICU length of stay that was 1.79 times greater than patients who were not mechanically ventilated (z = 31.451, 95% CI: [1.73, 1.86], p &lt; 0.0001).\n\n\nMechanical Ventilation in the During-Initiative Period\nFor those in the During period, patients who were mechanically ventilated had a mean MICU length of stay that was 1.43 times greater than patients who were not mechanically ventilated (z = 9.141, 95% CI: [1.32, 1.54], p &lt; 0.0001).\n\n\nMechanical Ventilation in the Post-Initiative Period\nFor those in the Post period, patients who were mechanically ventilated had a mean MICU length of stay that was 1.55 times greater than patients who were not mechanically ventilated (z = 23.09, 95% CI: [1.49, 1.61], p &lt; 0.0001).\n\n\nAdditional Interaction Comparisons\nThere are additional comparisons we can make in the interaction between initiative and mechanical ventilation, but they pose little clinical relevance and will thus not be examined further (e.g. no mechanical ventilation and Pre period vs mechanical ventilation and Post period).\n\n\n\nAge Range\nWhen accounting for multiple pairwise comparisons, age range is not a significant predictor of MICU length of stay while controlling for the other variables in the model (all p’s &gt; 0.05).\n\n\nAdmission Month\nWhen accounting for multiple pairwise comparisons, admission month is not a significant predictor of MICU length of stay while controlling for the other variables in the model (all p’s &gt; 0.05).\n\n\nPhysical Therapy\nReception of physical therapy was a signicant predictor of MICU length of stay, when controlling for the other variables in the model. Patients who received physical therapy had a mean MICU length of stay that was 1.09 times greater than those who did not receive physical therapy (z = 5.40, 95% CI: [1.05, 1.12], p = &lt; 0.0001).\n\n\nPhysical Therapy Visits\nNumber of physical therapy visits was a significant predictor of MICU length of stay, when controlling for the other variables in the model. On average, each 1 visit increase was associated with a 1.14 times increase in MICU length of stay (z = 44.23, 95% CI: [1.13,1.15], p &lt; 0.0001).\nTop of Tabset\n\n\n\n\nlibrary(interactions)\n\n# Change levels for optimal plot\ndata_ex$initiative &lt;- relevel(data_ex$initiative, ref = \"During\")\ndata_ex$initiative &lt;- relevel(data_ex$initiative, ref = \"Pre\")\n\n# Fit a Negative Binomial model\nmodel_MICU &lt;- glm.nb(Unit_LOS ~ initiative + Age_Range + Month + PT_VISITS + MV_YN + initiative*MV_YN + PT_YN, data = data_ex)\n\n# Use interactions package to plot interaction\ncat_plot(\n  model_MICU, \n  pred = initiative, \n  modx = MV_YN, \n  geom = \"line\", \n  colors = \"Pastel2\",\n  x.label = \"QI-Initiative Time Period\",\n  y.label = \"Predicted MICU Length of Stay\",\n  main.title = \"MICU Length of Stay by QI-Initiative Time Period and Mechanical Ventilation\")\n\n\n\n\n\n\n\n\n\nPlot Differences between Mechanical Ventilation Status\n\n# Relevel for proper output\ndata_ex$initiative &lt;- relevel(data_ex$initiative, ref = \"During\")\ndata_ex$initiative &lt;- relevel(data_ex$initiative, ref = \"Pre\")\n\n### this is from chat gpt\n# Create a new data frame for predictions\nnew_data &lt;- data_ex %&gt;%\n  expand.grid(\n    initiative = unique(data_ex$initiative),\n    MV_YN = unique(data_ex$MV_YN),\n    Age_Range = unique(data_ex$Age_Range),\n    Month = unique(data_ex$Month),\n    PT_VISITS = mean(data_ex$PT_VISITS, na.rm = TRUE),\n    PT_YN = unique(data_ex$PT_YN)\n  )\n\n# Predict using the model\nnew_data$predicted_LOS &lt;- predict(model_MICU, newdata = new_data, type = \"response\")\n\n# Summarize the differences\nnew_data_summ &lt;- new_data %&gt;% group_by(initiative, MV_YN) %&gt;% \n  summarize( mean_LOS = mean(predicted_LOS, na.rm = TRUE)) %&gt;% \n  mutate(diff_LOS = mean_LOS - lag(mean_LOS)) %&gt;% na.omit()\n\n`summarise()` has grouped output by 'initiative'. You can override using the\n`.groups` argument.\n\n# Examine the differences\npretty_print(new_data_summ)\n\n\n\n\ninitiative\nMV_YN\nmean_LOS\ndiff_LOS\n\n\n\n\nPre\n1\n199.8024\n88.34475\n\n\nDuring\n1\n114.5064\n34.33906\n\n\nPost\n1\n135.4859\n48.02445\n\n\n\n\n\n\n# Plot the differences\nggplot(new_data_summ, aes(x = initiative, y = diff_LOS, fill = initiative)) +\n  geom_col(size = 3, position = position_dodge()) +\n  scale_color_brewer(palette = \"Pastel2\", name = \"Mechanical Ventilation\") +\n  labs(\n    title = \"Differences in MICU Length of Stay between Mechanical Ventilation and Non MV\",\n    x = \"QI-Initiative Time Period\",\n    y = \"Predicted MICU Length of Stay\",\n    fill = \"QI-Initiative\") +\n  theme_minimal() + \n  scale_fill_brewer(palette = \"Pastel2\")\n\n\n\n\n\n\n\n\n\n\nPlot Main Effect\n\n# Extract the effect for initiative\neffect_plot &lt;- effect(\"initiative\", model_MICU)\n\nNOTE: initiative is not a high-order term in the model\n\n# Convert the effect object to a data frame for ggplot\neffect_data &lt;- as.data.frame(effect_plot)\n\n# Plot using ggplot2\nggplot(effect_data, aes(x = initiative, y = fit, group = 1)) +\n  geom_line(size = 1, color = \"aquamarine\") +\n  geom_point(size = 3) +\n  geom_errorbar(aes(ymin = lower, ymax = upper), width = 0.2, alpha = 0.5) +\n  labs(\n    title = \"Predicted Mechanical Ventilation Time by QI-Initiative Time Period\",\n    x = \"QI-Initiative\",\n    y = \"Average Mechanical Ventilation Time\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nTop of Tabset\n\n\n\nThere was a significant interaction between QI-initiative time period and mechanical ventilation on MICU length of stay, when controlling for age, admission month, reception of physical therapy, and number of physical therapy visits.\n\nMechanical Ventilation\nAcross the board, patients who were mechanically ventilated had a longer average MICU length of stay than those were not, regardless of QI-initiative time period.\n\n\nQI-Initiative Time Period\nThe QI-initiative was successful at reducing the MICU length of stay for both patients that were mechanically ventilated and those who were not.\nMoreover, the difference in MICU length of stay between patients who were mechanically ventilated and those who were not decreased in the During and Post periods compared to the Pre period. On average, those were mechanically ventilated in the Pre period had a predicted MICU length of stay that was 86.39 hours more than those were not, compared to only 33.69 hours in the During period, and 47.02 hours in the Post period.\nIn other words, the QI-initiative decreased the average MICU length of stay by at least 48 hours for those who were mechanically ventilated!\nTop of Tabset"
  },
  {
    "objectID": "Statistical_Consulting/Vieau_Project_01/Code/Megan_Watson_Project.html#MV",
    "href": "Statistical_Consulting/Vieau_Project_01/Code/Megan_Watson_Project.html#MV",
    "title": "Assessing the Impact of a Physical Therapy Quality-Improvement Initiative at a Colorado University Hospital",
    "section": "MV Time",
    "text": "MV Time\nPrimary question B of the study was whether the QI-initiative successfully decreased the total mechanical ventilation time.\nTo assess this we will run a Poisson or Negative Binomial Regression model predicting UNIT LOS:\n\\(Log( E(MV Total | QI-Initiative_i ) ) = β_0 + β_1 * QI-Initiative_i + β2 *Covariates_i + ...\\)\n\nCovariatesModel SelectionAnalysisInterpretationVisualizationAge RangeSummary\n\n\nThe same variables that were potential covariates for primary question A will be selected for this model. These are:\nPT_VISITS AVERAGE THERAPY LENGTH MV_YN initiativeMV_YN PT_YN initiativePT_YN\nAdditionally, since patients that were not mechanically ventilated have a MV_Total time of NA, an interaction term using MV_YN cannot be used, and thus will be dropped from consideration in this model.\nTop of Tabset\n\n\n\nCheck for Zero Inflation\n\n# Check if MV Total is zero inflated\nmin(data_ex$MV_Total, na.rm = T)\n\n[1] 1\n\n\nMV_Total does not have any values of 0, and thus cannot be zero inflated.\n\n\nCheck for Overdispersion\n\n# Compare mean and variance\npaste0(\"Mean = \", mean(data_ex$MV_Total, na.rm = T))\n\n[1] \"Mean = 6.10174685871897\"\n\n# Compare mean and variance\npaste0(\"Variance = \", var(data_ex$MV_Total, na.rm = T))\n\n[1] \"Variance = 26.135567149879\"\n\n\nThe variance is higher than the mean, and thus we have overdisperion and will use a negative binomial regression.\n\n\nFull Model\nWe will first begin with a saturated model of all potential covariates.\n\n# Change reference level\ndata_ex$initiative &lt;- relevel(data_ex$initiative, ref = \"Pre\")\n\n# Change reference level\ndata_ex$MV_YN &lt;- relevel(data_ex$MV_YN, ref = \"0\")\n\n# Fit a Negative Binomial model\nmodel_full &lt;- glm.nb(MV_Total ~ initiative + Age_Range + Month + PT_VISITS + `AVG THERAPY LENGTH` + PT_YN + initiative*PT_YN, data = data_ex)\n\n# Examine Model\nsummary(model_full)\n\n\nCall:\nglm.nb(formula = MV_Total ~ initiative + Age_Range + Month + \n    PT_VISITS + `AVG THERAPY LENGTH` + PT_YN + initiative * PT_YN, \n    data = data_ex, init.theta = 3.522878931, link = log)\n\nCoefficients:\n                        Estimate Std. Error z value             Pr(&gt;|z|)    \n(Intercept)             1.660254   0.050675  32.763 &lt; 0.0000000000000002 ***\ninitiativeDuring       -0.182827   0.108299  -1.688             0.091378 .  \ninitiativePost         -0.067207   0.046243  -1.453             0.146125    \nAge_Range40-49          0.045644   0.042371   1.077             0.281374    \nAge_Range50-59          0.039109   0.037009   1.057             0.290629    \nAge_Range60-69          0.015927   0.036911   0.431             0.666111    \nAge_Range70+            0.018048   0.039275   0.460             0.645851    \nMonth2                 -0.079008   0.055040  -1.435             0.151156    \nMonth3                 -0.135656   0.056051  -2.420             0.015511 *  \nMonth4                 -0.074602   0.054855  -1.360             0.173836    \nMonth5                 -0.171708   0.057890  -2.966             0.003016 ** \nMonth6                 -0.084213   0.058297  -1.445             0.148582    \nMonth7                 -0.153356   0.060965  -2.515             0.011888 *  \nMonth8                 -0.106208   0.058671  -1.810             0.070259 .  \nMonth9                  0.013727   0.056188   0.244             0.806998    \nMonth10                -0.168900   0.055660  -3.035             0.002409 ** \nMonth11                -0.046431   0.056163  -0.827             0.408395    \nMonth12                -0.001905   0.053954  -0.035             0.971840    \nPT_VISITS               0.116184   0.004477  25.950 &lt; 0.0000000000000002 ***\n`AVG THERAPY LENGTH`   -0.004124   0.001175  -3.511             0.000446 ***\nPT_YN                   0.271979   0.053225   5.110          0.000000322 ***\ninitiativeDuring:PT_YN -0.346605   0.119130  -2.909             0.003620 ** \ninitiativePost:PT_YN   -0.292881   0.057472  -5.096          0.000000347 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Negative Binomial(3.5229) family taken to be 1)\n\n    Null deviance: 4036.1  on 3255  degrees of freedom\nResidual deviance: 3158.3  on 3233  degrees of freedom\n  (3173 observations deleted due to missingness)\nAIC: 17271\n\nNumber of Fisher Scoring iterations: 1\n\n              Theta:  3.523 \n          Std. Err.:  0.135 \n\n 2 x log-likelihood:  -17222.941 \n\n# Examine multicollinearity\nvif(model_full, type = 'predictor')\n\nWarning in vif.lm(model_full, type = \"predictor\"): type = 'predictor' is available only for unweighted linear models;\n  type = 'terms' will be used\n\n\n                          GVIF Df GVIF^(1/(2*Df))\ninitiative           25.007318  2        2.236232\nAge_Range             1.042280  4        1.005190\nMonth                 1.099131 11        1.004306\nPT_VISITS             1.577196  1        1.255865\n`AVG THERAPY LENGTH`  3.548943  1        1.883864\nPT_YN                 4.367740  1        2.089914\ninitiative:PT_YN     40.417637  2        2.521406\n\n\nWe have multicollinearity on the initiative*PT_YN interaction again. We will remove this interaction.\n\n\nBackwards Elimination\n\n# Get cleaned data set with missing values omitted because this is necessary with the MASS package\ndata_ex_clean &lt;- data_ex |&gt; \n  dplyr::select(MV_Total, initiative, Age_Range, Month, PT_VISITS, `AVG THERAPY LENGTH`, PT_YN)\n\n# Remove rows with missing values\ndata_ex_clean &lt;- na.omit(data_ex_clean)\n\n# Fit a Negative Binomial model\nmodel_MV &lt;- glm.nb(MV_Total ~ initiative + Age_Range + Month + PT_VISITS + `AVG THERAPY LENGTH` + PT_YN, data = data_ex_clean)\n\n# Set n to the number of observations in the dataset\nn &lt;- nrow(data_ex_clean)\n\nscope\n\n$lower\n~initiative + Month + Age_Range\n\n$upper\nUnit_LOS ~ initiative + Age_Range + Month + PT_VISITS + `AVG THERAPY LENGTH` + \n    MV_YN + initiative * MV_YN + PT_YN\n\n# Define the scope to force the inclusion of specific variables\nscope &lt;- list(lower = MV_Total ~ initiative + Month + Age_Range, upper = formula(model_MV))\n\n# Perform backwards elimination using BIC\nbackwards_model_MV_bic &lt;- stepAIC(model_MV, direction = \"backward\", k = log(n), scope = scope)\n\nStart:  AIC=17421.88\nMV_Total ~ initiative + Age_Range + Month + PT_VISITS + `AVG THERAPY LENGTH` + \n    PT_YN\n\n                       Df   AIC\n&lt;none&gt;                    17422\n- PT_YN                 1 17423\n- `AVG THERAPY LENGTH`  1 17425\n- PT_VISITS             1 17978\n\n# Examine selected model\nsummary(backwards_model_MV_bic)\n\n\nCall:\nglm.nb(formula = MV_Total ~ initiative + Age_Range + Month + \n    PT_VISITS + `AVG THERAPY LENGTH` + PT_YN, data = data_ex_clean, \n    init.theta = 3.481985213, link = log)\n\nCoefficients:\n                       Estimate Std. Error z value             Pr(&gt;|z|)    \n(Intercept)           1.7278241  0.0493325  35.024 &lt; 0.0000000000000002 ***\ninitiativeDuring     -0.4187792  0.0458997  -9.124 &lt; 0.0000000000000002 ***\ninitiativePost       -0.2512336  0.0276680  -9.080 &lt; 0.0000000000000002 ***\nAge_Range40-49        0.0423662  0.0424712   0.998             0.318509    \nAge_Range50-59        0.0377523  0.0371155   1.017             0.309079    \nAge_Range60-69        0.0103329  0.0370215   0.279             0.780163    \nAge_Range70+          0.0115508  0.0393926   0.293             0.769353    \nMonth2               -0.0816017  0.0552061  -1.478             0.139373    \nMonth3               -0.1303677  0.0561933  -2.320             0.020342 *  \nMonth4               -0.0754712  0.0550260  -1.372             0.170202    \nMonth5               -0.1741105  0.0580487  -2.999             0.002705 ** \nMonth6               -0.0839495  0.0584616  -1.436             0.151009    \nMonth7               -0.1542573  0.0611603  -2.522             0.011663 *  \nMonth8               -0.1077130  0.0588588  -1.830             0.067247 .  \nMonth9                0.0241076  0.0563193   0.428             0.668614    \nMonth10              -0.1681626  0.0558234  -3.012             0.002592 ** \nMonth11              -0.0417061  0.0563135  -0.741             0.458932    \nMonth12               0.0007327  0.0540949   0.014             0.989193    \nPT_VISITS             0.1115364  0.0044348  25.150 &lt; 0.0000000000000002 ***\n`AVG THERAPY LENGTH` -0.0039679  0.0011782  -3.368             0.000758 ***\nPT_YN                 0.1399711  0.0476158   2.940             0.003286 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Negative Binomial(3.482) family taken to be 1)\n\n    Null deviance: 4007.0  on 3255  degrees of freedom\nResidual deviance: 3164.9  on 3235  degrees of freedom\nAIC: 17296\n\nNumber of Fisher Scoring iterations: 1\n\n              Theta:  3.482 \n          Std. Err.:  0.133 \n\n 2 x log-likelihood:  -17252.030 \n\n\nBackwards elimination includes all variables, and thus this will be selected as the final model.\nTop of Tabset\n\n\n\n\n# Change the reference level\ndata_ex$initiative &lt;- relevel(data_ex$initiative, ref = \"Pre\")\n\n# Change the reference level\ndata_ex$MV_YN &lt;- relevel(data_ex$MV_YN, ref = \"0\")\n\n# Fit the negative binomial regression\nmodel_MV &lt;- glm.nb(MV_Total ~ initiative + Age_Range + Month + PT_VISITS + `AVG THERAPY LENGTH` + PT_YN, data = data_ex)\n\n# Examine model\nsummary(model_MV)\n\n\nCall:\nglm.nb(formula = MV_Total ~ initiative + Age_Range + Month + \n    PT_VISITS + `AVG THERAPY LENGTH` + PT_YN, data = data_ex, \n    init.theta = 3.481985213, link = log)\n\nCoefficients:\n                       Estimate Std. Error z value             Pr(&gt;|z|)    \n(Intercept)           1.7278241  0.0493325  35.024 &lt; 0.0000000000000002 ***\ninitiativeDuring     -0.4187792  0.0458997  -9.124 &lt; 0.0000000000000002 ***\ninitiativePost       -0.2512336  0.0276680  -9.080 &lt; 0.0000000000000002 ***\nAge_Range40-49        0.0423662  0.0424712   0.998             0.318509    \nAge_Range50-59        0.0377523  0.0371155   1.017             0.309079    \nAge_Range60-69        0.0103329  0.0370215   0.279             0.780163    \nAge_Range70+          0.0115508  0.0393926   0.293             0.769353    \nMonth2               -0.0816017  0.0552061  -1.478             0.139373    \nMonth3               -0.1303677  0.0561933  -2.320             0.020342 *  \nMonth4               -0.0754712  0.0550260  -1.372             0.170202    \nMonth5               -0.1741105  0.0580487  -2.999             0.002705 ** \nMonth6               -0.0839495  0.0584616  -1.436             0.151009    \nMonth7               -0.1542573  0.0611603  -2.522             0.011663 *  \nMonth8               -0.1077130  0.0588588  -1.830             0.067247 .  \nMonth9                0.0241076  0.0563193   0.428             0.668614    \nMonth10              -0.1681626  0.0558234  -3.012             0.002592 ** \nMonth11              -0.0417061  0.0563135  -0.741             0.458932    \nMonth12               0.0007327  0.0540949   0.014             0.989193    \nPT_VISITS             0.1115364  0.0044348  25.150 &lt; 0.0000000000000002 ***\n`AVG THERAPY LENGTH` -0.0039679  0.0011782  -3.368             0.000758 ***\nPT_YN                 0.1399711  0.0476158   2.940             0.003286 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Negative Binomial(3.482) family taken to be 1)\n\n    Null deviance: 4007.0  on 3255  degrees of freedom\nResidual deviance: 3164.9  on 3235  degrees of freedom\n  (3173 observations deleted due to missingness)\nAIC: 17296\n\nNumber of Fisher Scoring iterations: 1\n\n              Theta:  3.482 \n          Std. Err.:  0.133 \n\n 2 x log-likelihood:  -17252.030 \n\n# Get 95% CIs\npretty_print(confint(model_MV))\n\nWaiting for profiling to be done...\n\n\n\n\n\n\n2.5 %\n97.5 %\n\n\n\n\n(Intercept)\n1.6309860\n1.8250625\n\n\ninitiativeDuring\n-0.5086721\n-0.3287499\n\n\ninitiativePost\n-0.3050203\n-0.1975103\n\n\nAge_Range40-49\n-0.0408593\n0.1256731\n\n\nAge_Range50-59\n-0.0350702\n0.1105221\n\n\nAge_Range60-69\n-0.0622156\n0.0828206\n\n\nAge_Range70+\n-0.0655918\n0.0886905\n\n\nMonth2\n-0.1898129\n0.0266320\n\n\nMonth3\n-0.2404426\n-0.0202635\n\n\nMonth4\n-0.1833071\n0.0323680\n\n\nMonth5\n-0.2879599\n-0.0602188\n\n\nMonth6\n-0.1985050\n0.0307507\n\n\nMonth7\n-0.2739578\n-0.0343711\n\n\nMonth8\n-0.2229273\n0.0076251\n\n\nMonth9\n-0.0863610\n0.1346781\n\n\nMonth10\n-0.2776651\n-0.0586800\n\n\nMonth11\n-0.1519512\n0.0686068\n\n\nMonth12\n-0.1051617\n0.1066180\n\n\nPT_VISITS\n0.1021283\n0.1210575\n\n\n`AVG THERAPY LENGTH`\n-0.0062506\n-0.0016832\n\n\nPT_YN\n0.0474993\n0.2325356\n\n\n\n\n\n\n\n\nChange Reference Category to During\n\n# Change the reference level\ndata_ex$initiative &lt;- relevel(data_ex$initiative, ref = \"During\")\n\n# Fit the negative binomial regression\nmodel_MV &lt;- glm.nb(MV_Total ~ initiative + Age_Range + Month + PT_VISITS + `AVG THERAPY LENGTH` + PT_YN, data = data_ex)\n\n# Examine model\nsummary(model_MV)\n\n\nCall:\nglm.nb(formula = MV_Total ~ initiative + Age_Range + Month + \n    PT_VISITS + `AVG THERAPY LENGTH` + PT_YN, data = data_ex, \n    init.theta = 3.481985213, link = log)\n\nCoefficients:\n                       Estimate Std. Error z value             Pr(&gt;|z|)    \n(Intercept)           1.3090449  0.0660577  19.817 &lt; 0.0000000000000002 ***\ninitiativePre         0.4187792  0.0458997   9.124 &lt; 0.0000000000000002 ***\ninitiativePost        0.1675456  0.0427248   3.922             0.000088 ***\nAge_Range40-49        0.0423662  0.0424712   0.998             0.318509    \nAge_Range50-59        0.0377523  0.0371155   1.017             0.309079    \nAge_Range60-69        0.0103329  0.0370215   0.279             0.780163    \nAge_Range70+          0.0115508  0.0393926   0.293             0.769353    \nMonth2               -0.0816017  0.0552061  -1.478             0.139373    \nMonth3               -0.1303677  0.0561933  -2.320             0.020342 *  \nMonth4               -0.0754712  0.0550260  -1.372             0.170202    \nMonth5               -0.1741105  0.0580487  -2.999             0.002705 ** \nMonth6               -0.0839495  0.0584616  -1.436             0.151009    \nMonth7               -0.1542573  0.0611603  -2.522             0.011663 *  \nMonth8               -0.1077130  0.0588588  -1.830             0.067247 .  \nMonth9                0.0241076  0.0563193   0.428             0.668614    \nMonth10              -0.1681626  0.0558234  -3.012             0.002592 ** \nMonth11              -0.0417061  0.0563135  -0.741             0.458932    \nMonth12               0.0007327  0.0540949   0.014             0.989193    \nPT_VISITS             0.1115364  0.0044348  25.150 &lt; 0.0000000000000002 ***\n`AVG THERAPY LENGTH` -0.0039679  0.0011782  -3.368             0.000758 ***\nPT_YN                 0.1399711  0.0476158   2.940             0.003286 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Negative Binomial(3.482) family taken to be 1)\n\n    Null deviance: 4007.0  on 3255  degrees of freedom\nResidual deviance: 3164.9  on 3235  degrees of freedom\n  (3173 observations deleted due to missingness)\nAIC: 17296\n\nNumber of Fisher Scoring iterations: 1\n\n              Theta:  3.482 \n          Std. Err.:  0.133 \n\n 2 x log-likelihood:  -17252.030 \n\n# Get 95% CIs\npretty_print(confint(model_MV))\n\nWaiting for profiling to be done...\n\n\n\n\n\n\n2.5 %\n97.5 %\n\n\n\n\n(Intercept)\n1.1797900\n1.4385311\n\n\ninitiativePre\n0.3287499\n0.5086721\n\n\ninitiativePost\n0.0834822\n0.2513735\n\n\nAge_Range40-49\n-0.0408593\n0.1256731\n\n\nAge_Range50-59\n-0.0350702\n0.1105221\n\n\nAge_Range60-69\n-0.0622156\n0.0828206\n\n\nAge_Range70+\n-0.0655918\n0.0886905\n\n\nMonth2\n-0.1898129\n0.0266320\n\n\nMonth3\n-0.2404426\n-0.0202635\n\n\nMonth4\n-0.1833071\n0.0323680\n\n\nMonth5\n-0.2879599\n-0.0602188\n\n\nMonth6\n-0.1985050\n0.0307507\n\n\nMonth7\n-0.2739578\n-0.0343711\n\n\nMonth8\n-0.2229273\n0.0076251\n\n\nMonth9\n-0.0863610\n0.1346781\n\n\nMonth10\n-0.2776651\n-0.0586800\n\n\nMonth11\n-0.1519512\n0.0686068\n\n\nMonth12\n-0.1051617\n0.1066180\n\n\nPT_VISITS\n0.1021283\n0.1210575\n\n\n`AVG THERAPY LENGTH`\n-0.0062506\n-0.0016832\n\n\nPT_YN\n0.0474993\n0.2325356\n\n\n\n\n\n\n\nTop of Tabset\n\n\n\n\nMain Effects\nQI-initiative time period is a significant predictor for mechanical ventilation time, while controlling for age, admission month, total physical therapy visits, and average physical therapy visit length. Compared to patients in the Pre time period, patients in the During time period had a mean mechanical ventilation time that was 0.66 that of those in the Pre period (z = -9.12, 95% CI: [0.60, 0.72], p &lt; 0.0001), and patients in the Post time period had a mean mechanical ventilation time that was 0.78 that of those in the pre period (z = -9.08, 95% CI: [0.74,0.82], p &lt; 0.0001). Additionally, compared to patients in the During time period, those in the Post time period had a mean mechanical ventilation time that was 1.18 that of those in the During period (z = 3.92, 95% CI: [1.09, 1.29], p &lt; 0.0001).\n\n\nAge\nAge was not a significant predictor in this model (all p’s &gt; 0.5), but was included in the final model to control for it.\n\n\nMonth\nAdmission Month was a significant predictor for mechanical ventilation time, while controlling for QI-initiative time period, age, total PT visits, and average PT visit length. For instance, those admitted during March had an average mechanical ventilation time that was 0.88 times that of those admitted during January z = -2.294, 95% CI: [0.79,0.98], p = 0.022).\nSince possible comparisons between months are myriad (12!), and the goal of this project was not to compare outcome variables between months (merely to control for these differences due to seasonality), further comparisons will not be made.\n\n\nReception of Physical Therapy\nReception of physical therapy was a signicant predictor of mechanical ventilation time, when controlling for the other variables in the model. Patients who received physical therapy had a mean mechanical ventilation time that was 1.15 times greater than those who did not receive physical therapy (z = 2.95, 95% CI: [1.11, 1.13], p = 0.003).\n\n\nTotal Physical Therapy Visits\nNumber of physical therapy visits was a significant predictor of mechanical ventilation time, when controlling for the other variables in the model. On average, each 1 visit increase was associated with a 1.12 times increase in mechanical ventilation time (z = 25.13, 95% CI: [1.11,1.13], p &lt; 0.0001).\n\n\nAverage Physical Therapy Length\nAverage physical therapy length was a significant predictor of mechanical ventilation time, when controlling for the other variables in the model. On average, each 1 hour increase in physical therapy visit length was associated with a 0.4% decrease in mechanical ventilation time (z = -3.37, 95% CI: [0.994, 0.998], p &lt; 0.0001).\nTop of Tabset\n\n\n\n\nMain Effect\n\n# Change reference level\ndata_ex$initiative &lt;- relevel(data_ex$initiative, ref = \"Pre\")\n\n# Get rid of spaces\ndata_ex &lt;- data_ex |&gt; \n  mutate(AVG_THERAPY_LENGTH = `AVG THERAPY LENGTH`)\n\n# Fit the negative binomial regression\nmodel_MV &lt;- glm.nb(MV_Total ~ initiative + Age_Range + Month + PT_VISITS + AVG_THERAPY_LENGTH + PT_YN, data = data_ex)\n\n# Extract the effect for initiative\neffect_plot &lt;- effect(\"initiative\", model_MV)\n\n# Convert the effect object to a data frame for ggplot\neffect_data &lt;- as.data.frame(effect_plot)\n\n# Plot using ggplot2\nggplot(effect_data, aes(x = initiative, y = fit, group = 1)) +\n  geom_line(size = 1, color = \"aquamarine\") +\n  geom_point(size = 3) +\n  geom_errorbar(aes(ymin = lower, ymax = upper), width = 0.2, alpha = 0.5) +\n  labs(\n    title = \"Predicted Mechanical Ventilation Time by QI-Initiative Time Period\",\n    x = \"QI-Initiative\",\n    y = \"Average Mechanical Ventilation Time\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\n# Extract the effect for initiative\neffect_plot &lt;- effect(\"Age_Range\", model_MV)\n\n# Convert the effect object to a data frame for ggplot\neffect_data &lt;- as.data.frame(effect_plot)\n\n# Plot using ggplot2\nggplot(effect_data, aes(x = Age_Range, y = fit, group = 1)) +\n  geom_line(size = 1, color = \"aquamarine\") +\n  geom_point(size = 3) +\n  geom_errorbar(aes(ymin = lower, ymax = upper), width = 0.2, alpha = 0.5) +\n  labs(\n    title = \"Predicted Mechanical Ventilation Time by Age Range\",\n    x = \"Age Range\",\n    y = \"Average Mechanical Ventilation Time\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nAdmission Month\n\n# Extract the effect for initiative\neffect_plot &lt;- effect(\"Month\", model_MV)\n\n# Convert the effect object to a data frame for ggplot\neffect_data &lt;- as.data.frame(effect_plot)\n\n# Plot using ggplot2\nggplot(effect_data, aes(x = Month, y = fit, group = 1)) +\n  geom_line(size = 1, color = \"aquamarine\") +\n  geom_point(size = 3) +\n  geom_errorbar(aes(ymin = lower, ymax = upper), width = 0.2, alpha = 0.5) +\n  labs(\n    title = \"Predicted Mechanical Ventilation Time by Admission Month\",\n    x = \"Month\",\n    y = \"Average Mechanical Ventilation Time\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nNumber of Physical Therapy Visits\n\n# Extract the effect for initiative\neffect_plot &lt;- effect(\"PT_VISITS\", model_MV)\n\n# Convert the effect object to a data frame for ggplot\neffect_data &lt;- as.data.frame(effect_plot)\n\n# Plot using ggplot2\nggplot(effect_data, aes(x = PT_VISITS, y = fit, group = 1)) +\n  geom_line(size = 1, color = \"aquamarine\") +\n  geom_point(size = 3) +\n  geom_errorbar(aes(ymin = lower, ymax = upper), width = 0.2, alpha = 0.5) +\n  labs(\n    title = \"Predicted Mechanical Ventilation Time by Number of PT Visits\",\n    x = \"Number of Physical Therapy Visits\",\n    y = \"Average Mechanical Ventilation Time\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nAverage Therapy Length\n\n# Extract the effect for initiative\neffect_plot &lt;- effect(\"AVG_THERAPY_LENGTH\", model_MV)\n\n# Convert the effect object to a data frame for ggplot\neffect_data &lt;- as.data.frame(effect_plot)\n\n# Plot using ggplot2\nggplot(effect_data, aes(x = AVG_THERAPY_LENGTH, y = fit, group = 1)) +\n  geom_line(size = 1, color = \"aquamarine\") +\n  geom_point(size = 3) +\n  geom_errorbar(aes(ymin = lower, ymax = upper), width = 0.2, alpha = 0.5) +\n  labs(\n    title = \"Predicted Mechanical Ventilation Time by Average Therapy Length\",\n    x = \"Average Therapy Length\",\n    y = \"Average Mechanical Ventilation Time\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nTop of Tabset\n\n\n\nQI-initiative time period is a significant predictor for mechanical ventilation time, while controlling for age, admission month, total physical therapy visits, and average physical therapy visit length. Compared to patients in the Pre time period, patients in the During time period had a mean mechanical ventilation time that was 0.66 that of those in the Pre period (p &lt; 0.0001), and patients in the Post time period had a mean mechanical ventilation time that was 0.78 that of those in the pre period (p &lt; 0.0001).\nAdditionally, there was a slight decrease in the impact of the the QI-initiative. Compared to patients in the During time period, those in the Post time period had a mean mechanical ventilation time that was 1.18 that of those in the During period (p &lt; 0.0001).\nTop of Tabset"
  },
  {
    "objectID": "Statistical_Consulting/Vieau_Project_01/Code/Megan_Watson_Project.html#Non",
    "href": "Statistical_Consulting/Vieau_Project_01/Code/Megan_Watson_Project.html#Non",
    "title": "Assessing the Impact of a Physical Therapy Quality-Improvement Initiative at a Colorado University Hospital",
    "section": "Non ICU Length of Stay",
    "text": "Non ICU Length of Stay\nSecondary Outcome of interest A is whether the QI-initiative decreased the Non-ICU length of stay.\nThis will be a Poisson or Negative Binomial Regression similar to the primary analysis\n\nCovariatesModel SelectionAnalysisInterpretationVisualizationSummary\n\n\nThe potential covariates for this analysis will be the same as the main analysis.\nTop of Tabset\n\n\n\nCheck for Zero Inflation\n\n# Check for zero inflation\nhist(data_ex$NON_ICU_LOS)\n\n\n\n\n\n\n\n\nWe appear to have zero inflation and will need to check if a zero inflated negative binomial model will improve performance over a simple negative binomial regression.\n\n\nCheck for Overdispersion\n\n# Compare mean and variance\npaste0(\"Mean = \", mean(data_ex$NON_ICU_LOS, na.rm = T))\n\n[1] \"Mean = 238.967957691709\"\n\n# Compare mean and variance\npaste0(\"Variance = \", var(data_ex$NON_ICU_LOS, na.rm = T))\n\n[1] \"Variance = 162873.976571139\"\n\n\nWe have over dispersion and thus the negative binomial model is preferred over a Poisson regression.\n\n\nTest Zero Inflation Model Performance\nWe will first fit the saturated model as both zero inflated negative binomial regression and a simple negative binomial model\n\n# Relevel \ndata_ex$initiative &lt;- relevel(data_ex$initiative, ref = \"Pre\")\n\n# Fit the zero-inflated negative binomial\nmodel_NON &lt;- zeroinfl(NON_ICU_LOS ~ initiative + Age_Range + Month + PT_VISITS + MV_YN + initiative*MV_YN + PT_YN| initiative, data = data_ex, dist = \"negbin\")\n\n# Examine Model\nsummary(model_NON)\n\n\nCall:\nzeroinfl(formula = NON_ICU_LOS ~ initiative + Age_Range + Month + PT_VISITS + \n    MV_YN + initiative * MV_YN + PT_YN | initiative, data = data_ex, \n    dist = \"negbin\")\n\nPearson residuals:\n    Min      1Q  Median      3Q     Max \n-0.7535 -0.6425 -0.3903  0.1532 24.2910 \n\nCount model coefficients (negbin with log link):\n                         Estimate Std. Error z value             Pr(&gt;|z|)    \n(Intercept)              5.393266   0.075458  71.473 &lt; 0.0000000000000002 ***\ninitiativeDuring         0.003212   0.083886   0.038             0.969461    \ninitiativePost           0.014612   0.051875   0.282             0.778193    \nAge_Range40-49          -0.063153   0.060268  -1.048             0.294700    \nAge_Range50-59          -0.099307   0.052852  -1.879             0.060250 .  \nAge_Range60-69          -0.070299   0.052609  -1.336             0.181470    \nAge_Range70+            -0.277091   0.054266  -5.106          0.000000329 ***\nMonth2                  -0.016078   0.080685  -0.199             0.842048    \nMonth3                  -0.117076   0.078755  -1.487             0.137126    \nMonth4                  -0.138345   0.078874  -1.754             0.079430 .  \nMonth5                  -0.198316   0.082207  -2.412             0.015848 *  \nMonth6                  -0.243802   0.084137  -2.898             0.003760 ** \nMonth7                  -0.077612   0.083504  -0.929             0.352664    \nMonth8                  -0.041371   0.081679  -0.507             0.612503    \nMonth9                  -0.079622   0.080773  -0.986             0.324256    \nMonth10                 -0.041182   0.079399  -0.519             0.603987    \nMonth11                 -0.177580   0.081545  -2.178             0.029428 *  \nMonth12                  0.017625   0.079969   0.220             0.825561    \nPT_VISITS                0.077789   0.009274   8.387 &lt; 0.0000000000000002 ***\nMV_YN1                   0.073545   0.050028   1.470             0.141543    \nPT_YN                    0.166018   0.043015   3.860             0.000114 ***\ninitiativeDuring:MV_YN1 -0.103961   0.116588  -0.892             0.372559    \ninitiativePost:MV_YN1   -0.136775   0.071453  -1.914             0.055596 .  \nLog(theta)              -0.565097   0.015164 -37.265 &lt; 0.0000000000000002 ***\n\nZero-inflation model coefficients (binomial with logit link):\n                  Estimate Std. Error z value Pr(&gt;|z|)\n(Intercept)       -20.1042   603.5634  -0.033    0.973\ninitiativeDuring   -0.9272  1828.8701  -0.001    1.000\ninitiativePost      0.5059   823.4400   0.001    1.000\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n\nTheta = 0.5683 \nNumber of iterations in BFGS optimization: 50 \nLog-likelihood: -4.053e+04 on 27 Df\n\n# Fit the  negative binomial\nmodel_NON_nb &lt;- glm.nb(NON_ICU_LOS ~ initiative + Age_Range + Month + PT_VISITS + MV_YN + initiative*MV_YN + PT_YN, data = data_ex)\n\n# Examine Model\nsummary(model_NON_nb)\n\n\nCall:\nglm.nb(formula = NON_ICU_LOS ~ initiative + Age_Range + Month + \n    PT_VISITS + MV_YN + initiative * MV_YN + PT_YN, data = data_ex, \n    init.theta = 0.5683045546, link = log)\n\nCoefficients:\n                         Estimate Std. Error z value             Pr(&gt;|z|)    \n(Intercept)              5.393259   0.075053  71.859 &lt; 0.0000000000000002 ***\ninitiativeDuring         0.003205   0.084299   0.038               0.9697    \ninitiativePost           0.014608   0.052233   0.280               0.7797    \nAge_Range40-49          -0.063151   0.059673  -1.058               0.2899    \nAge_Range50-59          -0.099307   0.052676  -1.885               0.0594 .  \nAge_Range60-69          -0.070303   0.052278  -1.345               0.1787    \nAge_Range70+            -0.277090   0.054333  -5.100           0.00000034 ***\nMonth2                  -0.016076   0.080514  -0.200               0.8417    \nMonth3                  -0.117071   0.078596  -1.490               0.1363    \nMonth4                  -0.138345   0.078682  -1.758               0.0787 .  \nMonth5                  -0.198313   0.081775  -2.425               0.0153 *  \nMonth6                  -0.243802   0.083996  -2.903               0.0037 ** \nMonth7                  -0.077608   0.083680  -0.927               0.3537    \nMonth8                  -0.041369   0.081490  -0.508               0.6117    \nMonth9                  -0.079621   0.080956  -0.984               0.3254    \nMonth10                 -0.041179   0.079221  -0.520               0.6032    \nMonth11                 -0.177577   0.081159  -2.188               0.0287 *  \nMonth12                  0.017628   0.079766   0.221               0.8251    \nPT_VISITS                0.077789   0.008113   9.588 &lt; 0.0000000000000002 ***\nMV_YN1                   0.073545   0.049863   1.475               0.1402    \nPT_YN                    0.166029   0.042170   3.937           0.00008246 ***\ninitiativeDuring:MV_YN1 -0.103957   0.116130  -0.895               0.3707    \ninitiativePost:MV_YN1   -0.136771   0.071051  -1.925               0.0542 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Negative Binomial(0.5683) family taken to be 1)\n\n    Null deviance: 8136.8  on 6412  degrees of freedom\nResidual deviance: 7887.3  on 6390  degrees of freedom\n  (16 observations deleted due to missingness)\nAIC: 81112\n\nNumber of Fisher Scoring iterations: 1\n\n              Theta:  0.56830 \n          Std. Err.:  0.00862 \n\n 2 x log-likelihood:  -81064.00600 \n\n\nAnd compare information criteria.\n\n# Compare model performance\nbic_nb &lt;- BIC(model_NON_nb)\nbic_zifb &lt;- BIC(model_NON)\n\naic_nb &lt;- AIC(model_NON_nb)\naic_zifb &lt;- AIC(model_NON)\n\n# Create a data frame to display the values \nmodel_comparison &lt;- data.frame(Model = c(\"Negative Binomial Model\", \"ZIF Negative Binomial Model\"), \n                               BIC = c(bic_nb, bic_zifb), \n                               AIC = c(aic_nb, aic_zifb))\n\n# Print resulting table\npretty_print(model_comparison)\n\n\n\n\nModel\nBIC\nAIC\n\n\n\n\nNegative Binomial Model\n81274.39\n81112.01\n\n\nZIF Negative Binomial Model\n81300.69\n81118.01\n\n\n\n\n\n\n\nAIC and BIC prefer the simple negative binomial model, and thus we do not have zero inflation!\n\n\nBackwards Elimination\n\n# Define the scope to force the inclusion of specific variables\nscope &lt;- list(lower = ~ initiative + Month + Age_Range, upper = formula(model_NON_nb))\n\n# Perform backwards elimination using BIC\nbackwards_model_NON_bic &lt;- stepAIC(model_NON_nb, direction = \"backward\", k = log(n), scope = scope)\n\nStart:  AIC=81250.04\nNON_ICU_LOS ~ initiative + Age_Range + Month + PT_VISITS + MV_YN + \n    initiative * MV_YN + PT_YN\n\n                   Df   AIC\n- initiative:MV_YN  2 81238\n&lt;none&gt;                81250\n- PT_YN             1 81257\n- PT_VISITS         1 81323\n\nStep:  AIC=81237.62\nNON_ICU_LOS ~ initiative + Age_Range + Month + PT_VISITS + MV_YN + \n    PT_YN\n\n            Df   AIC\n- MV_YN      1 81230\n&lt;none&gt;         81238\n- PT_YN      1 81246\n- PT_VISITS  1 81308\n\nStep:  AIC=81229.54\nNON_ICU_LOS ~ initiative + Age_Range + Month + PT_VISITS + PT_YN\n\n            Df   AIC\n&lt;none&gt;         81230\n- PT_YN      1 81238\n- PT_VISITS  1 81302\n\n# Examine final model\nsummary(backwards_model_NON_bic)\n\n\nCall:\nglm.nb(formula = NON_ICU_LOS ~ initiative + Age_Range + Month + \n    PT_VISITS + PT_YN, data = data_ex, init.theta = 0.5680587891, \n    link = log)\n\nCoefficients:\n                  Estimate Std. Error z value             Pr(&gt;|z|)    \n(Intercept)       5.431446   0.070876  76.633 &lt; 0.0000000000000002 ***\ninitiativeDuring -0.043640   0.061831  -0.706              0.48032    \ninitiativePost   -0.053696   0.038041  -1.412              0.15809    \nAge_Range40-49   -0.061716   0.059668  -1.034              0.30098    \nAge_Range50-59   -0.100560   0.052661  -1.910              0.05619 .  \nAge_Range60-69   -0.074376   0.052248  -1.424              0.15458    \nAge_Range70+     -0.278712   0.054256  -5.137          0.000000279 ***\nMonth2           -0.017205   0.080525  -0.214              0.83081    \nMonth3           -0.119259   0.078545  -1.518              0.12892    \nMonth4           -0.144106   0.078638  -1.833              0.06687 .  \nMonth5           -0.202770   0.081773  -2.480              0.01315 *  \nMonth6           -0.250015   0.083968  -2.977              0.00291 ** \nMonth7           -0.079208   0.083618  -0.947              0.34351    \nMonth8           -0.047918   0.081414  -0.589              0.55615    \nMonth9           -0.086284   0.080905  -1.066              0.28621    \nMonth10          -0.047240   0.079171  -0.597              0.55072    \nMonth11          -0.182991   0.081146  -2.255              0.02413 *  \nMonth12           0.009545   0.079749   0.120              0.90473    \nPT_VISITS         0.075407   0.007909   9.535 &lt; 0.0000000000000002 ***\nPT_YN             0.174069   0.041988   4.146          0.000033879 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Negative Binomial(0.5681) family taken to be 1)\n\n    Null deviance: 8133.4  on 6412  degrees of freedom\nResidual deviance: 7887.8  on 6393  degrees of freedom\n  (16 observations deleted due to missingness)\nAIC: 81110\n\nNumber of Fisher Scoring iterations: 1\n\n              Theta:  0.56806 \n          Std. Err.:  0.00861 \n\n 2 x log-likelihood:  -81067.77900 \n\n\nThe final model removes AVG THERAPY LENGTH and MV_YN, which makes sense, as if you didn’t go to the ER, then you didn’t require intensive therapy or mechanical ventilation.\nTop of Tabset\n\n\n\n\n# Relevel \ndata_ex$initiative &lt;- relevel(data_ex$initiative, ref = \"Pre\")\n\n# Run the model\nmodel_NON &lt;- glm.nb(NON_ICU_LOS ~ initiative + Age_Range + Month + PT_VISITS + PT_YN, data = data_ex, link = log)\n\n# Examine Model\nsummary(model_NON)\n\n\nCall:\nglm.nb(formula = NON_ICU_LOS ~ initiative + Age_Range + Month + \n    PT_VISITS + PT_YN, data = data_ex, link = log, init.theta = 0.5680587894)\n\nCoefficients:\n                  Estimate Std. Error z value             Pr(&gt;|z|)    \n(Intercept)       5.431444   0.070876  76.633 &lt; 0.0000000000000002 ***\ninitiativeDuring -0.043638   0.061831  -0.706              0.48034    \ninitiativePost   -0.053694   0.038041  -1.411              0.15810    \nAge_Range40-49   -0.061715   0.059668  -1.034              0.30099    \nAge_Range50-59   -0.100559   0.052661  -1.910              0.05619 .  \nAge_Range60-69   -0.074374   0.052248  -1.423              0.15459    \nAge_Range70+     -0.278710   0.054256  -5.137          0.000000279 ***\nMonth2           -0.017204   0.080525  -0.214              0.83083    \nMonth3           -0.119258   0.078545  -1.518              0.12893    \nMonth4           -0.144105   0.078638  -1.833              0.06687 .  \nMonth5           -0.202770   0.081773  -2.480              0.01315 *  \nMonth6           -0.250013   0.083968  -2.977              0.00291 ** \nMonth7           -0.079208   0.083618  -0.947              0.34351    \nMonth8           -0.047917   0.081414  -0.589              0.55615    \nMonth9           -0.086283   0.080905  -1.066              0.28621    \nMonth10          -0.047239   0.079171  -0.597              0.55072    \nMonth11          -0.182989   0.081146  -2.255              0.02413 *  \nMonth12           0.009545   0.079749   0.120              0.90473    \nPT_VISITS         0.075406   0.007909   9.534 &lt; 0.0000000000000002 ***\nPT_YN             0.174070   0.041988   4.146          0.000033875 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Negative Binomial(0.5681) family taken to be 1)\n\n    Null deviance: 8133.4  on 6412  degrees of freedom\nResidual deviance: 7887.8  on 6393  degrees of freedom\n  (16 observations deleted due to missingness)\nAIC: 81110\n\nNumber of Fisher Scoring iterations: 1\n\n              Theta:  0.56806 \n          Std. Err.:  0.00861 \n\n 2 x log-likelihood:  -81067.77900 \n\n# Get 95% CIs\npretty_print(confint(model_NON))\n\nWaiting for profiling to be done...\n\n\n\n\n\n\n2.5 %\n97.5 %\n\n\n\n\n(Intercept)\n5.2935136\n5.5726245\n\n\ninitiativeDuring\n-0.1622411\n0.0773043\n\n\ninitiativePost\n-0.1267003\n0.0193366\n\n\nAge_Range40-49\n-0.1795974\n0.0566555\n\n\nAge_Range50-59\n-0.2044159\n0.0028030\n\n\nAge_Range60-69\n-0.1776414\n0.0283267\n\n\nAge_Range70+\n-0.3851517\n-0.1725802\n\n\nMonth2\n-0.1753344\n0.1411822\n\n\nMonth3\n-0.2735474\n0.0348734\n\n\nMonth4\n-0.2987545\n0.0103628\n\n\nMonth5\n-0.3636162\n-0.0415060\n\n\nMonth6\n-0.4142783\n-0.0848340\n\n\nMonth7\n-0.2424838\n0.0848648\n\n\nMonth8\n-0.2076720\n0.1121815\n\n\nMonth9\n-0.2444270\n0.0721124\n\n\nMonth10\n-0.2028654\n0.1083008\n\n\nMonth11\n-0.3426818\n-0.0230156\n\n\nMonth12\n-0.1469838\n0.1660829\n\n\nPT_VISITS\n0.0579868\n0.0933824\n\n\nPT_YN\n0.0903232\n0.2576494\n\n\n\n\n\n\n\n\nChange Reference Category to During\n\n# Relevel \ndata_ex$initiative &lt;- relevel(data_ex$initiative, ref = \"During\")\n\n# Run the model\nmodel_NON &lt;- glm.nb(NON_ICU_LOS ~ initiative + Age_Range + Month + PT_VISITS + PT_YN, data = data_ex, link = log)\n\n# Examine Model\nsummary(model_NON)\n\n\nCall:\nglm.nb(formula = NON_ICU_LOS ~ initiative + Age_Range + Month + \n    PT_VISITS + PT_YN, data = data_ex, link = log, init.theta = 0.5680587894)\n\nCoefficients:\n                Estimate Std. Error z value             Pr(&gt;|z|)    \n(Intercept)     5.387805   0.092788  58.066 &lt; 0.0000000000000002 ***\ninitiativePre   0.043638   0.061831   0.706              0.48034    \ninitiativePost -0.010056   0.059053  -0.170              0.86478    \nAge_Range40-49 -0.061715   0.059668  -1.034              0.30099    \nAge_Range50-59 -0.100559   0.052661  -1.910              0.05619 .  \nAge_Range60-69 -0.074374   0.052248  -1.423              0.15459    \nAge_Range70+   -0.278710   0.054256  -5.137          0.000000279 ***\nMonth2         -0.017204   0.080525  -0.214              0.83083    \nMonth3         -0.119258   0.078545  -1.518              0.12893    \nMonth4         -0.144105   0.078638  -1.833              0.06687 .  \nMonth5         -0.202770   0.081773  -2.480              0.01315 *  \nMonth6         -0.250013   0.083968  -2.977              0.00291 ** \nMonth7         -0.079208   0.083618  -0.947              0.34351    \nMonth8         -0.047917   0.081414  -0.589              0.55615    \nMonth9         -0.086283   0.080905  -1.066              0.28621    \nMonth10        -0.047239   0.079171  -0.597              0.55072    \nMonth11        -0.182989   0.081146  -2.255              0.02413 *  \nMonth12         0.009545   0.079749   0.120              0.90473    \nPT_VISITS       0.075406   0.007909   9.534 &lt; 0.0000000000000002 ***\nPT_YN           0.174070   0.041988   4.146          0.000033875 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Negative Binomial(0.5681) family taken to be 1)\n\n    Null deviance: 8133.4  on 6412  degrees of freedom\nResidual deviance: 7887.8  on 6393  degrees of freedom\n  (16 observations deleted due to missingness)\nAIC: 81110\n\nNumber of Fisher Scoring iterations: 1\n\n              Theta:  0.56806 \n          Std. Err.:  0.00861 \n\n 2 x log-likelihood:  -81067.77900 \n\n# Get 95% CIs\npretty_print(confint(model_NON))\n\nWaiting for profiling to be done...\n\n\n\n\n\n\n2.5 %\n97.5 %\n\n\n\n\n(Intercept)\n5.2063551\n5.5722297\n\n\ninitiativePre\n-0.0773043\n0.1622411\n\n\ninitiativePost\n-0.1279697\n0.1054230\n\n\nAge_Range40-49\n-0.1795974\n0.0566555\n\n\nAge_Range50-59\n-0.2044159\n0.0028030\n\n\nAge_Range60-69\n-0.1776414\n0.0283267\n\n\nAge_Range70+\n-0.3851517\n-0.1725802\n\n\nMonth2\n-0.1753344\n0.1411822\n\n\nMonth3\n-0.2735474\n0.0348734\n\n\nMonth4\n-0.2987545\n0.0103628\n\n\nMonth5\n-0.3636162\n-0.0415060\n\n\nMonth6\n-0.4142783\n-0.0848340\n\n\nMonth7\n-0.2424838\n0.0848648\n\n\nMonth8\n-0.2076720\n0.1121815\n\n\nMonth9\n-0.2444270\n0.0721124\n\n\nMonth10\n-0.2028654\n0.1083008\n\n\nMonth11\n-0.3426818\n-0.0230156\n\n\nMonth12\n-0.1469838\n0.1660829\n\n\nPT_VISITS\n0.0579868\n0.0933824\n\n\nPT_YN\n0.0903232\n0.2576494\n\n\n\n\n\n\n\nTop of Tabset\n\n\n\n\nMain Effects\nQI-initiative time period is not a significant predictor for Non-ICU length of stay, while controlling for age, admission month, number of physical therapy visits, and reception of physical therapy. There was no difference in the mean Non-ICU length of stay between the Pre and During period (z = -0.84, 95% CI: [0.84, 1.07], p = 0.40), between the Pre and Post period (z = -1.51, 95% CI: [0.88, 1.02], p = 0.13), or between the During and Post periods (z = -0.093, 95% CI: [0.88, 1.12], p = 0.93).\n\n\nAge\nAge range was a significant predictor of non-ICU length of stay, while controlling for the other variables in the model. Specifically, patients who were 70+ had a mean non-ICU length of stay that was 0.76 that ofpatients who were 13-39 (z = -5.130, 95% CI: [0.68, 0.84], p &lt; 0.0001). Other age comparisons were not significant (p &gt; 0.05), and comparisons with baseline categories besides 13-39 were not conducted.\n\n\nAdmission Month\nAdmission Month was a significant predictor for non-ICU length of stay, while controlling for the other variables in the model. For instance, those admitted during May had an average mechanical ventilation time that was 0.78 times that of those admitted during January (z = -2.52, 95% CI: [0.69, 0.96], p = 0.01).\nSince possible comparisons between months are myriad (12!), and the goal of this project was not to compare outcome variables between months (merely to control for these differences due to seasonality), further comparisons were not made.\n\n\nNumber of Physical Therapy Visits\nNumber of physical therapy visits was a signifcant predictor of non-ICU length of stay. On average, every one visit increase was associated with a 1.08 times higher non-ICU length of stay (z = 9.63, 95% CI: [1.06,1.10], p &lt; 0.0001).\n\n\nReception of Physical Therapy\nReception of physical therapy was a significant predictor of non-ICU length of stay. On average, patients who received physical therapy had a mean non-ICU length of stay that was 1.19 times higher than patients who did not (z = 4.08, 95% CI: [1.09,1.29], p &lt; 0.0001).\nTop of Tabset\n\n\n\n\n# Relevel \ndata_ex$initiative &lt;- relevel(data_ex$initiative, ref = \"Pre\")\n\n# Run the model\nmodel_NON &lt;- glm.nb(NON_ICU_LOS ~ initiative + Age_Range + Month + PT_VISITS + PT_YN, data = data_ex, link = log)\n\n# Extract the effect for initiative\neffect_plot &lt;- effect(\"initiative\", model_NON)\n\n# Convert the effect object to a data frame for ggplot\neffect_data &lt;- as.data.frame(effect_plot)\n\n# Plot using ggplot2\nggplot(effect_data, aes(x = initiative, y = fit, group = 1)) +\n  geom_line(size = 1, color = \"aquamarine\") +\n  geom_point(size = 3) +\n  geom_errorbar(aes(ymin = lower, ymax = upper), width = 0.2, alpha = 0.5) +\n  labs(\n    title = \"Predicted Non-ICU Length of Stay by QI-Initiative Time Period\",\n    x = \"QI-Initiative\",\n    y = \"Non-ICU Length of Stay\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nTop of Tabset\n\n\nWe can conclude that the QI-initiative was not successful in reducing Non_ICU length of stay. Which makes sense, as physical therapy likely targets the patients in worse condition that could benefit from it compared to patients admitted for more benign injuries.\nTop of Tabset"
  },
  {
    "objectID": "Statistical_Consulting/Vieau_Project_01/Code/Megan_Watson_Project.html#Total_LOS",
    "href": "Statistical_Consulting/Vieau_Project_01/Code/Megan_Watson_Project.html#Total_LOS",
    "title": "Assessing the Impact of a Physical Therapy Quality-Improvement Initiative at a Colorado University Hospital",
    "section": "Total Hospital Length of Stay",
    "text": "Total Hospital Length of Stay\nSecondary Outcome of interest B is whether the QI-initiative decreased the total hospital length of stay.\nThis will be a Poisson or Negative Binomial Regression similar to the primary analysis .\n\nCovariatesModel SelectionAnalysisInterpretationVisualizationSummary\n\n\nThe potential covariates for this analysis are the same as the main analysis.\nTop of Tabset\n\n\n\nCheck for Zero Inflation\n\n# Check for Zeros\nmin(data_ex$`HOSPITAL LOS`)\n\n[1] 49\n\n\nYou cannot have a 0 minute hospital length of stay, thus we do not need to consider zero inflation in our model.\n\n\nCheck for Overdispersion\n\n# Compare mean and variance\npaste0(\"Mean = \", mean(data_ex$`HOSPITAL LOS`, na.rm = T))\n\n[1] \"Mean = 377.749261160367\"\n\n# Compare mean and variance\npaste0(\"Variance = \", var(data_ex$`HOSPITAL LOS`, na.rm = T))\n\n[1] \"Variance = 183898.93307545\"\n\n\nWe have overdispersion, a negative binomial regression will be chosen.\n\n\nBackwards Elimination\n\n# Relevel \ndata_ex$initiative &lt;- relevel(data_ex$initiative, ref = \"Pre\")\n\n# Get cleaned data set with missing values omitted because this is necessary with the MASS package\ndata_ex_clean &lt;- data_ex |&gt; \n  dplyr::select(`HOSPITAL LOS`, initiative, Age_Range, Month, PT_VISITS, `AVG THERAPY LENGTH`, MV_YN, PT_YN)\n\n# Remove rows with missing values\ndata_ex_clean &lt;- na.omit(data_ex_clean)\n\n# Run the model\nmodel_tot &lt;- glm.nb(`HOSPITAL LOS` ~ initiative + Age_Range + Month + PT_VISITS + PT_YN + `AVG THERAPY LENGTH` + MV_YN + initiative*MV_YN, data = data_ex_clean, link = log)\n\n# Define the scope to force the inclusion of specific variables\nscope &lt;- list(lower = ~ initiative + Month + Age_Range, upper = formula(model_tot))\n\n# Perform backwards elimination using BIC\nbackwards_model_tot_bic &lt;- stepAIC(model_tot, direction = \"backward\", k = log(n), scope = scope)\n\nStart:  AIC=87466.28\n`HOSPITAL LOS` ~ initiative + Age_Range + Month + PT_VISITS + \n    PT_YN + `AVG THERAPY LENGTH` + MV_YN + initiative * MV_YN\n\n                       Df   AIC\n- PT_YN                 1 87459\n&lt;none&gt;                    87466\n- initiative:MV_YN      2 87467\n- `AVG THERAPY LENGTH`  1 87488\n- PT_VISITS             1 87817\n\nStep:  AIC=87458.79\n`HOSPITAL LOS` ~ initiative + Age_Range + Month + PT_VISITS + \n    `AVG THERAPY LENGTH` + MV_YN + initiative:MV_YN\n\n                       Df   AIC\n&lt;none&gt;                    87459\n- initiative:MV_YN      2 87459\n- `AVG THERAPY LENGTH`  1 87506\n- PT_VISITS             1 87811\n\n# Examine final model\nsummary(backwards_model_tot_bic)\n\n\nCall:\nglm.nb(formula = `HOSPITAL LOS` ~ initiative + Age_Range + Month + \n    PT_VISITS + `AVG THERAPY LENGTH` + MV_YN + initiative:MV_YN, \n    data = data_ex_clean, init.theta = 1.706675816, link = log)\n\nCoefficients:\n                          Estimate Std. Error z value             Pr(&gt;|z|)    \n(Intercept)              5.7141463  0.0430473 132.741 &lt; 0.0000000000000002 ***\ninitiativeDuring        -0.1085199  0.0485788  -2.234              0.02549 *  \ninitiativePost          -0.0729731  0.0300860  -2.425              0.01529 *  \nAge_Range40-49          -0.0333751  0.0344891  -0.968              0.33319    \nAge_Range50-59          -0.0657986  0.0304475  -2.161              0.03069 *  \nAge_Range60-69          -0.0588501  0.0302327  -1.947              0.05159 .  \nAge_Range70+            -0.1971839  0.0313954  -6.281   0.0000000003371473 ***\nMonth2                  -0.0092354  0.0465774  -0.198              0.84282    \nMonth3                  -0.0845100  0.0454556  -1.859              0.06300 .  \nMonth4                  -0.0927520  0.0455189  -2.038              0.04158 *  \nMonth5                  -0.1462935  0.0472877  -3.094              0.00198 ** \nMonth6                  -0.1568557  0.0485668  -3.230              0.00124 ** \nMonth7                  -0.0558665  0.0483837  -1.155              0.24823    \nMonth8                  -0.0152168  0.0471583  -0.323              0.74694    \nMonth9                  -0.0392989  0.0468276  -0.839              0.40134    \nMonth10                 -0.0378652  0.0458265  -0.826              0.40865    \nMonth11                 -0.1069590  0.0469269  -2.279              0.02265 *  \nMonth12                  0.0403426  0.0461244   0.875              0.38177    \nPT_VISITS                0.0936973  0.0047424  19.757 &lt; 0.0000000000000002 ***\n`AVG THERAPY LENGTH`     0.0047051  0.0006204   7.584   0.0000000000000335 ***\nMV_YN1                   0.2588434  0.0288466   8.973 &lt; 0.0000000000000002 ***\ninitiativeDuring:MV_YN1 -0.1595678  0.0672666  -2.372              0.01768 *  \ninitiativePost:MV_YN1   -0.1605212  0.0410735  -3.908   0.0000930085101959 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Negative Binomial(1.7067) family taken to be 1)\n\n    Null deviance: 8106.2  on 6404  degrees of freedom\nResidual deviance: 6998.2  on 6382  degrees of freedom\nAIC: 87321\n\nNumber of Fisher Scoring iterations: 1\n\n              Theta:  1.7067 \n          Std. Err.:  0.0279 \n\n 2 x log-likelihood:  -87272.7600 \n\n\nThe final model drops PT_YN and the initiative*MV_YN interaction term.\nTop of Tabset\n\n\n\n\n# Relevel \ndata_ex$initiative &lt;- relevel(data_ex$initiative, ref = \"Pre\")\n\n# Run the model\nmodel_tot &lt;- glm.nb(`HOSPITAL LOS` ~ initiative + Age_Range + Month + PT_VISITS + `AVG THERAPY LENGTH` + MV_YN, data = data_ex, link = log)\n\n# Examine Model\nsummary(model_tot)\n\n\nCall:\nglm.nb(formula = `HOSPITAL LOS` ~ initiative + Age_Range + Month + \n    PT_VISITS + `AVG THERAPY LENGTH` + MV_YN, data = data_ex, \n    link = log, init.theta = 1.702916828)\n\nCoefficients:\n                      Estimate Std. Error z value             Pr(&gt;|z|)    \n(Intercept)           5.758068   0.041868 137.528 &lt; 0.0000000000000002 ***\ninitiativeDuring     -0.181866   0.035527  -5.119  0.00000030687045696 ***\ninitiativePost       -0.153099   0.021919  -6.985  0.00000000000284983 ***\nAge_Range40-49       -0.031253   0.034518  -0.905             0.365259    \nAge_Range50-59       -0.065947   0.030468  -2.164             0.030427 *  \nAge_Range60-69       -0.061447   0.030254  -2.031             0.042254 *  \nAge_Range70+         -0.197563   0.031425  -6.287  0.00000000032410509 ***\nMonth2               -0.010035   0.046625  -0.215             0.829582    \nMonth3               -0.086944   0.045494  -1.911             0.055988 .  \nMonth4               -0.098762   0.045547  -2.168             0.030133 *  \nMonth5               -0.151600   0.047339  -3.202             0.001363 ** \nMonth6               -0.162222   0.048598  -3.338             0.000844 ***\nMonth7               -0.058450   0.048427  -1.207             0.227437    \nMonth8               -0.021490   0.047185  -0.455             0.648786    \nMonth9               -0.047100   0.046847  -1.005             0.314706    \nMonth10              -0.044681   0.045843  -0.975             0.329732    \nMonth11              -0.112168   0.046964  -2.388             0.016922 *  \nMonth12               0.032102   0.046159   0.695             0.486762    \nPT_VISITS             0.090483   0.004691  19.289 &lt; 0.0000000000000002 ***\n`AVG THERAPY LENGTH`  0.004991   0.000618   8.076  0.00000000000000067 ***\nMV_YN1                0.172473   0.019665   8.771 &lt; 0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Negative Binomial(1.7029) family taken to be 1)\n\n    Null deviance: 8088.5  on 6404  degrees of freedom\nResidual deviance: 6999.4  on 6384  degrees of freedom\n  (24 observations deleted due to missingness)\nAIC: 87333\n\nNumber of Fisher Scoring iterations: 1\n\n              Theta:  1.7029 \n          Std. Err.:  0.0278 \n\n 2 x log-likelihood:  -87289.3850 \n\n# Get confidence intervals\npretty_print(confint(model_tot))\n\nWaiting for profiling to be done...\n\n\n\n\n\n\n2.5 %\n97.5 %\n\n\n\n\n(Intercept)\n5.6763881\n5.8407990\n\n\ninitiativeDuring\n-0.2505101\n-0.1124350\n\n\ninitiativePost\n-0.1953356\n-0.1108511\n\n\nAge_Range40-49\n-0.0991488\n0.0368078\n\n\nAge_Range50-59\n-0.1258433\n-0.0062158\n\n\nAge_Range60-69\n-0.1209400\n-0.0021411\n\n\nAge_Range70+\n-0.2590113\n-0.1362179\n\n\nMonth2\n-0.1014594\n0.0814717\n\n\nMonth3\n-0.1762229\n0.0022774\n\n\nMonth4\n-0.1881945\n-0.0093913\n\n\nMonth5\n-0.2445397\n-0.0585268\n\n\nMonth6\n-0.2573323\n-0.0668111\n\n\nMonth7\n-0.1531093\n0.0364691\n\n\nMonth8\n-0.1140185\n0.0711525\n\n\nMonth9\n-0.1386901\n0.0445725\n\n\nMonth10\n-0.1346373\n0.0452452\n\n\nMonth11\n-0.2043456\n-0.0199027\n\n\nMonth12\n-0.0584206\n0.1226230\n\n\nPT_VISITS\n0.0803614\n0.1007693\n\n\n`AVG THERAPY LENGTH`\n0.0037486\n0.0062392\n\n\nMV_YN1\n0.1340589\n0.2108852\n\n\n\n\n\n\n\n\nChange Reference Level to During\n\n# Relevel \ndata_ex$initiative &lt;- relevel(data_ex$initiative, ref = \"During\")\n\n# Relevel \ndata_ex$MV_YN &lt;- relevel(data_ex$MV_YN, ref = \"0\")\n\n# Run the model\nmodel_tot &lt;- glm.nb(`HOSPITAL LOS` ~ initiative + Age_Range + Month + PT_VISITS + `AVG THERAPY LENGTH` + MV_YN, data = data_ex, link = log)\n\n# Examine Model\nsummary(model_tot)\n\n\nCall:\nglm.nb(formula = `HOSPITAL LOS` ~ initiative + Age_Range + Month + \n    PT_VISITS + `AVG THERAPY LENGTH` + MV_YN, data = data_ex, \n    link = log, init.theta = 1.702916828)\n\nCoefficients:\n                      Estimate Std. Error z value             Pr(&gt;|z|)    \n(Intercept)           5.576201   0.053762 103.720 &lt; 0.0000000000000002 ***\ninitiativePre         0.181866   0.035527   5.119  0.00000030687045696 ***\ninitiativePost        0.028767   0.034196   0.841             0.400214    \nAge_Range40-49       -0.031253   0.034518  -0.905             0.365259    \nAge_Range50-59       -0.065947   0.030468  -2.164             0.030427 *  \nAge_Range60-69       -0.061447   0.030254  -2.031             0.042254 *  \nAge_Range70+         -0.197563   0.031425  -6.287  0.00000000032410509 ***\nMonth2               -0.010035   0.046625  -0.215             0.829582    \nMonth3               -0.086944   0.045494  -1.911             0.055988 .  \nMonth4               -0.098762   0.045547  -2.168             0.030133 *  \nMonth5               -0.151600   0.047339  -3.202             0.001363 ** \nMonth6               -0.162222   0.048598  -3.338             0.000844 ***\nMonth7               -0.058450   0.048427  -1.207             0.227437    \nMonth8               -0.021490   0.047185  -0.455             0.648786    \nMonth9               -0.047100   0.046847  -1.005             0.314706    \nMonth10              -0.044681   0.045843  -0.975             0.329732    \nMonth11              -0.112168   0.046964  -2.388             0.016922 *  \nMonth12               0.032102   0.046159   0.695             0.486762    \nPT_VISITS             0.090483   0.004691  19.289 &lt; 0.0000000000000002 ***\n`AVG THERAPY LENGTH`  0.004991   0.000618   8.076  0.00000000000000067 ***\nMV_YN1                0.172473   0.019665   8.771 &lt; 0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Negative Binomial(1.7029) family taken to be 1)\n\n    Null deviance: 8088.5  on 6404  degrees of freedom\nResidual deviance: 6999.4  on 6384  degrees of freedom\n  (24 observations deleted due to missingness)\nAIC: 87333\n\nNumber of Fisher Scoring iterations: 1\n\n              Theta:  1.7029 \n          Std. Err.:  0.0278 \n\n 2 x log-likelihood:  -87289.3850 \n\n# Get confidence intervals\npretty_print(confint(model_tot))\n\nWaiting for profiling to be done...\n\n\n\n\n\n\n2.5 %\n97.5 %\n\n\n\n\n(Intercept)\n5.4710371\n5.6823491\n\n\ninitiativePre\n0.1124350\n0.2505101\n\n\ninitiativePost\n-0.0388821\n0.0955995\n\n\nAge_Range40-49\n-0.0991488\n0.0368078\n\n\nAge_Range50-59\n-0.1258433\n-0.0062158\n\n\nAge_Range60-69\n-0.1209400\n-0.0021411\n\n\nAge_Range70+\n-0.2590113\n-0.1362179\n\n\nMonth2\n-0.1014594\n0.0814717\n\n\nMonth3\n-0.1762229\n0.0022774\n\n\nMonth4\n-0.1881945\n-0.0093913\n\n\nMonth5\n-0.2445397\n-0.0585268\n\n\nMonth6\n-0.2573323\n-0.0668111\n\n\nMonth7\n-0.1531093\n0.0364691\n\n\nMonth8\n-0.1140185\n0.0711525\n\n\nMonth9\n-0.1386901\n0.0445725\n\n\nMonth10\n-0.1346373\n0.0452452\n\n\nMonth11\n-0.2043456\n-0.0199027\n\n\nMonth12\n-0.0584206\n0.1226230\n\n\nPT_VISITS\n0.0803614\n0.1007693\n\n\n`AVG THERAPY LENGTH`\n0.0037486\n0.0062392\n\n\nMV_YN1\n0.1340589\n0.2108852\n\n\n\n\n\n\n# Check vifs\npretty_print(vif(model_tot))\n\n\n\n\n\nGVIF\nDf\nGVIF^(1/(2*Df))\n\n\n\n\ninitiative\n1.274062\n2\n1.062423\n\n\nAge_Range\n1.034605\n4\n1.004261\n\n\nMonth\n1.073062\n11\n1.003210\n\n\nPT_VISITS\n1.505159\n1\n1.226849\n\n\n`AVG THERAPY LENGTH`\n1.488256\n1\n1.219941\n\n\nMV_YN\n1.048987\n1\n1.024201\n\n\n\n\n\n\n\nTop of Tabset\n\n\n\n\nMain Effects\nQI-initiative time period was a significant predictor of total hospital length of stay, while controlling for age, admission month, mechanical ventilation, total physical therapy visits, and average therapy length. Compared to those in the Pre period, patients in the During period had a mean total hospital length of stay that was 0.83 times that of patients in the pre period (z = -5.27, 95% CI: [0.77,0.89], p &lt; 0.0001), and patients in the Period had a mean total hospital length of stay that was 0.86 times that of patients in the Pre period (z = -7.10, 95% CI: [0.82,0.89], p &lt; 0.0001). Those who were admitted in the During period did not have a significantly different total hospital length of stay compared to those enrolled in the Post period (z = 0.92, 95% CI: [0.96, 1.10], p = 0.36).\n\n\nAge\nAge range was a significant predictor of total hospital length of stay, while controlling for the other variables in the model. Specifically, patients who were 50-59 had 0.94 times (z = -2.16, 95% CI: [0.88, 0.99], p = 0.031), patients who were 60-69 had 0.94 times (z = -2.03, 95% CI: [0.89, 0.99], p = 0.043), and patients who were 70+ had 0.82 times the total hospital length of stay of patients who were 13-39 (z = -6.23, 95% CI: [0.77, 0.87], p = 0.0001). Patients who were 40-49 did not differ significantly from patients who were 13-39 (p = 0.37).\n\n\nAdmission Month\nAdmission Month was a significant predictor for total hospital length of stay, while controlling for the other variables in the model. For instance, those admitted during May had an average total hospital length of stay that was 0.86 times that of those admitted during January (z = -3.19, 95% CI: [0.78, -0.0578866], p = 0.0014).\nSince possible comparisons between months are myriad (12!), and the goal of this project was not to compare outcome variables between months (merely to control for these differences due to seasonality), further comparisons will not be made.\n\nNumber of Physical Therapy Visits\nTotal number of physical therapy visits was a significant predictor of total hospital length of stay, while controlling for the other variables in the model. On average, each 1 visit increase was associated with a 1.10 times increase in total hospital length stay (z = 19.37, 95% CI: [1.08, 1.11], p &lt; 0.0001).\n\n\nAverage Physical Therapy Length\nAverage physical therapy length was a significant predictor of total hospital length of stay, while controlling for the other variables in the model. On average, each 1 minute increase in average physical therapy visit length was assocaited with a 1.004 times increase total hospital length of stay (z = 8.03, 95% CI: [1.003, 1.006], p &lt; 0.0001).\n\n\nReception of Mechanical Ventilation\nReception of mechanical ventilation was a signicant predictor of total hospital length of stay. On average, patients who were mechanically ventilated had a mean total hospital length of stay that was 1.19 times greater than those who were not (z = 8.79, 95% CI: [1.14, 1.23], p &lt; 0.0001).\nTop of Tabset\n\n\n\n\n\n# Relevel \ndata_ex$initiative &lt;- relevel(data_ex$initiative, ref = \"Pre\")\n\n# Get rid of spaces\ndata_ex &lt;- data_ex |&gt; \n  mutate(HOSPITAL_LOS = `HOSPITAL LOS`)\n\n# Run the model\nmodel_tot &lt;- glm.nb(HOSPITAL_LOS ~ initiative + Age_Range + Month + PT_VISITS + AVG_THERAPY_LENGTH + MV_YN, data = data_ex, link = log)\n\n# Extract the effect for initiative\neffect_plot &lt;- effect(\"initiative\", model_tot)\n\n# Convert the effect object to a data frame for ggplot\neffect_data &lt;- as.data.frame(effect_plot)\n\n# Plot using ggplot2\nggplot(effect_data, aes(x = initiative, y = fit, group = 1)) +\n  geom_line(size = 1, color = \"aquamarine\") +\n  geom_point(size = 3) +\n  geom_errorbar(aes(ymin = lower, ymax = upper), width = 0.2, alpha = 0.5) +\n  labs(\n    title = \"Predicted Total Hospital Length of Stay by QI-Initiative Time Period\",\n    x = \"QI-Initiative\",\n    y = \"Total Hospital Length of Stay\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nTop of Tabset\n\n\nThe QI-initiative was successful at reducing the total hospital length of stay!\nQI-initiative time period was a significant predictor of total hospital length of stay, while controlling for age, admission month, mechanical ventilation, total physical therapy visits, and average therapy length. Compared to those in the Pre period, patients in the During period had a mean total hospital length of stay that was 0.83 times that of patients in the pre period (p &lt; 0.0001), and patients in the Period had a mean total hospital length of stay that was 0.86 times that of patients in the Pre period(p &lt; 0.0001). Those who were admitted in the During period did not have a significantly different total hospital length of stay compared to those enrolled in the Post period (z = 0.92, 95% CI: [0.96, 1.10], p = 0.36).\nTop of Tabset"
  }
]