{
  "hash": "c600c0a0c9ceb1852a29abdcf317b50f",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Advanced Data Analysis - Project 1\"\nauthor: \"Sean Vieau\"\ndate: \"September 9, 2024\"\noutput: html_document\ntoc: true\n---\n\n\n\n\n\n\n# Introduction\n\nThe aim of the current study is to assess whether a new gel treatment for gum disease results in lower whole-mouth average pocket depth and attachment loss after one year. Average pocket depth and attachment loss were taken at baseline, and participants were assigned into one of five groups (1 = placebo, 2 = no treatment, 3 = low concentration, 4 = medium concentration, 5 = high concentration) and instructed to apply the gel to their gums twice a day. After 1-year, average pocket depth and attachment loss were recorded again. Data was received as a .csv file containing treatment level, average pocket depth and attachment loss at baseline and at one-year, demographic information, gender, age, number of sites measured, and smoking status. The clinical hypothesis is that average pocket depth and attachment loss in participants who applied the gel will be lower compared to participants who did not. Gender, age, ethnicity, and smoking status will be investigated as potential covariates.\n\nThe project description provided by the PI is available below:\n\n![](Media/Project-1-Description.png)\n\n## Data Preparation\n\nFirst we begin by loading in the necessary packages\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(tidyverse)\nlibrary(knitr)\nlibrary(kableExtra) # This lets me pretty print the tables\nlibrary(naniar) # Used to visualize missing data\nlibrary(glue) # This is used for f-strings in R\nlibrary(purrr) # Used for summary tables\nlibrary(corrplot) # Used to make the correlation matrix\nlibrary(corrtable) # used to make the table for the correlation matrix\nlibrary(xtable) # Used to make the table for the correlation matrix\nlibrary(htmlTable) # Used to make Table 1\nlibrary(boot) # Used to make Table 1\nlibrary(table1) # Used to make Table 1\nlibrary(sjPlot) # Used to generate publication quality tables for regressions\nlibrary(mice) # Used for multiple imputation\nlibrary(car) # Used for outlier diagnostics with jackknife residuals\n```\n:::\n\n\n\n\nThen we will import the dataset\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Import dataset\ndata <- read.csv(\"C:\\\\Users\\\\sviea\\\\Documents\\\\Advanced Data Analysis\\\\Project 1 MLR with Covariates\\\\Project1_data.csv\")\nnames(data)[1] <- \"id\" # Renaming the weird character out of this colname\n```\n:::\n\n\n\n\nAnd visualize the dataset\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create a nicely formatted table, code adapted from ChatGPT\nkable(head(data), format = \"html\") %>%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\"))\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table table-striped table-hover table-condensed\" style=\"margin-left: auto; margin-right: auto;\">\n <thead>\n  <tr>\n   <th style=\"text-align:right;\"> id </th>\n   <th style=\"text-align:right;\"> trtgroup </th>\n   <th style=\"text-align:right;\"> gender </th>\n   <th style=\"text-align:right;\"> race </th>\n   <th style=\"text-align:right;\"> age </th>\n   <th style=\"text-align:right;\"> smoker </th>\n   <th style=\"text-align:right;\"> sites </th>\n   <th style=\"text-align:right;\"> attachbase </th>\n   <th style=\"text-align:right;\"> attach1year </th>\n   <th style=\"text-align:right;\"> pdbase </th>\n   <th style=\"text-align:right;\"> pd1year </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:right;\"> 101 </td>\n   <td style=\"text-align:right;\"> 4 </td>\n   <td style=\"text-align:right;\"> 2 </td>\n   <td style=\"text-align:right;\"> 5 </td>\n   <td style=\"text-align:right;\"> 44.57221 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 162 </td>\n   <td style=\"text-align:right;\"> 2.432099 </td>\n   <td style=\"text-align:right;\"> 2.577640 </td>\n   <td style=\"text-align:right;\"> 3.246914 </td>\n   <td style=\"text-align:right;\"> 3.407407 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 102 </td>\n   <td style=\"text-align:right;\"> 5 </td>\n   <td style=\"text-align:right;\"> 2 </td>\n   <td style=\"text-align:right;\"> 5 </td>\n   <td style=\"text-align:right;\"> 35.57290 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 162 </td>\n   <td style=\"text-align:right;\"> 2.543210 </td>\n   <td style=\"text-align:right;\"> NA </td>\n   <td style=\"text-align:right;\"> 3.006173 </td>\n   <td style=\"text-align:right;\"> NA </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 103 </td>\n   <td style=\"text-align:right;\"> 2 </td>\n   <td style=\"text-align:right;\"> 2 </td>\n   <td style=\"text-align:right;\"> 5 </td>\n   <td style=\"text-align:right;\"> 47.94524 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 144 </td>\n   <td style=\"text-align:right;\"> 2.881944 </td>\n   <td style=\"text-align:right;\"> 3.076389 </td>\n   <td style=\"text-align:right;\"> 3.118056 </td>\n   <td style=\"text-align:right;\"> 3.125000 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 104 </td>\n   <td style=\"text-align:right;\"> 3 </td>\n   <td style=\"text-align:right;\"> 2 </td>\n   <td style=\"text-align:right;\"> 5 </td>\n   <td style=\"text-align:right;\"> 55.17864 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 138 </td>\n   <td style=\"text-align:right;\"> 4.956522 </td>\n   <td style=\"text-align:right;\"> 5.304348 </td>\n   <td style=\"text-align:right;\"> 5.217391 </td>\n   <td style=\"text-align:right;\"> 4.891304 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 105 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 2 </td>\n   <td style=\"text-align:right;\"> 2 </td>\n   <td style=\"text-align:right;\"> 43.79740 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 168 </td>\n   <td style=\"text-align:right;\"> 1.773810 </td>\n   <td style=\"text-align:right;\"> 1.452381 </td>\n   <td style=\"text-align:right;\"> 3.363095 </td>\n   <td style=\"text-align:right;\"> 2.898810 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 106 </td>\n   <td style=\"text-align:right;\"> 4 </td>\n   <td style=\"text-align:right;\"> 2 </td>\n   <td style=\"text-align:right;\"> 5 </td>\n   <td style=\"text-align:right;\"> 42.14921 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 168 </td>\n   <td style=\"text-align:right;\"> 2.369048 </td>\n   <td style=\"text-align:right;\"> 1.922619 </td>\n   <td style=\"text-align:right;\"> 3.910714 </td>\n   <td style=\"text-align:right;\"> 3.083333 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n\n```{.r .cell-code}\n# Acquire number of variables and observations of the data set\ndim(data)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 130  11\n```\n\n\n:::\n:::\n\n\n\n\nOur data set is comprised of 130 observations, with 11 variables\n\n### Variables\n\nThe variables in the provided data set are as follows:\n\n-   [id:]{.underline} Patient ID\n-   [trtgroup]{.underline}: Treatment group (1 = placebo, 2 = no treatment control, 3 = low concentration, 4 = medium concentration, 5 = high concentration gel)\n-   [gender]{.underline}: Gender (1 = male, 2 = female)\n-   [race]{.underline}: Race, (1 = Native American, 2 = African American, 3 = Not used, 4 = Asian, 5 = White)\n-   [age]{.underline}: Age in years\n-   [smoker]{.underline}: Smoking status (0 = No, 1 = Yes)\n-   [sites]{.underline}: Number of sites gum measurements were averaged from\n-   [attachbase]{.underline}: Whole-mouth average attachment loss taken at base timepoint\n-   [attach1year]{.underline}: Whole-mouth average attachment loss after 1 year\n-   [pdbase]{.underline}: Whole-mouth average pocket loss at base timepoint\n-   [pd1year]{.underline}: Whole-mouth average pocket loss after 1 year\n\n::: callout-note\n## Note\n\nPocket depth and attachment loss are both measurements of how far the gums have pulled away from the teeth, hence *smaller values are better*\n:::\n\n# Data Management and Variable Creation {#Data_Management}\n\nHere we will perform several assessments to ensure fidelity of the data\n\n:::: panel-tabset\n## Erroneous Data Entry\n\nFirst we will perform a simple check to assess for correct entry of data\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create a function to summarize each column\nsummarize_column <- function(column) {\n  data.frame(\n    Min = min(column, na.rm = TRUE),\n    Max = max(column, na.rm = TRUE)\n  )\n}\n\n# Apply the function to each column and bind the results into a single data frame\nsummary_df <- map_dfr(data, summarize_column, .id = \"Column\")\n\n# Pretty print the table\nkable(summary_df, format = \"html\") %>%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\"))\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table table-striped table-hover table-condensed\" style=\"margin-left: auto; margin-right: auto;\">\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> Column </th>\n   <th style=\"text-align:right;\"> Min </th>\n   <th style=\"text-align:right;\"> Max </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> id </td>\n   <td style=\"text-align:right;\"> 101.0000000 </td>\n   <td style=\"text-align:right;\"> 270.000000 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> trtgroup </td>\n   <td style=\"text-align:right;\"> 1.0000000 </td>\n   <td style=\"text-align:right;\"> 5.000000 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> gender </td>\n   <td style=\"text-align:right;\"> 1.0000000 </td>\n   <td style=\"text-align:right;\"> 2.000000 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> race </td>\n   <td style=\"text-align:right;\"> 1.0000000 </td>\n   <td style=\"text-align:right;\"> 5.000000 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> age </td>\n   <td style=\"text-align:right;\"> 28.5722108 </td>\n   <td style=\"text-align:right;\"> 74.532512 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> smoker </td>\n   <td style=\"text-align:right;\"> 0.0000000 </td>\n   <td style=\"text-align:right;\"> 1.000000 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> sites </td>\n   <td style=\"text-align:right;\"> 114.0000000 </td>\n   <td style=\"text-align:right;\"> 168.000000 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> attachbase </td>\n   <td style=\"text-align:right;\"> 0.8950617 </td>\n   <td style=\"text-align:right;\"> 5.089286 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> attach1year </td>\n   <td style=\"text-align:right;\"> 0.8653846 </td>\n   <td style=\"text-align:right;\"> 5.304348 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> pdbase </td>\n   <td style=\"text-align:right;\"> 2.2628205 </td>\n   <td style=\"text-align:right;\"> 5.217391 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> pd1year </td>\n   <td style=\"text-align:right;\"> 1.9642857 </td>\n   <td style=\"text-align:right;\"> 4.891304 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n\n\nAll of these values look believable, and we can conclude that there was at least no grievous mistake during data entry (e.g. no participants with attachment losses of 55, or ages of 180, etc.)\n\n<a href=\"#Data_Management\">Back to top of tabset</a>\n\n## Outliers\n\nFirst we will make a visual inspection for outliers using boxplots\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Visualize data to identify outliers\n#### Outlier code modified from Joey Kim's code\n# This code creates a boxplot for the dvs, and labels the values of any outliers\noutlier_attachbase <- boxplot(data$attachbase, main = \"Boxplot for Attachment Loss at baseline\")$out\ntext(x = rep(1.2, length(outlier_attachbase)),\n     y = outlier_attachbase, labels = outlier_attachbase, col = 'red', cex = 0.8)\n```\n\n::: {.cell-output-display}\n![](Project_1_Regression_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# Repeating for attachment loss at 1 year\noutlier_attach1year <- boxplot(data$attach1year, main = \"Boxplot for Attachment Loss at 1 year\")$out\ntext(x = rep(1.2, length(outlier_attach1year)),\n     y = outlier_attach1year, labels = outlier_attach1year, col = 'red', cex = 0.8)\n```\n\n::: {.cell-output-display}\n![](Project_1_Regression_files/figure-html/unnamed-chunk-5-2.png){width=672}\n:::\n\n```{.r .cell-code}\n# Repeating for pocket depth at base\noutlier_pdbase <- boxplot(data$pdbase, main = \"Boxplot for Pocket Depth at baseline\")$out\ntext(x = rep(1.2, length(outlier_pdbase)),\n     y = outlier_pdbase, labels = outlier_pdbase, col = 'red', cex = 0.8)\n```\n\n::: {.cell-output-display}\n![](Project_1_Regression_files/figure-html/unnamed-chunk-5-3.png){width=672}\n:::\n\n```{.r .cell-code}\n# Repeating for pocket depth at 1 year\noutlier_pd1year <- boxplot(data$pd1year, main = \"Boxplot for Pocket Depth at 1 year\")$out\ntext(x = rep(1.2, length(outlier_pd1year)),\n     y = outlier_pd1year, labels = outlier_pd1year, col = 'red', cex = 0.8)\n```\n\n::: {.cell-output-display}\n![](Project_1_Regression_files/figure-html/unnamed-chunk-5-4.png){width=672}\n:::\n:::\n\n\n\n\nInterestingly, there are 3 outliers for attachment loss at base, and only 1 after 1 year. Likewise, there are 4 outliers for pocket depth at base, but only 1 at 1 year. These outlier participants may have dropped out of the study or have missing values, or their values have returned to be within 3 SDs of the mean values. It will be important to assess this!\n\nOutliers will be assessed statistically later using the jackknife residuals. A value will be considered an outlier if the jackknife residual is outside of the range of +/- 3, and has high leverage and influence in the model.\n\n<a href=\"#Data_Management\">Back to top of tabset</a>\n\n## Variable Creation\n\nSince we have two timepoints of both attachment loss and pocket depth loss, it will make the most sense to create a change score for each measurement and use those as the dependent variable. This will make the analysis significantly easier to perform and understand/explain, while still being an accurate analysis.\n\n::: callout-note\n## Note\n\nattachchange and pdchange are coded such that *higher scores signify a greater loss over time*\n:::\n\nFor example, for subject 101:\n\n$$\n2.577(1 year)-2.43(base) = 0.14 5 (change in attachment loss)\n$$\n\nNegative values mean that attachment loss/ pocket depth *IMPROVED* for that participant (i.e., they gained gum attachment back)\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create new variables for the change score of attachment loss and pocket depth \ndata[\"attachchange\"] <- data[\"attach1year\"] - data[\"attachbase\"]\ndata[\"pdchange\"] <- data[\"pd1year\"] - data[\"pdbase\"]\n\n# Check that our change scores are added correctly to the dataframe\nkable(head(data), format = \"html\") %>%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\"))\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table table-striped table-hover table-condensed\" style=\"margin-left: auto; margin-right: auto;\">\n <thead>\n  <tr>\n   <th style=\"text-align:right;\"> id </th>\n   <th style=\"text-align:right;\"> trtgroup </th>\n   <th style=\"text-align:right;\"> gender </th>\n   <th style=\"text-align:right;\"> race </th>\n   <th style=\"text-align:right;\"> age </th>\n   <th style=\"text-align:right;\"> smoker </th>\n   <th style=\"text-align:right;\"> sites </th>\n   <th style=\"text-align:right;\"> attachbase </th>\n   <th style=\"text-align:right;\"> attach1year </th>\n   <th style=\"text-align:right;\"> pdbase </th>\n   <th style=\"text-align:right;\"> pd1year </th>\n   <th style=\"text-align:right;\"> attachchange </th>\n   <th style=\"text-align:right;\"> pdchange </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:right;\"> 101 </td>\n   <td style=\"text-align:right;\"> 4 </td>\n   <td style=\"text-align:right;\"> 2 </td>\n   <td style=\"text-align:right;\"> 5 </td>\n   <td style=\"text-align:right;\"> 44.57221 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 162 </td>\n   <td style=\"text-align:right;\"> 2.432099 </td>\n   <td style=\"text-align:right;\"> 2.577640 </td>\n   <td style=\"text-align:right;\"> 3.246914 </td>\n   <td style=\"text-align:right;\"> 3.407407 </td>\n   <td style=\"text-align:right;\"> 0.1455410 </td>\n   <td style=\"text-align:right;\"> 0.1604938 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 102 </td>\n   <td style=\"text-align:right;\"> 5 </td>\n   <td style=\"text-align:right;\"> 2 </td>\n   <td style=\"text-align:right;\"> 5 </td>\n   <td style=\"text-align:right;\"> 35.57290 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 162 </td>\n   <td style=\"text-align:right;\"> 2.543210 </td>\n   <td style=\"text-align:right;\"> NA </td>\n   <td style=\"text-align:right;\"> 3.006173 </td>\n   <td style=\"text-align:right;\"> NA </td>\n   <td style=\"text-align:right;\"> NA </td>\n   <td style=\"text-align:right;\"> NA </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 103 </td>\n   <td style=\"text-align:right;\"> 2 </td>\n   <td style=\"text-align:right;\"> 2 </td>\n   <td style=\"text-align:right;\"> 5 </td>\n   <td style=\"text-align:right;\"> 47.94524 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 144 </td>\n   <td style=\"text-align:right;\"> 2.881944 </td>\n   <td style=\"text-align:right;\"> 3.076389 </td>\n   <td style=\"text-align:right;\"> 3.118056 </td>\n   <td style=\"text-align:right;\"> 3.125000 </td>\n   <td style=\"text-align:right;\"> 0.1944444 </td>\n   <td style=\"text-align:right;\"> 0.0069444 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 104 </td>\n   <td style=\"text-align:right;\"> 3 </td>\n   <td style=\"text-align:right;\"> 2 </td>\n   <td style=\"text-align:right;\"> 5 </td>\n   <td style=\"text-align:right;\"> 55.17864 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 138 </td>\n   <td style=\"text-align:right;\"> 4.956522 </td>\n   <td style=\"text-align:right;\"> 5.304348 </td>\n   <td style=\"text-align:right;\"> 5.217391 </td>\n   <td style=\"text-align:right;\"> 4.891304 </td>\n   <td style=\"text-align:right;\"> 0.3478261 </td>\n   <td style=\"text-align:right;\"> -0.3260870 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 105 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 2 </td>\n   <td style=\"text-align:right;\"> 2 </td>\n   <td style=\"text-align:right;\"> 43.79740 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 168 </td>\n   <td style=\"text-align:right;\"> 1.773810 </td>\n   <td style=\"text-align:right;\"> 1.452381 </td>\n   <td style=\"text-align:right;\"> 3.363095 </td>\n   <td style=\"text-align:right;\"> 2.898810 </td>\n   <td style=\"text-align:right;\"> -0.3214286 </td>\n   <td style=\"text-align:right;\"> -0.4642857 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 106 </td>\n   <td style=\"text-align:right;\"> 4 </td>\n   <td style=\"text-align:right;\"> 2 </td>\n   <td style=\"text-align:right;\"> 5 </td>\n   <td style=\"text-align:right;\"> 42.14921 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 168 </td>\n   <td style=\"text-align:right;\"> 2.369048 </td>\n   <td style=\"text-align:right;\"> 1.922619 </td>\n   <td style=\"text-align:right;\"> 3.910714 </td>\n   <td style=\"text-align:right;\"> 3.083333 </td>\n   <td style=\"text-align:right;\"> -0.4464286 </td>\n   <td style=\"text-align:right;\"> -0.8273810 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n\n\nI am curious to look at the outliers for our change scores\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Creating boxplots to examine outliers for attachment loss change scores\noutlier_attachchange <- boxplot(data$attachchange, main = \"Boxplot for Attachment Loss Change Score\")$out\ntext(x = rep(1.2, length(outlier_attachchange)),\n     y = outlier_attachchange, labels = outlier_attachchange, col = 'red', cex = 0.8)\n```\n\n::: {.cell-output-display}\n![](Project_1_Regression_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# Creating boxplots to examine outliers for pocket depth change scores\n\noutlier_pdchange <- boxplot(data$pdchange, main = \"Boxplot for Pocket Depth Change Score\")$out\n```\n\n::: {.cell-output-display}\n![](Project_1_Regression_files/figure-html/unnamed-chunk-7-2.png){width=672}\n:::\n\n```{.r .cell-code}\n# Note: no outliers for this variable, commenting out the following code as it throws an error message as a result and prevents the code from knitting.\n# text(x = rep(1.2, length(outlier_pdchange)),\n#      y = outlier_pdchange, labels = outlier_pdchange, col = 'red', cex = 0.8)\n```\n:::\n\n\n\n\nInterestingly, there are no outliers for pocket depth change, and 3 for attachment loss change.\n\nWe will also need to create dummy codes all categorical values.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Adding dummy codes for all categorical variables with >2 levels\ndata$placebo <- ifelse(data$trtgroup == 1, 1, 0)\ndata$control <- ifelse(data$trtgroup == 2, 1, 0)\ndata$low <- ifelse(data$trtgroup == 3, 1, 0)\ndata$medium <- ifelse(data$trtgroup == 4, 1, 0)\ndata$high <- ifelse(data$trtgroup == 5, 1, 0)\ndata$trt <- ifelse(data$trtgroup == 3 | data$trtgroup == 4 | data$trtgroup ==5, 1, 0) # Collapsing all treatment levels into one category of treatment\n\ndata$trt3groups <- ifelse(data$trtgroup ==1, 1, ifelse(data$trtgroup == 2, 2, 3))\n\nkable(head(data), format = \"html\") %>%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\"))\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table table-striped table-hover table-condensed\" style=\"margin-left: auto; margin-right: auto;\">\n <thead>\n  <tr>\n   <th style=\"text-align:right;\"> id </th>\n   <th style=\"text-align:right;\"> trtgroup </th>\n   <th style=\"text-align:right;\"> gender </th>\n   <th style=\"text-align:right;\"> race </th>\n   <th style=\"text-align:right;\"> age </th>\n   <th style=\"text-align:right;\"> smoker </th>\n   <th style=\"text-align:right;\"> sites </th>\n   <th style=\"text-align:right;\"> attachbase </th>\n   <th style=\"text-align:right;\"> attach1year </th>\n   <th style=\"text-align:right;\"> pdbase </th>\n   <th style=\"text-align:right;\"> pd1year </th>\n   <th style=\"text-align:right;\"> attachchange </th>\n   <th style=\"text-align:right;\"> pdchange </th>\n   <th style=\"text-align:right;\"> placebo </th>\n   <th style=\"text-align:right;\"> control </th>\n   <th style=\"text-align:right;\"> low </th>\n   <th style=\"text-align:right;\"> medium </th>\n   <th style=\"text-align:right;\"> high </th>\n   <th style=\"text-align:right;\"> trt </th>\n   <th style=\"text-align:right;\"> trt3groups </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:right;\"> 101 </td>\n   <td style=\"text-align:right;\"> 4 </td>\n   <td style=\"text-align:right;\"> 2 </td>\n   <td style=\"text-align:right;\"> 5 </td>\n   <td style=\"text-align:right;\"> 44.57221 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 162 </td>\n   <td style=\"text-align:right;\"> 2.432099 </td>\n   <td style=\"text-align:right;\"> 2.577640 </td>\n   <td style=\"text-align:right;\"> 3.246914 </td>\n   <td style=\"text-align:right;\"> 3.407407 </td>\n   <td style=\"text-align:right;\"> 0.1455410 </td>\n   <td style=\"text-align:right;\"> 0.1604938 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 3 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 102 </td>\n   <td style=\"text-align:right;\"> 5 </td>\n   <td style=\"text-align:right;\"> 2 </td>\n   <td style=\"text-align:right;\"> 5 </td>\n   <td style=\"text-align:right;\"> 35.57290 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 162 </td>\n   <td style=\"text-align:right;\"> 2.543210 </td>\n   <td style=\"text-align:right;\"> NA </td>\n   <td style=\"text-align:right;\"> 3.006173 </td>\n   <td style=\"text-align:right;\"> NA </td>\n   <td style=\"text-align:right;\"> NA </td>\n   <td style=\"text-align:right;\"> NA </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 3 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 103 </td>\n   <td style=\"text-align:right;\"> 2 </td>\n   <td style=\"text-align:right;\"> 2 </td>\n   <td style=\"text-align:right;\"> 5 </td>\n   <td style=\"text-align:right;\"> 47.94524 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 144 </td>\n   <td style=\"text-align:right;\"> 2.881944 </td>\n   <td style=\"text-align:right;\"> 3.076389 </td>\n   <td style=\"text-align:right;\"> 3.118056 </td>\n   <td style=\"text-align:right;\"> 3.125000 </td>\n   <td style=\"text-align:right;\"> 0.1944444 </td>\n   <td style=\"text-align:right;\"> 0.0069444 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 2 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 104 </td>\n   <td style=\"text-align:right;\"> 3 </td>\n   <td style=\"text-align:right;\"> 2 </td>\n   <td style=\"text-align:right;\"> 5 </td>\n   <td style=\"text-align:right;\"> 55.17864 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 138 </td>\n   <td style=\"text-align:right;\"> 4.956522 </td>\n   <td style=\"text-align:right;\"> 5.304348 </td>\n   <td style=\"text-align:right;\"> 5.217391 </td>\n   <td style=\"text-align:right;\"> 4.891304 </td>\n   <td style=\"text-align:right;\"> 0.3478261 </td>\n   <td style=\"text-align:right;\"> -0.3260870 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 3 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 105 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 2 </td>\n   <td style=\"text-align:right;\"> 2 </td>\n   <td style=\"text-align:right;\"> 43.79740 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 168 </td>\n   <td style=\"text-align:right;\"> 1.773810 </td>\n   <td style=\"text-align:right;\"> 1.452381 </td>\n   <td style=\"text-align:right;\"> 3.363095 </td>\n   <td style=\"text-align:right;\"> 2.898810 </td>\n   <td style=\"text-align:right;\"> -0.3214286 </td>\n   <td style=\"text-align:right;\"> -0.4642857 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 106 </td>\n   <td style=\"text-align:right;\"> 4 </td>\n   <td style=\"text-align:right;\"> 2 </td>\n   <td style=\"text-align:right;\"> 5 </td>\n   <td style=\"text-align:right;\"> 42.14921 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 168 </td>\n   <td style=\"text-align:right;\"> 2.369048 </td>\n   <td style=\"text-align:right;\"> 1.922619 </td>\n   <td style=\"text-align:right;\"> 3.910714 </td>\n   <td style=\"text-align:right;\"> 3.083333 </td>\n   <td style=\"text-align:right;\"> -0.4464286 </td>\n   <td style=\"text-align:right;\"> -0.8273810 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 3 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n\n\n<a href=\"#Data_Management\">Back to top of tabset</a>\n\n## Missingness\n\nHere we will investigate the missingness of the dataset\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Print number of missing data for each column \nkable(colSums(is.na(data)), format = \"html\", col.names = \"Missing\") %>%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\"))\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table table-striped table-hover table-condensed\" style=\"margin-left: auto; margin-right: auto;\">\n <thead>\n  <tr>\n   <th style=\"text-align:left;\">   </th>\n   <th style=\"text-align:right;\"> Missing </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> id </td>\n   <td style=\"text-align:right;\"> 0 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> trtgroup </td>\n   <td style=\"text-align:right;\"> 0 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> gender </td>\n   <td style=\"text-align:right;\"> 0 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> race </td>\n   <td style=\"text-align:right;\"> 0 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> age </td>\n   <td style=\"text-align:right;\"> 1 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> smoker </td>\n   <td style=\"text-align:right;\"> 1 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> sites </td>\n   <td style=\"text-align:right;\"> 0 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> attachbase </td>\n   <td style=\"text-align:right;\"> 0 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> attach1year </td>\n   <td style=\"text-align:right;\"> 27 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> pdbase </td>\n   <td style=\"text-align:right;\"> 0 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> pd1year </td>\n   <td style=\"text-align:right;\"> 27 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> attachchange </td>\n   <td style=\"text-align:right;\"> 27 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> pdchange </td>\n   <td style=\"text-align:right;\"> 27 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> placebo </td>\n   <td style=\"text-align:right;\"> 0 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> control </td>\n   <td style=\"text-align:right;\"> 0 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> low </td>\n   <td style=\"text-align:right;\"> 0 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> medium </td>\n   <td style=\"text-align:right;\"> 0 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> high </td>\n   <td style=\"text-align:right;\"> 0 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> trt </td>\n   <td style=\"text-align:right;\"> 0 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> trt3groups </td>\n   <td style=\"text-align:right;\"> 0 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n\n```{.r .cell-code}\n# Here is a handy package that lets us visualize missing data\nvis_miss(data)\n```\n\n::: {.cell-output-display}\n![](Project_1_Regression_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\n\n\nThere is 1 missing value for age and smoking status, and 27 missing values for the one year measurements of attachment loss and pocket depth.\n\nThis is 27/130 = 20.77% of our sample size, quite large! We will still have 103 participants, which should be sufficient to run our analysis if there are 20-30 participants in each condition. Let's check\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create a new dataframe that removes participants with NA outcome variables\ndata_missing <- data %>%\n  filter(complete.cases(pdchange, attachchange)) \ndim(data_missing)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 103  20\n```\n\n\n:::\n\n```{.r .cell-code}\n# Count the number of participants in each group\ngroup_counts <- data_missing %>%\n  count(trtgroup)\nprint(group_counts)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  trtgroup  n\n1        1 23\n2        2 23\n3        3 21\n4        4 20\n5        5 16\n```\n\n\n:::\n:::\n\n\n\n\nThere are only 16 participants in the high concentrations (5) group.\n\nThis could cause problems! We will either have to combine all the treatment conditions into one group, or perform some multiple imputations to handle the missing data. This will have to be noted in the discussion.\n\nSee <a href=\"#Multiple_imputation\">Multiple Imputation section</a> for analysis with multiple imputation.\n\n<a href=\"#Data_Management\">Back to top of tabset</a>\n::::\n\n## Descriptive Statistics {#Descriptives}\n\nHere we will acquire the descriptive statistics of our data set and create Table 1. Code modified from [cran.r-project.org](https://cran.r-project.org/web/packages/table1/vignettes/table1-examples.html)\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Duplicate the dataset so we are not modifying the original\ndata2 <- data\n\n# Factor the basic variables that we're interested in\ndata2$trtgroup <- factor(data2$trtgroup,\n                                levels = c(1,2,3,4,5),\n                                labels = c(\"Placebo\", \"Control\", \"Low\", \"Medium\", \"High\"))\n\ndata2$gender <- factor(data2$gender,\n                              levels = c(1,2),\n                              labels = c(\"Male\", \"Female\"))\n\ndata2$race <- factor(data2$race,\n                             levels = c(1,2,4,5),\n                             labels = c(\"Native American\", \"African American\", \"White\", \"Asian\"))\n                             \ndata2$smoker <- factor(data2$smoker,\n                               levels = c(0,1),\n                               labels = c(\"Non-Smoker\", \"Smoker\"))\n\n# Create labels to make the names of each variable more professional\nlabel(data2$gender) <- \"Gender\"\nlabel(data2$race) <- \"Race\"\nlabel(data2$age) <- \"Age (Years)\"\nlabel(data2$smoker) <- \"Smoking Status\"\nlabel(data2$sites) <- \"Sites\"\nlabel(data2$attachbase) <- \"Attachment Loss at Baseline\"\nlabel(data2$attach1year) <- \"Attachment Loss at 1 Year\"\nlabel(data2$pdbase) <- \"Pocket Depth at Baseline\"\nlabel(data2$pd1year) <- \"Pocket Depth at 1 Year\"\nlabel(data2$attachchange) <- \"Attachment Loss Change\"\nlabel(data2$pdchange) <- \"Pocket Depth Change\"\n\n\n# Create table 1\ntable1 <- table1(~ gender + race + age + smoker + sites + attachbase + attach1year + pdbase + pd1year + attachchange + pdchange| trtgroup,  data = data2, caption = \"Descriptive Statistics\", overall = c(left=\"Total\"))\ntable1\n```\n\n::: {.cell-output-display}\n\n```{=html}\n<div class=\"Rtable1\"><table class=\"Rtable1\"><caption>Descriptive Statistics</caption>\n\n<thead>\n<tr>\n<th class='rowlabel firstrow lastrow'></th>\n<th class='firstrow lastrow'><span class='stratlabel'>Total<br><span class='stratn'>(N=130)</span></span></th>\n<th class='firstrow lastrow'><span class='stratlabel'>Placebo<br><span class='stratn'>(N=26)</span></span></th>\n<th class='firstrow lastrow'><span class='stratlabel'>Control<br><span class='stratn'>(N=26)</span></span></th>\n<th class='firstrow lastrow'><span class='stratlabel'>Low<br><span class='stratn'>(N=26)</span></span></th>\n<th class='firstrow lastrow'><span class='stratlabel'>Medium<br><span class='stratn'>(N=26)</span></span></th>\n<th class='firstrow lastrow'><span class='stratlabel'>High<br><span class='stratn'>(N=26)</span></span></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td class='rowlabel firstrow'>Gender</td>\n<td class='firstrow'></td>\n<td class='firstrow'></td>\n<td class='firstrow'></td>\n<td class='firstrow'></td>\n<td class='firstrow'></td>\n<td class='firstrow'></td>\n</tr>\n<tr>\n<td class='rowlabel'>Male</td>\n<td>54 (41.5%)</td>\n<td>11 (42.3%)</td>\n<td>10 (38.5%)</td>\n<td>11 (42.3%)</td>\n<td>11 (42.3%)</td>\n<td>11 (42.3%)</td>\n</tr>\n<tr>\n<td class='rowlabel lastrow'>Female</td>\n<td class='lastrow'>76 (58.5%)</td>\n<td class='lastrow'>15 (57.7%)</td>\n<td class='lastrow'>16 (61.5%)</td>\n<td class='lastrow'>15 (57.7%)</td>\n<td class='lastrow'>15 (57.7%)</td>\n<td class='lastrow'>15 (57.7%)</td>\n</tr>\n<tr>\n<td class='rowlabel firstrow'>Race</td>\n<td class='firstrow'></td>\n<td class='firstrow'></td>\n<td class='firstrow'></td>\n<td class='firstrow'></td>\n<td class='firstrow'></td>\n<td class='firstrow'></td>\n</tr>\n<tr>\n<td class='rowlabel'>Native American</td>\n<td>4 (3.1%)</td>\n<td>0 (0%)</td>\n<td>1 (3.8%)</td>\n<td>1 (3.8%)</td>\n<td>0 (0%)</td>\n<td>2 (7.7%)</td>\n</tr>\n<tr>\n<td class='rowlabel'>African American</td>\n<td>9 (6.9%)</td>\n<td>2 (7.7%)</td>\n<td>1 (3.8%)</td>\n<td>5 (19.2%)</td>\n<td>0 (0%)</td>\n<td>1 (3.8%)</td>\n</tr>\n<tr>\n<td class='rowlabel'>White</td>\n<td>3 (2.3%)</td>\n<td>1 (3.8%)</td>\n<td>1 (3.8%)</td>\n<td>0 (0%)</td>\n<td>1 (3.8%)</td>\n<td>0 (0%)</td>\n</tr>\n<tr>\n<td class='rowlabel lastrow'>Asian</td>\n<td class='lastrow'>114 (87.7%)</td>\n<td class='lastrow'>23 (88.5%)</td>\n<td class='lastrow'>23 (88.5%)</td>\n<td class='lastrow'>20 (76.9%)</td>\n<td class='lastrow'>25 (96.2%)</td>\n<td class='lastrow'>23 (88.5%)</td>\n</tr>\n<tr>\n<td class='rowlabel firstrow'>Age (Years)</td>\n<td class='firstrow'></td>\n<td class='firstrow'></td>\n<td class='firstrow'></td>\n<td class='firstrow'></td>\n<td class='firstrow'></td>\n<td class='firstrow'></td>\n</tr>\n<tr>\n<td class='rowlabel'>Mean (SD)</td>\n<td>49.9 (10.0)</td>\n<td>47.1 (8.61)</td>\n<td>50.7 (9.90)</td>\n<td>51.9 (10.8)</td>\n<td>49.0 (9.49)</td>\n<td>50.8 (11.2)</td>\n</tr>\n<tr>\n<td class='rowlabel'>Median [Min, Max]</td>\n<td>48.6 [28.6, 74.5]</td>\n<td>44.7 [30.4, 67.1]</td>\n<td>49.2 [36.1, 73.3]</td>\n<td>51.5 [36.9, 71.9]</td>\n<td>48.1 [28.6, 70.9]</td>\n<td>49.9 [34.1, 74.5]</td>\n</tr>\n<tr>\n<td class='rowlabel lastrow'>Missing</td>\n<td class='lastrow'>1 (0.8%)</td>\n<td class='lastrow'>1 (3.8%)</td>\n<td class='lastrow'>0 (0%)</td>\n<td class='lastrow'>0 (0%)</td>\n<td class='lastrow'>0 (0%)</td>\n<td class='lastrow'>0 (0%)</td>\n</tr>\n<tr>\n<td class='rowlabel firstrow'>Smoking Status</td>\n<td class='firstrow'></td>\n<td class='firstrow'></td>\n<td class='firstrow'></td>\n<td class='firstrow'></td>\n<td class='firstrow'></td>\n<td class='firstrow'></td>\n</tr>\n<tr>\n<td class='rowlabel'>Non-Smoker</td>\n<td>81 (62.3%)</td>\n<td>15 (57.7%)</td>\n<td>17 (65.4%)</td>\n<td>18 (69.2%)</td>\n<td>14 (53.8%)</td>\n<td>17 (65.4%)</td>\n</tr>\n<tr>\n<td class='rowlabel'>Smoker</td>\n<td>48 (36.9%)</td>\n<td>11 (42.3%)</td>\n<td>9 (34.6%)</td>\n<td>8 (30.8%)</td>\n<td>11 (42.3%)</td>\n<td>9 (34.6%)</td>\n</tr>\n<tr>\n<td class='rowlabel lastrow'>Missing</td>\n<td class='lastrow'>1 (0.8%)</td>\n<td class='lastrow'>0 (0%)</td>\n<td class='lastrow'>0 (0%)</td>\n<td class='lastrow'>0 (0%)</td>\n<td class='lastrow'>1 (3.8%)</td>\n<td class='lastrow'>0 (0%)</td>\n</tr>\n<tr>\n<td class='rowlabel firstrow'>Sites</td>\n<td class='firstrow'></td>\n<td class='firstrow'></td>\n<td class='firstrow'></td>\n<td class='firstrow'></td>\n<td class='firstrow'></td>\n<td class='firstrow'></td>\n</tr>\n<tr>\n<td class='rowlabel'>Mean (SD)</td>\n<td>158 (11.3)</td>\n<td>160 (10.1)</td>\n<td>154 (10.9)</td>\n<td>161 (8.54)</td>\n<td>155 (15.7)</td>\n<td>157 (9.65)</td>\n</tr>\n<tr>\n<td class='rowlabel lastrow'>Median [Min, Max]</td>\n<td class='lastrow'>162 [114, 168]</td>\n<td class='lastrow'>162 [138, 168]</td>\n<td class='lastrow'>159 [126, 168]</td>\n<td class='lastrow'>162 [138, 168]</td>\n<td class='lastrow'>162 [114, 168]</td>\n<td class='lastrow'>159 [138, 168]</td>\n</tr>\n<tr>\n<td class='rowlabel firstrow'>Attachment Loss at Baseline</td>\n<td class='firstrow'></td>\n<td class='firstrow'></td>\n<td class='firstrow'></td>\n<td class='firstrow'></td>\n<td class='firstrow'></td>\n<td class='firstrow'></td>\n</tr>\n<tr>\n<td class='rowlabel'>Mean (SD)</td>\n<td>2.15 (0.797)</td>\n<td>1.79 (0.646)</td>\n<td>2.46 (0.687)</td>\n<td>2.07 (0.987)</td>\n<td>2.17 (0.656)</td>\n<td>2.24 (0.858)</td>\n</tr>\n<tr>\n<td class='rowlabel lastrow'>Median [Min, Max]</td>\n<td class='lastrow'>2.03 [0.895, 5.09]</td>\n<td class='lastrow'>1.71 [0.899, 3.64]</td>\n<td class='lastrow'>2.48 [1.22, 4.39]</td>\n<td class='lastrow'>1.77 [0.895, 4.96]</td>\n<td class='lastrow'>2.12 [1.02, 4.01]</td>\n<td class='lastrow'>1.97 [1.26, 5.09]</td>\n</tr>\n<tr>\n<td class='rowlabel firstrow'>Attachment Loss at 1 Year</td>\n<td class='firstrow'></td>\n<td class='firstrow'></td>\n<td class='firstrow'></td>\n<td class='firstrow'></td>\n<td class='firstrow'></td>\n<td class='firstrow'></td>\n</tr>\n<tr>\n<td class='rowlabel'>Mean (SD)</td>\n<td>2.10 (0.772)</td>\n<td>1.74 (0.542)</td>\n<td>2.33 (0.551)</td>\n<td>2.08 (1.06)</td>\n<td>2.24 (0.652)</td>\n<td>2.15 (0.915)</td>\n</tr>\n<tr>\n<td class='rowlabel'>Median [Min, Max]</td>\n<td>1.98 [0.865, 5.30]</td>\n<td>1.64 [0.964, 3.10]</td>\n<td>2.23 [1.46, 3.49]</td>\n<td>1.74 [0.865, 5.30]</td>\n<td>2.25 [1.35, 3.83]</td>\n<td>1.71 [1.22, 4.04]</td>\n</tr>\n<tr>\n<td class='rowlabel lastrow'>Missing</td>\n<td class='lastrow'>27 (20.8%)</td>\n<td class='lastrow'>3 (11.5%)</td>\n<td class='lastrow'>3 (11.5%)</td>\n<td class='lastrow'>5 (19.2%)</td>\n<td class='lastrow'>6 (23.1%)</td>\n<td class='lastrow'>10 (38.5%)</td>\n</tr>\n<tr>\n<td class='rowlabel firstrow'>Pocket Depth at Baseline</td>\n<td class='firstrow'></td>\n<td class='firstrow'></td>\n<td class='firstrow'></td>\n<td class='firstrow'></td>\n<td class='firstrow'></td>\n<td class='firstrow'></td>\n</tr>\n<tr>\n<td class='rowlabel'>Mean (SD)</td>\n<td>3.14 (0.437)</td>\n<td>3.09 (0.372)</td>\n<td>3.28 (0.473)</td>\n<td>3.17 (0.593)</td>\n<td>3.05 (0.402)</td>\n<td>3.11 (0.273)</td>\n</tr>\n<tr>\n<td class='rowlabel lastrow'>Median [Min, Max]</td>\n<td class='lastrow'>3.10 [2.26, 5.22]</td>\n<td class='lastrow'>3.11 [2.47, 4.08]</td>\n<td class='lastrow'>3.11 [2.65, 4.77]</td>\n<td class='lastrow'>3.07 [2.26, 5.22]</td>\n<td class='lastrow'>3.09 [2.42, 3.91]</td>\n<td class='lastrow'>3.14 [2.62, 3.60]</td>\n</tr>\n<tr>\n<td class='rowlabel firstrow'>Pocket Depth at 1 Year</td>\n<td class='firstrow'></td>\n<td class='firstrow'></td>\n<td class='firstrow'></td>\n<td class='firstrow'></td>\n<td class='firstrow'></td>\n<td class='firstrow'></td>\n</tr>\n<tr>\n<td class='rowlabel'>Mean (SD)</td>\n<td>2.88 (0.488)</td>\n<td>2.75 (0.482)</td>\n<td>2.95 (0.455)</td>\n<td>3.02 (0.578)</td>\n<td>2.84 (0.469)</td>\n<td>2.80 (0.423)</td>\n</tr>\n<tr>\n<td class='rowlabel'>Median [Min, Max]</td>\n<td>2.90 [1.96, 4.89]</td>\n<td>2.70 [1.96, 3.75]</td>\n<td>2.90 [2.24, 4.07]</td>\n<td>2.97 [2.16, 4.89]</td>\n<td>2.90 [2.05, 3.78]</td>\n<td>2.87 [2.04, 3.40]</td>\n</tr>\n<tr>\n<td class='rowlabel lastrow'>Missing</td>\n<td class='lastrow'>27 (20.8%)</td>\n<td class='lastrow'>3 (11.5%)</td>\n<td class='lastrow'>3 (11.5%)</td>\n<td class='lastrow'>5 (19.2%)</td>\n<td class='lastrow'>6 (23.1%)</td>\n<td class='lastrow'>10 (38.5%)</td>\n</tr>\n<tr>\n<td class='rowlabel firstrow'>Attachment Loss Change</td>\n<td class='firstrow'></td>\n<td class='firstrow'></td>\n<td class='firstrow'></td>\n<td class='firstrow'></td>\n<td class='firstrow'></td>\n<td class='firstrow'></td>\n</tr>\n<tr>\n<td class='rowlabel'>Mean (SD)</td>\n<td>-0.0995 (0.276)</td>\n<td>-0.0871 (0.242)</td>\n<td>-0.222 (0.280)</td>\n<td>-0.0178 (0.266)</td>\n<td>-0.00656 (0.231)</td>\n<td>-0.165 (0.326)</td>\n</tr>\n<tr>\n<td class='rowlabel'>Median [Min, Max]</td>\n<td>-0.0679 [-1.05, 0.452]</td>\n<td>-0.0247 [-0.599, 0.452]</td>\n<td>-0.123 [-0.901, 0.194]</td>\n<td>0.0298 [-0.705, 0.348]</td>\n<td>-0.0160 [-0.446, 0.339]</td>\n<td>-0.0579 [-1.05, 0.199]</td>\n</tr>\n<tr>\n<td class='rowlabel lastrow'>Missing</td>\n<td class='lastrow'>27 (20.8%)</td>\n<td class='lastrow'>3 (11.5%)</td>\n<td class='lastrow'>3 (11.5%)</td>\n<td class='lastrow'>5 (19.2%)</td>\n<td class='lastrow'>6 (23.1%)</td>\n<td class='lastrow'>10 (38.5%)</td>\n</tr>\n<tr>\n<td class='rowlabel firstrow'>Pocket Depth Change</td>\n<td class='firstrow'></td>\n<td class='firstrow'></td>\n<td class='firstrow'></td>\n<td class='firstrow'></td>\n<td class='firstrow'></td>\n<td class='firstrow'></td>\n</tr>\n<tr>\n<td class='rowlabel'>Mean (SD)</td>\n<td>-0.294 (0.268)</td>\n<td>-0.350 (0.277)</td>\n<td>-0.338 (0.232)</td>\n<td>-0.206 (0.279)</td>\n<td>-0.203 (0.272)</td>\n<td>-0.382 (0.245)</td>\n</tr>\n<tr>\n<td class='rowlabel'>Median [Min, Max]</td>\n<td>-0.284 [-0.858, 0.455]</td>\n<td>-0.383 [-0.858, 0.161]</td>\n<td>-0.367 [-0.759, 0.0145]</td>\n<td>-0.244 [-0.661, 0.455]</td>\n<td>-0.200 [-0.827, 0.175]</td>\n<td>-0.347 [-0.845, 0.0536]</td>\n</tr>\n<tr>\n<td class='rowlabel lastrow'>Missing</td>\n<td class='lastrow'>27 (20.8%)</td>\n<td class='lastrow'>3 (11.5%)</td>\n<td class='lastrow'>3 (11.5%)</td>\n<td class='lastrow'>5 (19.2%)</td>\n<td class='lastrow'>6 (23.1%)</td>\n<td class='lastrow'>10 (38.5%)</td>\n</tr>\n</tbody>\n</table>\n</div>\n```\n\n:::\n:::\n\n\n\n\n## Preliminary Evaluation of Assumptions {#Preliminary}\n\nBefore we begin digging into the data, let's take a closer look at the relationships between our variables.\n\nWe will first run a correlation matrix to assess the correlations between our IVs. Then we will explore any high correlations that which could affect our analysis.\n\nFinally we will make histograms of attachment loss and pocket depth change to gauge whether they will be normally distributed or if we need to run some transformations.\n\n::: panel-tabset\n## Correlation Matrix\n\nFirst we will begin by making a correlation matrix to assess whether any of our IVs are related to each other (multicollinearity). This will inform which variables to incorporate into the final model.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Since we made dummy codes, we will get spurious correlations that will obfuscate the main relationships we are interested in (e.g. between 'medium' and 'trtgroup'. So we will first make a separate dataset excluding the dummy coded variables\ndata_for_matrix <- select(data_missing, -placebo, -control, -low, -medium,  -high, -trt, -trt3groups)\n\n# Make a correlation matrix with all variables of the trimmed data set\ncorrelation_matrix <- cor(data_for_matrix, use = \"complete.obs\")\n\n# Plot the matrix\ncorrplot(correlation_matrix, method = \"circle\")\n```\n\n::: {.cell-output-display}\n![](Project_1_Regression_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# Trim the matrix\ncorrelation_matrix[upper.tri(correlation_matrix)] <- NA\n\n# Save the matrix as a LaTex file for paper\ncor_table <- xtable(correlation_matrix, caption = \"Correlation Matrix\", label = \"tab:correlation\")\nprint(cor_table, type = \"latex\", file = \"correlation_matrix999.tex\")\n\nkable(correlation_matrix, format = \"html\", table.attr = \"class='table table-bordered'\")\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class='table table-bordered'>\n <thead>\n  <tr>\n   <th style=\"text-align:left;\">   </th>\n   <th style=\"text-align:right;\"> id </th>\n   <th style=\"text-align:right;\"> trtgroup </th>\n   <th style=\"text-align:right;\"> gender </th>\n   <th style=\"text-align:right;\"> race </th>\n   <th style=\"text-align:right;\"> age </th>\n   <th style=\"text-align:right;\"> smoker </th>\n   <th style=\"text-align:right;\"> sites </th>\n   <th style=\"text-align:right;\"> attachbase </th>\n   <th style=\"text-align:right;\"> attach1year </th>\n   <th style=\"text-align:right;\"> pdbase </th>\n   <th style=\"text-align:right;\"> pd1year </th>\n   <th style=\"text-align:right;\"> attachchange </th>\n   <th style=\"text-align:right;\"> pdchange </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> id </td>\n   <td style=\"text-align:right;\"> 1.0000000 </td>\n   <td style=\"text-align:right;\"> NA </td>\n   <td style=\"text-align:right;\"> NA </td>\n   <td style=\"text-align:right;\"> NA </td>\n   <td style=\"text-align:right;\"> NA </td>\n   <td style=\"text-align:right;\"> NA </td>\n   <td style=\"text-align:right;\"> NA </td>\n   <td style=\"text-align:right;\"> NA </td>\n   <td style=\"text-align:right;\"> NA </td>\n   <td style=\"text-align:right;\"> NA </td>\n   <td style=\"text-align:right;\"> NA </td>\n   <td style=\"text-align:right;\"> NA </td>\n   <td style=\"text-align:right;\"> NA </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> trtgroup </td>\n   <td style=\"text-align:right;\"> -0.1452850 </td>\n   <td style=\"text-align:right;\"> 1.0000000 </td>\n   <td style=\"text-align:right;\"> NA </td>\n   <td style=\"text-align:right;\"> NA </td>\n   <td style=\"text-align:right;\"> NA </td>\n   <td style=\"text-align:right;\"> NA </td>\n   <td style=\"text-align:right;\"> NA </td>\n   <td style=\"text-align:right;\"> NA </td>\n   <td style=\"text-align:right;\"> NA </td>\n   <td style=\"text-align:right;\"> NA </td>\n   <td style=\"text-align:right;\"> NA </td>\n   <td style=\"text-align:right;\"> NA </td>\n   <td style=\"text-align:right;\"> NA </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> gender </td>\n   <td style=\"text-align:right;\"> -0.8306185 </td>\n   <td style=\"text-align:right;\"> 0.1428866 </td>\n   <td style=\"text-align:right;\"> 1.0000000 </td>\n   <td style=\"text-align:right;\"> NA </td>\n   <td style=\"text-align:right;\"> NA </td>\n   <td style=\"text-align:right;\"> NA </td>\n   <td style=\"text-align:right;\"> NA </td>\n   <td style=\"text-align:right;\"> NA </td>\n   <td style=\"text-align:right;\"> NA </td>\n   <td style=\"text-align:right;\"> NA </td>\n   <td style=\"text-align:right;\"> NA </td>\n   <td style=\"text-align:right;\"> NA </td>\n   <td style=\"text-align:right;\"> NA </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> race </td>\n   <td style=\"text-align:right;\"> 0.1319767 </td>\n   <td style=\"text-align:right;\"> -0.0314771 </td>\n   <td style=\"text-align:right;\"> -0.0955531 </td>\n   <td style=\"text-align:right;\"> 1.0000000 </td>\n   <td style=\"text-align:right;\"> NA </td>\n   <td style=\"text-align:right;\"> NA </td>\n   <td style=\"text-align:right;\"> NA </td>\n   <td style=\"text-align:right;\"> NA </td>\n   <td style=\"text-align:right;\"> NA </td>\n   <td style=\"text-align:right;\"> NA </td>\n   <td style=\"text-align:right;\"> NA </td>\n   <td style=\"text-align:right;\"> NA </td>\n   <td style=\"text-align:right;\"> NA </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> age </td>\n   <td style=\"text-align:right;\"> 0.0593032 </td>\n   <td style=\"text-align:right;\"> 0.1100051 </td>\n   <td style=\"text-align:right;\"> -0.0453862 </td>\n   <td style=\"text-align:right;\"> 0.1787040 </td>\n   <td style=\"text-align:right;\"> 1.0000000 </td>\n   <td style=\"text-align:right;\"> NA </td>\n   <td style=\"text-align:right;\"> NA </td>\n   <td style=\"text-align:right;\"> NA </td>\n   <td style=\"text-align:right;\"> NA </td>\n   <td style=\"text-align:right;\"> NA </td>\n   <td style=\"text-align:right;\"> NA </td>\n   <td style=\"text-align:right;\"> NA </td>\n   <td style=\"text-align:right;\"> NA </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> smoker </td>\n   <td style=\"text-align:right;\"> -0.2742888 </td>\n   <td style=\"text-align:right;\"> -0.0495029 </td>\n   <td style=\"text-align:right;\"> 0.0206456 </td>\n   <td style=\"text-align:right;\"> 0.0377211 </td>\n   <td style=\"text-align:right;\"> -0.2229620 </td>\n   <td style=\"text-align:right;\"> 1.0000000 </td>\n   <td style=\"text-align:right;\"> NA </td>\n   <td style=\"text-align:right;\"> NA </td>\n   <td style=\"text-align:right;\"> NA </td>\n   <td style=\"text-align:right;\"> NA </td>\n   <td style=\"text-align:right;\"> NA </td>\n   <td style=\"text-align:right;\"> NA </td>\n   <td style=\"text-align:right;\"> NA </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> sites </td>\n   <td style=\"text-align:right;\"> 0.0520821 </td>\n   <td style=\"text-align:right;\"> -0.0393742 </td>\n   <td style=\"text-align:right;\"> -0.1291127 </td>\n   <td style=\"text-align:right;\"> 0.0505164 </td>\n   <td style=\"text-align:right;\"> -0.0274695 </td>\n   <td style=\"text-align:right;\"> -0.0226381 </td>\n   <td style=\"text-align:right;\"> 1.0000000 </td>\n   <td style=\"text-align:right;\"> NA </td>\n   <td style=\"text-align:right;\"> NA </td>\n   <td style=\"text-align:right;\"> NA </td>\n   <td style=\"text-align:right;\"> NA </td>\n   <td style=\"text-align:right;\"> NA </td>\n   <td style=\"text-align:right;\"> NA </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> attachbase </td>\n   <td style=\"text-align:right;\"> -0.1641513 </td>\n   <td style=\"text-align:right;\"> 0.1440999 </td>\n   <td style=\"text-align:right;\"> 0.2416242 </td>\n   <td style=\"text-align:right;\"> 0.0313438 </td>\n   <td style=\"text-align:right;\"> 0.1354434 </td>\n   <td style=\"text-align:right;\"> 0.1456234 </td>\n   <td style=\"text-align:right;\"> -0.4237813 </td>\n   <td style=\"text-align:right;\"> 1.0000000 </td>\n   <td style=\"text-align:right;\"> NA </td>\n   <td style=\"text-align:right;\"> NA </td>\n   <td style=\"text-align:right;\"> NA </td>\n   <td style=\"text-align:right;\"> NA </td>\n   <td style=\"text-align:right;\"> NA </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> attach1year </td>\n   <td style=\"text-align:right;\"> -0.1440182 </td>\n   <td style=\"text-align:right;\"> 0.1671499 </td>\n   <td style=\"text-align:right;\"> 0.2021284 </td>\n   <td style=\"text-align:right;\"> 0.0671713 </td>\n   <td style=\"text-align:right;\"> 0.0850851 </td>\n   <td style=\"text-align:right;\"> 0.1987327 </td>\n   <td style=\"text-align:right;\"> -0.4042743 </td>\n   <td style=\"text-align:right;\"> 0.9450189 </td>\n   <td style=\"text-align:right;\"> 1.0000000 </td>\n   <td style=\"text-align:right;\"> NA </td>\n   <td style=\"text-align:right;\"> NA </td>\n   <td style=\"text-align:right;\"> NA </td>\n   <td style=\"text-align:right;\"> NA </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> pdbase </td>\n   <td style=\"text-align:right;\"> -0.2010672 </td>\n   <td style=\"text-align:right;\"> -0.0224838 </td>\n   <td style=\"text-align:right;\"> 0.2645908 </td>\n   <td style=\"text-align:right;\"> 0.0182289 </td>\n   <td style=\"text-align:right;\"> -0.0884687 </td>\n   <td style=\"text-align:right;\"> 0.2614919 </td>\n   <td style=\"text-align:right;\"> -0.1805543 </td>\n   <td style=\"text-align:right;\"> 0.6047132 </td>\n   <td style=\"text-align:right;\"> 0.6093346 </td>\n   <td style=\"text-align:right;\"> 1.0000000 </td>\n   <td style=\"text-align:right;\"> NA </td>\n   <td style=\"text-align:right;\"> NA </td>\n   <td style=\"text-align:right;\"> NA </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> pd1year </td>\n   <td style=\"text-align:right;\"> -0.0685783 </td>\n   <td style=\"text-align:right;\"> 0.0126731 </td>\n   <td style=\"text-align:right;\"> 0.1362804 </td>\n   <td style=\"text-align:right;\"> 0.0606735 </td>\n   <td style=\"text-align:right;\"> -0.1246106 </td>\n   <td style=\"text-align:right;\"> 0.2447030 </td>\n   <td style=\"text-align:right;\"> -0.1901940 </td>\n   <td style=\"text-align:right;\"> 0.5601052 </td>\n   <td style=\"text-align:right;\"> 0.6685457 </td>\n   <td style=\"text-align:right;\"> 0.8436653 </td>\n   <td style=\"text-align:right;\"> 1.0000000 </td>\n   <td style=\"text-align:right;\"> NA </td>\n   <td style=\"text-align:right;\"> NA </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> attachchange </td>\n   <td style=\"text-align:right;\"> 0.0964391 </td>\n   <td style=\"text-align:right;\"> 0.0296043 </td>\n   <td style=\"text-align:right;\"> -0.1696216 </td>\n   <td style=\"text-align:right;\"> 0.0928969 </td>\n   <td style=\"text-align:right;\"> -0.1742586 </td>\n   <td style=\"text-align:right;\"> 0.1135740 </td>\n   <td style=\"text-align:right;\"> 0.1578683 </td>\n   <td style=\"text-align:right;\"> -0.3976360 </td>\n   <td style=\"text-align:right;\"> -0.0757224 </td>\n   <td style=\"text-align:right;\"> -0.1342018 </td>\n   <td style=\"text-align:right;\"> 0.1679506 </td>\n   <td style=\"text-align:right;\"> 1.0000000 </td>\n   <td style=\"text-align:right;\"> NA </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> pdchange </td>\n   <td style=\"text-align:right;\"> 0.2251487 </td>\n   <td style=\"text-align:right;\"> 0.0622762 </td>\n   <td style=\"text-align:right;\"> -0.2123192 </td>\n   <td style=\"text-align:right;\"> 0.0789059 </td>\n   <td style=\"text-align:right;\"> -0.0731704 </td>\n   <td style=\"text-align:right;\"> -0.0091784 </td>\n   <td style=\"text-align:right;\"> -0.0323859 </td>\n   <td style=\"text-align:right;\"> -0.0317726 </td>\n   <td style=\"text-align:right;\"> 0.1579534 </td>\n   <td style=\"text-align:right;\"> -0.2031295 </td>\n   <td style=\"text-align:right;\"> 0.3543035 </td>\n   <td style=\"text-align:right;\"> 0.5400668 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n\n\n\n\nVisually, we can see a cluster of high positive correlations between all of attachment loss and pocket depth at baseline and 1 year. This makes sense, as all measurements were taken from the same sites in the gums. This will be explored later <a href=\"#Exploratory\">(See Exploratory Data Analysis - Dependent Variables)</a>\n\nJust for sanity (and practice), let's make a table of our correlation coefficients.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Convert the matrix to a dataframe for better formatting\ncorrelation_df <- as.data.frame(correlation_matrix)\n\n# Use Kable to pretty print the table\nkable(correlation_df, caption = \"Correlation Matrix\", format = \"html\") %>%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\"))\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table table-striped table-hover table-condensed\" style=\"margin-left: auto; margin-right: auto;\">\n<caption>Correlation Matrix</caption>\n <thead>\n  <tr>\n   <th style=\"text-align:left;\">   </th>\n   <th style=\"text-align:right;\"> id </th>\n   <th style=\"text-align:right;\"> trtgroup </th>\n   <th style=\"text-align:right;\"> gender </th>\n   <th style=\"text-align:right;\"> race </th>\n   <th style=\"text-align:right;\"> age </th>\n   <th style=\"text-align:right;\"> smoker </th>\n   <th style=\"text-align:right;\"> sites </th>\n   <th style=\"text-align:right;\"> attachbase </th>\n   <th style=\"text-align:right;\"> attach1year </th>\n   <th style=\"text-align:right;\"> pdbase </th>\n   <th style=\"text-align:right;\"> pd1year </th>\n   <th style=\"text-align:right;\"> attachchange </th>\n   <th style=\"text-align:right;\"> pdchange </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> id </td>\n   <td style=\"text-align:right;\"> 1.0000000 </td>\n   <td style=\"text-align:right;\"> NA </td>\n   <td style=\"text-align:right;\"> NA </td>\n   <td style=\"text-align:right;\"> NA </td>\n   <td style=\"text-align:right;\"> NA </td>\n   <td style=\"text-align:right;\"> NA </td>\n   <td style=\"text-align:right;\"> NA </td>\n   <td style=\"text-align:right;\"> NA </td>\n   <td style=\"text-align:right;\"> NA </td>\n   <td style=\"text-align:right;\"> NA </td>\n   <td style=\"text-align:right;\"> NA </td>\n   <td style=\"text-align:right;\"> NA </td>\n   <td style=\"text-align:right;\"> NA </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> trtgroup </td>\n   <td style=\"text-align:right;\"> -0.1452850 </td>\n   <td style=\"text-align:right;\"> 1.0000000 </td>\n   <td style=\"text-align:right;\"> NA </td>\n   <td style=\"text-align:right;\"> NA </td>\n   <td style=\"text-align:right;\"> NA </td>\n   <td style=\"text-align:right;\"> NA </td>\n   <td style=\"text-align:right;\"> NA </td>\n   <td style=\"text-align:right;\"> NA </td>\n   <td style=\"text-align:right;\"> NA </td>\n   <td style=\"text-align:right;\"> NA </td>\n   <td style=\"text-align:right;\"> NA </td>\n   <td style=\"text-align:right;\"> NA </td>\n   <td style=\"text-align:right;\"> NA </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> gender </td>\n   <td style=\"text-align:right;\"> -0.8306185 </td>\n   <td style=\"text-align:right;\"> 0.1428866 </td>\n   <td style=\"text-align:right;\"> 1.0000000 </td>\n   <td style=\"text-align:right;\"> NA </td>\n   <td style=\"text-align:right;\"> NA </td>\n   <td style=\"text-align:right;\"> NA </td>\n   <td style=\"text-align:right;\"> NA </td>\n   <td style=\"text-align:right;\"> NA </td>\n   <td style=\"text-align:right;\"> NA </td>\n   <td style=\"text-align:right;\"> NA </td>\n   <td style=\"text-align:right;\"> NA </td>\n   <td style=\"text-align:right;\"> NA </td>\n   <td style=\"text-align:right;\"> NA </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> race </td>\n   <td style=\"text-align:right;\"> 0.1319767 </td>\n   <td style=\"text-align:right;\"> -0.0314771 </td>\n   <td style=\"text-align:right;\"> -0.0955531 </td>\n   <td style=\"text-align:right;\"> 1.0000000 </td>\n   <td style=\"text-align:right;\"> NA </td>\n   <td style=\"text-align:right;\"> NA </td>\n   <td style=\"text-align:right;\"> NA </td>\n   <td style=\"text-align:right;\"> NA </td>\n   <td style=\"text-align:right;\"> NA </td>\n   <td style=\"text-align:right;\"> NA </td>\n   <td style=\"text-align:right;\"> NA </td>\n   <td style=\"text-align:right;\"> NA </td>\n   <td style=\"text-align:right;\"> NA </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> age </td>\n   <td style=\"text-align:right;\"> 0.0593032 </td>\n   <td style=\"text-align:right;\"> 0.1100051 </td>\n   <td style=\"text-align:right;\"> -0.0453862 </td>\n   <td style=\"text-align:right;\"> 0.1787040 </td>\n   <td style=\"text-align:right;\"> 1.0000000 </td>\n   <td style=\"text-align:right;\"> NA </td>\n   <td style=\"text-align:right;\"> NA </td>\n   <td style=\"text-align:right;\"> NA </td>\n   <td style=\"text-align:right;\"> NA </td>\n   <td style=\"text-align:right;\"> NA </td>\n   <td style=\"text-align:right;\"> NA </td>\n   <td style=\"text-align:right;\"> NA </td>\n   <td style=\"text-align:right;\"> NA </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> smoker </td>\n   <td style=\"text-align:right;\"> -0.2742888 </td>\n   <td style=\"text-align:right;\"> -0.0495029 </td>\n   <td style=\"text-align:right;\"> 0.0206456 </td>\n   <td style=\"text-align:right;\"> 0.0377211 </td>\n   <td style=\"text-align:right;\"> -0.2229620 </td>\n   <td style=\"text-align:right;\"> 1.0000000 </td>\n   <td style=\"text-align:right;\"> NA </td>\n   <td style=\"text-align:right;\"> NA </td>\n   <td style=\"text-align:right;\"> NA </td>\n   <td style=\"text-align:right;\"> NA </td>\n   <td style=\"text-align:right;\"> NA </td>\n   <td style=\"text-align:right;\"> NA </td>\n   <td style=\"text-align:right;\"> NA </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> sites </td>\n   <td style=\"text-align:right;\"> 0.0520821 </td>\n   <td style=\"text-align:right;\"> -0.0393742 </td>\n   <td style=\"text-align:right;\"> -0.1291127 </td>\n   <td style=\"text-align:right;\"> 0.0505164 </td>\n   <td style=\"text-align:right;\"> -0.0274695 </td>\n   <td style=\"text-align:right;\"> -0.0226381 </td>\n   <td style=\"text-align:right;\"> 1.0000000 </td>\n   <td style=\"text-align:right;\"> NA </td>\n   <td style=\"text-align:right;\"> NA </td>\n   <td style=\"text-align:right;\"> NA </td>\n   <td style=\"text-align:right;\"> NA </td>\n   <td style=\"text-align:right;\"> NA </td>\n   <td style=\"text-align:right;\"> NA </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> attachbase </td>\n   <td style=\"text-align:right;\"> -0.1641513 </td>\n   <td style=\"text-align:right;\"> 0.1440999 </td>\n   <td style=\"text-align:right;\"> 0.2416242 </td>\n   <td style=\"text-align:right;\"> 0.0313438 </td>\n   <td style=\"text-align:right;\"> 0.1354434 </td>\n   <td style=\"text-align:right;\"> 0.1456234 </td>\n   <td style=\"text-align:right;\"> -0.4237813 </td>\n   <td style=\"text-align:right;\"> 1.0000000 </td>\n   <td style=\"text-align:right;\"> NA </td>\n   <td style=\"text-align:right;\"> NA </td>\n   <td style=\"text-align:right;\"> NA </td>\n   <td style=\"text-align:right;\"> NA </td>\n   <td style=\"text-align:right;\"> NA </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> attach1year </td>\n   <td style=\"text-align:right;\"> -0.1440182 </td>\n   <td style=\"text-align:right;\"> 0.1671499 </td>\n   <td style=\"text-align:right;\"> 0.2021284 </td>\n   <td style=\"text-align:right;\"> 0.0671713 </td>\n   <td style=\"text-align:right;\"> 0.0850851 </td>\n   <td style=\"text-align:right;\"> 0.1987327 </td>\n   <td style=\"text-align:right;\"> -0.4042743 </td>\n   <td style=\"text-align:right;\"> 0.9450189 </td>\n   <td style=\"text-align:right;\"> 1.0000000 </td>\n   <td style=\"text-align:right;\"> NA </td>\n   <td style=\"text-align:right;\"> NA </td>\n   <td style=\"text-align:right;\"> NA </td>\n   <td style=\"text-align:right;\"> NA </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> pdbase </td>\n   <td style=\"text-align:right;\"> -0.2010672 </td>\n   <td style=\"text-align:right;\"> -0.0224838 </td>\n   <td style=\"text-align:right;\"> 0.2645908 </td>\n   <td style=\"text-align:right;\"> 0.0182289 </td>\n   <td style=\"text-align:right;\"> -0.0884687 </td>\n   <td style=\"text-align:right;\"> 0.2614919 </td>\n   <td style=\"text-align:right;\"> -0.1805543 </td>\n   <td style=\"text-align:right;\"> 0.6047132 </td>\n   <td style=\"text-align:right;\"> 0.6093346 </td>\n   <td style=\"text-align:right;\"> 1.0000000 </td>\n   <td style=\"text-align:right;\"> NA </td>\n   <td style=\"text-align:right;\"> NA </td>\n   <td style=\"text-align:right;\"> NA </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> pd1year </td>\n   <td style=\"text-align:right;\"> -0.0685783 </td>\n   <td style=\"text-align:right;\"> 0.0126731 </td>\n   <td style=\"text-align:right;\"> 0.1362804 </td>\n   <td style=\"text-align:right;\"> 0.0606735 </td>\n   <td style=\"text-align:right;\"> -0.1246106 </td>\n   <td style=\"text-align:right;\"> 0.2447030 </td>\n   <td style=\"text-align:right;\"> -0.1901940 </td>\n   <td style=\"text-align:right;\"> 0.5601052 </td>\n   <td style=\"text-align:right;\"> 0.6685457 </td>\n   <td style=\"text-align:right;\"> 0.8436653 </td>\n   <td style=\"text-align:right;\"> 1.0000000 </td>\n   <td style=\"text-align:right;\"> NA </td>\n   <td style=\"text-align:right;\"> NA </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> attachchange </td>\n   <td style=\"text-align:right;\"> 0.0964391 </td>\n   <td style=\"text-align:right;\"> 0.0296043 </td>\n   <td style=\"text-align:right;\"> -0.1696216 </td>\n   <td style=\"text-align:right;\"> 0.0928969 </td>\n   <td style=\"text-align:right;\"> -0.1742586 </td>\n   <td style=\"text-align:right;\"> 0.1135740 </td>\n   <td style=\"text-align:right;\"> 0.1578683 </td>\n   <td style=\"text-align:right;\"> -0.3976360 </td>\n   <td style=\"text-align:right;\"> -0.0757224 </td>\n   <td style=\"text-align:right;\"> -0.1342018 </td>\n   <td style=\"text-align:right;\"> 0.1679506 </td>\n   <td style=\"text-align:right;\"> 1.0000000 </td>\n   <td style=\"text-align:right;\"> NA </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> pdchange </td>\n   <td style=\"text-align:right;\"> 0.2251487 </td>\n   <td style=\"text-align:right;\"> 0.0622762 </td>\n   <td style=\"text-align:right;\"> -0.2123192 </td>\n   <td style=\"text-align:right;\"> 0.0789059 </td>\n   <td style=\"text-align:right;\"> -0.0731704 </td>\n   <td style=\"text-align:right;\"> -0.0091784 </td>\n   <td style=\"text-align:right;\"> -0.0323859 </td>\n   <td style=\"text-align:right;\"> -0.0317726 </td>\n   <td style=\"text-align:right;\"> 0.1579534 </td>\n   <td style=\"text-align:right;\"> -0.2031295 </td>\n   <td style=\"text-align:right;\"> 0.3543035 </td>\n   <td style=\"text-align:right;\"> 0.5400668 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n\n\nOther than that, there is a correlation between gender and ID which seems spurious. Let's investigate that in the next tab.\n\n<a href=\"#Preliminary\">Back to top of tabset</a>\n\n## Gender and Missingness\n\nThere is a correlation between gender and ID. Let's make a simple plot to investigate.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Creating a simple plot of id and gender\nggplot(data_missing, aes(x = factor(gender), y = id)) + \n  geom_point() + \n  labs(title = \"ID by Gender\")\n```\n\n::: {.cell-output-display}\n![](Project_1_Regression_files/figure-html/unnamed-chunk-15-1.png){width=672}\n:::\n:::\n\n\n\n\nInterestingly, it appears that the experimenters assigned ID based on gender. That is, females received ID's starting at 101, and males received ID's starting at 201 (for some reason there's a few females with ID's \\> 200).\n\nIt will be important to double check with the PI's how they assigned participants to treatment group to ensure it was in fact random.\n\nLet's make a contingency table to see what the breakdown between gender and treatment group is.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# First make a contingency table of both variables\ncontingency_table <- table(data_missing$gender, data_missing$trtgroup)\n\n# Set the row and column names\ndimnames(contingency_table) <- list(\n  \"Gender\" = c(\"Male\", \"Female\"),\n  \"Treatment Condition\" = c(\"Placebo\", \"Control\", \"Low\", \"Medium\", \"High\"))\n\n# Convert the table to a dataframe for better formatting (from ChatGPT)\ncontingency_df <- as.data.frame.matrix(contingency_table)\n\n# Pretty print the table using kable\nkable(contingency_df, format = \"html\") %>%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\"))\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table table-striped table-hover table-condensed\" style=\"margin-left: auto; margin-right: auto;\">\n <thead>\n  <tr>\n   <th style=\"text-align:left;\">   </th>\n   <th style=\"text-align:right;\"> Placebo </th>\n   <th style=\"text-align:right;\"> Control </th>\n   <th style=\"text-align:right;\"> Low </th>\n   <th style=\"text-align:right;\"> Medium </th>\n   <th style=\"text-align:right;\"> High </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> Male </td>\n   <td style=\"text-align:right;\"> 10 </td>\n   <td style=\"text-align:right;\"> 7 </td>\n   <td style=\"text-align:right;\"> 9 </td>\n   <td style=\"text-align:right;\"> 7 </td>\n   <td style=\"text-align:right;\"> 3 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Female </td>\n   <td style=\"text-align:right;\"> 13 </td>\n   <td style=\"text-align:right;\"> 16 </td>\n   <td style=\"text-align:right;\"> 12 </td>\n   <td style=\"text-align:right;\"> 13 </td>\n   <td style=\"text-align:right;\"> 13 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n\n\nThat's not good! It looks like males were less likely to be in the high treatment condition compared to females.\n\nThis could be because males were more likely to drop out then females. Let's make a quick table using the original data set before we dropped the missing variables.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# First make a contingency table of both variables\ncontingency_table_clean <- table(data$gender, data$trtgroup)\n\n# Set the row and column names\ndimnames(contingency_table_clean) <- list(\n  \"Gender\" = c(\"Male\", \"Female\"),\n  \"Treatment Condition\" = c(\"Placebo\", \"Control\", \"Low\", \"Medium\", \"High\"))\n\n# Convert the table to a dataframe for better formatting (from ChatGPT)\ncontingency_df_clean <- as.data.frame.matrix(contingency_table_clean)\n\n# Pretty print the table using kable\nkable(contingency_df_clean, format = \"html\") %>%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\"))\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table table-striped table-hover table-condensed\" style=\"margin-left: auto; margin-right: auto;\">\n <thead>\n  <tr>\n   <th style=\"text-align:left;\">   </th>\n   <th style=\"text-align:right;\"> Placebo </th>\n   <th style=\"text-align:right;\"> Control </th>\n   <th style=\"text-align:right;\"> Low </th>\n   <th style=\"text-align:right;\"> Medium </th>\n   <th style=\"text-align:right;\"> High </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> Male </td>\n   <td style=\"text-align:right;\"> 11 </td>\n   <td style=\"text-align:right;\"> 10 </td>\n   <td style=\"text-align:right;\"> 11 </td>\n   <td style=\"text-align:right;\"> 11 </td>\n   <td style=\"text-align:right;\"> 11 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Female </td>\n   <td style=\"text-align:right;\"> 15 </td>\n   <td style=\"text-align:right;\"> 16 </td>\n   <td style=\"text-align:right;\"> 15 </td>\n   <td style=\"text-align:right;\"> 15 </td>\n   <td style=\"text-align:right;\"> 15 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n\n\nIt looks more balanced before I took out participants with missing data.\n\nChi-square is known to be unsuitable if a cell has \\< 5 counts, which we have in this case (3 males in high concentration condition). So I will run Fisher's test to see if that difference is statistically significant.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfisher_test <- fisher.test(contingency_table)\nfisher_test\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tFisher's Exact Test for Count Data\n\ndata:  contingency_table\np-value = 0.506\nalternative hypothesis: two.sided\n```\n\n\n:::\n:::\n\n\n\n\nThe p-value is not signficant (p = 0.506).\n\nWe know from visualizing the missing data that there were 27 missing data points for the gum measurement DVs, and these all belong to the same people. Furthermore, it appears that males in the treatment groups were more likely to have missing values than in the placebo (and maybe control) group. Is it possible that the gel was having an adverse effect on these participants? Does the gel have an adverse effect only on males and not females for some reason? Let's explore.\n\nFirst, I want to investigate if males were more likely to have missing data points. It's possible if their gums were hurting they simply rejected or avoided having these measurements taken.\n\nLet's repeat this process and make a contingency table of gender and missing variables.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# First let's add a new dummy code for if a participant is missing any data points\ndata$missing <- ifelse(apply(data, 1, function(row) any(is.na(row))), 1, 0)\n\n# First make a contingency table of both variables\ncontingency_table_missing <- table(data$gender, data$missing)\n\n# Set the row and column names\ndimnames(contingency_table_missing) <- list(\n  \"Gender\" = c(\"Male\", \"Female\"),\n  \"Missing\" = c(\"Not Missing\", \"Missing\"))\n\n# Convert the table to a dataframe for better formatting (from ChatGPT)\ncontingency_df_missing <- as.data.frame.matrix(contingency_table_missing)\n\n# Pretty print the table using kable\nkable(contingency_df_missing, format = \"html\") %>%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\"))\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table table-striped table-hover table-condensed\" style=\"margin-left: auto; margin-right: auto;\">\n <thead>\n  <tr>\n   <th style=\"text-align:left;\">   </th>\n   <th style=\"text-align:right;\"> Not Missing </th>\n   <th style=\"text-align:right;\"> Missing </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> Male </td>\n   <td style=\"text-align:right;\"> 35 </td>\n   <td style=\"text-align:right;\"> 19 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Female </td>\n   <td style=\"text-align:right;\"> 66 </td>\n   <td style=\"text-align:right;\"> 10 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n\n\nProportionally, it appears that males may be more likely to have missing variables than females. Let's run a chi-square to check.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nchi_square_test <- chisq.test(contingency_df_missing)\nchi_square_test\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tPearson's Chi-squared test with Yates' continuity correction\n\ndata:  contingency_df_missing\nX-squared = 7.6127, df = 1, p-value = 0.005796\n```\n\n\n:::\n:::\n\n\n\n\nSuccess! Males were more likely to have missing values compared to females (p = 0.005796). This could be a problem (counfound) if something was causing males to avoid having their gums measured compared to females (such as adverse reactions from the gel)\n\nLet's do a quick chi square test to check if there is a relationship between missing values and treatment condition.\n\nWe start off the same way by making a contingency table and running a chi-square test.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# First make a contingency table of both variables\ncontingency_table_missing2 <- table(data$missing, data$trtgroup)\n\n# Set the row and column names\ndimnames(contingency_table_missing2) <- list(\"Missing\" = c(\"Not Missing\", \"Missing\"),\n                                             \"Treatmtent Condition\" = c(\"Placebo\", \"Control\", \"Low\", \"Medium\", \"High\"))\n\n# Convert the table to a dataframe for better formatting (from ChatGPT)\ncontingency_df_missing2 <- as.data.frame.matrix(contingency_table_missing2)\n\n# Pretty print the table using kable\nkable(contingency_df_missing2, format = \"html\") %>%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\"))\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table table-striped table-hover table-condensed\" style=\"margin-left: auto; margin-right: auto;\">\n <thead>\n  <tr>\n   <th style=\"text-align:left;\">   </th>\n   <th style=\"text-align:right;\"> Placebo </th>\n   <th style=\"text-align:right;\"> Control </th>\n   <th style=\"text-align:right;\"> Low </th>\n   <th style=\"text-align:right;\"> Medium </th>\n   <th style=\"text-align:right;\"> High </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> Not Missing </td>\n   <td style=\"text-align:right;\"> 22 </td>\n   <td style=\"text-align:right;\"> 23 </td>\n   <td style=\"text-align:right;\"> 21 </td>\n   <td style=\"text-align:right;\"> 19 </td>\n   <td style=\"text-align:right;\"> 16 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Missing </td>\n   <td style=\"text-align:right;\"> 4 </td>\n   <td style=\"text-align:right;\"> 3 </td>\n   <td style=\"text-align:right;\"> 5 </td>\n   <td style=\"text-align:right;\"> 7 </td>\n   <td style=\"text-align:right;\"> 10 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n\n\nIt does appear that there are more missing variables in the high concentration condition. Is it statistically significant?\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Run a Fisher's Exact Test (since we have < 5 observations in cells)\nfisher_test <- fisher.test(contingency_df_missing2)\nfisher_test\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tFisher's Exact Test for Count Data\n\ndata:  contingency_df_missing2\np-value = 0.1675\nalternative hypothesis: two.sided\n```\n\n\n:::\n:::\n\n\n\n\nNot significant (p = 0.1675). So we can conclude that there is no difference in gender or missing values based on treatment condition (i.e., participants in all treatment conditions were equally likely to be male or female, or have missing values)\n\n**However**, across the board, males were more likely to have missing values than females. This will be important to note as a caveat during interpretation of the final results.\n\n<a href=\"#Preliminary\">Back to top of tabset</a>\n\n## Normality of Dependent Variables\n\nHere we will simply plot the histograms of attachment loss and pocket depth changes scores, to assess if they appear normally distributed or if we will have to perform a transformation of some kind.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Plot simple histogram of attachment loss change score\nhist(data_missing$attachchange,\n     main  = \"Histogram of Attachment Loss Change\",\n     xlab = \"Attachment Loss Change\",\n     ylab = \"Frequency\")\n```\n\n::: {.cell-output-display}\n![](Project_1_Regression_files/figure-html/unnamed-chunk-23-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# Plot simple histogram of pocket depth change score\nhist(data_missing$pdchange,\n     main  = \"Histogram of Attachment Loss Change\",\n     xlab = \"Pocket Depth Change\",\n     ylab = \"Frequency\")\n```\n\n::: {.cell-output-display}\n![](Project_1_Regression_files/figure-html/unnamed-chunk-23-2.png){width=672}\n:::\n:::\n\n\n\n\nBoth attachment loss and pocket depth change appear to be normally distributed and will not need to be transformed. Attachment loss change is slightly left-tailed but this could be due to outliers, or may just not impact the analysis.\n:::\n\n## Exploratory Data Analysis {#Exploratory}\n\nHere I will plot the data and perform a number of simple linear regressions to examine relationships between variables in order to determine which covariates to include in the model.\n\n:::::: panel-tabset\n## Primary Explanatory Variable\n\n::: panel-tabset\n## 5 Treatment Groups\n\nLet's examine if there appears to be a difference in the average attachment loss and pocket depth change according to treatment level\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(data_missing, aes(x = factor(trtgroup), y = attachchange)) + \n  geom_boxplot() +\n  labs(title = \"Boxplot of Attachment Loss change by Treatment\")\n```\n\n::: {.cell-output-display}\n![](Project_1_Regression_files/figure-html/unnamed-chunk-24-1.png){width=672}\n:::\n\n```{.r .cell-code}\nggplot(data_missing, aes(x = factor(trtgroup), y = pdchange)) + \n  geom_boxplot()  +\n  labs(title = \"Boxplot of Pocket Depth Change by treatment\")\n```\n\n::: {.cell-output-display}\n![](Project_1_Regression_files/figure-html/unnamed-chunk-24-2.png){width=672}\n:::\n:::\n\n\n\n\n[Attachment Loss Change]{.underline}\n\nVisually, there does not appear to be a difference between the treatment group levels (low, medium, high concentration gel) compared to each other. Additionally, there does not seem to be a difference between the treatment groups and the placebo for attachment loss change.\n\n[Pocket Depth Change]{.underline}\n\nInterestingly, the high concentration condition appears to have had a negative effect. Additionally, there seems to be a difference in the treatment groups compared to the no treatment control, but NOT when compared to the placebo. The exception is the low concentration condition compared to the placebo when looking at pocket depth change. Further analysis will reveal whether these differences are statistically significant or not.\n\n<a href=\"#Exploratory\">Back to top of tabset</a>\n\n## 3 Treatment Groups\n\nThe relationship between low vs medium vs high gel concentration does not look very strong. Furthermore, we technically do not have a large enough sample size in the high concentration condition to include it.\n\nFor those reasons, let's make the same comparisons but while combining all treatment levels into one group called treatment.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(data_missing, aes(x = factor(trt3groups), y = attachchange)) + \n  geom_boxplot()\n```\n\n::: {.cell-output-display}\n![](Project_1_Regression_files/figure-html/unnamed-chunk-25-1.png){width=672}\n:::\n\n```{.r .cell-code}\nggplot(data_missing, aes(x = factor(trt3groups), y = attachchange)) +\n  geom_boxplot() +\n  labs(title = \"Boxplot of Attachment Loss Change by Treatment\")\n```\n\n::: {.cell-output-display}\n![](Project_1_Regression_files/figure-html/unnamed-chunk-25-2.png){width=672}\n:::\n\n```{.r .cell-code}\nggplot(data_missing, aes(x = factor(trt3groups), y = pdchange)) + \n  geom_boxplot() +\n  labs(title = \"Boxplot of Pocket Depth Change by Treatment\")\n```\n\n::: {.cell-output-display}\n![](Project_1_Regression_files/figure-html/unnamed-chunk-25-3.png){width=672}\n:::\n:::\n\n\n\n\nBy eye, it appears as if there is no difference between the placebo and collapsed treatment groups in attachment loss change or pocket depth change. However, both placebo and any treatment conditions appear to have decreased (?) attachment loss and pocket depth. It may be that this study has null results, unless including one of the covariates changes the results. We will see come the analysis section.\n\n<a href=\"#Exploratory\">Back to top of tabset</a>\n:::\n\n## Covariates {#covariates}\n\nNow we will exlore the relationships between our potential covariates and attachment loss and pocket depth change\n\n::: panel-tabset\n## Age\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Plot age vs attachment loss change\nggplot(data_missing, aes(x = age, y = attachchange)) + \n  geom_point() + \n  labs(title = \"Scatterplot of Age vs Attachment Loss Change\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_point()`).\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](Project_1_Regression_files/figure-html/unnamed-chunk-26-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# Plot age vs pocket depth change\nggplot(data_missing, aes(x = age, y = pdchange)) +\n  geom_point() + \n  labs(title = \"Scatterplot of Age vs Pocket Depth Change\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_point()`).\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](Project_1_Regression_files/figure-html/unnamed-chunk-26-2.png){width=672}\n:::\n:::\n\n\n\n\nThere does not appear to be any kind of linear relationship between age and attachment loss change or pocket depth change. I will run a model where I include age, but it will likely be removed in the final model.\n\n<a href=\"#Exploratory\">Back to top of tabset</a>\n\n## Sites\n\nIs there any relationship between the number of sites measured from and attachment loss change or pocket depth change (e.g. a lower number of sites could lead to less accurate readings)? If so this could be something to include in our data analysis\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Plot sites vs attachment change loss\nggplot(data_missing, aes(x = sites, y = attachchange)) + \n  geom_point() +\n  labs(title = \"Scatterplot of Attachment Loss Change vs Sites\")\n```\n\n::: {.cell-output-display}\n![](Project_1_Regression_files/figure-html/unnamed-chunk-27-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# Plot sites vs pocket depth chagne\nggplot(data_missing, aes(x = sites, y = pdchange)) + \n  geom_point() +\n  labs(title = \"Scatterplot of Pocket Depth Change by Sites\")\n```\n\n::: {.cell-output-display}\n![](Project_1_Regression_files/figure-html/unnamed-chunk-27-2.png){width=672}\n:::\n:::\n\n\n\n\nThere does not seem to be a dramatic difference in attachment loss or pocket depth change based on the number of sites measured from. It's a bit hard to tell with those two low site subjects, but the points are similar enough to the rest of the dataset that I do not think we have to worry about site number in our analysis.\n\n<a href=\"#Exploratory\">Back to top of tabset</a>\n\n## Gender\n\nIt is conceivable that there are sex differences in regards to gum health. Let's examine if there is a difference in attachment loss or pocket depth change based on gender\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Plot gender vs attachment loss change\nggplot(data_missing, aes(x = factor(gender), y = attachchange)) +\n  geom_boxplot() + \n  labs(title = \"Boxplot of Attachment Loss Change by Gender\",\n      x = \"Gender\")\n```\n\n::: {.cell-output-display}\n![](Project_1_Regression_files/figure-html/unnamed-chunk-28-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# Plot gender vs pocket depth change\nggplot(data_missing, aes(x = factor(gender), y = pdchange)) +\n  geom_boxplot() + \n  labs(title = \"Boxplot of Pocket Depth Change by Gender\",\n      x = \"Gender\")\n```\n\n::: {.cell-output-display}\n![](Project_1_Regression_files/figure-html/unnamed-chunk-28-2.png){width=672}\n:::\n:::\n\n\n\n\nBy eye, it appears that males may have more pocket depth loss compared with females. This will be a good variable to include as a covariate.\n\nLet's try a t-test to see if there is any difference in attachment loss or pocket depth change based on gender\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Running a t-test on attachment loss change by gender\nmale <- data_missing$attachchange[data_missing$gender == 1]\nfemale <- data_missing$attachchange[data_missing$gender == 2]\nt_test_result <- t.test(male, female)\nt_test_result\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tWelch Two Sample t-test\n\ndata:  male and female\nt = 2.1969, df = 100.8, p-value = 0.03032\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n 0.01003279 0.19682842\nsample estimates:\n  mean of x   mean of y \n-0.03217094 -0.13560154 \n```\n\n\n:::\n:::\n\n\n\n\nThere is a significant difference in attachment loss change score based on gender (t = 2.0502, p = 0.04299)\n\nLet's repeat for pocket depth change\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Running a t-test on pocket depth change by gender\nmale <- data_missing$pdchange[data_missing$gender == 1]\nfemale <- data_missing$pdchange[data_missing$gender == 2]\nt_test_result <- t.test(male, female)\nt_test_result\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tWelch Two Sample t-test\n\ndata:  male and female\nt = 2.3534, df = 81.683, p-value = 0.02101\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n 0.01884469 0.22488042\nsample estimates:\n mean of x  mean of y \n-0.2150844 -0.3369470 \n```\n\n\n:::\n:::\n\n\n\n\nThere is also a statistically significant difference in pocket depth change based on gender (t = 2.2626, p = 0.02641).\n\n**Therefore I will include gender as a covariate in the analysis!**\n\n<a href=\"#Exploratory\">Back to top of tabset</a>\n\n## Race\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Plot race vs attachment loss change\nggplot(data_missing, aes(x = factor(race), y = attachchange)) +\n  geom_boxplot() +\n  labs(title = \"Boxplot of Attachment Change by Race\",\n       x = \"Race\")\n```\n\n::: {.cell-output-display}\n![](Project_1_Regression_files/figure-html/unnamed-chunk-31-1.png){width=672}\n:::\n\n```{.r .cell-code}\nggplot(data_missing, aes(x = factor(race), y = pdchange)) + \n  geom_boxplot() +\n  labs(title = \"Boxplot of Pocket Depth Change by Race\",\n  x = \"Race\")\n```\n\n::: {.cell-output-display}\n![](Project_1_Regression_files/figure-html/unnamed-chunk-31-2.png){width=672}\n:::\n:::\n\n\n\n\nThere does not seem to be much of a difference in attachment loss or pocket depth based on race. MAYBE African Americans (2) have more pocket depth loss compared to Asians(4), but the sample size was very small for both races (88.1% of the sample identified as White)\n\n<a href=\"#Exploratory\">Back to top of tabset</a>\n\n## Smoking Status\n\nWhether the participant is a smoker or not likely has a dramatic effect on gum health. Let's assess\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Plot smoking status vs attachment loss change\nggplot(data_missing, aes(x = factor(smoker), y = attachchange)) +\n  geom_boxplot() +\n  labs(title = \"Attachment loss by smoking status\",\n       x = \"Smoking Status\")\n```\n\n::: {.cell-output-display}\n![](Project_1_Regression_files/figure-html/unnamed-chunk-32-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# Plot smoking status vs pocket depth change\nggplot(data_missing, aes(x = factor(smoker), y = pdchange)) +\n  geom_boxplot() +\n  labs(title = \"Attachment loss by smoking status\",\n       x = \"Smoking Status\")\n```\n\n::: {.cell-output-display}\n![](Project_1_Regression_files/figure-html/unnamed-chunk-32-2.png){width=672}\n:::\n:::\n\n\n\n\nThere does not appear to be any difference in attachment loss or pocket depth change based on smoking status. I will test a model with smoking status included, but it will likely be dropped in the final model.\n\n<a href=\"#Exploratory\">Back to top of tabset</a>\n:::\n\n## Dependent Variables\n\n::: panel-tabset\n## Baseline vs 1 Year\n\nI am curious what the relationship between base line and 1 year measurements are. The study is an RCT so even if baseline measurements affect 1 year measurements, participants should have been randomly assigned to groups so it is essentially controlled for in the study design. It will still be important to assess this relationship as a moderator however. Maybe treatment only worked for those with high attachment loss or pocket depth at the beginning?\n\nFirst let's make a simple plot of attachment loss at baseline and at 1 year\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Creating a scatter plot of attachment loss at baseline and 1 year\nggplot(data_missing, aes(x = attachbase, y = attach1year)) + \n  geom_point() +\n  labs(title = \"Scatterplot of Attachment Loss at Baseline and 1 Year\")\n```\n\n::: {.cell-output-display}\n![](Project_1_Regression_files/figure-html/unnamed-chunk-33-1.png){width=672}\n:::\n:::\n\n\n\n\nLet's check the correlation coefficient.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Run a correlation test between attachment at base and 1 year\ncorrelation <- cor.test(data_missing$attachbase, data_missing$attach1year)\ncorrelation\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tPearson's product-moment correlation\n\ndata:  data_missing$attachbase and data_missing$attach1year\nt = 29.198, df = 101, p-value < 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.9204657 0.9628839\nsample estimates:\n      cor \n0.9455558 \n```\n\n\n:::\n:::\n\n\n\n\nThose are highly correlated (R = 0.946, p \\<.0001)! Let's run a simple linear regression\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Run a regression with attachment loss at 1 year as the DV and attachment at base as the IV\nmodel <- lm(attach1year ~ attachbase, data = data_missing)\nsummary(model)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = attach1year ~ attachbase, data = data_missing)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.55683 -0.15641 -0.01841  0.15202  0.82062 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  0.19872    0.06975   2.849  0.00531 ** \nattachbase   0.86452    0.02961  29.198  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2525 on 101 degrees of freedom\nMultiple R-squared:  0.8941,\tAdjusted R-squared:  0.893 \nF-statistic: 852.5 on 1 and 101 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n\n```{.r .cell-code}\n# Plot the model\nggplot(data_missing, aes(x = attachbase, y = attach1year)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", col = \"blue\") +\n  labs(title = \"Simple Linear Regression of Attachment Loss at Baseline and 1 Year\",\n      x = \"Attachment Loss at Baseline\",\n      y = \"Attachment Loss at 1 Year\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`geom_smooth()` using formula = 'y ~ x'\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](Project_1_Regression_files/figure-html/unnamed-chunk-35-1.png){width=672}\n:::\n:::\n\n\n\n\nAttachment loss at baseline is a significant predictor of attachment loss at 1 year (t = 29.20, p \\<.0001). This is important! We should account for attachment loss at baseline by including it as a covariate in our final model!\n\nLet's do the same process of pocket depth at baseline and 1 year\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create a scatterplot of pocket depth at base vs 1 year\nggplot(data_missing, aes(x = pdbase, y = pd1year)) +\n  geom_point() +\n  labs(title = \"Scatterplot of Pocket Depth at Baseline and 1 Year\")\n```\n\n::: {.cell-output-display}\n![](Project_1_Regression_files/figure-html/unnamed-chunk-36-1.png){width=672}\n:::\n:::\n\n\n\n\nSimilar to attachment loss, we see a relationship between pocket depth at baseline and 1 year. Let's run the correlation.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Run a correlation between pocket depth at baseline and 1 year\ncorrelation <- cor.test(data_missing$pdbase, data_missing$pd1year)\ncorrelation\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tPearson's product-moment correlation\n\ndata:  data_missing$pdbase and data_missing$pd1year\nt = 15.767, df = 101, p-value < 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.7764571 0.8913340\nsample estimates:\n      cor \n0.8432691 \n```\n\n\n:::\n:::\n\n\n\n\nWhile not as strong as attachment loss, there is still a strong relationship between pocket depth at baseline and 1 year (R = 0.84, p \\<.0001). Let's run the SLR.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Run an SLR with pocket depth at 1 year as the DV and pocket depth at baseline as the DV\nmodel <- lm(pd1year ~ pdbase, data = data_missing)\nsummary(model)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = pd1year ~ pdbase, data = data_missing)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.55105 -0.17484  0.01996  0.19627  0.64381 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  0.07504    0.17948   0.418    0.677    \npdbase       0.88346    0.05603  15.767   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2634 on 101 degrees of freedom\nMultiple R-squared:  0.7111,\tAdjusted R-squared:  0.7082 \nF-statistic: 248.6 on 1 and 101 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n\n```{.r .cell-code}\n# Plot the model\nggplot(data_missing, aes(x = pdbase, y = pd1year)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", col = \"blue\") +\n  labs(title = \"Simple Linear Regression of Pocket Depth at Baseline and 1 Year\",\n       x = \"Pocket Depth at Baseline\",\n       y = \"Pocket Depth at 1 year\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`geom_smooth()` using formula = 'y ~ x'\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](Project_1_Regression_files/figure-html/unnamed-chunk-38-1.png){width=672}\n:::\n:::\n\n\n\n\nPocket depth at baseline is also a significant predictor of pocket depth at 1 year (t = 15.78, p = \\<.0001). We should also therefore include pocket depth at baseline as a covariate in our model to control for it!\n\n<a href=\"#Exploratory\">Back to top of tabset</a>\n\n## Attachment Loss vs Pocket Depth\n\nI am interested in how attachment loss and pocket depth change are related. Since they are both measurements taken from the same sites in the gums, they are likely to be highly correlated. This could have implications on how we perform the analysis and interpret the results.\n\nFirst, we plot attachment loss change against pocket depth change\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Creating a scatter plot of attachment loss change against pocket depth change \nggplot(data_missing, aes(x = attachchange, y = pdchange)) + \n  geom_point() +\n  labs(title = \"Scatterplot of Attachment Loss Change and Pocket Depth Change\")\n```\n\n::: {.cell-output-display}\n![](Project_1_Regression_files/figure-html/unnamed-chunk-39-1.png){width=672}\n:::\n:::\n\n\n\n\nThat looks like a linear relationship! Let's run a correlation and an SLR.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Running the correlation\ncorrelation <- cor.test(data_missing$attachchange, data_missing$pdchange)\ncorrelation\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tPearson's product-moment correlation\n\ndata:  data_missing$attachchange and data_missing$pdchange\nt = 6.3717, df = 101, p-value = 5.621e-09\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.3814636 0.6605362\nsample estimates:\n      cor \n0.5354593 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Running the regression\nmodel <- lm(attachchange ~ pdchange, data = data_missing)\nsummary(model)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = attachchange ~ pdchange, data = data_missing)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.83130 -0.12683  0.00563  0.14913  0.46480 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  0.06312    0.03441   1.835   0.0695 .  \npdchange     0.55231    0.08668   6.372 5.62e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2343 on 101 degrees of freedom\nMultiple R-squared:  0.2867,\tAdjusted R-squared:  0.2797 \nF-statistic:  40.6 on 1 and 101 DF,  p-value: 5.621e-09\n```\n\n\n:::\n\n```{.r .cell-code}\n# Creating the plot\nggplot(data_missing, aes(x = attachchange, y = pdchange)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", col = \"blue\") +\n  labs(title = \"Simple Linear Regression of Attachment Loss and Pocket Depth Change\",\n       x = \"Pocket Depth Change\",\n       y = \"Attachment Loss Change\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`geom_smooth()` using formula = 'y ~ x'\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](Project_1_Regression_files/figure-html/unnamed-chunk-40-1.png){width=672}\n:::\n:::\n\n\n\n\nThe correlation shows that attachment loss and pocket depth change are very correlated (R = 0.54, p \\< .0001). The simple linear regression shows that pocket depth change significantly predicts attachment loss change (t = 6.37, p \\<.0001).\n\nHowever, multicollinearity is only an issue when IVs are correlated with each other. We can still run a multivariate multiple linear regression even though the DVs are correlated. In fact this is often the case, and is one of the justifications for using a multivariate MLR in the first place! <a href=\"#Exploratory\">Back to top of tabset</a>\n:::\n\n## Summary\n\n[Treatment Condition]{.underline}\n\nCollapsing the low, medium, and high concentration gel groups into 1 group does not seem to improve the relationship between treatment and attachment loss or pocket depth change. Only models including all 5 treatment groups will therefore be considered from here on out to keep in alignment with the original study design.\n\n[Gender]{.underline}\n\nGender is related to attachment loss and pocket depth change, and as such will be included in the final model.\n\n[Supressor / Counfound Variables]{.underline}\n\nA model will be tested with all covariates to assess for possible suppressor / confound variables. But the other potential covariates of race, age, sites, and smoking status were not related to attachment loss or pocket depth change by themselves.\n\n[Baseline vs 1 Year Measurements]{.underline}\n\nThe baseline measurements of attachment loss and pocket depth were significant predictors of attachment loss and pocket depth at 1 year, respectively. While the RCT nature of the study should ensure that participants were randomly assigned into treatment condition regardless of their baseline measurements, it will still be good practice to include baseline attachment loss and pocket depth into the final model.\n\n[Attachment Loss vs Pocket Depth Change Scores]{.underline}\n\nAttachment loss and pocket depth change are highly related to each other, but this should not impact the analysis. PI's will need to be consulted to interpret the clinical significance of findings, and to help fully understand the implications of any possible differences that may arise between attachment loss and pocket depth change in the analysis.\n\n<a href=\"#Exploratory\">Back to top of tabset</a>\n::::::\n\n# Data Analysis\n\nNow we can perform the actual analysis!\n\nThe clinical hypothesis for this study is that average pocket depth and attachment loss in participants who applied the gel will be lower compared to participants who did not.\n\nThe statistical hypotheses are:\n\n [Null Hypothesis:]{.underline}\n\n$$\nH_0: _{placebo} = _{no treatment} = _{low} = _{medium} = _{high} \n$$\n\nThe average pocket depth and attachment loss for all conditions will be equal to each other.\n\n [Alternative Hypothesis]{.underline}:\n\n$$\nH_A: _{placebo} \\neq _{no treatment} \\neq _{low} \\neq _{medium} \\neq _{high} \n$$\n\nAt least one of the groups will have different average pocket depth or attachment loss.\n\n## Running the Model {#Model_1}\n\nFor ease of interpretation I will be performing this analysis as two separate linear regressions, one for each outcome variable of either attachment loss or pocket depth change. I will begin with a simple linear regression only including the PEV and either attachment loss change or pocket depth change.\n\n::: panel-tabset\n## Attachment Loss Change\n\nWe start by constructing a model with attachment loss change as the dependent variable, and treatment group as the independent variable.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Running a SLR with attachment loss change as the DV, treatment group as the IV with no treatment as the reference group\nmodel_attach1 <- lm(attachchange ~ placebo + low +  medium + high, data = data_missing)\nsummary(model_attach1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = attachchange ~ placebo + low + medium + high, data = data_missing)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.88283 -0.14813  0.05115  0.17174  0.53945 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -0.22169    0.05590  -3.966 0.000139 ***\nplacebo      0.13462    0.07906   1.703 0.091771 .  \nlow          0.20388    0.08092   2.520 0.013365 *  \nmedium       0.21514    0.08197   2.625 0.010063 *  \nhigh         0.05690    0.08728   0.652 0.515950    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2681 on 98 degrees of freedom\nMultiple R-squared:  0.09368,\tAdjusted R-squared:  0.05669 \nF-statistic: 2.532 on 4 and 98 DF,  p-value: 0.0451\n```\n\n\n:::\n\n```{.r .cell-code}\n# Apply Bonferroni correction\np_values <- summary(model_attach1)$coefficients[,4] # This line selects the fourth column of the resulting coefficients table from summary(model_attach), which is the p-values\np_adjusted <- p.adjust(p_values, method = \"bonferroni\")\n\n# Compare adjusted p-values to unadjusted p-values\np_comparison <- cbind(p_values, p_adjusted)\np_comparison\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                p_values   p_adjusted\n(Intercept) 0.0001392148 0.0006960742\nplacebo     0.0917709125 0.4588545624\nlow         0.0133650749 0.0668253747\nmedium      0.0100625287 0.0503126435\nhigh        0.5159498143 1.0000000000\n```\n\n\n:::\n\n```{.r .cell-code}\n# Get confidence intervals\nconf_intervals <- confint(model_attach1)\nconf_intervals\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                  2.5 %     97.5 %\n(Intercept) -0.33262557 -0.1107577\nplacebo     -0.02226569  0.2915028\nlow          0.04330175  0.3644540\nmedium       0.05247446  0.3777966\nhigh        -0.11629489  0.2300962\n```\n\n\n:::\n:::\n\n\n\n\nThe overall model is significant (F~(4,98)~ = 2.53, p = 0.0451). Looking at the individual t-statistics, we see that - following adjustment for multiple pairwise comparisons - the placebo group (p = 0.549), low concentration group (p = 0.0668), and high concentration group (p = 1.000) are not statistically different from the control group. The medium concentration group is approaching significance (p = 0.0503).\n\nThat was with the no treatment control group as the reference. Let's see if any of our treatment conditions were different from the placebo group.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Running a SLR with attachment loss change as the DV, treatment group as the IV with placebo as the reference group\nmodel_attach2 <- lm(attachchange ~ control + low +  medium + high, data = data_missing)\nsummary(model_attach2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = attachchange ~ control + low + medium + high, data = data_missing)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.88283 -0.14813  0.05115  0.17174  0.53945 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)  \n(Intercept) -0.08707    0.05590  -1.558   0.1225  \ncontrol     -0.13462    0.07906  -1.703   0.0918 .\nlow          0.06926    0.08092   0.856   0.3941  \nmedium       0.08052    0.08197   0.982   0.3284  \nhigh        -0.07772    0.08728  -0.890   0.3754  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2681 on 98 degrees of freedom\nMultiple R-squared:  0.09368,\tAdjusted R-squared:  0.05669 \nF-statistic: 2.532 on 4 and 98 DF,  p-value: 0.0451\n```\n\n\n:::\n\n```{.r .cell-code}\n# Apply Bonferroni correction\np_values <- summary(model_attach2)$coefficients[,4] # This line selects the fourth column of the resulting coefficients table from summary(model_attach), which is the p-values\np_adjusted <- p.adjust(p_values, method = \"bonferroni\")\n\n# Compare adjusted p-values to unadjusted p-values\np_comparison <- cbind(p_values, p_adjusted)\np_comparison\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n              p_values p_adjusted\n(Intercept) 0.12254523  0.6127262\ncontrol     0.09177091  0.4588546\nlow         0.39412117  1.0000000\nmedium      0.32836700  1.0000000\nhigh        0.37538435  1.0000000\n```\n\n\n:::\n\n```{.r .cell-code}\n# Get confidence intervals\nconf_intervals <- confint(model_attach2)\nconf_intervals\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                  2.5 %     97.5 %\n(Intercept) -0.19800701 0.02386082\ncontrol     -0.29150281 0.02226569\nlow         -0.09131681 0.22983549\nmedium      -0.08214410 0.24317801\nhigh        -0.25091345 0.09547760\n```\n\n\n:::\n:::\n\n\n\n\nAfter applying a bonferroni correction, none of the groups are significantly different from the placebo group (p \\> .05).\n\n[Conclusion]{.underline}\n\nWhile the overall model was significant, the only groups that were statistically different from the no treatment control were the low and medium concentration gel groups (p \\< 0.05). However, since this was a placebo-controlled RCT, and none of the treatment groups were significantly different from the placebo group, we can conclude that we ***fail to reject the null hypothesis*** that the average attachment loss over 1 year is the same between all groups.\n\n\n\n\n\n\n\n\n\n[Tables]{.underline}\n\n<img src=\"Media/Regression1_model_attach.png\" alt=\"Regression Model\" width=\"50%\" height=\"50%\"/>\n\n<img src=\"Media/Regression2_model_attach.png\" alt=\"Regression Model\" width=\"50%\" height=\"50%\"/>\n\nNote: p-values are unadjusted.\n\n<a href=\"#Model_1\">Back to top of tabset</a>\n\n## Pocket Depth Change\n\nNow we will create our model with pocket depth change as the dependent variable and treatment group as the independent variable.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Running the regression with pocket depth change as the DV, treatment group as the IV with no treatment as the reference group\nmodel_pd <- lm(pdchange ~ placebo + low + medium + high, data = data_missing)\nsummary(model_pd)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = pdchange ~ placebo + low + medium + high, data = data_missing)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.62483 -0.14595 -0.01768  0.16029  0.66130 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -0.33817    0.05466  -6.187 1.43e-08 ***\nplacebo     -0.01152    0.07730  -0.149   0.8818    \nlow          0.13200    0.07912   1.668   0.0984 .  \nmedium       0.13562    0.08015   1.692   0.0938 .  \nhigh        -0.04413    0.08534  -0.517   0.6063    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2621 on 98 degrees of freedom\nMultiple R-squared:  0.07806,\tAdjusted R-squared:  0.04043 \nF-statistic: 2.074 on 4 and 98 DF,  p-value: 0.08994\n```\n\n\n:::\n:::\n\n\n\n\nThe overall model is not statistically significant (F~(4,98)~= 2.074, p = 0.0899). Based on this model, it appears that the gel treatment has no effect on pocket depth change after 1 year.\n\n[Conclusion]{.underline}\n\nNo groups were significantly different from each other in pocket depth change after 1 year (F~(4,98)~= 2.074, p = 0.0899), and we ***fail to reject the null hypothesis*** that the average pocket depth change over 1 year is the same between all groups.\n\n[Tables]{.underline}\n\n\n\n\n\n\n\n\n\n<img src=\"Media/Regression3_model_pd.png\" alt=\"Regression Model\" width=\"50%\"/>\n\nNote: p-values are unadjusted\n\n<a href=\"#Model_1\">Back to top of tabset</a>\n\n## Post-Hoc Analysis\n\nWe did not find a main effect based on treatment group. However I am still interested if including any of the covariates changes the results.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Run a SLR with attachment loss change as the DV and gender as a covariate\nmodel2_attach <- lm(attachchange ~ placebo + low + medium + high + gender, data = data_missing) \nsummary(model2_attach)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = attachchange ~ placebo + low + medium + high + gender, \n    data = data_missing)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.86656 -0.17533  0.02334  0.16395  0.57717 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)  \n(Intercept) -0.07460    0.10988  -0.679   0.4988  \nplacebo      0.12330    0.07883   1.564   0.1210  \nlow          0.19310    0.08064   2.395   0.0186 *\nmedium       0.21118    0.08143   2.593   0.0110 *\nhigh         0.06704    0.08690   0.771   0.4423  \ngender      -0.08675    0.05593  -1.551   0.1242  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2662 on 97 degrees of freedom\nMultiple R-squared:  0.1156,\tAdjusted R-squared:  0.07003 \nF-statistic: 2.536 on 5 and 97 DF,  p-value: 0.03346\n```\n\n\n:::\n\n```{.r .cell-code}\n# Apply Bonferroni correction\np_values <- summary(model2_attach)$coefficients[,4] # This line selects the fourth column of the resulting coefficients table from summary(model_attach), which is the p-values\np_adjusted <- p.adjust(p_values, method = \"bonferroni\")\n\n# Compare adjusted p-values to unadjusted p-values\np_comparison <- cbind(p_values, p_adjusted)\np_comparison\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n              p_values p_adjusted\n(Intercept) 0.49883151 1.00000000\nplacebo     0.12104977 0.72629859\nlow         0.01856222 0.11137333\nmedium      0.01097151 0.06582903\nhigh        0.44234244 1.00000000\ngender      0.12415264 0.74491582\n```\n\n\n:::\n\n```{.r .cell-code}\n# Get confidence intervals\nconf_intervals <- confint(model2_attach)\nconf_intervals\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                  2.5 %     97.5 %\n(Intercept) -0.29268916 0.14349242\nplacebo     -0.03315881 0.27976619\nlow          0.03304940 0.35315426\nmedium       0.04956814 0.37278247\nhigh        -0.10544029 0.23951403\ngender      -0.19775102 0.02425639\n```\n\n\n:::\n:::\n\n\n\n\nA model including gender does not seem to help anything. What about with all covariates (just for fun)?\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Run a SLR with attachment loss change as the DV and gender as a covariate\nmodel3_attach <- lm(attachchange ~ placebo + low + medium + high + gender + race + age + smoker + sites + attachbase + pdbase , data = data_missing) \nsummary(model3_attach)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = attachchange ~ placebo + low + medium + high + gender + \n    race + age + smoker + sites + attachbase + pdbase, data = data_missing)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.60415 -0.16360  0.02131  0.16645  0.57481 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  0.160630   0.480855   0.334 0.739127    \nplacebo      0.017898   0.079113   0.226 0.821541    \nlow          0.151477   0.077598   1.952 0.054074 .  \nmedium       0.172196   0.079066   2.178 0.032060 *  \nhigh         0.045948   0.081336   0.565 0.573557    \ngender      -0.050360   0.054953  -0.916 0.361924    \nrace         0.032325   0.026871   1.203 0.232181    \nage         -0.002472   0.002709  -0.912 0.364067    \nsmoker       0.069384   0.055052   1.260 0.210844    \nsites       -0.001501   0.002539  -0.591 0.555885    \nattachbase  -0.165154   0.043233  -3.820 0.000246 ***\npdbase       0.094444   0.072226   1.308 0.194370    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2456 on 89 degrees of freedom\n  (2 observations deleted due to missingness)\nMultiple R-squared:  0.2908,\tAdjusted R-squared:  0.2032 \nF-statistic: 3.318 on 11 and 89 DF,  p-value: 0.0007435\n```\n\n\n:::\n\n```{.r .cell-code}\n# Apply Bonferroni correction\np_values <- summary(model3_attach)$coefficients[,4] # This line selects the fourth column of the resulting coefficients table from summary(model_attach), which is the p-values\np_adjusted <- p.adjust(p_values, method = \"bonferroni\")\n\n# Compare adjusted p-values to unadjusted p-values\np_comparison <- cbind(p_values, p_adjusted)\np_comparison\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n               p_values  p_adjusted\n(Intercept) 0.739127379 1.000000000\nplacebo     0.821540779 1.000000000\nlow         0.054074263 0.648891153\nmedium      0.032060232 0.384722790\nhigh        0.573556835 1.000000000\ngender      0.361924208 1.000000000\nrace        0.232180658 1.000000000\nage         0.364067210 1.000000000\nsmoker      0.210844428 1.000000000\nsites       0.555885079 1.000000000\nattachbase  0.000246493 0.002957916\npdbase      0.194369803 1.000000000\n```\n\n\n:::\n\n```{.r .cell-code}\n# Get confidence intervals\nconf_intervals <- confint(model3_attach)\nconf_intervals\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                   2.5 %       97.5 %\n(Intercept) -0.794818756  1.116078333\nplacebo     -0.139298371  0.175094025\nlow         -0.002709329  0.305662938\nmedium       0.015093048  0.329298305\nhigh        -0.115665991  0.207561289\ngender      -0.159551369  0.058830891\nrace        -0.021067104  0.085716311\nage         -0.007855313  0.002911679\nsmoker      -0.040003508  0.178771331\nsites       -0.006546011  0.003543891\nattachbase  -0.251056725 -0.079251471\npdbase      -0.049067286  0.237955593\n```\n\n\n:::\n:::\n\n\n\n\nNope. Interestingly the best predictor of attachment loss at 1 year is attachment loss at baseline. The results for pocket depth are likely the same.\n\nI also want to assess if controlling for baseline attachment loss or pocket depth change affects things, since those were strong predictors of each measurement at 1 year (still need to add those SLRs).\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel4_attach <- lm(attachchange ~ placebo + low + medium + high + attachbase, data = data_missing)\nsummary(model4_attach)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = attachchange ~ placebo + low + medium + high + attachbase, \n    data = data_missing)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.52469 -0.16400  0.02084  0.15231  0.73422 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  0.10707    0.09304   1.151   0.2527    \nplacebo      0.04202    0.07616   0.552   0.5824    \nlow          0.14605    0.07592   1.924   0.0573 .  \nmedium       0.17586    0.07622   2.307   0.0232 *  \nhigh         0.02665    0.08087   0.330   0.7425    \nattachbase  -0.12902    0.03039  -4.246 4.99e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2475 on 97 degrees of freedom\nMultiple R-squared:  0.2357,\tAdjusted R-squared:  0.1963 \nF-statistic: 5.984 on 5 and 97 DF,  p-value: 7.236e-05\n```\n\n\n:::\n\n```{.r .cell-code}\n# Apply Bonferroni correction\np_values <- summary(model4_attach)$coefficients[,4] # This line selects the fourth column of the resulting coefficients table from summary(model_attach), which is the p-values\np_adjusted <- p.adjust(p_values, method = \"bonferroni\")\n\n# Compare adjusted p-values to unadjusted p-values\np_comparison <- cbind(p_values, p_adjusted)\np_comparison\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                p_values   p_adjusted\n(Intercept) 2.526878e-01 1.0000000000\nplacebo     5.824313e-01 1.0000000000\nlow         5.731590e-02 0.3438953943\nmedium      2.317045e-02 0.1390227138\nhigh        7.424826e-01 1.0000000000\nattachbase  4.989769e-05 0.0002993861\n```\n\n\n:::\n:::\n\n\n\n\nAfter controlling for treatment group, the only significant predictor of attachment loss change is baseline attachment loss scores (p~adj~ = 0.001)\n\n\n\n\n\n\n\n\n\nThese models were just for exploration, fun, and practice. The final models selected are the SLR's with treatment group as the IV and either attachment loss change or pocket depth change as the DV. <a href=\"#Model_1\">Back to top of tabset</a>\n:::\n\n## Evaluating Assumptions {#Assumptions}\n\nIn order to evaluate the assumptions of our models, we will first gather the residuals of the model predicting attachment loss change score and the model predicting pocket depth change score.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Calculate the jackknife residuals of model_attach\njackknife_residuals_attach <- rstudent(model_attach1)\n\n# Calculate the jackknife residuals of model_pd\njackknife_residuals_pd <- rstudent(model_pd)\n```\n:::\n\n\n\n\nNow that we have our residuals, we can take a closer look at the assumptions.\n\n::: panel-tabset\n## Linearity\n\nSince the IV is categorical, we do not need to assess linearity.\n\n<a href=\"#Assumptions\">Back to top of tabset</a>\n\n## Independence\n\nIndependence can be assessed in part by the study design and how the data was collected. Based on the information provided by the PI, I will assume subjects are independent from each other (e.g. not siblings).\n\nAdditionally, we can examine a scatter plot of the model's residuals against any time point variable (such as ID). Let's do that.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create a scatterplot of jackknife residuals vs ID to assess independence for pocket depth change\nggplot(data_missing, aes(x = id, y = jackknife_residuals_attach)) + \n  geom_point() +\n  geom_hline(yintercept = 0) +\n  labs(title = \"Scatterplot of Jackknife Residuals vs ID for Attachment Loss Change\",\n       x = \"ID\",\n       y = \"Jackknife Residuals\")\n```\n\n::: {.cell-output-display}\n![](Project_1_Regression_files/figure-html/unnamed-chunk-51-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# Create a scatterplot of jackknife residuals vs ID to assess independence for pocket depth change\nggplot(data_missing, aes(x = id, y = jackknife_residuals_pd)) + \n  geom_point() +\n  geom_hline(yintercept = 0) +\n  labs(title = \"Scatterplot of Residuals vs ID for Pocket Depth Change\",\n       x = \"ID\",\n       y = \"Jackknife Residuals\")\n```\n\n::: {.cell-output-display}\n![](Project_1_Regression_files/figure-html/unnamed-chunk-51-2.png){width=672}\n:::\n:::\n\n\n\n\nThe pattern appears random, suggesting independence.\n\nNote: The gap between ID's 170 and 200 looks odd, but is an artifact from how the experimenters assigned ID, with females starting at 101, and males starting at 201.\n\n<a href=\"#Assumptions\">Back to top of tabset</a>\n\n## Normality\n\nHere we will asses that, for any fixed value of X, Y has a normal distribution. We will do this using Q-Q plots and histograms of the residuals.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Make the Q-Q plots using the jackknife residuals for attachment loss change\nqqnorm(jackknife_residuals_attach, main = \"Q-Q plots of Jackknife Residuals for model_attach\")\nqqline(jackknife_residuals_attach, col = \"black\")\n```\n\n::: {.cell-output-display}\n![](Project_1_Regression_files/figure-html/unnamed-chunk-52-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# Create histogram of jackknife residuals for model_attach\nhist(jackknife_residuals_attach, main = \"Histogram of Jackknife Residuals\",\n     xlab = \"Jackknife Residuals\",\n     col = \"lightblue\",\n     border = \"black\")\n```\n\n::: {.cell-output-display}\n![](Project_1_Regression_files/figure-html/unnamed-chunk-52-2.png){width=672}\n:::\n\n```{.r .cell-code}\n# Make the Q-Q plots using the jackknife residuals for pocket depth change\nqqnorm(jackknife_residuals_pd, main = \"Q-Q plots of Jackknife Residuals for model_pd\")\nqqline(jackknife_residuals_pd, col = \"black\")\n```\n\n::: {.cell-output-display}\n![](Project_1_Regression_files/figure-html/unnamed-chunk-52-3.png){width=672}\n:::\n\n```{.r .cell-code}\n# Create histogram of jackknife residuals for model_pd\nhist(jackknife_residuals_pd, main = \"Histogram of Jackknife Residuals\",\n     xlab = \"Jackknife Residuals\",\n     col = \"lightblue\",\n     border = \"black\")\n```\n\n::: {.cell-output-display}\n![](Project_1_Regression_files/figure-html/unnamed-chunk-52-4.png){width=672}\n:::\n:::\n\n\n\n\nThe histogram for attachment loss change is a little left-tailed. This could be from an outlier. Comparatively, the Q-Q plot and histogram of the residuals for pocket depth change are normally distributed.\n\nWe can also include the Shapiro-Wilk test of normality, which provides a p-value and allows us to numerically establish that the assumption of normality is met. If the p-value is \\< 0.05 you conclude that the assumption of normality is *not* met.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nshapiro.test(jackknife_residuals_attach)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tShapiro-Wilk normality test\n\ndata:  jackknife_residuals_attach\nW = 0.9604, p-value = 0.003601\n```\n\n\n:::\n\n```{.r .cell-code}\nshapiro.test(jackknife_residuals_pd)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tShapiro-Wilk normality test\n\ndata:  jackknife_residuals_pd\nW = 0.99059, p-value = 0.6933\n```\n\n\n:::\n:::\n\n\n\n\nThe assumption of normality is violated for the attachment loss change model (p = 0.0036), but not for the pocket depth change model (p = 0.6933). Looking at the histogram of the residuals for attachment loss change, this is likely due to an outlier.\n\n[Summary]{.underline}\n\nFor attachment loss change, we have a slight violation of normality, which could be due to the presence of an outlier. However, regressions are robust to violations of assumptions and this may not actually be an issue. Outliers in the model will be assessed using jackknife residuals to confirm that these points do not have an excessive amount of influence on the model.\n\nBased on the Q-Q plots and histograms of the residuals for pocket depth change, we can conclude that we satisfy the assumption of normality.\n\n<a href=\"#Assumptions\">Back to top of tabset</a>\n\n## Equal Variances (Homoscedasticity)\n\nUsing the scale-location plot will allow us to evaluate the constant variance assumption. This will allow us to see whether the variability of the residuals is roughly constant between each group ([Source](https://tuos-bio-data-skills.github.io/intro-stats-book/regression-diagnostics-in-r.html)).\n\nTo assess homoscedasticity we examine the residual scatterplots by treatment group. The warning sign to look for here is if the variance differs greatly across groups.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Make the residual scatterplot using the jackknife residuals for model_attach\nggplot(data_missing, aes(x = trtgroup, y = jackknife_residuals_attach)) +\n  geom_point() + \n  labs(title = \"Jackknife Residuals vs Treatment Condition for Attachment Loss Change\",\n       x = \"Treatment Condition\",\n       y = \"JackKnife Residuals\")\n```\n\n::: {.cell-output-display}\n![](Project_1_Regression_files/figure-html/unnamed-chunk-54-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# Make the residual scatterplot using the jackknife residuals for model_pd\nggplot(data_missing, aes(x = trtgroup, y = jackknife_residuals_pd)) +\n  geom_point() + \n  labs(title = \"Jackknife Residuals vs Treatment Condition for Pocket Depth Change\",\n       x = \"Treatment Condition\",\n       y = \"JackKnife Residuals\")\n```\n\n::: {.cell-output-display}\n![](Project_1_Regression_files/figure-html/unnamed-chunk-54-2.png){width=672}\n:::\n:::\n\n\n\n\nIt appears that our variances in all groups are equal!\n\nAdditionally, while it is not recommended to perform a statistical test to assess for equality of variances (because formal tests of equality of variance are not very powerful), we can still do this using Bartlett's test.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbartlett.test(attachchange ~ trtgroup, data = data_missing)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tBartlett test of homogeneity of variances\n\ndata:  attachchange by trtgroup\nBartlett's K-squared = 2.5462, df = 4, p-value = 0.6364\n```\n\n\n:::\n:::\n\n\n\n\nThe null hypothesis of the Bartlett test is that the *variances are equal*. Thus failing to reject the null (p \\> 0.05) indicates that the data are consistent with the equal variance assumption.\n\nSo we meet the assumption of equality of variances, looking good!\n\n<a href=\"#Assumptions\">Back to top of tabset</a>\n\n## Residuals Centered Around Zero\n\nTo start, we can simply check that the mean of our residuals is close to 0.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#### For attachment loss change score\n# Generate the mean of the jackknife residuals for attachment loss change. Should be close to 0.\nmean(jackknife_residuals_attach)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] -0.004132486\n```\n\n\n:::\n:::\n\n\n\n\nWe are looking good for attachment loss change score. What about for pocket depth change?\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#### For pocket depth change score\n# Generate the mean of the jackknife residuals for attachment loss change. Should be close to 0.\nmean(jackknife_residuals_pd)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] -6.695748e-06\n```\n\n\n:::\n:::\n\n\n\n\nA more sophisticated approach is to plot the fitted values vs jackknife residuals. We can then compare the trend between groups to see if they are random. The fitted line should gravitate around 0 with no obvious trends.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Generate vectors containing the fitted values vs jackknife residuals so we can plot them \nfitted_values_pd <- fitted(model_pd)\n\nggplot(data_missing, aes(x = fitted_values_pd, y = jackknife_residuals_pd)) + \n  geom_point() +\n  geom_smooth(method = \"lm\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`geom_smooth()` using formula = 'y ~ x'\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](Project_1_Regression_files/figure-html/unnamed-chunk-58-1.png){width=672}\n:::\n\n```{.r .cell-code}\n  labs(title = \"Jackknife Residuals vs Fitted Values for Attachment Loss Change\",\n       x = \"Fitted Values\",\n       y = \"Jackknife Residuals\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$x\n[1] \"Fitted Values\"\n\n$y\n[1] \"Jackknife Residuals\"\n\n$title\n[1] \"Jackknife Residuals vs Fitted Values for Attachment Loss Change\"\n\nattr(,\"class\")\n[1] \"labels\"\n```\n\n\n:::\n:::\n\n\n\n\nThe pattern looks random and we can conclude we meet this assumption.\n\n<a href=\"#Assumptions\">Back to top of tabset</a>\n\n## Summary\n\nWe meet the assumptions of independence, equal variances, and errors centered around zero required for this analysis. There is a slight violation of normality for the attachment loss change model, but this could be due to an outlier.\n:::\n\n# Results\n\nThe total population of the study was 130 participants. Of that total, 54 were male and 76 were female. Each treatment group was evenly divided with 26 subjects. The study team found no cause for alarm within patient demographics (See Table 1). Primary outcome data was only collected on 103 subjects as there were 27 patients that were lost-to-follow-up. These subjects were not included in the final analysis. Additionally, Box-and-Whisker Plots were created to assess outliers in the dataset (See Figure 1). No values were excluded for this analysis. The team ran a correlation matrix to determine if any of the covariates had preexisting relationships. Figures 2A and 2B reveal that there was not any strong correlation between covariates. However, there was a strong relationship between both baseline attachment loss and pocket depth and their subsequent 1-year follow up (rs \\> 0.80). Additionally, there was a slight association between gender and attachment loss (r = -.17) and pocket depth change (r = -0.21). A two-sample t-test revealed a significant difference between means for males and females (t = 2.20, p = .0303). As a result, we only included gender as a potential demographic covariate. Given the lack of correlations between the outcome variables and the remaining demographic covariates, the team felt assured to move into regression modeling while primarily focusing on treatment group and the primary outcome variables.\n\nTo better determine the effect of randomization status on our primary outcome variables, the team created change-score variables to isolate the 1-year change in attachment loss and pocket depth from baseline. A series of linear regressions were conducted with attachment loss or pocket depth change score as the outcome variable and treatment group as the independent variable. Our first model to assess this question was a simple linear regression with treatment group as the independent variable and change in attachment loss as the dependent variable, using the control group as the reference category. A follow up regression with placebo group as the reference category allowed us to assess for any changes between the treatment groups and the placebo. Both of these models allow us to assess the effect of treatment status in comparison to the control or placebo group.\n\nThe results of the analyses can be seen in Figures 3-5. For attachment loss change, the overall model is significant (F(4,98) = 2.53, p = 0.0451). Looking at the individual t-statistics, we see that - following adjustment for multiple pairwise comparisons - the placebo group (p = 0.549), low concentration group (p = 0.0668), and high concentration group (p = 1.000) are not statistically different from the control group. The medium concentration group is approaching significance (95% CI: \\[0.052, 0.38\\], p = 0.0503). After changing the reference group to the placebo group and applying a Bonferroni correction, none of the groups are significantly different from the placebo group (all ps \\> 0.05). When looking at pocket depth change, the overall model is not statistically significant (F(4,96)= 1.958, p = 0.107). Neither model changed in significance when adding gender as a covariate. Assumptions for a regression were assessed using Q-Q plots, histograms of the residuals, scale-location plots, and scatterplots of the residuals against the fitted-values. All assumptions were met, except for a slight violation of normality for the model examining attachment loss, which is potentially due to outliers in the data.\n\n# Discussion\n\nWhile the overall model for attachment loss change was significant, the only groups that were statistically different from each other were the no treatment control and low concentration gel groups . However, since this was a placebo-controlled RCT, and none of the treatment groups were significantly different from the placebo group, we can conclude that we fail to reject the null hypothesis that the average attachment loss over 1 year is the same between all groups. The model for pocket depth change was not significant and we can conclude that we fail to reject the null hypothesis that the average pocket depth change over 1 year is the same between all groups. Therefore based on the results of both regressions, our final conclusion is that the gel treatment did not reduce attachment loss or pocket depth change compared to a placebo. In fact, there may be evidence that those in the medium-dose treatment group possibly have worse health outcomes compared to the control group, as that comparison was trending towards significance (p = .0503).\n\n-   Note: Next paragraph written by classmate Dominick DeMarsico\n\nHowever, we must address the effect of missingness on our interpretation of the data. There were 27 subjects that were lost-to-follow-up and were unable to provide outcome data (19 of which were male). The lack of males in the final dataset may make this data less predictive of treatment outcomes in males. Additionally, there were more individuals lost-to-follow-up in the treatment groups than the control or placebo groups (11.5% in the placebo and control groups, 19.2% in the low-dose group, 23.1% in the medium-dose group, and 38.5% in the high-dose group), which could further support a potential adverse effect of the gel. Alternatively, there may have been another factor affecting the treatment group which increased the rate of subject attrition that the experimenters are better poised to identify.\n\n# Bonus / Practice\n\nIn this section I will perform multiple imputation and examine outliers using the jackknife residuals, both of which we're not covered in class up to this point. This is self study.\n\n## Multiple Imputation {#Multiple_imputation}\n\n::: panel-tabset\n## Background\n\nNote: This information is taken from Biostats II (BIOS 6612) slides, week 13 lecture 20, and from [Flexible Imputation of Missing Data](https://stefvanbuuren.name/fimd/) 2nd edition.\n\n[Missing Data]{.underline}\n\nTo review, there are several assumptions for missing data. The first is MCAR (Missing Completely at Random). This is when the probability of having a missing value is the same for everyone, and is rarely ever the case with real data.\n\nThe second is MAR (missing at random). This is when the probability of having a missing value is the same *within groups defined by the observed data*. That is, if we know that second variable by which the data is NOT MCAR, in addition to the measurements, we can assume MCAR within that second variable. For instance, if we know the missing data is not equal between the sexes, then we can assume MCAR within groups defined by sex. MAR is often observed in situations such as males missing more data than females, or older participants missing more data than younger ones, etc. MAR is more general and more realistic than MCAR. Modern missing data assumptions typically start from the MAR assumption.\n\nMNAR (Missing Not At Random) is the final category of missingness, and is when the the probability of having a missing value varies for reasons that are unknown to you. For example, in public opinion research, those with weaker opinions may respond less than those with stronger opinions, and you may not know this ahead of time. MNAR is the most complex and if you have it, you have your work cut out for you. In that situation, you can find more data about the causes for the missingness, or run lots and lots of sensitivity analyses.\n\nAnd as they say, the best way to handle missing data is not to have it.\n\n[Multiple Imputation]{.underline}\n\nIn the current experiment, we know that data is not MCAR (males are missing more data than females), and thus cannot simply throw out subjects with missing values. If the data are not MCAR, listwise deletion (deleting any subject with a missing value) can severely bias estimates of means, regression coefficients and correlations.\n\nMultiple Imputation is a commonly used method to handle missing data when data is not MCAR, but is MAR. It is a process by which you use observed data to \"predict\" missing data, then use those \"imputed\" values in further analyses. Multiple imputation builds upon and pools together \"single\" imputation approaches.\n\nFor example, there exists very simple ways of imputing data, such as plugging in the average or median values in place of missing values. This is not sophisticated and kind of sketchy.\n\nThen there exists regression imputation, where for each variable you fit a regression model of it to the observed data, and then use that model to predict missing values, and use those predictions in place of the missing values. However, this injects bias into the estimate for the correlation between X and Y (since the values fall perfectly in line with the hypothesis that X and Y have a non zero correlation. (and are in a perfect line)).\n\n<img src=\"Media/regression_imputation.png\" width=\"65%\" height=\"65%\"/>\n\nThen there is stochastic regression imputation, which is like regression imputation but *adds noise back into the imputations based on the variance of their residuals*. This helps account for variance in the results due to missing data.\n\n<img src=\"Media/stochastic_regression_imputation.png\" width=\"65%\" height=\"65%\"/>\n\nMultiple imputation works by fitting multiple stochastic imputation models (can be 100's or 1000's), analyzing each dataset separately to produce estimates of interest, and finally pooling together these statistics of interest. If done properly, the pooled statistics are *unbiased under MAR, and the SE's will be correct*!\n\n<img src=\"Media/multiple_imputation.png\" width=\"65%\" height=\"65%\"/>\n\nMultiple Imputation is \"simple, elegant and powerful. It is simple because it fills the holes in the data with plausible values. It is elegant because the uncertainty about the unknown data is coded in the data itself. And it is powerful because it can solve other problems that are actually missing data problems in disguise.\" - Stef van Buuren\n\n<a href=\"#Multiple_imputation\">Back to top of tabset</a>\n\n## Coding Example\n\nFirst we will begin with an example of multiple imputation using the built in airquality dataset in R.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Examining data set and missingness\nhead(airquality)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  Ozone Solar.R Wind Temp Month Day\n1    41     190  7.4   67     5   1\n2    36     118  8.0   72     5   2\n3    12     149 12.6   74     5   3\n4    18     313 11.5   62     5   4\n5    NA      NA 14.3   56     5   5\n6    28      NA 14.9   66     5   6\n```\n\n\n:::\n\n```{.r .cell-code}\ncolSums(is.na(airquality))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  Ozone Solar.R    Wind    Temp   Month     Day \n     37       7       0       0       0       0 \n```\n\n\n:::\n\n```{.r .cell-code}\nvis_miss(airquality)\n```\n\n::: {.cell-output-display}\n![](Project_1_Regression_files/figure-html/unnamed-chunk-59-1.png){width=672}\n:::\n:::\n\n\n\n\nWe're missing 37 values in the Ozone column (24% of values!). We will have to address this missing data somehow.\n\n[Mean Imputation]{.underline}\n\nWe could quickly fill in missing data with mean imputation...\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Here's how you do mean imputation in mice\nimp <- mice(airquality, method = \"mean\", m = 1, maxit = 1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n iter imp variable\n  1   1  Ozone  Solar.R\n```\n\n\n:::\n:::\n\n\n\n\nBut mean imputation will \"underestimate the variance, disturb the relations between variables, bias almost any estimate other than the mean and bias the estimate of the mean when data are not MCAR.\" You can use mean imputation as a quick fix when there's a handful of missing values, but it should be avoided in general.\n\n[Regression Imputation]{.underline}\n\nWe could alternatively do regression imputation, where we fit a model and then use that model to predict the missing values. In regression imputation, you are essentially using the observed data to predict the missing data.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Performing regression imputation using the mice package\ndata_example <- airquality[, c(\"Ozone\", \"Solar.R\")]\nimp <- mice(data_example, method = \"norm.predict\", seed = 1,\n           m = 1, print = FALSE)\n\n# Plotting missing vs observed values\nxyplot(imp, Ozone ~ Solar.R,\n       main = \"Missing vs Observed values for Ozone ~ Solar.R\",\n       auto.key = list(corner = c(0,1),\n                  points = TRUE, \n                  text = c(\"Observed\", \"Missing\"), col = c(\"steelblue\", \"red3\")))\n```\n\n::: {.cell-output-display}\n![](Project_1_Regression_files/figure-html/unnamed-chunk-61-1.png){width=672}\n:::\n:::\n\n\n\n\nYou predict the values of the missing datapoints with the regression equation resulting from your model. That is, the imputed values correspond to the most likely values under that model. However, the imputed values (red) vary less than the observed values (blue). So each value is the best under the model, but it is very unlikely that the real values would have had this distribution.\n\nRegression imputation yields unbiased estimates of the means and of the regression weights of the model under MCAR. It also does so under MAR, provided that the factors that influence missinginess are part of the model. However, correlations are biased to be greater/higher.\n\n\"Regression imputation, as well as its modern incarnations in machine learning is probably the most dangerous of all methods described here. We may be led to believe that were to do a good job by preserving the relations between the variables. In reality however, regression imputation artificially strengthens the relations in the data. Correlations are biased upwards. Variability is underestimated. Imputations are too good to be true. Regression imputation is a recipe for false positive and spurious relations.\"\n\n[Stochastic regression imputation]{.underline}\n\nStochastic regression imputation is a refinement of regression imputation that attempts to address correlation bias by adding noise back into the predictions of the missing values.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Impute Ozone from Solar.R by stochastic regression imputation using the mice package.\ndata_example <- airquality[, c(\"Ozone\", \"Solar.R\")]\n\n# Perform stochastic regression imputation\nimp <- mice(data_example, method = \"norm.nob\", m = 1, maxit = 1, seed = 1, print = FALSE)\n\n# Plotting missing vs observed values\nxyplot(imp, Ozone ~ Solar.R,\n       main = \"Missing vs Observed values for Ozone ~ Solar.R\",\n       auto.key = list(corner = c(0,1),\n                  points = TRUE, \n                  text = c(\"Observed\", \"Missing\"), col = c(\"steelblue\", \"red3\")))\n```\n\n::: {.cell-output-display}\n![](Project_1_Regression_files/figure-html/unnamed-chunk-62-1.png){width=672}\n:::\n:::\n\n\n\n\n\"The method = norm.nob argument requests a plain, non-Bayesian, stochastic regression method. This method first estimates the intercept, slope and residual variance under the linear model, then calculates the predicted value for each missing value, and adds a random draw from the residual to the prediction\".\n\nThis can create issues though, such as we have one imputed value that is negative! Which might not be plausible (such as in this case, there is no such thing as a negative ozone level).\n\nA more convenient solution is multiple imputation\n\n[Multiple Imputation]{.underline}\n\nMultiple imputation creates m \\> 1 complete data sets. The m results are then pooled into a final point estimate plus standard error. So each data set is identical in observed values, but differs in imputed values.\n\nWe then estimate the parameters of interest from each dataset. This is done by applying the analytic method you would have used if the dataset was complete in the first place (here, a regression). The results of the model on each dataset will differ because the data is different. These differences are caused by the uncertainty of what value to impute.\n\nThe last step is that these parameter estimates are then pooled together into a single value, and its variance estimated. The variance is assessed by combining the \"within-imputation variance with the extra variance caused by the missing data (between-imputation variance)\". So under the appropriate conditions the pooled estimates are unbiased. MI solves the problem of 'too small' SEs of other imputation methods we just covered.\n\nNow let's perform the multiple imputation on the airquality data set.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Perform multiple imputation on the airquality data set\nimp <- mice(airquality, seed = 1, m = 20, print = FALSE) # This line imputes the missing data 20 times\n\n# Fit a linear regression \nmodel_imp <- with(imp, lm(Ozone ~ Wind + Temp + Solar.R)) # You have to run the regression with \"with(imp, lm(etc))\n                  \nsummary(model_imp) # This runs a regression on each imputed dataset (so 20 different regressions)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 80  6\n   term        estimate std.error statistic  p.value  nobs\n   <chr>          <dbl>     <dbl>     <dbl>    <dbl> <int>\n 1 (Intercept) -60.7      17.9        -3.40 8.78e- 4   153\n 2 Wind         -2.95      0.514      -5.75 4.96e- 8   153\n 3 Temp          1.53      0.197       7.74 1.42e-12   153\n 4 Solar.R       0.0671    0.0184      3.64 3.70e- 4   153\n 5 (Intercept) -56.8      18.5        -3.07 2.56e- 3   153\n 6 Wind         -3.33      0.532      -6.25 4.04e- 9   153\n 7 Temp          1.56      0.205       7.58 3.49e-12   153\n 8 Solar.R       0.0554    0.0191      2.89 4.37e- 3   153\n 9 (Intercept) -74.0      18.7        -3.95 1.19e- 4   153\n10 Wind         -2.69      0.540      -4.99 1.67e- 6   153\n#  70 more rows\n```\n\n\n:::\n\n```{.r .cell-code}\nsummary(pool(model_imp)) # This pools together the parameters of all 20 regressions (statistic is Wald's test) into a single model.\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n         term     estimate   std.error statistic       df      p.value\n1 (Intercept) -65.87829658 23.09377412 -2.852643 69.97033 5.696702e-03\n2        Wind  -3.01897171  0.66252377 -4.556775 70.51194 2.125022e-05\n3        Temp   1.63483547  0.25107557  6.511328 75.99913 7.203792e-09\n4     Solar.R   0.05861581  0.02267832  2.584662 90.10797 1.135441e-02\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# mids workflow using pipes\nest3 <- airquality %>%\n  mice(seed = 1, print = FALSE, m = 20) %>%\n  with(lm(formula = Ozone ~ Wind + Temp + Solar.R)) %>%\n  pool()\nsummary(est3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n         term     estimate   std.error statistic       df      p.value\n1 (Intercept) -65.87829658 23.09377412 -2.852643 69.97033 5.696702e-03\n2        Wind  -3.01897171  0.66252377 -4.556775 70.51194 2.125022e-05\n3        Temp   1.63483547  0.25107557  6.511328 75.99913 7.203792e-09\n4     Solar.R   0.05861581  0.02267832  2.584662 90.10797 1.135441e-02\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Run a regression on the imputed airquality data set. \nimp <- mice(airquality, seed = 1, print = FALSE)\n\nfit <- with(imp, lm(Ozone ~ Solar.R))\n\n# Print out the estimates for the first and second data set\ncoef(fit$analyses[[1]])\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n(Intercept)     Solar.R \n 22.2963201   0.1057437 \n```\n\n\n:::\n\n```{.r .cell-code}\ncoef(fit$analyses[[2]])\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n(Intercept)     Solar.R \n 20.7891622   0.1140081 \n```\n\n\n:::\n:::\n\n\n\n\nNotice that the paremter estimates differ because of the uncertainty created by the missing data.\n\nApplying the standard pooling rules is done with\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nest <- pool(fit)\nsummary(est)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n         term   estimate  std.error statistic       df      p.value\n1 (Intercept) 21.5321395 5.67991951  3.790923 136.3544 0.0002245626\n2     Solar.R  0.1091546 0.02846503  3.834693 102.7169 0.0002170873\n```\n\n\n:::\n:::\n\n\n\n\nAny R expression produced by ~expression()~ can be used on the multiply imputed data...\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Extract residuals and fitted values\n\n# This gets you ALL the residuals for ALL the models/dataset\nimp_residuals <- sapply(model_imp[[4]], residuals)\n\n# Same thing but for the fitted\nimp_fitted <- sapply(model_imp[[4]],fitted)\n\n# Don't know where to go from here. Apparently you can take the average of all of these to get the average residuals, but it's not published anywhere.\n```\n:::\n\n\n\n\nWe can compare the results of our multiple imputation model with the listwise deletion model to see how different they came out.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_non_imp <- lm(Ozone ~ Wind + Temp + Solar.R, data = airquality, na.action = na.omit)\nsummary(model_non_imp)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = Ozone ~ Wind + Temp + Solar.R, data = airquality, \n    na.action = na.omit)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-40.485 -14.219  -3.551  10.097  95.619 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -64.34208   23.05472  -2.791  0.00623 ** \nWind         -3.33359    0.65441  -5.094 1.52e-06 ***\nTemp          1.65209    0.25353   6.516 2.42e-09 ***\nSolar.R       0.05982    0.02319   2.580  0.01124 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 21.18 on 107 degrees of freedom\n  (42 observations deleted due to missingness)\nMultiple R-squared:  0.6059,\tAdjusted R-squared:  0.5948 \nF-statistic: 54.83 on 3 and 107 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n\n```{.r .cell-code}\nsummary(pool(model_imp))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n         term     estimate   std.error statistic       df      p.value\n1 (Intercept) -65.87829658 23.09377412 -2.852643 69.97033 5.696702e-03\n2        Wind  -3.01897171  0.66252377 -4.556775 70.51194 2.125022e-05\n3        Temp   1.63483547  0.25107557  6.511328 75.99913 7.203792e-09\n4     Solar.R   0.05861581  0.02267832  2.584662 90.10797 1.135441e-02\n```\n\n\n:::\n:::\n\n\n\n\nOur regression using MI and the complete case (i.e. listwise deletion) data set are comparable!\n\nNote: what we are NOT doing in multiple imputation is taking an average of the imputed data and running a model on that as if its a single, complete data set. \"Researchers are often tempted to average the multiply imputed data, and analyze the averaged data as if it were complete. This method yields incorrect standard errors, confidence intervals and p-values, and thus should not be used if any form of statistical testing or uncertainty analysis is to be done on the imputed data. The reason is that the procedure ignores the between-imputation variability, and hence shares all the drawbacks of single imputation\".\n\nNote: It's recommended to *impute then transform*, because if you create variables based on other variables, there are relationships between those variables that MI doesn't account for (it might create combinations of variables that are unrealistic)\n\nYou can do this as\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Example of impute then transform method (i.e. how to add variables to an imputed dataset)\ndata_ex <- boys[, c(\"age\", \"hgt\", \"wgt\", \"hc\", \"reg\")]\nimp <- mice(data_ex, print = FALSE, seed = 1)\n \n# put the data in long format\nlong <- mice::complete(imp, \"long\", include = TRUE)\n\nlong$new_var <- with(long, 100 * wgt / hgt)\n\nimp.itt <- as.mids(long)\n```\n:::\n\n\n\n\n<a href=\"#Multiple_imputation\">Back to top of tabset</a>\n\n## Performing the Multiple Imputation\n\nNow that we have reviewed the background and gone through coding examples of multiple imputation, we are ready to perform it for the dataset for Project 1.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Pool the parameters of interest for a regression on the imputed values for attach change control as reference\nmodel_imp <- data %>%\n  mice(seed = 1, print = FALSE, m = 20) %>%\n  with(lm(attachchange ~ placebo + low + medium + high)) %>%\n  pool()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Number of logged events: 1158\n```\n\n\n:::\n:::\n\n\n\n\nNone of the p-values for the pooled regressions on the MI data are significant (p \\> 0.05).\n\nHow do those parameter estimates compare to the complete case analysis model?\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Compare coefficients to complete case analysis for attachment loss with control as reference category\nMI <- summary(model_imp)\nMI\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n         term   estimate  std.error  statistic       df     p.value\n1 (Intercept) -0.1853493 0.06920536 -2.6782512 65.79196 0.009337056\n2     placebo  0.1158204 0.09076258  1.2760813 91.37003 0.205161055\n3         low  0.1608502 0.10027097  1.6041549 59.48514 0.113976747\n4      medium  0.1612700 0.11393844  1.4154130 37.61151 0.165174849\n5        high  0.0930271 0.15435067  0.6026997 18.41109 0.554060413\n```\n\n\n:::\n\n```{.r .cell-code}\ncomplete_case <- summary(model_attach1)\ncomplete_case$coefficients\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n               Estimate Std. Error    t value     Pr(>|t|)\n(Intercept) -0.22169165 0.05590110 -3.9657832 0.0001392148\nplacebo      0.13461856 0.07905610  1.7028232 0.0917709125\nlow          0.20387790 0.08091650  2.5196086 0.0133650749\nmedium       0.21513551 0.08196711  2.6246567 0.0100625287\nhigh         0.05690063 0.08727557  0.6519652 0.5159498143\n```\n\n\n:::\n:::\n\n\n\n\nThey are drastically different, although still not significant.\n\nWe can also plot the imputed values for a sample of m dataset (not too many or it's hard to see!) in order to visually assess that our imputed values are comparabe to the observed values.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Plot imputed values for m = 5 data set\nimpy <- mice(data, seed = 1, print = FALSE, m = 5)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Number of logged events: 285\n```\n\n\n:::\n\n```{.r .cell-code}\nmodel_impy <- with(imp, attachchange ~ trtgroup)\n\n# This is cool, here's how I can plot the imputed values for attach change \nxyplot(impy, attachchange ~ trtgroup,\n       main = \"Example of Imputed Values for m = 5 Dataset\",\n       auto.key = list(space = \"right\",\n        points = TRUE, \n        text = c(\"Observed\", \"Missing\"), col = c(\"blue4\", \"red3\")))\n```\n\n::: {.cell-output-display}\n![](Project_1_Regression_files/figure-html/unnamed-chunk-72-1.png){width=672}\n:::\n:::\n\n\n\n\nWhat about for the same model but with placebo as the reference group?\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Pool the parameters of interest for a regression on the imputed values for attach change placebo as reference\nmodel_imp <- data %>%\n  mice(seed = 1, print = FALSE, m = 20) %>%\n  with(lm(attachchange ~ control + low + medium + high)) %>%\n  pool()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Number of logged events: 1158\n```\n\n\n:::\n\n```{.r .cell-code}\n# Compare coefficients for complete case analysis with attachment with placebo as reference category\nsummary(model_imp)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n         term    estimate  std.error  statistic       df   p.value\n1 (Intercept) -0.06952890 0.06697153 -1.0381859 75.81298 0.3024834\n2     control -0.11582043 0.09076258 -1.2760813 91.37003 0.2051611\n3         low  0.04502973 0.11027780  0.4083299 41.79227 0.6851161\n4      medium  0.04544952 0.11870540  0.3828766 33.28878 0.7042453\n5        high -0.02279333 0.16350701 -0.1394028 16.62753 0.8908064\n```\n\n\n:::\n\n```{.r .cell-code}\nsummary(model_attach2)$coefficients\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n               Estimate Std. Error    t value   Pr(>|t|)\n(Intercept) -0.08707309 0.05590110 -1.5576275 0.12254523\ncontrol     -0.13461856 0.07905610 -1.7028232 0.09177091\nlow          0.06925934 0.08091650  0.8559360 0.39412117\nmedium       0.08051695 0.08196711  0.9823081 0.32836700\nhigh        -0.07771793 0.08727557 -0.8904889 0.37538435\n```\n\n\n:::\n:::\n\n\n\n\nThe estimates are more comparable, although still pretty different. Still not significant with both methods.\n\nAnd finally for pocket depth change.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Pool the parameters of interest for a regression on the imputed values for pocket depth  change placebo as reference\nmodel_imp <- data %>%\n  mice(seed = 1, print = FALSE, m = 20) %>%\n  with(lm(pdchange ~ placebo + low + medium + high)) %>%\n  pool()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Number of logged events: 1158\n```\n\n\n:::\n\n```{.r .cell-code}\n# Compare coefficients with complete case analysis for pocket depth change\nMI <- summary(model_imp)\nMI\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n         term    estimate  std.error  statistic       df      p.value\n1 (Intercept) -0.33597389 0.06144440 -5.4679330 66.87925 7.296278e-07\n2     placebo -0.00982096 0.08061544 -0.1218248 92.74234 9.033013e-01\n3         low  0.11415425 0.07995226  1.4277802 96.05307 1.565984e-01\n4      medium  0.11260463 0.08900896  1.2650932 60.47912 2.106907e-01\n5        high -0.02704497 0.10936047 -0.2473011 30.38938 8.063384e-01\n```\n\n\n:::\n:::\n\n\n\n\nHere the models are a LOT more comparable, although still not significant. It seems we can conclude that missing values impact the attachment loss measurements more than pocket depth change.\n\nLet's finish by plotting the imputed values for pocket depth change\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Plot imputed values for m = 5 data set\nimpy <- mice(data, seed = 1, print = FALSE, m = 5)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Number of logged events: 285\n```\n\n\n:::\n\n```{.r .cell-code}\nmodel_impy <- with(imp, pdchange ~ trtgroup)\n\n# This is cool, here's how I can plot the imputed values for attach change \nxyplot(impy, pdchange ~ trtgroup,\n       main = \"Example of Imputed Values for m = 5 Dataset\",\n       auto.key = list(space = \"right\",\n        points = TRUE, \n        text = c(\"Observed\", \"Missing\"), col = c(\"blue4\", \"red3\")))\n```\n\n::: {.cell-output-display}\n![](Project_1_Regression_files/figure-html/unnamed-chunk-75-1.png){width=672}\n:::\n:::\n\n\n\n\nLooks pretty good.\n\n<a href=\"#Multiple_imputation\">Back to top of tabset</a>\n:::\n\n## Identify Outliers using Jackknife Residuals {#Jackknife}\n\nIn this section we will explore how our analysis would have changed if we identified outliers using the jackknife residuals.\n\n:::::: panel-tabset\n## Background\n\n\\*Note: This information is from BIOS 6602 Week 6 Lecture 10\n\nJackknife residuals are a type of residual used in regressions that allows us to detect outliers that have a significant impact on the regression coefficients of the model.\n\nJackknife residuals are calculated by systematically leaving out one observation (i) at a time from a dataset, fitting the regression model to the remaining dataset, and predicting the left out observation. The residual for each observation is then computed based on that prediction.\n\nThis process follows three step:\n\n-   Fit the regression model to the data, excluding observation (i)\n\n-   Use the model to predict the left-out observation (i)\n\n-   Calculate the jackknife residual as the difference between the actual value and that fitted value\n\nJackknife residuals are similar to standardized residuals. Standardized residuals are standardized by the standard error of the regression, and are primarily used for identifying outliers. Jackknife residuals on the other hand allow you to assess the leverage of individuals data points, to see how much they actually influence the model ([Source](https://stats.stackexchange.com/questions/204708/is-studentized-residuals-v-s-standardized-residuals-in-lm-model))\n\n<img src=\"Media/rstudent-jackknife.png\" width=\"95%\"/>\n\nJackknife residuals follow exactly a t(n-p-2) distribution, where approximately 5% of residuals are expected to exceed 1.96 in absolute value. Jackknife residuals make suspicious values more obvious compared to other residuals.\n\nDefinitions vary, but we generally consider a residual to be an outlier if the jackknife residual is +- 3.\n\nHowever, a potential outlier value may not actually have that dramatic of an impact on the model (which is what we are concerned that outliers will do). That is why we use jackknife residuals to investigate leverage and influence.\n\n-   Extreme X values can have *high leverage*.\n-   Extreme X values can have *high influence*\n-   A high-leverage point becomes an *influential* point if its Y value doesn't follow the pattern of the rest of the data (i.e. is too low or too high)\n\n<img src=\"Media/leverage_influence.png\" width=\"80%\"/>\n\nAn observation is influential if removing it substantially changes the estimate of the coefficients for that model. We use five measurements to assess influence: Jackknife residuals, Leverage, Cook's Distance (Cook's D), DFFITS, and DFBETAS.\n\n-   Leverage: Measures how far a measurement deviates from the mean. You want to examine values greater than 2(p+1)/n\n\n-   Cook's D: Measures the influence of an observation on regression predictions. You want to examine observations with d_i \\> 1.0\n\n-   DFFITS: Measures the influence of an observation on regression predictions (related to Cook's D). You want to examine observations outside the range of +-2(sqrt(p+10n)). If h_i is near zero, then that observation has little effect.\n\n-   DDFBETAS: Measure the influence of an observation on \\*individual\\* coefficient estimates. You want to examine estimates outside the range of +- 2/sqrt(n). A large DFBETA for variable k indicates that the i-th observation has a sizeable impact on the k-th regression coefficient.\n\nNote: p = the number of variables in the model\n\n<img src=\"Media/jackknife_cheet_sheet.png\" width=\"90%\"/>\n\n<a href=\"#Jackknife\">Back to top of tabset</a>\n\n## Identifying Outliers\n\n[This website](https://rpubs.com/liamroel13/stat312_mod4_les15) was used for coding information for this section, alongside the notes from BIOS 6602\n\nNow that we have covered the background for jackknife residuals, we can apply that knowledge to assess for outliers in our dataset for this project!\n\n::::: panel-tabset\n## Attachment Loss\n\n::: panel-tabset\n## Set up\n\nRecall that we computed the jackknife residuals earlier.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Calculate the jackknife residuals of model_attach\njackknife_residuals_attach <- rstudent(model_attach1)\n\n# Calculate the jackknife residuals of model_pd\njackknife_residuals_pd <- rstudent(model_pd)\n```\n:::\n\n\n\n\nLet's generate a table so we can handily compare: ID, jackknife residual, leverage (diagonal hat), DFFITS, and DFBETAs for each participant.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Get leverage values (hat values)\nhat_values_attach <- hatvalues(model_attach1)\n\n# Get Cook's D values\ncooks_d_attach <- cooks.distance(model_attach1)\n\n# Get the DFFITS values\ndffits_attach <- dffits(model_attach1)\n\n# Get the DFBETAS\ndfbetas_attach <- dfbetas(model_attach1)\n\n# Make a table with ID and all diagnostic values\ndiagnostics_attach <- data.frame(id = data_missing$id, jackknife_residuals = jackknife_residuals_attach, leverage = hat_values_attach, cooks_D = cooks_d_attach, dffits = dffits_attach, dfbetas = dfbetas_attach)\n\nkable(head(diagnostics_attach), caption = \"Diagnostics for Attachment Loss Change Model\", format = \"html\") %>%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\"))\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table table-striped table-hover table-condensed\" style=\"margin-left: auto; margin-right: auto;\">\n<caption>Diagnostics for Attachment Loss Change Model</caption>\n <thead>\n  <tr>\n   <th style=\"text-align:right;\"> id </th>\n   <th style=\"text-align:right;\"> jackknife_residuals </th>\n   <th style=\"text-align:right;\"> leverage </th>\n   <th style=\"text-align:right;\"> cooks_D </th>\n   <th style=\"text-align:right;\"> dffits </th>\n   <th style=\"text-align:right;\"> dfbetas..Intercept. </th>\n   <th style=\"text-align:right;\"> dfbetas.placebo </th>\n   <th style=\"text-align:right;\"> dfbetas.low </th>\n   <th style=\"text-align:right;\"> dfbetas.medium </th>\n   <th style=\"text-align:right;\"> dfbetas.high </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:right;\"> 101 </td>\n   <td style=\"text-align:right;\"> 0.5800959 </td>\n   <td style=\"text-align:right;\"> 0.0500000 </td>\n   <td style=\"text-align:right;\"> 0.0035664 </td>\n   <td style=\"text-align:right;\"> 0.1330831 </td>\n   <td style=\"text-align:right;\"> 0.0000000 </td>\n   <td style=\"text-align:right;\"> 0.0000000 </td>\n   <td style=\"text-align:right;\"> 0.0000000 </td>\n   <td style=\"text-align:right;\"> 0.0973313 </td>\n   <td style=\"text-align:right;\"> 0.0000000 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 103 </td>\n   <td style=\"text-align:right;\"> 1.5996714 </td>\n   <td style=\"text-align:right;\"> 0.0434783 </td>\n   <td style=\"text-align:right;\"> 0.0228989 </td>\n   <td style=\"text-align:right;\"> 0.3410511 </td>\n   <td style=\"text-align:right;\"> 0.3410511 </td>\n   <td style=\"text-align:right;\"> -0.2411595 </td>\n   <td style=\"text-align:right;\"> -0.2356149 </td>\n   <td style=\"text-align:right;\"> -0.2325949 </td>\n   <td style=\"text-align:right;\"> -0.2184475 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 104 </td>\n   <td style=\"text-align:right;\"> 1.4044559 </td>\n   <td style=\"text-align:right;\"> 0.0476190 </td>\n   <td style=\"text-align:right;\"> 0.0195311 </td>\n   <td style=\"text-align:right;\"> 0.3140459 </td>\n   <td style=\"text-align:right;\"> 0.0000000 </td>\n   <td style=\"text-align:right;\"> 0.0000000 </td>\n   <td style=\"text-align:right;\"> 0.2270548 </td>\n   <td style=\"text-align:right;\"> 0.0000000 </td>\n   <td style=\"text-align:right;\"> 0.0000000 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 105 </td>\n   <td style=\"text-align:right;\"> -0.8928811 </td>\n   <td style=\"text-align:right;\"> 0.0434783 </td>\n   <td style=\"text-align:right;\"> 0.0072626 </td>\n   <td style=\"text-align:right;\"> -0.1903629 </td>\n   <td style=\"text-align:right;\"> 0.0000000 </td>\n   <td style=\"text-align:right;\"> -0.1346069 </td>\n   <td style=\"text-align:right;\"> 0.0000000 </td>\n   <td style=\"text-align:right;\"> 0.0000000 </td>\n   <td style=\"text-align:right;\"> 0.0000000 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 106 </td>\n   <td style=\"text-align:right;\"> -1.6995151 </td>\n   <td style=\"text-align:right;\"> 0.0500000 </td>\n   <td style=\"text-align:right;\"> 0.0298289 </td>\n   <td style=\"text-align:right;\"> -0.3898955 </td>\n   <td style=\"text-align:right;\"> 0.0000000 </td>\n   <td style=\"text-align:right;\"> 0.0000000 </td>\n   <td style=\"text-align:right;\"> 0.0000000 </td>\n   <td style=\"text-align:right;\"> -0.2851530 </td>\n   <td style=\"text-align:right;\"> 0.0000000 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 107 </td>\n   <td style=\"text-align:right;\"> -2.7107885 </td>\n   <td style=\"text-align:right;\"> 0.0476190 </td>\n   <td style=\"text-align:right;\"> 0.0690131 </td>\n   <td style=\"text-align:right;\"> -0.6061507 </td>\n   <td style=\"text-align:right;\"> 0.0000000 </td>\n   <td style=\"text-align:right;\"> 0.0000000 </td>\n   <td style=\"text-align:right;\"> -0.4382463 </td>\n   <td style=\"text-align:right;\"> 0.0000000 </td>\n   <td style=\"text-align:right;\"> 0.0000000 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n\n\n<a href=\"#Jackknife\">Back to top of tabset</a>\n\n## Jackknife Residuals\n\nWe can start by making a plot of the jackknife residuals to assess if there are outliers \\< -3 or \\> 3 SDs).\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Plot jackknife outiers and label any values that are > 3 or < -3.\nggplot(data_missing, aes(x = id, y = jackknife_residuals_attach)) + geom_point() + \n  geom_hline(yintercept = c(-3,0,3)) + ylim(-4,4) + \n  labs(y = \"Jackknife Residuals\",\n       title = \"Jackknife Residuals vs ID\") + \n  geom_text(aes(label = ifelse(jackknife_residuals_attach > 3 | jackknife_residuals_attach < -3, id, '')), \n            vjust = -1)\n```\n\n::: {.cell-output-display}\n![](Project_1_Regression_files/figure-html/unnamed-chunk-78-1.png){width=672}\n:::\n:::\n\n\n\n\nParticipant 168 is a potential outlier based on the residual value.\n\nLet's look at this participant more closely.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Examine participant 168 to assess what values could make them an outlier\ndata_missing[data_missing$id == 168,]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n    id trtgroup gender race      age smoker sites attachbase attach1year\n58 168        5      2    5 54.20397      0   168   5.089286    4.041667\n     pdbase  pd1year attachchange   pdchange placebo control low medium high\n58 3.410714 2.904762    -1.047619 -0.5059524       0       0   0      0    1\n   trt trt3groups\n58   1          3\n```\n\n\n:::\n\n```{.r .cell-code}\n# Sort our dataset by descending order of attachment at baseline to see if participant 168 is the highest\n\nkable(head(data_missing[order(-data_missing$attachbase),]), caption = \"Diagnostics for Attachment Loss Change Model\", format = \"html\") %>%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\"))\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table table-striped table-hover table-condensed\" style=\"margin-left: auto; margin-right: auto;\">\n<caption>Diagnostics for Attachment Loss Change Model</caption>\n <thead>\n  <tr>\n   <th style=\"text-align:left;\">   </th>\n   <th style=\"text-align:right;\"> id </th>\n   <th style=\"text-align:right;\"> trtgroup </th>\n   <th style=\"text-align:right;\"> gender </th>\n   <th style=\"text-align:right;\"> race </th>\n   <th style=\"text-align:right;\"> age </th>\n   <th style=\"text-align:right;\"> smoker </th>\n   <th style=\"text-align:right;\"> sites </th>\n   <th style=\"text-align:right;\"> attachbase </th>\n   <th style=\"text-align:right;\"> attach1year </th>\n   <th style=\"text-align:right;\"> pdbase </th>\n   <th style=\"text-align:right;\"> pd1year </th>\n   <th style=\"text-align:right;\"> attachchange </th>\n   <th style=\"text-align:right;\"> pdchange </th>\n   <th style=\"text-align:right;\"> placebo </th>\n   <th style=\"text-align:right;\"> control </th>\n   <th style=\"text-align:right;\"> low </th>\n   <th style=\"text-align:right;\"> medium </th>\n   <th style=\"text-align:right;\"> high </th>\n   <th style=\"text-align:right;\"> trt </th>\n   <th style=\"text-align:right;\"> trt3groups </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> 58 </td>\n   <td style=\"text-align:right;\"> 168 </td>\n   <td style=\"text-align:right;\"> 5 </td>\n   <td style=\"text-align:right;\"> 2 </td>\n   <td style=\"text-align:right;\"> 5 </td>\n   <td style=\"text-align:right;\"> 54.20397 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 168 </td>\n   <td style=\"text-align:right;\"> 5.089286 </td>\n   <td style=\"text-align:right;\"> 4.041667 </td>\n   <td style=\"text-align:right;\"> 3.410714 </td>\n   <td style=\"text-align:right;\"> 2.904762 </td>\n   <td style=\"text-align:right;\"> -1.0476190 </td>\n   <td style=\"text-align:right;\"> -0.5059524 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 3 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> 3 </td>\n   <td style=\"text-align:right;\"> 104 </td>\n   <td style=\"text-align:right;\"> 3 </td>\n   <td style=\"text-align:right;\"> 2 </td>\n   <td style=\"text-align:right;\"> 5 </td>\n   <td style=\"text-align:right;\"> 55.17864 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 138 </td>\n   <td style=\"text-align:right;\"> 4.956522 </td>\n   <td style=\"text-align:right;\"> 5.304348 </td>\n   <td style=\"text-align:right;\"> 5.217391 </td>\n   <td style=\"text-align:right;\"> 4.891304 </td>\n   <td style=\"text-align:right;\"> 0.3478261 </td>\n   <td style=\"text-align:right;\"> -0.3260870 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 3 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> 40 </td>\n   <td style=\"text-align:right;\"> 144 </td>\n   <td style=\"text-align:right;\"> 2 </td>\n   <td style=\"text-align:right;\"> 2 </td>\n   <td style=\"text-align:right;\"> 2 </td>\n   <td style=\"text-align:right;\"> 49.07598 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 126 </td>\n   <td style=\"text-align:right;\"> 4.388889 </td>\n   <td style=\"text-align:right;\"> 3.488000 </td>\n   <td style=\"text-align:right;\"> 3.666667 </td>\n   <td style=\"text-align:right;\"> 3.230159 </td>\n   <td style=\"text-align:right;\"> -0.9008889 </td>\n   <td style=\"text-align:right;\"> -0.4365079 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 2 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> 102 </td>\n   <td style=\"text-align:right;\"> 269 </td>\n   <td style=\"text-align:right;\"> 3 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 5 </td>\n   <td style=\"text-align:right;\"> 64.90075 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 162 </td>\n   <td style=\"text-align:right;\"> 4.080247 </td>\n   <td style=\"text-align:right;\"> 3.685185 </td>\n   <td style=\"text-align:right;\"> 3.370370 </td>\n   <td style=\"text-align:right;\"> 3.265432 </td>\n   <td style=\"text-align:right;\"> -0.3950617 </td>\n   <td style=\"text-align:right;\"> -0.1049383 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 3 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> 18 </td>\n   <td style=\"text-align:right;\"> 121 </td>\n   <td style=\"text-align:right;\"> 4 </td>\n   <td style=\"text-align:right;\"> 2 </td>\n   <td style=\"text-align:right;\"> 5 </td>\n   <td style=\"text-align:right;\"> 54.49692 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 138 </td>\n   <td style=\"text-align:right;\"> 4.014493 </td>\n   <td style=\"text-align:right;\"> 3.825000 </td>\n   <td style=\"text-align:right;\"> 3.608696 </td>\n   <td style=\"text-align:right;\"> 3.783333 </td>\n   <td style=\"text-align:right;\"> -0.1894928 </td>\n   <td style=\"text-align:right;\"> 0.1746377 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 3 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> 17 </td>\n   <td style=\"text-align:right;\"> 120 </td>\n   <td style=\"text-align:right;\"> 3 </td>\n   <td style=\"text-align:right;\"> 2 </td>\n   <td style=\"text-align:right;\"> 5 </td>\n   <td style=\"text-align:right;\"> 41.83710 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 150 </td>\n   <td style=\"text-align:right;\"> 3.886667 </td>\n   <td style=\"text-align:right;\"> 3.920000 </td>\n   <td style=\"text-align:right;\"> 4.200000 </td>\n   <td style=\"text-align:right;\"> 3.880000 </td>\n   <td style=\"text-align:right;\"> 0.0333333 </td>\n   <td style=\"text-align:right;\"> -0.3200000 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 3 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n\n\nWe have confirmed that participant 168 has the largest values for attachment loss at baseline (5.09). If that's the case however, participant 3 is not far behind them (4.96) (who incidentally has the highest baseline pocket depth measurement), followed by participant 40 a large amount lower (4.39).\n\nWe can also (apparently) run a test statistic to test if we have an outlier using the 'car' package\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\noutlierTest(model_attach1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nNo Studentized residuals with Bonferroni p < 0.05\nLargest |rstudent|:\n    rstudent unadjusted p-value Bonferroni p\n58 -3.602896         0.00049863     0.051359\n```\n\n\n:::\n:::\n\n\n\n\nThis is a test of a hypothesis that we do not have an outlier. We reject that hypothesis (p \\< 0.05) so we have an outlier (I think).\n\n<a href=\"#Jackknife\">Back to top of tabset</a>\n\n## Leverage\n\n**Leverage** is a measure of geometric distance of an observation's predictor point from the center point of the predictor space. In other words, leverage is a measurement of how far that observation deviates from the mean of that variable. High leverage observations have the potential to be very influential, but they are not *necessarily* influential. It's possible for a high leverage point to not be influential, but very difficult for a low leverage point to be influential.\n\nLeverage is calculated as\n\n\\<img src=\"Media/leverage.png\" width=\"90%\\>\n\nIf X \\_i is close to Xbar, then h_i is small (i.e. the i-th point has low leverage).\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Caculate cutoff for leverage\ncutoff_leverage <- 2*((4+1)/103)\ncutoff_leverage\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.09708738\n```\n\n\n:::\n\n```{.r .cell-code}\n# Make leverage plot in ggplot. Set a horizontal line at the cutoff point, and label ID's for participants that pass that line\nggplot(diagnostics_attach, aes(x = id, y = leverage)) +\n  geom_point() + \n  geom_hline(yintercept = cutoff_leverage) +\n  labs(title = \"Leverage Plot with Cutoff Line\",\n       x = \"ID\",\n       y = \"Hat Values\") + \n  geom_text(aes(label = ifelse(leverage > cutoff_leverage, id, \"\")), vjust = -0.5)\n```\n\n::: {.cell-output-display}\n![](Project_1_Regression_files/figure-html/unnamed-chunk-81-1.png){width=672}\n:::\n:::\n\n\n\n\nNone of our data points pass the cutoff point for leverage/hat values. I think this may be because the IV is categorical which changes the pattern and interpretation. Will check with a professor.\n\n<a href=\"#Jackknife\">Back to top of tabset</a>\n\n## Cook's D\n\nCook's D combines information about the residuals and leverage into a single value. A higher Cook's D value signifies that the data point woud greatly change the regression coefficients, and is therefore influential and may impact the model's accuracy.\n\nApparently we actually want to look at a cutoff of Cook's D as 4/(n-p-1). Let's calculate that and store it.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Calculate cutoff for Cook's D\ncutoff_d <- 4/((nrow(data_missing) - length(model_attach1$coefficients) - 1))\ncutoff_d\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.04123711\n```\n\n\n:::\n\n```{.r .cell-code}\n# Make Cook's D plot in ggplot. Set a horizontal line at the cutoff point, and label ID's for participants that pass that line\nggplot(diagnostics_attach, aes(x = id, y = cooks_D)) +\n  geom_point() + \n  geom_hline(yintercept = cutoff_d) +\n  labs(title = \"Cook's D with Cutoff Line\",\n       x = \"ID\",\n       y = \"Cook's D\") + \n  geom_text(aes(label = ifelse(cooks_D > cutoff_d, id, \"\")), vjust = -0.5)\n```\n\n::: {.cell-output-display}\n![](Project_1_Regression_files/figure-html/unnamed-chunk-82-1.png){width=672}\n:::\n:::\n\n\n\n\nWe see that participants 107, 144, 146 and 168 are past the cutoff for Cook's D. As when looking at the residuals plot, participant 168 is drastically different compared to the other potential outliers.\n\n<a href=\"#Jackknife\">Back to top of tabset</a>\n\n## DFFITS\n\nNow we will examine DFFITS (Difference in fits). DFFITS is a measure used to identify influential data points in a regression analysis. It quantifes how much the predicted values (Y) of a model change when that particular observation is left out from the analysis. So in this case it is the difference in attachment loss change if we take out observation i from the analysis.\n\nLet's make a plot where we label the values that are past the cutoff.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Calculate the cutoff for DFFITS\ncutoff_dffits <- 2 * sqrt((4+1)/103)\ncutoff_dffits\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.4406526\n```\n\n\n:::\n\n```{.r .cell-code}\n# Make DFFITS plot in ggplot. Set a horizontal line at the cutoff points, and label ID's for participants that pass those lines.\nggplot(diagnostics_attach, aes(x = id, y = dffits)) + \n  geom_point() +\n  geom_hline(yintercept = c(cutoff_dffits, -cutoff_dffits)) + \n  geom_text(aes(label = ifelse(dffits < -cutoff_dffits | dffits > cutoff_dffits, id, \"\")), vjust = -0.5) +\n  labs(title = \"DFFITS by ID\",\n       x = \"ID\",\n       y = \"DFFITS\")\n```\n\n::: {.cell-output-display}\n![](Project_1_Regression_files/figure-html/unnamed-chunk-83-1.png){width=672}\n:::\n:::\n\n\n\n\nAgain we are flagging participants 107, 113, 144, 168. The new addition is 113, who is right on the cutoff line. Again participant 168 is the most egregious, and the other potential outliers are a lot closer to the cutoff.\n\n<a href=\"#Jackknife\">Back to top of tabset</a>\n\n## DFBETAS\n\nNow we can look at the DFBETAs (Difference in Betas). DFBETAS quantify how much the beta coefficients for each variable in the model change when you exclude observation i from the analysis.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Calculate the cutoff for DFBETAS\ncutoff_dfbetas <- 2/sqrt(103)\ncutoff_dfbetas\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.1970659\n```\n\n\n:::\n\n```{.r .cell-code}\n# Make DFBETAS plot in ggplot. Set a horizontal line at the cutoff points, and label ID's for participants that pass those lines.\n\n# For placebo group\nggplot(diagnostics_attach, aes(x = id, y = dfbetas.placebo)) + \n  geom_point() +\n  geom_hline(yintercept = c(cutoff_dfbetas, -cutoff_dfbetas)) + \n  geom_text(aes(label = ifelse(dfbetas.placebo < -cutoff_dfbetas | dfbetas.placebo > cutoff_dfbetas, id, \"\")), vjust = -0.5) +\n  labs(title = \"DFBETAS by ID for Placebo\",\n       x = \"ID\",\n       y = \"DFBETA\")\n```\n\n::: {.cell-output-display}\n![](Project_1_Regression_files/figure-html/unnamed-chunk-84-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# For low dose group\nggplot(diagnostics_attach, aes(x = id, y = dfbetas.low)) + \n  geom_point() +\n  geom_hline(yintercept = c(cutoff_dfbetas, -cutoff_dfbetas)) + \n  geom_text(aes(label = ifelse(dfbetas.low < -cutoff_dfbetas | dfbetas.low > cutoff_dfbetas, id, \"\")), vjust = -0.5) +\n  labs(title = \"DFBETAS by ID for Low Dose\",\n       x = \"ID\",\n       y = \"DFBETA\")\n```\n\n::: {.cell-output-display}\n![](Project_1_Regression_files/figure-html/unnamed-chunk-84-2.png){width=672}\n:::\n\n```{.r .cell-code}\n# For medium dose group\nggplot(diagnostics_attach, aes(x = id, y = dfbetas.medium)) + \n  geom_point() +\n  geom_hline(yintercept = c(cutoff_dfbetas, -cutoff_dfbetas)) + \n  geom_text(aes(label = ifelse(dfbetas.medium < -cutoff_dfbetas | dfbetas.medium > cutoff_dfbetas, id, \"\")), vjust = -0.5) +\n  labs(title = \"DFBETAS by ID for Medium Dose\",\n       x = \"ID\",\n       y = \"DFBETA\")\n```\n\n::: {.cell-output-display}\n![](Project_1_Regression_files/figure-html/unnamed-chunk-84-3.png){width=672}\n:::\n\n```{.r .cell-code}\n# For high does group\nggplot(diagnostics_attach, aes(x = id, y = dfbetas.high)) + \n  geom_point() +\n  geom_hline(yintercept = c(cutoff_dfbetas, -cutoff_dfbetas)) + \n  geom_text(aes(label = ifelse(dfbetas.high < -cutoff_dfbetas | dfbetas.high > cutoff_dfbetas, id, \"\")), vjust = -0.5) +\n  labs(title = \"DFBETAS by ID for High Dose\",\n       x = \"ID\",\n       y = \"DFBETA\")\n```\n\n::: {.cell-output-display}\n![](Project_1_Regression_files/figure-html/unnamed-chunk-84-4.png){width=672}\n:::\n:::\n\n\n\n\nThat's a lot of points that are past the cutoff! I think for these plots it might matter less if we're pass that point. I think the idea here is we get a fine tune look at, for instance, how participant 168 drastically changes the beta for high, compared to how much any other point changes the betas. Other possible values of concern are 144 and 146. All the other data points are still roughly around the cutoff of .20, but 168 is around .7!\n\n<a href=\"#Jackknife\">Back to top of tabset</a>\n\n## Additional Plots\n\nThe car package in R has some handy plots that we can use to help us with diagnosing outliers.\n\nThe first is the influence plot, which essentially combines the jackknife residuals plot with the Cook's D plot.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Influence plot\ninfluencePlot(model_attach1, main = \"Influence Plot\",\n              sub = \"Circle size is proportional to Cook's Distance\")\n```\n\n::: {.cell-output-display}\n![](Project_1_Regression_files/figure-html/unnamed-chunk-85-1.png){width=672}\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      StudRes        Hat       CookD\n6  -2.7107885 0.04761905 0.069013121\n8  -0.5238664 0.06250000 0.003686439\n12  1.4073671 0.06250000 0.026147438\n58 -3.6028957 0.06250000 0.154223694\n```\n\n\n:::\n:::\n\n\n\n\nUsing this plot, we can see that obseration 58 (ID 168) has the largest residual value and Cook's D value by a large margin!\n\nAdditionally, a single line of code provides us with some useful diagnostic plots.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# infIndexPlot gives us a series of plots that we need to investigate influence points\ninfIndexPlot(model_attach1)\n```\n\n::: {.cell-output-display}\n![](Project_1_Regression_files/figure-html/unnamed-chunk-86-1.png){width=672}\n:::\n:::\n\n\n\n\nThe first plot is the Cook's D plot, the second plot is the studentized residuals plot, and the leverage/hat values plot, all of which we plotted before. New is the third plot which is the Bonferroni P-value plot.\n\nBased on these plots we can see pretty readily that participant 168 is a true outlier. That is, they are a point that has high leverage AND influence in this model.\n\n<a href=\"#Jackknife\">Back to top of tabset</a>\n\n## Final Look and Conclusion\n\nWe saw that participant 168 is the most egregious offender, and participants 107, 144, 146, and MAYBE 113 are flagging past our different cutoff points.\n\nLet's take one last look at the table of these participants\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Pretty print diagnostics table of potential outliers\nkable(diagnostics_attach[diagnostics_attach$id %in% c(107,113,144,146,168),], caption = \"Correlation Matrix\", format = \"html\") %>%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\"))\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table table-striped table-hover table-condensed\" style=\"margin-left: auto; margin-right: auto;\">\n<caption>Correlation Matrix</caption>\n <thead>\n  <tr>\n   <th style=\"text-align:left;\">   </th>\n   <th style=\"text-align:right;\"> id </th>\n   <th style=\"text-align:right;\"> jackknife_residuals </th>\n   <th style=\"text-align:right;\"> leverage </th>\n   <th style=\"text-align:right;\"> cooks_D </th>\n   <th style=\"text-align:right;\"> dffits </th>\n   <th style=\"text-align:right;\"> dfbetas..Intercept. </th>\n   <th style=\"text-align:right;\"> dfbetas.placebo </th>\n   <th style=\"text-align:right;\"> dfbetas.low </th>\n   <th style=\"text-align:right;\"> dfbetas.medium </th>\n   <th style=\"text-align:right;\"> dfbetas.high </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> 6 </td>\n   <td style=\"text-align:right;\"> 107 </td>\n   <td style=\"text-align:right;\"> -2.710789 </td>\n   <td style=\"text-align:right;\"> 0.0476190 </td>\n   <td style=\"text-align:right;\"> 0.0690131 </td>\n   <td style=\"text-align:right;\"> -0.6061507 </td>\n   <td style=\"text-align:right;\"> 0.0000000 </td>\n   <td style=\"text-align:right;\"> 0.0000000 </td>\n   <td style=\"text-align:right;\"> -0.4382463 </td>\n   <td style=\"text-align:right;\"> 0.0000000 </td>\n   <td style=\"text-align:right;\"> 0.0000000 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> 11 </td>\n   <td style=\"text-align:right;\"> 113 </td>\n   <td style=\"text-align:right;\"> 2.092587 </td>\n   <td style=\"text-align:right;\"> 0.0434783 </td>\n   <td style=\"text-align:right;\"> 0.0384816 </td>\n   <td style=\"text-align:right;\"> 0.4461411 </td>\n   <td style=\"text-align:right;\"> 0.0000000 </td>\n   <td style=\"text-align:right;\"> 0.3154694 </td>\n   <td style=\"text-align:right;\"> 0.0000000 </td>\n   <td style=\"text-align:right;\"> 0.0000000 </td>\n   <td style=\"text-align:right;\"> 0.0000000 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> 40 </td>\n   <td style=\"text-align:right;\"> 144 </td>\n   <td style=\"text-align:right;\"> -2.670168 </td>\n   <td style=\"text-align:right;\"> 0.0434783 </td>\n   <td style=\"text-align:right;\"> 0.0610008 </td>\n   <td style=\"text-align:right;\"> -0.5692818 </td>\n   <td style=\"text-align:right;\"> -0.5692818 </td>\n   <td style=\"text-align:right;\"> 0.4025431 </td>\n   <td style=\"text-align:right;\"> 0.3932880 </td>\n   <td style=\"text-align:right;\"> 0.3882470 </td>\n   <td style=\"text-align:right;\"> 0.3646322 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> 42 </td>\n   <td style=\"text-align:right;\"> 146 </td>\n   <td style=\"text-align:right;\"> -2.291912 </td>\n   <td style=\"text-align:right;\"> 0.0434783 </td>\n   <td style=\"text-align:right;\"> 0.0457672 </td>\n   <td style=\"text-align:right;\"> -0.4886373 </td>\n   <td style=\"text-align:right;\"> -0.4886373 </td>\n   <td style=\"text-align:right;\"> 0.3455188 </td>\n   <td style=\"text-align:right;\"> 0.3375748 </td>\n   <td style=\"text-align:right;\"> 0.3332479 </td>\n   <td style=\"text-align:right;\"> 0.3129784 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> 58 </td>\n   <td style=\"text-align:right;\"> 168 </td>\n   <td style=\"text-align:right;\"> -3.602896 </td>\n   <td style=\"text-align:right;\"> 0.0625000 </td>\n   <td style=\"text-align:right;\"> 0.1542237 </td>\n   <td style=\"text-align:right;\"> -0.9302637 </td>\n   <td style=\"text-align:right;\"> 0.0000000 </td>\n   <td style=\"text-align:right;\"> 0.0000000 </td>\n   <td style=\"text-align:right;\"> 0.0000000 </td>\n   <td style=\"text-align:right;\"> 0.0000000 </td>\n   <td style=\"text-align:right;\"> -0.7143938 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n\n\nAcross the board we can see that participant 168 has worse values for almost everything, in particular the jackknife residual, Cook's D, and DFFITS values. I can confidently conclude that we should remove participant 168 as an outlier, and feel justified in keeping participants 107, 113, 144, and 146 based on the closeness to the rest of the data points on the previous plots.\n:::\n\n## Pocket Depth\n\n::: panel-tabset\n## Set Up\n\nLet's generate a table so we can handily compare: ID, jackknife residual, leverage (diagonal hat), DFFITS, and DFBETAs for each participant.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Get leverage values (hat values)\nhat_values_pd <- hatvalues(model_pd)\n\n# Get Cook's D values\ncooks_d_pd <- cooks.distance(model_pd)\n\n# Get the DFFITS values\ndffits_pd <- dffits(model_pd)\n\n# Get the DFBETAS\ndfbetas_pd <- dfbetas(model_pd)\n\n# Make a table with ID and all diagnostic values\ndiagnostics_pd <- data.frame(id = data_missing$id, jackknife_residuals = jackknife_residuals_pd, leverage = hat_values_pd, cooks_D = cooks_d_pd, dffits = dffits_pd, dfbetas = dfbetas_pd)\n\nkable(head(diagnostics_pd), caption = \"Diagnostics for Attachment Loss Change Model\", format = \"html\") %>%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\"))\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table table-striped table-hover table-condensed\" style=\"margin-left: auto; margin-right: auto;\">\n<caption>Diagnostics for Attachment Loss Change Model</caption>\n <thead>\n  <tr>\n   <th style=\"text-align:right;\"> id </th>\n   <th style=\"text-align:right;\"> jackknife_residuals </th>\n   <th style=\"text-align:right;\"> leverage </th>\n   <th style=\"text-align:right;\"> cooks_D </th>\n   <th style=\"text-align:right;\"> dffits </th>\n   <th style=\"text-align:right;\"> dfbetas..Intercept. </th>\n   <th style=\"text-align:right;\"> dfbetas.placebo </th>\n   <th style=\"text-align:right;\"> dfbetas.low </th>\n   <th style=\"text-align:right;\"> dfbetas.medium </th>\n   <th style=\"text-align:right;\"> dfbetas.high </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:right;\"> 101 </td>\n   <td style=\"text-align:right;\"> 1.4284091 </td>\n   <td style=\"text-align:right;\"> 0.0500000 </td>\n   <td style=\"text-align:right;\"> 0.0212518 </td>\n   <td style=\"text-align:right;\"> 0.3276995 </td>\n   <td style=\"text-align:right;\"> 0.0000000 </td>\n   <td style=\"text-align:right;\"> 0.0000000 </td>\n   <td style=\"text-align:right;\"> 0.0000000 </td>\n   <td style=\"text-align:right;\"> 0.2396655 </td>\n   <td style=\"text-align:right;\"> 0.0000000 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 103 </td>\n   <td style=\"text-align:right;\"> 1.3517762 </td>\n   <td style=\"text-align:right;\"> 0.0434783 </td>\n   <td style=\"text-align:right;\"> 0.0164727 </td>\n   <td style=\"text-align:right;\"> 0.2881997 </td>\n   <td style=\"text-align:right;\"> 0.2881997 </td>\n   <td style=\"text-align:right;\"> -0.2037879 </td>\n   <td style=\"text-align:right;\"> -0.1991025 </td>\n   <td style=\"text-align:right;\"> -0.1965505 </td>\n   <td style=\"text-align:right;\"> -0.1845955 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 104 </td>\n   <td style=\"text-align:right;\"> -0.4668576 </td>\n   <td style=\"text-align:right;\"> 0.0476190 </td>\n   <td style=\"text-align:right;\"> 0.0021971 </td>\n   <td style=\"text-align:right;\"> -0.1043925 </td>\n   <td style=\"text-align:right;\"> 0.0000000 </td>\n   <td style=\"text-align:right;\"> 0.0000000 </td>\n   <td style=\"text-align:right;\"> -0.0754757 </td>\n   <td style=\"text-align:right;\"> 0.0000000 </td>\n   <td style=\"text-align:right;\"> 0.0000000 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 105 </td>\n   <td style=\"text-align:right;\"> -0.4451251 </td>\n   <td style=\"text-align:right;\"> 0.0434783 </td>\n   <td style=\"text-align:right;\"> 0.0018161 </td>\n   <td style=\"text-align:right;\"> -0.0949010 </td>\n   <td style=\"text-align:right;\"> 0.0000000 </td>\n   <td style=\"text-align:right;\"> -0.0671051 </td>\n   <td style=\"text-align:right;\"> 0.0000000 </td>\n   <td style=\"text-align:right;\"> 0.0000000 </td>\n   <td style=\"text-align:right;\"> 0.0000000 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 106 </td>\n   <td style=\"text-align:right;\"> -2.5107398 </td>\n   <td style=\"text-align:right;\"> 0.0500000 </td>\n   <td style=\"text-align:right;\"> 0.0629491 </td>\n   <td style=\"text-align:right;\"> -0.5760032 </td>\n   <td style=\"text-align:right;\"> 0.0000000 </td>\n   <td style=\"text-align:right;\"> 0.0000000 </td>\n   <td style=\"text-align:right;\"> 0.0000000 </td>\n   <td style=\"text-align:right;\"> -0.4212642 </td>\n   <td style=\"text-align:right;\"> 0.0000000 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 107 </td>\n   <td style=\"text-align:right;\"> -1.7949702 </td>\n   <td style=\"text-align:right;\"> 0.0476190 </td>\n   <td style=\"text-align:right;\"> 0.0315049 </td>\n   <td style=\"text-align:right;\"> -0.4013675 </td>\n   <td style=\"text-align:right;\"> 0.0000000 </td>\n   <td style=\"text-align:right;\"> 0.0000000 </td>\n   <td style=\"text-align:right;\"> -0.2901882 </td>\n   <td style=\"text-align:right;\"> 0.0000000 </td>\n   <td style=\"text-align:right;\"> 0.0000000 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n\n\n<a href=\"#Jackknife\">Back to top of tabset</a>\n\n## Jackknife Residuals\n\nWe can start by making a plot of the jackknife residuals to assess if there are outliers \\< -3 or \\> 3 SDs).\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Plot jackknife outiers and label any values that are > 3 or < -3.\nggplot(data_missing, aes(x = id, y = jackknife_residuals_pd)) + geom_point() + \n  geom_hline(yintercept = c(-3,0,3)) + ylim(-4,4) + \n  labs(y = \"Jackknife Residuals\",\n       title = \"Jackknife Residuals vs ID\") + \n  geom_text(aes(label = ifelse(jackknife_residuals_pd > 3 | jackknife_residuals_pd < -3, id, '')), \n            vjust = -1)\n```\n\n::: {.cell-output-display}\n![](Project_1_Regression_files/figure-html/unnamed-chunk-89-1.png){width=672}\n:::\n:::\n\n\n\n\nWe have no jackknife residuals +- 3. That's a good sign!\n\nLet's sort our dataset and see who has the highest pocket depth at baseline.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Sort our dataset by descending order of pocket depth at baseline to see who is the highest\nkable(head(data_missing[order(-data_missing$pdbase),]), caption = \"Diagnostics for Attachment Loss Change Model\", format = \"html\") %>%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\"))\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table table-striped table-hover table-condensed\" style=\"margin-left: auto; margin-right: auto;\">\n<caption>Diagnostics for Attachment Loss Change Model</caption>\n <thead>\n  <tr>\n   <th style=\"text-align:left;\">   </th>\n   <th style=\"text-align:right;\"> id </th>\n   <th style=\"text-align:right;\"> trtgroup </th>\n   <th style=\"text-align:right;\"> gender </th>\n   <th style=\"text-align:right;\"> race </th>\n   <th style=\"text-align:right;\"> age </th>\n   <th style=\"text-align:right;\"> smoker </th>\n   <th style=\"text-align:right;\"> sites </th>\n   <th style=\"text-align:right;\"> attachbase </th>\n   <th style=\"text-align:right;\"> attach1year </th>\n   <th style=\"text-align:right;\"> pdbase </th>\n   <th style=\"text-align:right;\"> pd1year </th>\n   <th style=\"text-align:right;\"> attachchange </th>\n   <th style=\"text-align:right;\"> pdchange </th>\n   <th style=\"text-align:right;\"> placebo </th>\n   <th style=\"text-align:right;\"> control </th>\n   <th style=\"text-align:right;\"> low </th>\n   <th style=\"text-align:right;\"> medium </th>\n   <th style=\"text-align:right;\"> high </th>\n   <th style=\"text-align:right;\"> trt </th>\n   <th style=\"text-align:right;\"> trt3groups </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> 3 </td>\n   <td style=\"text-align:right;\"> 104 </td>\n   <td style=\"text-align:right;\"> 3 </td>\n   <td style=\"text-align:right;\"> 2 </td>\n   <td style=\"text-align:right;\"> 5 </td>\n   <td style=\"text-align:right;\"> 55.17864 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 138 </td>\n   <td style=\"text-align:right;\"> 4.956522 </td>\n   <td style=\"text-align:right;\"> 5.304348 </td>\n   <td style=\"text-align:right;\"> 5.217391 </td>\n   <td style=\"text-align:right;\"> 4.891304 </td>\n   <td style=\"text-align:right;\"> 0.3478261 </td>\n   <td style=\"text-align:right;\"> -0.3260870 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 3 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> 20 </td>\n   <td style=\"text-align:right;\"> 124 </td>\n   <td style=\"text-align:right;\"> 2 </td>\n   <td style=\"text-align:right;\"> 2 </td>\n   <td style=\"text-align:right;\"> 5 </td>\n   <td style=\"text-align:right;\"> 41.01574 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 162 </td>\n   <td style=\"text-align:right;\"> 2.901235 </td>\n   <td style=\"text-align:right;\"> 2.808642 </td>\n   <td style=\"text-align:right;\"> 4.771605 </td>\n   <td style=\"text-align:right;\"> 4.067901 </td>\n   <td style=\"text-align:right;\"> -0.0925926 </td>\n   <td style=\"text-align:right;\"> -0.7037037 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 2 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> 17 </td>\n   <td style=\"text-align:right;\"> 120 </td>\n   <td style=\"text-align:right;\"> 3 </td>\n   <td style=\"text-align:right;\"> 2 </td>\n   <td style=\"text-align:right;\"> 5 </td>\n   <td style=\"text-align:right;\"> 41.83710 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 150 </td>\n   <td style=\"text-align:right;\"> 3.886667 </td>\n   <td style=\"text-align:right;\"> 3.920000 </td>\n   <td style=\"text-align:right;\"> 4.200000 </td>\n   <td style=\"text-align:right;\"> 3.880000 </td>\n   <td style=\"text-align:right;\"> 0.0333333 </td>\n   <td style=\"text-align:right;\"> -0.3200000 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 3 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> 77 </td>\n   <td style=\"text-align:right;\"> 234 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 2 </td>\n   <td style=\"text-align:right;\"> 5 </td>\n   <td style=\"text-align:right;\"> 51.12115 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 144 </td>\n   <td style=\"text-align:right;\"> 2.756944 </td>\n   <td style=\"text-align:right;\"> 2.527778 </td>\n   <td style=\"text-align:right;\"> 4.083333 </td>\n   <td style=\"text-align:right;\"> 3.631944 </td>\n   <td style=\"text-align:right;\"> -0.2291667 </td>\n   <td style=\"text-align:right;\"> -0.4513889 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> 5 </td>\n   <td style=\"text-align:right;\"> 106 </td>\n   <td style=\"text-align:right;\"> 4 </td>\n   <td style=\"text-align:right;\"> 2 </td>\n   <td style=\"text-align:right;\"> 5 </td>\n   <td style=\"text-align:right;\"> 42.14921 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 168 </td>\n   <td style=\"text-align:right;\"> 2.369048 </td>\n   <td style=\"text-align:right;\"> 1.922619 </td>\n   <td style=\"text-align:right;\"> 3.910714 </td>\n   <td style=\"text-align:right;\"> 3.083333 </td>\n   <td style=\"text-align:right;\"> -0.4464286 </td>\n   <td style=\"text-align:right;\"> -0.8273810 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 3 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> 6 </td>\n   <td style=\"text-align:right;\"> 107 </td>\n   <td style=\"text-align:right;\"> 3 </td>\n   <td style=\"text-align:right;\"> 2 </td>\n   <td style=\"text-align:right;\"> 5 </td>\n   <td style=\"text-align:right;\"> 37.15811 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 156 </td>\n   <td style=\"text-align:right;\"> 3.544872 </td>\n   <td style=\"text-align:right;\"> 2.839744 </td>\n   <td style=\"text-align:right;\"> 3.897436 </td>\n   <td style=\"text-align:right;\"> 3.237179 </td>\n   <td style=\"text-align:right;\"> -0.7051282 </td>\n   <td style=\"text-align:right;\"> -0.6602564 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 3 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n\n\nParticipant 104 has the highest baseline pocket depth. Interestingly it is not 168, who was our outlier for attachment loss.\n\nLet's run that test we did earlier to see if there's an outlier in the pocket depth model.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\noutlierTest(model_pd)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nNo Studentized residuals with Bonferroni p < 0.05\nLargest |rstudent|:\n   rstudent unadjusted p-value Bonferroni p\n82 2.664155          0.0090393      0.93105\n```\n\n\n:::\n:::\n\n\n\n\nThis is a test of a hypothesis that we do not have an outlier. We fail to reject the null and thus have more evidence that we do not have outliers for the pocket depth model.\n\n<a href=\"#Jackknife\">Back to top of tabset</a>\n\n## Leverage\n\nLet's plot leverage for our model on pocket depth change.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Caculate cutoff for leverage\ncutoff_leverage <- 2*((4+1)/103)\ncutoff_leverage\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.09708738\n```\n\n\n:::\n\n```{.r .cell-code}\n# Make leverage plot in ggplot. Set a horizontal line at the cutoff point, and label ID's for participants that pass that line\nggplot(diagnostics_pd, aes(x = id, y = leverage)) +\n  geom_point() + \n  geom_hline(yintercept = cutoff_leverage) +\n  labs(title = \"Leverage Plot with Cutoff Line\",\n       x = \"ID\",\n       y = \"Hat Values\") + \n  geom_text(aes(label = ifelse(leverage > cutoff_leverage, id, \"\")), vjust = -0.5)\n```\n\n::: {.cell-output-display}\n![](Project_1_Regression_files/figure-html/unnamed-chunk-92-1.png){width=672}\n:::\n:::\n\n\n\n\nAgain we are good on leverage.\n\n<a href=\"#Jackknife\">Back to top of tabset</a>\n\n## Cook's D\n\nLet's make our plot for Cook's D for pocket depth change.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Calculate cutoff for Cook's D\ncutoff_d <- 4/((nrow(data_missing) - length(model_attach1$coefficients) - 1))\ncutoff_d\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.04123711\n```\n\n\n:::\n\n```{.r .cell-code}\n# Make Cook's D plot in ggplot. Set a horizontal line at the cutoff point, and label ID's for participants that pass that line\nggplot(diagnostics_pd, aes(x = id, y = cooks_D)) +\n  geom_point() + \n  geom_hline(yintercept = cutoff_d) +\n  labs(title = \"Cook's D with Cutoff Line\",\n       x = \"ID\",\n       y = \"Cook's D\") + \n  geom_text(aes(label = ifelse(cooks_D > cutoff_d, id, \"\")), vjust = -0.5)\n```\n\n::: {.cell-output-display}\n![](Project_1_Regression_files/figure-html/unnamed-chunk-93-1.png){width=672}\n:::\n:::\n\n\n\n\nWe see that participants 106, 118, and 239 have Cook's D values past the cutoff. Interestingly participant 104 who had the highest baseline pocket depth did not flag here.\n\n<a href=\"#Jackknife\">Back to top of tabset</a>\n\n## DFFITS\n\nLet's make the DFFITS plot for pocket depth change.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Calculate the cutoff for DFFITS\ncutoff_dffits <- 2 * sqrt((4+1)/103)\ncutoff_dffits\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.4406526\n```\n\n\n:::\n\n```{.r .cell-code}\n# Make DFFITS plot in ggplot. Set a horizontal line at the cutoff points, and label ID's for participants that pass those lines.\nggplot(diagnostics_pd, aes(x = id, y = dffits)) + \n  geom_point() +\n  geom_hline(yintercept = c(cutoff_dffits, -cutoff_dffits)) + \n  geom_text(aes(label = ifelse(dffits < -cutoff_dffits | dffits > cutoff_dffits, id, \"\")), vjust = -0.5) +\n  labs(title = \"DFFITS by ID\",\n       x = \"ID\",\n       y = \"DFFITS\")\n```\n\n::: {.cell-output-display}\n![](Project_1_Regression_files/figure-html/unnamed-chunk-94-1.png){width=672}\n:::\n:::\n\n\n\n\nParticipants 106, 118, and 239 are flagging here again. In addition we have 137 and 233, which are near enough to the cutoff that we can ignore them.\n\n<a href=\"#Jackknife\">Back to top of tabset</a>\n\n## DFBETAS\n\nNow we can look at the DFBETAS plot for pocket depth change.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Calculate the cutoff for DFBETAS\ncutoff_dfbetas <- 2/sqrt(103)\ncutoff_dfbetas\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.1970659\n```\n\n\n:::\n\n```{.r .cell-code}\n# Make DFBETAS plot in ggplot. Set a horizontal line at the cutoff points, and label ID's for participants that pass those lines.\n\n# For placebo group\nggplot(diagnostics_pd, aes(x = id, y = dfbetas.placebo)) + \n  geom_point() +\n  geom_hline(yintercept = c(cutoff_dfbetas, -cutoff_dfbetas)) + \n  geom_text(aes(label = ifelse(dfbetas.placebo < -cutoff_dfbetas | dfbetas.placebo > cutoff_dfbetas, id, \"\")), vjust = -0.5) +\n  labs(title = \"DFBETAS by ID for Placebo\",\n       x = \"ID\",\n       y = \"DFBETA\")\n```\n\n::: {.cell-output-display}\n![](Project_1_Regression_files/figure-html/unnamed-chunk-95-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# For low dose group\nggplot(diagnostics_pd, aes(x = id, y = dfbetas.low)) + \n  geom_point() +\n  geom_hline(yintercept = c(cutoff_dfbetas, -cutoff_dfbetas)) + \n  geom_text(aes(label = ifelse(dfbetas.low < -cutoff_dfbetas | dfbetas.low > cutoff_dfbetas, id, \"\")), vjust = -0.5) +\n  labs(title = \"DFBETAS by ID for Low Dose\",\n       x = \"ID\",\n       y = \"DFBETA\")\n```\n\n::: {.cell-output-display}\n![](Project_1_Regression_files/figure-html/unnamed-chunk-95-2.png){width=672}\n:::\n\n```{.r .cell-code}\n# For medium dose group\nggplot(diagnostics_pd, aes(x = id, y = dfbetas.medium)) + \n  geom_point() +\n  geom_hline(yintercept = c(cutoff_dfbetas, -cutoff_dfbetas)) + \n  geom_text(aes(label = ifelse(dfbetas.medium < -cutoff_dfbetas | dfbetas.medium > cutoff_dfbetas, id, \"\")), vjust = -0.5) +\n  labs(title = \"DFBETAS by ID for Medium Dose\",\n       x = \"ID\",\n       y = \"DFBETA\")\n```\n\n::: {.cell-output-display}\n![](Project_1_Regression_files/figure-html/unnamed-chunk-95-3.png){width=672}\n:::\n\n```{.r .cell-code}\n# For high does group\nggplot(diagnostics_pd, aes(x = id, y = dfbetas.high)) + \n  geom_point() +\n  geom_hline(yintercept = c(cutoff_dfbetas, -cutoff_dfbetas)) + \n  geom_text(aes(label = ifelse(dfbetas.high < -cutoff_dfbetas | dfbetas.high > cutoff_dfbetas, id, \"\")), vjust = -0.5) +\n  labs(title = \"DFBETAS by ID for High Dose\",\n       x = \"ID\",\n       y = \"DFBETA\")\n```\n\n::: {.cell-output-display}\n![](Project_1_Regression_files/figure-html/unnamed-chunk-95-4.png){width=672}\n:::\n:::\n\n\n\n\nAgain we have a lot of points that are past the cutoff here. None as far off as 168 in the attachment loss model, 239 is the only one that is concerning.\n\n<a href=\"#Jackknife\">Back to top of tabset</a>\n\n## Additional Plots\n\nThe car package in R has some handy plots that we can use to help us with diagnosing outliers.\n\nThe first is the influence plot, which essentially combines the jackknife residuals plot with the Cook's D plot.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Influence plot\ninfluencePlot(model_pd, main = \"Influence Plot\",\n              sub = \"Circle size is proportional to Cook's Distance\")\n```\n\n::: {.cell-output-display}\n![](Project_1_Regression_files/figure-html/unnamed-chunk-96-1.png){width=672}\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      StudRes        Hat       CookD\n5  -2.5107398 0.05000000 0.062949101\n8  -0.8158788 0.06250000 0.008905825\n12  0.4184503 0.06250000 0.002354494\n82  2.6641548 0.04761905 0.066819586\n```\n\n\n:::\n:::\n\n\n\n\nHere we can see that none of our residuals are +- 3, but we do have some concerns with large Cook's distance, particularly with observation 82 (ID = 239).\n\nAdditionally, a single line of code provides us with some useful diagnostic plots.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# infIndexPlot gives us a series of plots that we need to investigate influence points\ninfIndexPlot(model_pd)\n```\n\n::: {.cell-output-display}\n![](Project_1_Regression_files/figure-html/unnamed-chunk-97-1.png){width=672}\n:::\n:::\n\n\n\n\nBased on these plots, it is arguable that participant 239 is an outlier for pocket depth. Though they do not have an extreme jackknife residual, the large Cook's distance suggests that this data point has substantial infuence on the model's parameters.\n\nHowever, a closer look reveals that the influence and leverage values for the pocket depth model are all comparable to the potential outliers in the attachmnet loss model we chose to keep (Cook's D \\~0.06). Specifically, while it appears that participant 239 here has a large Cook's D (0.068) compared to the rest of the values, it is nowhere near as high as participant 168 was in the attachment loss model (0.15)!\n\n<a href=\"#Jackknife\">Back to top of tabset</a>\n\n## Final Look and Conclusion\n\nParticipants 106 and 239 were the only potential concerns here. However, as noted, their Cook's D is comparable to the values we kept in the attachment loss model, and are \\< 1/2 of what the Cook's D was for participant 168 who we plan to remove as an outlier in the attachment loss model!\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Pretty print diagnostics table of potential outliers\nkable(diagnostics_pd[diagnostics_attach$id %in% c(106,239),], caption = \"Correlation Matrix\", format = \"html\") %>%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\"))\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table table-striped table-hover table-condensed\" style=\"margin-left: auto; margin-right: auto;\">\n<caption>Correlation Matrix</caption>\n <thead>\n  <tr>\n   <th style=\"text-align:left;\">   </th>\n   <th style=\"text-align:right;\"> id </th>\n   <th style=\"text-align:right;\"> jackknife_residuals </th>\n   <th style=\"text-align:right;\"> leverage </th>\n   <th style=\"text-align:right;\"> cooks_D </th>\n   <th style=\"text-align:right;\"> dffits </th>\n   <th style=\"text-align:right;\"> dfbetas..Intercept. </th>\n   <th style=\"text-align:right;\"> dfbetas.placebo </th>\n   <th style=\"text-align:right;\"> dfbetas.low </th>\n   <th style=\"text-align:right;\"> dfbetas.medium </th>\n   <th style=\"text-align:right;\"> dfbetas.high </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> 5 </td>\n   <td style=\"text-align:right;\"> 106 </td>\n   <td style=\"text-align:right;\"> -2.510740 </td>\n   <td style=\"text-align:right;\"> 0.050000 </td>\n   <td style=\"text-align:right;\"> 0.0629491 </td>\n   <td style=\"text-align:right;\"> -0.5760032 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 0.0000000 </td>\n   <td style=\"text-align:right;\"> -0.4212642 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> 82 </td>\n   <td style=\"text-align:right;\"> 239 </td>\n   <td style=\"text-align:right;\"> 2.664155 </td>\n   <td style=\"text-align:right;\"> 0.047619 </td>\n   <td style=\"text-align:right;\"> 0.0668196 </td>\n   <td style=\"text-align:right;\"> 0.5957231 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 0.4307071 </td>\n   <td style=\"text-align:right;\"> 0.0000000 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n\n:::\n:::::\n\n## Rerunning the Model with Outliers Removed\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Remove participant 168 from the dataset\ndata_missing_outlier <- data_missing[data_missing$id != 168,]\n\n# Run model 1 attachment loss\nmodel_attach1_outlier <- lm(attachchange ~ placebo + low + medium + high, data = data_missing_outlier)\nsummary(model_attach1_outlier)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = attachchange ~ placebo + low + medium + high, data = data_missing_outlier)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.68731 -0.16279  0.04936  0.16823  0.53945 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -0.22169    0.05277  -4.201  5.9e-05 ***\nplacebo      0.13462    0.07463   1.804  0.07435 .  \nlow          0.20388    0.07638   2.669  0.00891 ** \nmedium       0.21514    0.07737   2.780  0.00652 ** \nhigh         0.11576    0.08399   1.378  0.17130    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2531 on 97 degrees of freedom\nMultiple R-squared:  0.09493,\tAdjusted R-squared:  0.05761 \nF-statistic: 2.543 on 4 and 97 DF,  p-value: 0.04442\n```\n\n\n:::\n\n```{.r .cell-code}\n# Run model 2 attachmnent loss\nmodel_attach2_outlier <- lm(attachchange ~ control + low + medium + high, data = data_missing_outlier)\nsummary(model_attach2_outlier)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = attachchange ~ control + low + medium + high, data = data_missing_outlier)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.68731 -0.16279  0.04936  0.16823  0.53945 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)  \n(Intercept) -0.08707    0.05277  -1.650   0.1022  \ncontrol     -0.13462    0.07463  -1.804   0.0743 .\nlow          0.06926    0.07638   0.907   0.3668  \nmedium       0.08052    0.07737   1.041   0.3006  \nhigh        -0.01886    0.08399  -0.225   0.8228  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2531 on 97 degrees of freedom\nMultiple R-squared:  0.09493,\tAdjusted R-squared:  0.05761 \nF-statistic: 2.543 on 4 and 97 DF,  p-value: 0.04442\n```\n\n\n:::\n\n```{.r .cell-code}\n# Run the pocket depth model\nmodel_pd_outlier <- lm(pdchange ~ control + low + medium + high, data = data_missing_outlier)\nsummary(model_pd_outlier)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = pdchange ~ control + low + medium + high, data = data_missing_outlier)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-0.6248 -0.1513 -0.0129  0.1616  0.6613 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -0.34969    0.05488  -6.372 6.26e-09 ***\ncontrol      0.01152    0.07761   0.148   0.8823    \nlow          0.14352    0.07943   1.807   0.0739 .  \nmedium       0.14714    0.08046   1.829   0.0705 .  \nhigh        -0.02437    0.08734  -0.279   0.7809    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2632 on 97 degrees of freedom\nMultiple R-squared:  0.07456,\tAdjusted R-squared:  0.0364 \nF-statistic: 1.954 on 4 and 97 DF,  p-value: 0.1077\n```\n\n\n:::\n:::\n\n\n\n\nThere is no difference in our models after removing outlier 168.\n::::::\n\n# To do List\n\n[Important]{.underline}\n\ncheck for outliers using jackknife residuals\n\n[Aesthetic]{.underline}\n\nTry adding captions to all graphs and tables (eg #\\| fig-cap )\n\nmess around with a final clean version that makes use of echo = FALSE to hide unneeded code chunks\n\nquestion for a TA or prof:\n\n-   consuslting Prof: Did he make the slides for jackknife residuals? Why the cutoff in Cooks D of 1.0, interwebs says other equation.\n\n-   what is leverage vs half normal quantile plot? in detecting outliers.\n\n-   good way to get correlation matrix or just how do you get good tables out of R for papers? What's you rmethodology/pipeline?\n\nSuppose that the complete-data model is a regression with outcome Y and predictors X . If the missing data occur in Y only, complete-case analysis and multiple imputation are equivalent, so then complete-case analysis is preferred since it is easier, more efficient and more robust (Von Hippel 2007).\n\nComplete case analysis is the same as listwise deletion, which we did anyway.\n\nIf the missing data occur in y only, then it is possible to correct the variance formulas of method norm.predict. However, if the missing data occur in X , norm.predict is severely biased, so then variance correction is not useful.\n\n...\n\ngood format to follow for write up. \"I used mice (Multiple imputation by chained equations) to impute my dataset with missing variables. Next, I did a linear regression on the imputed datasets (m=50) and pooled the results according to Rubin's Rules.\"\n\nLook into Little's Test for MCAR\n\n------------------------------------------------------------------------\n\nMeeting with Zoey\n\n\\*\\*Look into making R shiny apps\\*\\*\n\nBe careful with Kable cause it can cause conflicts (and crashes)\n\nWork with base R so it's not effected by updates of packages\n\nTable 1 is good enough, if not use custom table. You can create a matrix first.\n\nLook into R custom table (could also use kable)(but can crash)\n\nplotly for interactive graphs\n\nr-graph-galler.com/interactive-charts.html plotly gampinder, highchart interactive line plot in R (plotly is more popular) (highchart requires minimal coding, seems simpler. but is limited)\n\ncan use R to directly send an email to someone\n\nOptions: Directly upload photos to online cloud and link to those Everytime you share the doc, share as folders and include link to those folders. Use setwd()./(test) command and set the working directory at the start(the . means the current folder) Use github (?) (be careful of automatic updates function, )\n\nalexkaizer.com/bios_6618 - can ask him how to turn my qmd into a website.\n\nlook into Tableau for interactive graphs (she says its easy).",
    "supporting": [
      "Project_1_Regression_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"../../site_libs/kePrint-0.0.1/kePrint.js\"></script>\n<link href=\"../../site_libs/lightable-0.0.1/lightable.css\" rel=\"stylesheet\" />\n<link href=\"../../site_libs/table1-1.0/table1_defaults.css\" rel=\"stylesheet\" />\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}