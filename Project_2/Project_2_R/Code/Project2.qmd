---
title: "Advanced Data Analysis - Project 2"
author: "Sean Vieau"
date: "October 9, 2024"
editor: visual
output: html_document
toc: true
---

```{r setup, include=FALSE}
# Sets the default for all chunks as echo = TRUE (ensures they show unless specified not to)
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction

The aim of the current study is to assess how treatment response differs for HIV+ patients 2 years after initiating Highly Active Antiretroviral Therapy (HAART) based on hard drug usage (such as heroin or cocaine). This study is of particular scientific interest because it is unclear whether the use of hard drugs inhibits the immune system in humans; treatment strategies may differ based on these results. The researchers are interested in comparing subjects who never used hard drugs to current hard drug users (those that use hard drugs at year 2) or previous hard drug users (those who used drugs at year 0 or 1). Outcomes of interest are: viral load (HIV copies in a mL of blood), CD4+ T cell count (a measure of immunologic health), and aggregate physical and quality of life scores from the SF-36.

The clinical hypothesis is that, if hard drugs inhibit the immune system in humans, subjects who currently or previously used hard drugs will have higher viral load and lower CD4+ T cell counts than those who never used hard drugs. Additionally, the researchers are interested in knowing if potential differences between the drug use groups can be explained by differences in adherence to the treatment regimen. The researchers are agnostic on how quality of life changes after treatment, since side effects of the treatment are significant.

The project description provided by the PI is available below:

::: {style="text-align: center;"}
<img src="/Project_2/Project_2_R/Media/Project2_description1.png" width="85%"/>
:::

# Method

**Study Design**

This is a secondary data analysis of the Multicenter AIDS Cohort Study, an ongoing prospective cohort study investigating the natural and treated disease progression of HIV-1 in bisexual men in 4 major cities in the U.S. Measurements for all variables were taken once per year over an 8-year time period; however, the current analysis is only concerned with treatment outcomes after 2 years of HAART. Data was received as a longform .csv file containing 33 columns along with a data dictionary. The main outcomes of interest are viral load, CD4+ T cell count, and aggregate physical and quality of life scores. Adherence to treatment regiment will be investigated as a potential confounder.

Potential covariates of interest include: marijuana usage since last visit and frequency of usage, income, BMI, high blood pressure, diabetes, liver disease stage 3 / 4, kidney disease, frailty related phenotype, total cholesterol, triglycerides, fasting LDL, dyslipidemia, depression score, smoking status, alcohol use since last visit, heroin or opiate use since last visit, intravenous drug use since last visit, race, education at baseline, age, if they took ART at the visit or if they have ever taken it before, and years since initiating ART.

# Data Preparation

First we load the necessary packages

```{r, message = FALSE}
library(dplyr)
library(ggplot2)
library(tidyverse)
library(naniar) # Used to visualize missing data
library(kableExtra) # Used for pretty printing (kable_styling)
library(table1) # Used to make Table 1
library(tidyr) # Used for reshaping
library(bestNormalize) # Used in selecting best transformation for a variable
```

Then we import the data set.

```{r, message = FALSE}
# Read in data
data <- read_csv("C:/Users/sviea/Documents/Portfolio/Project_2/Project_2_R/RawData/hiv_dataset.csv")
```

And take a look.

```{r}
# Examine data
glimpse(data)
```

Everything appears properly imported, however all our categorical variables are coded as doubles.

## Labeling Categorical Variables

Let's factor and label our categorical variables so they are appropriately represented (and not doubles, which will yield incorrect results in models)

```{r}
# Converting all appropriate variables from doubles to categorical variables

data$HASHV <- factor(data$HASHV,
                     levels = c(1, 2),
                     labels = c("No", "Yes"))

data$HASHF <- factor(data$HASHF,
                     levels = c(0, 1, 2, 3, 4),
                     labels = c("Never", "Daily", "Weekly", "Monthly", "Less Often"))

data$income <- factor(data$income,
                      levels = c(1, 2, 3, 4, 5, 6, 7, 9),
                      labels = c("Less than $10,000", "$10,000-$19,999", "$20,000-$29,999", "$30,000-$39,999", "$40,000-$49,999", "$50,000-$59,999", "$60,000 or more", "Do not wish to answer"))

data$HBP <- factor(data$HBP,
                   levels = c(1, 2, 3, 4, 9, -1),
                   labels = c("No", "Yes", "No, based on data trajectory", "Yes, based on data trajectory", "Insufficient data, may include reported treatment without diagnosis", "Improbable Value"))

data$DIAB <- factor(data$DIAB,
                    levels = c(1, 2, 3, 4, 9),
                    labels = c("No", "Yes", "No, based on data trajectory", "Yes, based on data trajectory", "Insufficient data"))
                      
data$LIV34 <- factor(data$LIV34,
                     levels = c(1, 2, 9),
                     labels = c("No", "Yes", "Insufficient Data"))

data$KID <- factor(data$KID,
                   levels = c(1, 2, 3, 4, 9),
                   labels = c("No", "Yes", "No, based on data trajectory", "Yes, based on data trajectory", "Insufficient data"))

data$FRP <- factor(data$FRP,
                   levels = c(1,2,9),
                   labels = c("No", "Yes", "Insufficient Data"))

data$FP <- factor(data$FP,
                  levels = c(1,2,9),
                  labels = c("No", "Yes", "Insufficient Data"))

data$DYSLIP <- factor(data$DYSLIP,
                      levels = c(1, 2, 3, 4, 9),
                      labels = c("No", "Yes", "No, based on data trajectory", "Yes, based on data trajectory", "Insufficient data"))

data$SMOKE <- factor(data$SMOKE,
                     levels = c(1, 2, 3),
                     labels = c("Never Smoked", "Former Smoker", "Current Smoker"))

data$DKGRP <- factor(data$DKGRP,
                     levels = c(0, 1, 2, 3),
                     labels = c("None", "1-3 drinks/week", "4-13 drinks/week", ">13 drinks/week"))

data$HEROPIATE <- factor(data$HEROPIATE,
                         levels = c(1, 2, -9),
                         labels = c("No", "Yes", "Not Specified"))

data$IDU <- factor(data$IDU,
                   levels = c(1, 2),
                   labels = c("No", "Yes"))

data$ADH <- factor(data$ADH,
                   levels = c(1, 2, 3, 4),
                   labels = c("100%", "95-99%", "75-94%", "<75%"))

data$RACE <- factor(data$RACE,
                    levels = c(1, 2, 3, 4, 5, 6, 7),
                    labels = c("White, non-Hispanic", "White, Hispanic", "Black, non-Hispanic ", "Black, Hispanic",  "American Indian or Alaskan Native", "Asian or Pacific Islander", "Other Hispanic"))

data$EDUCBAS <- factor(data$EDUCBAS,
                       levels = c(1, 2, 3, 4, 5, 6, 7),
                       labels = c("8th grade or less ", "9,10, or 11th grade", "12th grade", "At least one year college but no degree", "Four years college / got degree ", "Some graduate work", "Post-graduate degree"))

data$hard_drugs <- factor(data$hard_drugs,
                          levels = c(0, 1),
                          labels = c("No", "Yes"))
```

Let's take another look to check that those variables are no longer doubles.

```{r}
# Examine data
glimpse(data)
```

Looks good.

## Filtering Data Set

Now let's take a look at the header to get a good feeling for our data.

```{r}
# Pretty print data header
kable(head(data), format = "html", full_width = TRUE) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"))
```

Hmm, we have 8 years worth of data points, but the experimenters are only interested in the first 2 years.

Out of curiosity, let's look at how many participants they had each year.

```{r}
# Visualize patient drop off over 8 years of study
barplot(table(data$years))

# Check number of patients in each year
kable(table(data$years), format = "html", full_width = FALSE) %>%
  kable_styling(bootstrap_options = c("condensed", "hover", "striped"))
```

This is interesting, we don't seem to have as drastic a drop off as I expected. The researchers managed to retain all participants for the first 2 years, and 50% by the end of the 8-year study.

Let's filter to only include values from the first 2 years.

```{r}
# Filter long form data set to be include only first 2 years
data_2 <- data[data$years <= 2,]
```

```{r}
# Double check if any patients dropped out within the first 2 years
any(is.na(data_2$years))
```

Luckily, all participants have at least 2 years of visits!

## Transpose to Wideform

We can also see that the provided data set is in longform. Let's convert that to wideform.

```{r}
# Create new wideform data set for first 2 years of study              
data_wide_2 <- pivot_wider(data_2, id_cols = newid, names_from = years, values_from = -c(newid, years))
```

And take a look at the header to check that was done correctly.

```{r}
# Pretty print header of wideform data
kable(head(data_wide_2), format = "html", full_width = TRUE) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"))
```

Good. now we have a long and wide form of the data set for the first two years of the study.

Finally, let's just clean that wide data set up a bit to drop repeat measures of variables that are constant over time (race, education at baseline, HIV serostatus, everART)

```{r}
# Clean up the wide data set a bit by deleting multiple observations across time for constant variables such as race
data_wide_2 <- data_wide_2 %>% select(-RACE_1, -RACE_2, -EDUCBAS_1, -EDUCBAS_2, -hivpos_1, - hivpos_2, -everART_1, -everART_2)
```

Now that our data sets are adequately prepared, we can move on to performing our data checks to ensure fidelity of the data set.

# Data Quality Checks

Here we will perform several assessments on each variable to ensure fidelity of the data

## Missingness

First we begin by examining missingness in our data set

```{r}
# Check missingness for long form data
gg_miss_var(data_2)
```

This shows that we are missing the most values for `LDL`, `TRIG`, `ADH`, `TCHOL`, and `income`.

A closer examination reveals...

```{r}
# Visualize missing values for longform data
vis_miss(data_2)
```

53% of `LDL`, 53% of `TRIG`, 33% of `ADH`, 32% of `TCHOL`, and 24% of `income` values are missing.

`LDL` and`TRIG` have egregious amounts of missing data (\> 50%). `TCHOL` and `income` are in a range where we may be able to save them with MI or a linear mixed model that allows for missing data. We will have to see.

`ADH` is missing 33% of values. That could be problematic as that's a key variable the researchers are interested in.

But there's an odd, systematic pattern there... what if we look at the wide form of the data?

```{r}
# Visualize missing values for wideform data
vis_miss(data_wide_2)
```

Ah, 1/3 of the values for `ADH` are missing because there are 3 time points and you can't have baseline adherence to a protocol you just started (i.e. `ADH_0`).

There's a small blip there that looks like someone DOES have a value for `ADH_0`, I wonder what that's about...

```{r}
# For some reason participant 426 has an adherence of 1 at baseline
adh_at_baseline <- data_wide_2 %>%
  filter(ADH_0 == "100%") %>% 
  select(newid, ADH_0)

# Pretty print
kable(adh_at_baseline, format = "html", full_width = FALSE)
```

Apparently if you're participant 426, you can have 100% adherence to a protocol you've just started (clearly a typo).

Conclusion is we can still use `ADH` as a variable! We just have to use adherence at years 1 or 2.

## Data Cleaning

We just examined missingness as a preliminary check. However there is more work to be done.

The dataset we received has variables that were coded inconsistently. For instance, some variables are coded so that missing values are represented by a blank, and some variables (like `CESD`) are coded so that missing values are represented by -1. In other cases, such as with `BMI`, improbable values are coded as 999.

We can see this if we examine the mins and maxes for each numerical variable.

```{r}
# Code from ChatGPT
# This function summarizes the mins and maxes of numeric variables
summarize_column <- function(column) {
  if (is.numeric(column)) {
    return(data.frame(
      Type = "Numeric",
      Min = min(column, na.rm = TRUE),
      Max = max(column, na.rm = TRUE)
    ))
  }
}

# Apply the function to each column and bind the results into a single data frame
summary_df <- map_dfr(data_2, summarize_column, .id = "Column") %>%
  mutate(across(everything(), ~ format(., scientific = FALSE))) # Eliminates scientific notation

# Pretty print the mins and maxes of longform data_2
kable(summary_df, format = "html") %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"))
```

In effect, our data is not correctly showing all missing values. Let's clean all that up, variable by variable.

### Cleaning Dependent Variables {#Clean_DVs}

First we will begin by examining and cleaning our 4 primary outcomes of interest.

The first two are laboratory measures.

-   [Viral load (VLOAD):]{.underline} The number of HIV copies in a mL of blood
-   [CD4+ T cell count (LEU3N):]{.underline} A measure of immunologic health.

In untreated HIV infection, viral load increases over time and CD4+ T cell counts decline as the immune system is attacked by the virus. Once treatment is initiated, we expect viral load to decrease rapidly and CD4 counts to recover.

Our last two measures are quality of life measures from the [SF-36](https://www.rand.org/health-care/surveys_tools/mos/36-item-short-form.html).

-   [Aggregate physical quality of life score (AGG_PHYS)]{.underline}

-   [Aggregate mental quality of life score (AGG_MENT)]{.underline}

These scores range from 0 to 100, with higher scores indicating better quality of life. The researchers are not sure what happens to quality of life after initiating treatment. While in theory subjects’ improving health should result in increased quality of life, the side effects of these treatments are significant. If subjects experience declines in quality of life after initiating treatment, we would be concerned that they would stop treatment.

::: panel-tabset
## Viral Load

Standardized viral load

-   0 = 0 copies/ml

-   999,999,999 = 999,999,999 copies/ml

-   Blank = Missing

Our min max function earlier showed the max VLOAD was 190695039.60. I wonder if this is real or a data error?

```{r}
# create histogram of viral load
hist(data_2$VLOAD)

# Create qqplots of viral load
qqnorm(data_2$VLOAD)
qqline(data_2$VLOAD)
```

Yeah, looks like there are about 3 data points throwing off our qqplot from being normal.

Let's investigate.

```{r}
# Sort data set by descending viral load
sorted_data <- data_2[order(-data_2$VLOAD),] %>%
  select(newid, VLOAD, years)

# Pretty print resulting table
kable(head(sorted_data), format = "html") %>%
  kable_styling(bootstrap_options = c("condensed", "striped", "hover"))
```

So the highest VLOAD value is 75x the 5th highest, and the 4th highest is 2x 5th highest. All these values are from different patients at the baseline.

Based on the data dictionary provided, these values fall below the specified range of 999,999,999 copies/ml. That the PI's specified this range could mean these are real data points. Maybe immediately after when someone is first exposed to HIV the viral load is incredibly high, and these 4 or so patients fell in that time period?

I will first check if removing them makes our data normally distributed.

We will then add them back into the data set and keep them in mind. Checking with the jackknife residuals after we run our model will tell us if they are high leverage points.

```{r}
# Create boxplot to assess for outliers
outlier_vload <- boxplot(data_2$VLOAD, main = "Boxplot for VLOAD")$out
text(x = rep(1.2, length(outlier_vload)),
     y = outlier_vload, labels = outlier_vload, col = 'red', cex = 0.8)
```

Indeed the boxplot shows these values really mess with our data.

These top 4 patients based on VLOAD are 224, 78, 437, and 196. Patient 196 has double the VLOAD of the next highest person, which means this could be an outlier or real data, but let's remove them just to see.

```{r}
# Remove 4 highest viral load visits
data_vload_removed <- data
data_vload_removed$VLOAD[data_vload_removed$newid %in% c(224, 78, 437, 196)] <- NA

# Create plots to assess for normality
hist(data_vload_removed$VLOAD)
qqnorm(data_vload_removed$VLOAD)
```

Oh, that makes more sense. Those might not have been outliers, we just need to log transform viral load. Right, viral load is often used as a real world example of a biological measurement that is logarithmic...

Let's do that log transform, and just pretend we remembered that from the start.

```{r}
# Log transform viral load in the long form data set
data_2$VLOAD_log <- log(data_2$VLOAD)

# Create a histogram of log transformed viral load
hist(data_2$VLOAD_log)

# Create qqplot of log transformed viral load
qqnorm(data_2$VLOAD_log)
qqline(data_2$VLOAD_log)

# Create boxplot of log transformed viral load to assess for potential outliers
outlier_vload <- boxplot(data_2$VLOAD_log, main = "Boxplot for VLOAD")
```

That looks much better!

I'd say that's roughly normally distributed, maybe a bit right tailed but likely still acceptable.

Looks like this took care of those values that were showing up as outliers before.

`VLOAD` has now been cleaned!

<a href="#Clean_DVs">Back to top of tabset</a>

## CD4+ T Cell Count

A measure of immunologic health.

Number of CD4 positive cells (helpers)

-   0 - 9999 cells
-   Blank = Missing

```{r}
# Create histogram of CD4 T Cells
hist(data_2$LEU3N)

# Create qqplot of CD4 T Cells
qqnorm(data_2$LEU3N)
qqline(data_2$LEU3N)

# Sort data set by descending CD4 T Cell
sorted_data <- data_2[order(-data_2$LEU3N),] %>%
  select(newid, LEU3N, years)

# Pretty print resulting table
kable(head(sorted_data), format = "html") %>%
  kable_styling(bootstrap_options = c("condensed", "striped", "hover"))
```

These values all look believable and like there was no errors during data collection or entering. It is unclear whether this variable is right tailed because of outliers, or if it needs to be transformed.

Let's look at potential outliers.

```{r}
# Create boxplot of CD4 T cell count to assess for outliers
outlier_leu3n <- boxplot(data_2$LEU3N, main = "Boxplot for Leu3n")$out
text(x = rep(1.2, length(outlier_leu3n)),
     y = outlier_leu3n, labels = outlier_leu3n, col = 'red', cex = 0.8)
```

Yeah, the boxplot is showing a lot of outliers.

Let's try a log transform.

```{r}
# Log transform CD4 T Cell count
data_2$LEU3N_log <- log(data_2$LEU3N)

# Create histogram of log transformed CD4 T cell count
hist(data_2$LEU3N_log)

# Create qqplot of log transformed CD4 T cell count
qqnorm(data_2$LEU3N_log)
qqline(data_2$LEU3N_log)

```

That... didn't work. Maybe let's try standardization.

```{R}
# Perform a standardization transformation of CD4 T Cell Count
data_2$LEU3N_standard <- scale(data_2$LEU3N)

# Create histogram of standardized CD4 T Cell count
hist(data_2$LEU3N_standard)

# Create qqplot of standardized CD4 T Cell count.
qqnorm(data_2$LEU3N_standard)
qqline(data_2$LEU3N_standard)
```

Hmm, that didn't do the trick.

At this point I asked my professor in passing and he recommended the bestNormalize package (which he happened to write) to help in selecting the best transformation for a variable.

Let's take a shot at it.

```{r}
# Use bestNormalize R package to select the best transformation for CD4 T Cell count
BNObject <- bestNormalize(data_2$LEU3N)
BNObject
```

The bestNormalize function selects the best transformation according to the Pearson P statistic (divided by its degrees of freedom), as calculated by the nortest package. There are a variety of normality tests out there, but the benefit of the Pearson P / df is that it is a relatively interpretable goodness of fit test, and the ratio P / df can be compared between transformations as an absolute measure of the departure from normality (if the data follows close to a normal distribution, this ratio will be close to 1).

Here we can see that orderNorm (1.14), Yeo-Johnson (1.26), and Box-Cox (1.26) all perform relatively similar to each other. Let's see what those plots look like if I do those transformations.

```{r}
# Peform ordernNorm transformation of CD4 T Cell count
data_2$LEU3N_orderNorm <- orderNorm(data_2$LEU3N)$x.t

# Peform Box-Cox transformation of CD4 T Cell count
data_2$LEU3N_boxcox <- boxcox(data_2$LEU3N)$x.t

# Peform Yeo-Johnson transformation of CD4 T Cell count
data_2$LEU3N_yeojohnson <- yeojohnson(data_2$LEU3N)$x.t

# Plot all histograms using MASS
par(mfrow = c(3,1))
MASS::truehist(data_2$LEU3N_orderNorm, main = "OrderNorm transformation", nbins = 24)
MASS::truehist(data_2$LEU3N_boxcox, main = "Box Cox transformation", nbins = 24)
MASS::truehist(data_2$LEU3N_yeojohnson, main = "Yeo-Johnson transformation", nbins = 24)
```

```{r}
# This function visualizes the estimated normality statistics obtained for each fold and repeat of cross-validation via boxplots. It allows you to compare transformation methods
boxplot(log10(BNObject$oos_preds), yaxt = 'n')
axis(2, at=log10(c(.1,.5, 1, 2, 5, 10)), labels=c(.1,.5, 1, 2, 5, 10))
```

I will select Box-Cox as those two names are more familiar to me so I trust it more per the availability heuristic (and because orderNorm looks TOO good to be true).

It looks like we are good on `LEU3N` and can move forward!

More information on Box-Cox Transformation [here](https://www.statisticshowto.com/probability-and-statistics/normal-distributions/box-cox-transformation/)

<a href="#Clean_DVs">Back to top of tabset</a>

## Aggregate Mental QOL Score

The values for `AGG_MENT` in our data set range from 7.229315 to 73.31224, which is believable and leads us to conclude there were no data entry errors here.

Let's examine normality.

```{r}
# Create a histogram for aggregate mental QOL score
hist(data_2$AGG_MENT)

# Create qqplot for aggregate mental QOL score
qqnorm(data_2$AGG_MENT)
qqline(data_2$AGG_MENT)

# Sort by descending to examine highest values
sorted_data_2 <- data_2[order(data_2$AGG_MENT),] %>%
  select(newid, AGG_MENT, years)

# Pretty print resulting table
kable(head(sorted_data_2), format = "html") %>%
  kable_styling(bootstrap_options = c("hover", "striped", "condensed"))
```

It appears that `AGG_MENT` is also not normally distributed, it is left-tailed. Let's address that.

```{r}
# Use bestNormalize function to test which transformation performs the best
BNobject <- bestNormalize(data_2$AGG_MENT)
BNobject
```

The orderNorm transformation beats out the other transformations by a mile. Let's perform that.

```{r}
# Perform orderNorm transformation of aggregate mental QOL score
data_2$AGG_MENT_orderNorm <- orderNorm(data_2$AGG_MENT)$x.t
MASS::truehist(data_2$AGG_MENT_orderNorm, main = "OrderNorm transformation", nbins = 24)
```

That appears to be what we have to do but I have some misgivings with orderNorm transforming everything...

Come back to this

<a href="#Clean_DVs">Back to top of tabset</a>

## Aggregate Physical QOL Score

AGG_PHYS has a min of 9.12 and a max of 73.57. These are within the specified range of 0 - 100, and it appears there were no data error entries.

```{r}
# Creat histogram of aggregate physical QOL score
hist(data_2$AGG_PHYS)

# Create qqplots of aggregate physical QOL score
qqnorm(data_2$AGG_PHYS)
qqline(data_2$AGG_PHYS)
```

`AGG_PHYS` is not normally distributed, it is left-tailed.

Let's test which type of transformation might suit it.

```{r}
# Use bestNormalize function to test which transformation performs the best
BNobject <- bestNormalize(data_2$AGG_PHYS)
BNobject
```

Again orderNorm performs the best.

```{r}
# Perform orderNorm transformation of aggregate mental QOL score
data_2$AGG_PHYS_orderNorm <- orderNorm(data_2$AGG_PHYS)$x.t
MASS::truehist(data_2$AGG_PHYS_orderNorm, main = "OrderNorm transformation", nbins = 24)
```

OrderNorm transforming all our DVs might make interpration difficult...

Come back to this.

<a href="#Clean_DVs">Back to top of tabset</a>
:::

### Cleaning Covariates {#Clean_IVs}

Now let's perform data quality checks on our covariates.

::: panel-tabset
## Hash Use

Hash/marijuana use since last visit

-   1 = no
-   2 = yes
-   blank = missing

```{r}
# Create a barplot for hash use
barplot(table(data_2$HASHV))
```

Missing data is correctly handled for this variable.

We have more visits where participants used hash since the last visit than visits where participants did not use hash.

<a href="#Clean_IVs">Back to top of tabset</a>

## Hash Frequency

Frequency hash/marijuana was used since last visit.

-   0 = Never
-   1 = Daily
-   2 = Weekly
-   3 = Monthly
-   4 = Less Often
-   Blank = Missing

```{r}
# Create barplot for hash frequency
barplot(table(data_2$HASHF))
```

This variable is coded correctly. Most participants answered they have never used Hash.

<a href="#Clean_IVs">Back to top of tabset</a>

## Income

Income

-   1 = Less than \$10,000
-   2 = \$10,000 - \$19,999
-   3 = \$20,000 - \$29,999
-   4 = \$30,000 - \$39,999
-   5 = \$40,000 - \$49,999
-   6 = \$50,000 - \$59,999
-   7 = \$60,000 or more
-   9 = Do not wish to answer

The min and max for income are 1 - 9, which matches that data dictionary.

```{r}
# Create barplot for income
barplot(table(data_2$income))

# Get values for each income level
kable(table(data_2$income), format = "html") %>%
  kable_styling(bootstrap_options = c("condensed", "hover", "striped"))
```

We have to convert those values do not wish to answer to be NA.

```{r}
# Converting scores of 9 (do not wish to answer) to be NA
data$income[data_2$income == 9] <- NA
barplot(table(data_2$income))
```

Looks good, we just converted 38 participants from do not wish to answer, to count as missing.

<a href="#Clean_IVs">Back to top of tabset</a>

## BMI

Body Mass Index

We have a min of -1 and a max of 1000.

-   -1: Improbable values

-   999: Insufficient data (why it shows up with decimals and is not exactly 999, who knows).

```{r}
# Create histogram of BMI
hist(data_2$BMI)
```

Let's convert those values of -1 and \>= 998 into missing values.

```{r}
# Convert missing and improbably values to NA
data_2$BMI[data_2$BMI < 0 | data_2$BMI >= 998] <- NA
```

And check out the histogram again and the qqplot.

```{r}
# Create histogram of BMI
hist(data_2$BMI, breaks = 20)

# Create qqplot of BMI
qqnorm(data_2$BMI)
qqline(data_2$BMI)

summarize_column(data_2$BMI)
```

Looks better. Now we have a BMI range of 15.94 - 52.83.

The histogram and qqplots show BMI is slightly right skewed, with more morbidly obese patients than underweight. Is this close enough to normal to ignore, if we take out outliers?

The patient with a BMI of 52.83 may be an outlier based on the qqplot.

```{r}
# Investigating highest BMI value to see if its an outlier
highest_bmi <- data[data$newid == 206,]

# Plot BMI for each year for this patient
plot(highest_bmi$years, highest_bmi$BMI)

```

Interestingly, participant 206 got heavier over the first year, then dropped weight in the proceeding years. Either that or that second year entry point was an error and was meant to be 42.83

```{r}
# Testing to see if these plots look normal after taking the participant with BMI of 52.83 out
data_2$BMI[data_2$newid == 206 & data_2$years == 1] <- NA
hist(data_2$BMI, breaks = 20)
qqnorm(data_2$BMI)
qqline(data_2$BMI)
 
# Add back in that value we removed
data_2$BMI[data_2$newid == 206 & data_2$years == 1] <- 52.832
```

After removing that highest BMI value, the histogram is still right tailed.

What do the boxplots look like?

```{r}
outlier_bmi <- boxplot(data_2$BMI, main = "Boxplot for BMI")$out
text(x = rep(1.2, length(outlier_bmi)),
     y = outlier_bmi, labels = outlier_bmi, col = 'red', cex = 0.8)
```

That's a lot of potential outliers for BMI. If we really want to use this variable we may have to remove these values to keep BMI normally distributed.

<a href="#Clean_IVs">Back to top of tabset</a>

## High Blood Pressure

High Blood Pressure (SBP \>= 140 or DBP \>= 90 or (diagnosed with hypertension and use of medication)

-   1 = No
-   2 = Yes
-   3 = No, based on data trajectory
-   4 = Yes, based on data trajectory
-   9 = Insufficient data, may include reported treatment without diagnosis
-   -1 = improbable value

We will have to exclude values of 9 or -1.

```{r}
# Create barplot of high blood pressure
barplot(table(data_2$HBP))
```

```{r}
# Get values for high blood pressure category
kable(table(data_2$HBP), format = "html") %>%
  kable_styling(bootstrap_options = c("condensed", "hover", "striped"))
```

There are 137 participants with insufficient data. Let's purge them from the data set.

```{r}
# Convert values of insufficient data to NA for high blood pressure
data_2$HBP[data_2$HBP == "Insufficient data, may include reported treatment without diagnosis"] <- NA

# Drop empty levels
data_2$HBP <- droplevels(data_2$HBP) 

# Create barplot of high blood pressure 
barplot(table(data_2$HBP))

# Pretty print table
kable(table(data_2$HBP), format = "html") %>%
  kable_styling(bootstrap_options = c("condensed", "hover", "striped"))
```

Looks better.

Only 34 visits where participants had no based on trajectory, and 4 that had yes based on trajectory.

We will have to decide to either exclude these or merge them into the no or yes groups, respectively. We can do that after we run our correlation matrix to see if there's any relationship here worth pursuing.

<a href="#Clean_IVs">Back to top of tabset</a>

## Diabetes

Diabetes (GLUC 2 \>= 126 or (diagnosed with diabetes and use of medication))

-   1 = No
-   2 = Yes
-   3 = No, based on data trajectory
-   4 = Yes, based on data trajectory
-   9 = Insufficient data

```{r}
# Create barplot of diabetes
barplot(table(data_2$DIAB))

# Pretty print table
kable(table(data_2$DIAB), format = "html") %>%
  kable_styling(bootstrap_options = c("condensed", "hover", "striped"))
```

There are 881 visits with patient who had insuffiicent data to make a diabetes diagnosis!

Let's change those values to NA.

```{r}
# Convert values of insufficient data to NA for diabetes
data_2$DIAB[data_2$DIAB == "Insufficient data"] <- NA

# Drop empty levels
data_2$DIAB <- droplevels(data_2$DIAB)

# Create a barplot for diabetes
barplot(table(data_2$DIAB))

# Pretty print table
kable(table(data_2$DIAB), format = "html") %>%
  kable_styling(bootstrap_options = c("condensed", "hover", "striped"))
```

Great, `HBP` is now cleaned.

Notably, there were no visits with a yes, based on trajectory.

<a href="#Clean_IVs">Back to top of tabset</a>

## Liver Disease

Liver disease stage 3/4 (SGPT or SGOP \> 150), preliminary algorithm

-   1 = No
-   2 = Yes
-   9 = Insufficient data

```{r}
# Create barplot of liver disease stage
barplot(table(data_2$LIV34))

# Pretty print table
kable(table(data_2$LIV34), format = "html") %>%
  kable_styling(bootstrap_options = c("condensed", "hover", "striped"))
```

There are 507 patients with insufficient data for a liver disease diagnosis.

Let's convert those values to NA to reflect this.

```{r}
# Convert values of insufficient data to NA for liver disease stage
data_2$LIV34[data_2$LIV34 == "Insufficient Data"] <- NA

# Drop empty levels
data_2$LIV34 <- droplevels(data_2$LIV34)

# Create barplot of cleaned liver stage disease
barplot(table(data_2$LIV34))

# Pretty print table
kable(table(data_2$LIV34), format = "html") %>%
  kable_styling(bootstrap_options = c("condensed", "hover", "striped"))
```

Looks good, `LIV34` is now cleaned!

<a href="#Clean_IVs">Back to top of tabset</a>

## Kidney Disease

Kidney disease (EGFR \< 60 or UPRCR \>= 200)

-   1 = No

-   2 = Yes

-   3 = No, based on data trajectory

-   4 = Yes, based on data trajectory

-   \- 9 = Insufficient data

```{r}
# Create barplots of kidney disease
barplot(table(data_2$KID))

# Pretty print table
kable(table(data_2$KID), format = "html") %>%
  kable_styling(bootstrap_options = c("condensed", "hover", "striped"))
```

There are 1068 visits where there was insufficient data for a diagnosis.

Let's convert those to NA values.

```{r}
# Convert values of insufficient data to NA for kidney disease
data_2$KID[data_2$KID == "Insufficient data"] <- NA

# Drop empty levels
data_2$KID <- droplevels(data_2$KID)

# Create barplot of kidney disease
barplot(table(data_2$KID))
```

Looks good, `KID` is now cleaned!

<a href="#Clean_IVs">Back to top of tabset</a>

## Frailty Related Phenotype

Frailty Related Phenotype (3 out of 4 conditions = YES; WTLOS, PHDWA, HLTWB, HLTVA

-   1 = No
-   2 = Yes
-   9 = Insufficient data

```{r}
# Create barplot of frailty related phenotype
barplot(table(data_2$FRP))

# Pretty print table
kable(table(data_2$FRP), format = "html") %>%
  kable_styling(bootstrap_options = c("condensed", "hover", "striped"))
```

Only 3 patients with insufficient data.

Let's convert them to NA.

```{r}
# Convert values of insufficient data to NA for frailty related phenotype
data_2$FRP[data_2$FRP == "Insufficient Data"] <- NA

# Drop empty levels
data_2$FRP <- droplevels(data_2$FRP)

# Create barplot of frailty related phenotype
barplot(table(data_2$FRP))
```

Looks good, `FRP` is now cleaned.

<a href="#Clean_IVs">Back to top of tabset</a>

## Frailty Phenotype

Frailty Phenotype (3 out of 5 conditions = YES: WTLOS, PHWDA, HLTVA, SLOW, WEAK)

-   1 = No
-   2 = Yes
-   9 = Insufficient Data

```{r}
# Create barplot of frailty phenotype
barplot(table(data_2$FP))

# Pretty print table
kable(table(data_2$FP), format = "html") %>%
  kable_styling(bootstrap_options = c("condensed", "hover", "striped"))
```

357 visits with insufficient data. Let's convert to NA.

```{r}
# Convert values of insufficient data to NA for frailty phenotype
data_2$FP[data_2$FP == "Insufficient Data"] <- NA

# Drop empty levels
data_2$FP <- droplevels(data_2$FP)

# Create barplot of frailty phenotype
barplot(table(data_2$FP))
```

Looks good, `FP` is now cleaned!

<a href="#Clean_IVs">Back to top of tabset</a>

## Total Cholesterol

Total cholesterol mg/dL

```{r}
# Create histogram for total cholesterol
hist(data_2$TCHOL)

# Create qqplot for total cholesterol
qqnorm(data_2$TCHOL)
qqline(data_2$TCHOL)
```

The histogram and qq plot show what may be outliers for total cholesterol at the higher range. How many values are potential outliers?

```{r}
# Create boxplot to assess for outliers for total cholesterol
outlier_tchol <- boxplot(data_2$TCHOL, main = "Boxplot for Total Cholesterol")$out
text(x = rep(1.2, length(outlier_tchol)),
     y = outlier_tchol, labels = outlier_tchol, col = 'red', cex = 0.8)

# Sort by descending total cholesterol
sorted_data <- data_2[order(-data_2$TCHOL),] %>%
  select(newid, TCHOL, years)

# Pretty print table
kable(head(sorted_data), format = "html") %>%
  kable_styling(bootstrap_options = c("condensed", "hover", "striped"))
```

The highest cholesterol value is \~2x higher than the next highest value. Let's see what happens if we remove it.

```{r}
# Delete highest total cholesterol value
data_2$TCHOL[data_2$TCHOL == 613] <- NA 

# Create histogram of total cholesterol
hist(data_2$TCHOL)

# Create qqplot of total cholesterol
qqnorm(data_2$TCHOL)
qqline(data_2$TCHOL)
```

Looks better but still slightly right skewed. This variable had \~30% missing values, so we may end up not using it.

<a href="#Clean_IVs">Back to top of tabset</a>

## Triglycerides

Triglycerides, mg/dL

```{r}
# Create histogram of triglycerides
hist(data_2$TRIG)

# Create qqplot of triglycerides
qqnorm(data_2$TRIG)
qqline(data_2$TRIG)
```

VERY skewed! Based on the qqplots, it looks like we would have to perform a log transform on `TRIG` if we wanted to use it. However we have nearly 50% missing values for this variable, so we should drop it as a covariate.

<a href="#Clean_IVs">Back to top of tabset</a>

## LoW Density Lipoprotein

Low Density Lipoprotein (fasting) mg/dL

```{r}
# Create a histogram for LDL
hist(data_2$LDL)
```

Looks like we may have an erroneous value at the highest range there.

```{r}
# Sort by descending total cholesterol
sorted_data <- data_2[order(-data_2$LDL),] %>%
  select(newid, LDL, years)

# Pretty print table
kable(head(sorted_data), format = "html") %>%
  kable_styling(bootstrap_options = c("condensed", "hover", "striped"))
```

Patients 19 and 413 have the same value of 704 at baseline. Clearly an error with the measurement process.

`LDL` has close to 50% missing values and we will not be using it in our model, so I will move on. But good to know we can't just blindly trust all the values to be correct!

<a href="#Clean_IVs">Back to top of tabset</a>

## Dyslipidemia

Dyslipidemia at visit. fasting TC \>=200 mg/dl or \>=130 mg/dl or HDL \< 40 mg/dl or triglycerides \>=150 mg/dl or use of lipid lowering medications (HICHOLRX) with self report or clinical diagnosis in the past.

-   1 = No
-   2 = Yes
-   3 = No, based on data trajectory
-   4 = Yes from data trajectory
-   9 = Insufficient data

```{r}
# Create barplot of dyslipidemia
barplot(table(data_2$DYSLIP))

# Pretty print table
kable(table(data_2$DYSLIP), format = "html") %>%
  kable_styling(bootstrap_options = c("condensed", "hover", "striped"))
```

There are 718 visits with insufficient data for a dyslipidemia diagnosis.

Let's convert those to NAs to reflect this.

```{r}
# Convert values of insufficient data to NA for dyslipidemia
data_2$DYSLIP[data_2$DYSLIP == "Insufficient data"] <- NA

# Drop empty levels
data_2$DYSLIP <- droplevels(data_2$DYSLIP)

# Create barplot of dyslipidemia
barplot(table(data_2$DYSLIP))
```

Looks good, `DYSLIP` is now cleaned!

<a href="#Clean_IVs">Back to top of tabset</a>

## Depression

Center for Epidemiological Studies Depression Scale ( \>= 16 is depressed).

-   0 - 60

-   -1 = missing

```{r}
# Create histogram for depression score
hist(data_2$CESD)
```

Let's correctly reflect those -1's as NA's

```{r}
# Remove depression scores that were coded as missing
data_2$CESD[data_2$CESD == -1] <- NA

# Create histogram of depression score
hist(data_2$CESD)
```

Looks good. `CESD` is now cleaned!

It IS right skewed though. Will have to handle that if we want to use it.

<a href="#Clean_IVs">Back to top of tabset</a>

## Smoking Status

Smoking status

-   1 = Never smoked
-   2 = Former smoker
-   3 = Current smoker
-   Blank = missing

```{r}
# Create barplot of smoking status
barplot(table(data_2$SMOKE))
```

Looks good, nothing to do here.

<a href="#Clean_IVs">Back to top of tabset</a>

## Drinking Group

Alcohol use since last visit

-   0 = None
-   1 = 1 to 3 drinks/week
-   2 = 4 to 13 drinks/week
-   3 = More than 13 drinks/week
-   Blank = Missing

```{r}
# Create barplot of drinking group
barplot(table(data_2$DKGRP))
```

Looks good, nothing to do here.

<a href="#Clean_IVs">Back to top of tabset</a>

## Heroin or Opiate Use

Took heroin or other opiates since last visit?

-   1 = No
-   2 = Yes
-   -9 = Not specified in form
-   Blank = Missing

```{r}
# Create barplot of heroin or opiate use
barplot(table(data_2$HEROPIATE))

# Pretty print table
kable(table(data_2$HEROPIATE), format = "html") %>%
  kable_styling(bootstrap_options = c("condensed", "hover", "striped"))
```

Only 20 visits where participants did not specify drinking frequency on their form.

Let's correct those to be NA.

```{r}
# Convert values of insufficient data to NA for heroin or opiate use
data_2$HEROPIATE[data_2$HEROPIATE == "Not Specified"] <- NA

# Drop empty levels
data_2$HEROPIATE <- droplevels(data_2$HEROPIATE)

# Create barplot of heroin or opiate use
barplot(table(data_2$HEROPIATE))
```

Looks good. `HEROPIATE` is now cleaned!

<a href="#Clean_IVs">Back to top of tabset</a>

## Intravenous Drug Use

Took/used drugs with a needle since last visit?

-   1 = No
-   2 = Yes
-   Blank = Missing

```{r}
# Create barplot of intravenous drug use
barplot(table(data_2$IDU))
```

Looks good. Nothing to do here

<a href="#Clean_IVs">Back to top of tabset</a>

## Adherence

Adherence to meds taken since last visit

-   1 = 100%
-   2 = 95-99%
-   3 = 75-94%
-   4 \<75%
-   Blank = Missing

```{r}
# Create bar plot of adherence
barplot(table(data_2$ADH))

# Pretty print table
kable(table(data_2$ADH), format = "html") %>%
  kable_styling(bootstrap_options = c("condensed", "hover", "striped"))
```

VERY interesting. I was thinking that 100% vs 95-99% adherence was an arbitrary difference to choose to divide groups on, and was actually planning to merge the two. However, this shows why the experimenters likely made that decision: both groups have close to the same amount of observations (\~500)\> That's really good to know.

We could still play with the idea of simplifying this into two groups: \>= 95% and \< 95%. We will revisit that in the model selection.

<a href="#Clean_IVs">Back to top of tabset</a>

## Race

Race

-   1 = White, non-Hispanic
-   2 = White, Hispanic
-   3 = Black, non-Hispanic
-   4 = Black, Hispanic
-   5 = American Indian or Alaskan Native
-   6 = Asian or Pacific Islander
-   7 = Other 8 = Other Hispanic (created for 2001-03 new recruits)
-   Blank = Missing

```{r}
# Create barplot of race
barplot(table(data_2$RACE))

# Pretty print table
kable(table(data_2$RACE), format = "html") %>%
  kable_styling(bootstrap_options = c("condensed", "hover", "striped"))
```

This all looks coded properly. As is a common thing I am seeing, we have a predominant proportion of participants who are white, non-Hispanic. The data set might be large enough that we can use race as a covariate.

It might be worth dummy coding as white vs non white and see if there are any differences. That's not the main focus of this project though so I will leave that to if I have extra time at the end.

<a href="#Clean_IVs">Back to top of tabset</a>

## Education at Baseline

Baseline or earliest reported education (highest grade or level)

-   1 = 8th grade or less
-   2 = 9,10, or 11th grade
-   3 = 12th grade
-   4 = At least one year college but no degree
-   5 = Four years college / got degree
-   6 = Some graduate work
-   7 = Post-graduate degree
-   Blank = Missing

```{r}
# Create barplot of education at baseline
barplot(table(data_2$EDUCBAS))

# Pretty print table
kable(table(data_2$EDUCBAS), format = "html") %>%
  kable_styling(bootstrap_options = c("condensed", "hover", "striped"))
```

This all checks out. And it looks like there are enough participants in each group (except for 8th grade or less) to run analyses with this variable. It will be interesting to see what relationships arise, as I expect there to be a strong association between education and HIV exposure.

<a href="#Clean_IVs">Back to top of tabset</a>

## HIV Serostatus

HIV Serostatus

-   0 = Negative
-   1 = Positive

```{r}
# Checking that all patients are HIV pos
any(is.na(data_2$hivpos))
```

All patients in this data set are HIV+

<a href="#Clean_IVs">Back to top of tabset</a>

## Age

Age at visit

```{r}
# Create histogram for age
hist(data_2$age)

# Create qqplot for age
qqnorm(data_2$age)
qqline(data_2$age)
```

Nice and normally distributed, how we like it.

<a href="#Clean_IVs">Back to top of tabset</a>

## Antiretroviral Therapy

Take ART at visit

-   0 = NO
-   1 = YES

```{r}
# Create barplot of antiretroviral therapy
barplot(table(data_2$ART))

# Pretty print table
kable(table(data_2$ART), format = "html") %>%
  kable_styling(bootstrap_options = c("condensed", "hover", "striped"))
```

I'm not too sure how useful this variable will be. It just means there were some visits where patients were not given ART, I suppose. But most visits had participants receiving ART.

<a href="#Clean_IVs">Back to top of tabset</a>

## everART

Ever took ART.

-   0 = NO
-   1 = YES

```{r}
# Create barplot of everART
barplot(table(data_2$everART))

# Pretty print table
kable(table(data_2$everART), format = "html") %>%
  kable_styling(bootstrap_options = c("condensed", "hover", "striped"))
```

This has the exact same split as `ART`. Which makes me think they are exactly the same values for each participant

```{r}
# Check if everART and ART are identical
all(data_2$everART == data_2$ART)
```

Yup, this is either an accidental duplicate of `ART`, or there is no distinction of significance between the two. What exactly does "Ever took ART" (the explanation provided by the data dictionary) mean? Was this taken at baseline?

Either way looks like we're not using this variable.

<a href="#Clean_IVs">Back to top of tabset</a>

## Hard Drug Use

Hard drug use (either injection drugs or illicit heroin/opiate use) since last visit

-   0 = No
-   1 = Yes
-   Blank = Missing

```{r}
# Create barplot of hard drug use category
barplot(table(data_2$hard_drugs))

# Pretty print table
kable(table(data_2$hard_drugs), format = "html") %>%
  kable_styling(bootstrap_options = c("condensed", "hover", "striped"))
```

There were 198 visits where participants had used hard_drugs since the last visit.

This variable looks good. We will just have to do some dummy coding to create the categories the researchers were interested in.

<a href="#Clean_IVs">Back to top of tabset</a>

## Summary

For the CONSORT diagram, we just removed:

38 visits where patients did not report income.

X BMI values that were missing and y values that were improbable.

137 visits with insufficient data for a HBP diagnosis

881 visits with insufficient data for a DIAB diagnosis.

507 visits with insufficient data for a LIV34 diagnosis

1068 visits with insuffcient data for a kidney disease diagnosis

3 visits with insuffiient data for FRP diagnosis

357 visits with insufficient data for FP diagnosis

718 visits with insufficent data for dyslipidemia diagnosis

X visits with missing values for CESD

20 visits where heroin or opiate use was not specified

COME BACK TO THIS BECAUSE i THINK I NEED TO COME BACK AND DELETE OUTLIERS.

<a href="#Clean_IVs">Back to top of tabset</a>
:::

## Missingness Redux

We first examined missingness before performing data cleaning just to get a sense of the data set.

Let's compare what our missingness looked like pre- and post-data cleaning.

```{r}
# Visualize missingness for pre-cleaned data
gg_miss_var(data)

# Visualize missingness for post-cleaned data
gg_miss_var(data_2)
```

The order for missingness has changed, now with `KID` at the top, followed by `DIAB`, `LDL`, `TRIG`, and `DYSLIP`.

`TCHOL`, `LIV34`, and `income` are further behind, with levels of missingness that may be salvageable (\~30%).

```{r}
# Visualize missingness for pre-cleaned dataset
vis_miss(data)

# Visualize missingness for post-cleaned dataset
vis_miss(data_2)
```
And for good measure let's now examine missingness in the wide form data set.

```{r}
# Create new wideform data set for first 2 years of study              
data_wide_2 <- pivot_wider(data_2, id_cols = newid, names_from = years, values_from = -c(newid, years))

# Visualize missing values in the wideform data set
vis_miss(data_wide_2)
```

There are no real trends that become apparent when looking at this plot.

To summarize, it appears that diagnoses that were determined by algorithm often had insufficient data to make a diagnosis, so perhaps this is an issue with those algorithms. Additionally, lab measurements of `LDL` and `TRIG` seem to have been too onerous for participants to have gotten. Maybe they opted out of those tests, or maybe the tests were only ordered under certain circumstances.

These would be valuable questions to bring forth to the PI. But for now it appear as if we won't be able to use these variables.




[Variables with a small amount of missingness that we can ignore and let them drop from the analysis when we run the model (i.e. \< 5%)]{.underline}

AGG_MENT, AGG_PHYS, HASH_V, HASHF, FRP, SMOKE, DKGRP, HEROPIATE, IDU, LEU3N, VLOAD, ADH, EDUCBASE, AGE, ART, years, hard-drugs

[Variables with enough missingness we have to address (i.e. 5-20%)]{.underline}

baseline cases (\~8%) BMI, HBP

[Variables with \>20% that are edge cases, may have to drop, maybe can salvage.]{.underline}

TCHOL

income

LIV34

[Variables with so much missing data we have to drop those variables \>40%]{.underline}

-   `LDL`

-   `TRIG`

-   DIAB

-   KID

-   DYSLIP

# Variable Creation

Here we will create our change scores for our dependent variables `LEU3N`, `VLOAD`, `AGG_MENT`, and `AGG_PHYS` to assess treatment response to ART

```{r}
# Create change scores for outcome variables
data_wide_2$VLOAD_CHANGE <- data_wide_2$VLOAD_2 - data_wide_2$VLOAD_0
data_wide_2$LEU3N_CHANGE <- data_wide_2$LEU3N_2 - data_wide_2$LEU3N_0
data_wide_2$AGG_MENT_CHANGE <- data_wide_2$AGG_MENT_2 - data_wide_2$AGG_MENT_0
data_wide_2$AGG_MENT_PHYS <- data_wide_2$AGG_PHYS_2 - data_wide_2$AGG_PHYS_0
```

We need to make dummy codes for our hard drug use groups.

```{r}
# Making dummy codes for hard drug use group
data_wide_2$current_drug <- ifelse(data_wide_2$hard_drugs_2 == "Yes", 1, 0) 
data_wide_2$previous_drug <- ifelse((data_wide_2$hard_drugs_1 == "Yes" | data_wide_2$hard_drugs_0 == "Yes") & data_wide_2$hard_drugs_2 == "No", 1, 0)
data_wide_2$never_drug <- ifelse(data_wide_2$hard_drugs_1 == "No" & data_wide_2$hard_drugs_0 == "No", 1, 0)
```


# Correlation Matrix

```{r}

# # Have to filter down to only numeric variables here
# 
# 
# # Make a correlation matrix with all variables of the trimmed data set
# correlation_matrix <- cor(data_wide_2, use = "complete.obs")
# 
# # Plot the matrix
# corrplot(correlation_matrix, method = "circle")
# 
# # Trim the matrix
# correlation_matrix[upper.tri(correlation_matrix)] <- NA

```










```{r}
# Gets average viral load per patient for example, can do min and max etc.
summary_table <- data %>%
  group_by(newid) %>%
  summarize(mean_vload = mean(VLOAD))
summary_table

colnames(data)
```


# To do

-   Go back and add title and axes labels to all your plots dude
-   add #greentext to every chunk
-   organize tabs by outcome variable / covariates
-   investigate these to analyze non normal data: quantile regression, best normalized r package, gamma link GLM

"The transformations contained in this package and implemented in bestNormalize are reversible (i.e., 1-1), which allows for straight-forward interpretation and consistency. In other words, any analysis performed on the normalized data can be interpreted using the original unit (see application)." Cool.


```{r}
library(mice)

imputed_data <- mice(mtcars, m = 5, method = 'pmm', maxit = 50, seed = 500)

# Check the imputed data
summary(imputed_data)

complete_data <- complete(imputed_data, 1) # 1 refers to the first imputed dataset
complete_data

```

