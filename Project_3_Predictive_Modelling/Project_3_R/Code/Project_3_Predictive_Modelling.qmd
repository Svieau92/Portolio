---
title: "Predictive Modelling"
subtitle: "Advanced Data Analysis - Project 3"
author: "Sean Vieau"
date: October 30, 2024
format: html
smooth-scroll: true
editor: visual
toc: true
bibliography: references.bib
---

```{r setup, include=FALSE}
# Sets the default for all chunks as echo = TRUE (ensures they show unless specified not to)
knitr::opts_chunk$set(echo = TRUE)

# Create a function to pretty print our dataframes
pretty_print <- function(x) {
  kable(x, format = "html") |>
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"))
}

# Create a function to pretty print useful parameters of a regression model
model_results <- function(x) {
  # Create a table of the coefficients of the model
  coefficients_ <- summary(x)$coefficients[]
  
  # Perform Bonferroni correction 
  p_values <- summary(x)$coefficients[,4] # This line selects the fourth column of the resulting coefficients      table from summary(model), which is the p-values
  p_adjusted <- p.adjust(p_values, method = "bonferroni")
  
  # Get the 95% CIs
  conf_intervals <- confint(x)
  
  # Compare adjusted p-values to unadjusted p-values, with 95% CI's
  model_output <- cbind(coefficients_, p_adjusted, conf_intervals)
  
  # Pretty print results
  pretty_print(model_output)
}

# Set options to avoid scientific notation
options(scipen = 999)

# Make a function for quick plotting histograms and qq plot
distr_plots <- function(data, variable, bins_choose = 30) {
  
hist_plot <- ggplot(data, aes_string(x = variable)) +
  geom_histogram(bins = bins_choose) +
  theme_minimal() +
  labs(title = paste("Histogram of", variable),
       x = variable,
       y = "Count")

# Create qqplot
qq_plot <- ggplot(data, aes_string(sample = variable)) +
  geom_qq() +
  geom_qq_line() +
  theme_minimal() +
  labs(title = paste("QQ Plot of", variable))

# Plot side by side
grid.arrange(hist_plot, qq_plot, ncol = 2)
}
```

# Introduction

The aim of the current study is to investigate whether MRI image features extracted from baseline kidney images can enhance the prediction of disease progression in young Autosomal Dominant Polycystic Kidney Disease (ADPKD) patients. This study is important because recent literature has demonstrated the inclusion of MRI image features improves the prognostic accuracy of models in adults, but this has not yet been demonstrated in children. Improving prediction models could greatly assist in diagnosing and predicting kidney disease outcomes in children, which is more challenging than in adults and could lead to improved treatment.

::: {style="text-align: center;"}
<img src="/Project_3_Predictive_Modelling/Project_3_R/Media/Project_Description.png" style="width:80%;"/>
:::

#### Task One

![](/Project_3_Predictive_Modelling/Project_3_R/Media/Task_One.png){fig-align="center" width="80%"}

#### Task Two

![](/Project_3_Predictive_Modelling/Project_3_R/Media/Task_Two.png){fig-align="center" width="80%"}

### Clinical Hypotheses

The two clinical hypotheses for this study are that including MRI image features will improve model performance compared to using baseline kidney volume alone in models predicting

1.  Percentage change of total kidney volume growth

2.  Classification of a patient as having fast or slow progression of the disease.

# Background

Here we will review background information on the methodology and variables provided in this data set.

## Gabor Transform

#### What is it

The Gabor Transform, named after Dennis Gabor, is a powerful technique for analyzing the [**texture**]{.underline} of MRI images. By convolving the image with a Gabor filter, it becomes possible to discriminate between features based on intensity differences. This allows the target region and background to be differentiated if they possess distinct features, as they will exhibit different intensity levels. Moreover, the Gabor Transform has tunable parameters, such as the frequency of the sinusoidal wave, which can be adjusted to extract specific textures from the images. Higher frequencies are ideal for capturing fine textures, while lower frequencies are better suited for coarse textures.

#### How it Works

The Gabor Filter first applies a Gaussian Envelope to focus on a small region of the image. It then applies a sinusoidal wave that oscillates at a specific frequency and captures the variation in intensities at that frequency and orientation within that region.

#### Example of Gabor Transform

![Example of applying a Gaussian, then Gabor, and Laplacian Filter (GGL) for differentiation between two different textures (+'s vs L's) ([2](https://link.springer.com/article/10.1007/BF00204594)).](images/clipboard-573562689.png){fig-align="center"}

#### Image Features Provided by the Gabor Transform

In general, Gabor functions can easily extract features of:

-   Spatial Frequency (e.g. how often pixel intensity changes in a given area)

-   Density (e.g. concentration of features within a certain area)

-   Orientation (e.g. Textures or edges at 0°, 45°, 90°, 135°)

-   Phase (e.g. alignment/distance of features)

-   Energy (e.g. overall intensity)

Sources: [1](https://link.springer.com/article/10.1007/s11042-020-09635-6), [2](https://link.springer.com/article/10.1007/BF00204594)

## Gray Level Co-Occurrence Matrix

#### What is it

The Gray Level Co-Occurrence Matrix (GLCM) is another powerful tool for extracting **texture** features from an MRI image. GLCM works under the assumption that the textural information in an image is contained in the "average" spatial relationship that the gray tones in the image have to each other. For example, when a small patch of the picture has little variation of features, the dominant property is tone; When a small patch of a picture has high variation, the dominant property is texture ([3](https://ieeexplore.ieee.org/document/4309314)).

#### How it Works

GLCM examines the spatial relationship between pairs of pixels in an image and calculates how often pairs of pixel values occur at a specified distance and orientation. It does this by computing a set of gray-tone spacial-dependence matrices for various angular relationships and distances between neighboring resolution cell pairs on the image ([3](https://ieeexplore.ieee.org/document/4309314)).

#### Example of GLCM

![Example of textural features extracted from two different land-use category images ([3](https://ieeexplore.ieee.org/document/4309314)).](images/clipboard-759848695.png){width="80%"}

#### Image Features Provided by GLCM

In general, GLCM provides information on the following features:

-   Homogeneity

-   Linear Structure

-   Contrast

-   Number and Nature of Boundaries

-   Complexity of the Image

## Local Binary Pattern

#### What is it

The Local Binary Pattern (LBP) is a third powerful method for extracting [**texture**]{.underline} features from an image. An important and fundamental property of texture is how uniform the patterns are, and LBP captures this by detecting these uniform patterns in circular neighborhoods at any rotation and spatial resolution. LBP is rotation invariant, meaning it does not matter what rotation the image is at; it will always extract very similar features) ([4](https://ieeexplore.ieee.org/document/1017623)).

#### How it Works

LBP works by comparing the intensity of a central pixel with its neighboring pixels and encoding this relationship into a binary pattern. For each neighbor, if it's intensity is greater or equal to the central pixel, it gets assigned a 1 (otherwise a 0). The binary values of all neighbors are then concatenated to form a binary number, and this number is converted into a decimal that represent the LBP for the central pixel ([4](https://ieeexplore.ieee.org/document/1017623)).

#### Example of LBP

![Example of the 36 unique comparisons that can be made between neighboring pixels ([4](https://ieeexplore.ieee.org/document/1017623)).](images/clipboard-3679699747.png){fig-align="center"}

#### Code Information

![](/Project_3_Predictive_Modelling/Project_3_R/Media/Code_Information.png){fig-align="center" width="80%"}

#### Image Features Provided by the LBP

In general, the LBP provides image features on:

-   Uniformity

-   Local Contrast

-   Texture Description

-   Spatial Patterns

-   Gray Level Distribution

# Method

## Study Design

The investigators recruited 71 young patients with ADPKD and collected MRI data at baseline and after 3 years. Additionally, the height corrected kidney volume for each patient was collected at baseline and 3 years by a physician, and the percentage change calculated. Patients were classified as having slow or fast progression of the disease based on this percentage change. Image features were extracted from the baseline MRI images including 2 image features on kidney geometric information, 5 features based on Gabor transform, 2 features based on gray level co-occurrence matrix, 5 features based on image textures, and 5 features based on local binary pattern.

### Statistical Hypotheses

1.  A **linear regression model** predicting **percentage change of total kidney volume growth** including MRI image features and baseline kidney volume will have better performance than a model with baseline kidney volume alone, as determined by specificity, sensitivity, PPV,  NPV,  accuracy, and AUC.

2.  A **logistic regression model** predicting **classification of disease progression as slow or fast** including MRI image features and baseline kidney volume will have better performance than a model with baseline kidney volume alone, as determined by specificity, sensitivity, PPV,  NPV,  accuracy, and AUC.

# Data Preparation

#### Load Packages

```{r, message = FALSE}
library(dplyr)
library(ggplot2)
library(tidyverse)
library(plotly) # Used for interactive plots
library(kableExtra) # Used for pretty printing (kable_styling)
library(ggridges) # Used for plotting distributions
library(naniar) # Used to visualize missingness
library(patchwork) # Used to chain ggplots together
library(gridExtra) # Used to plot ggplots side by side
library(corrplot) # Used for correlation matrix
library(caret) # Used for K-fold cross validation
library(bestNormalize) # Used to find best transformation
```

#### Read .CSV

```{r}
# Read in data
data <- read_csv("C:/Users/sviea/Documents/Portfolio/Project_3_Predictive_Modelling/Project_3_R/Raw_Data/Project3_data.csv")
```

#### Examine Data Set

```{r}
# Examine data
pretty_print(head(data))

# Check data types
glimpse(data)
```

#### Create Factors

Everything is appropriately coded as a double.

Let's just convert `progression` into a factor that contains the levels for "slow" or "fast and we're good to go. We will also rename some variables for convenience.

```{r}
# Convert progression into a factor
data <- data |> 
  mutate(progression = factor(progression,
         levels = c(0,1),
         labels = c("Slow", "Fast")))

# Rename Subject ID and kidney volume variables so they are easier to access
data <- data |> 
  rename(Subject_ID = `Subject ID`,
         kidvol_base = tkvht_base,
         kidvol_visit2 = tkvht_visit2,
         kidvol_change = tkvht_change) 

# Double check data types
glimpse(data)
```

#### Summary

Our data set consists of:

-   71 patients
-   19 MRI image features
-   4 kidney volume variables

# Missingness

```{r}
# Examine missingness
gg_miss_var(data)

# Examine missingness percents
vis_miss(data)
```

We have 0 missing values!

# Data Split

#### Background

We have an N of 71, which is notably small.

In order to increase the power of the study, we will perform a 5-fold cross validation. Thus, all patients will be in both the training and validation sets, providing a comprehensive evaluation.

![](images/clipboard-802573844.png)

#### Perform the Split

To begin with, we will divide our data set into 5 even folds.

First we randomly shuffle the data so it is not organized by Subject ID

```{r}
# Set seed for reproducibility
set.seed(123)

# Shuffle the rows
data <- data |> slice_sample(n = nrow(data))
```

Then we create 5-folds of our data set.

```{r}
# Create labels for Folds1-5
data |> 
  mutate(fold_number = rep(c("Fold1", "Fold2", "Fold3", "Fold4", "Fold5"), length.out = n())) -> data

#### Create separate data frames for each fold

# Create Fold 1
data1_train <- data |> 
  filter(fold_number != "Fold1")
data1_test <- data |> 
  filter(fold_number == "Fold1")

# Create fold 2
data2_train <- data |> 
  filter(fold_number != "Fold2")
data2_test <- data |> 
  filter(fold_number == "Fold2")

# Create fold 3
data3_train <- data |> 
  filter(fold_number != "Fold3")
data3_test <- data |> 
  filter(fold_number == "Fold3")

# Create fold 4
data4_train <- data |> 
  filter(fold_number != "Fold4")
data4_test <- data |> 
  filter(fold_number == "Fold4")

# Create fold 5
data5_train <- data |> 
  filter(fold_number != "Fold5")
data5_test <- data |> 
  filter(fold_number == "Fold5")
```

And examine `Fold1` and ensure the dimensions check out.

```{r}
# Check dimensions of Fold1
dim(data1_train)

# Check dimensions of Fold1
dim(data1_test)

# Check for any overlapping patients
inner_join(data1_train, data1_test, by = "Subject_ID")
```

We have 56 patients in the fold 1 training set, and 15 in the test set, with no overlapping IDs. Perfect!

# Data Examination

Examining the distributions of the entire data set before splitting can cause subtle biases. For example, transformations or decisions based on the entire data set might inadvertently incorporate patterns from the test data into the training process!

We essentially want to mimic real world situations, where we would not know the distributions of our test set prior to analysis.

Thus, we will perform the initial examination of the distribution of our variables, using `Fold1_train`.

## Univariate Distributions

### Outcome Variables {#Outcomes_Uni}

::: panel-tabset
## Kidney Volume

#### Baseline Kidney Volume

```{r}
# Visualize baseline kidney volume
ggplot(data1_train, aes(x = kidvol_base)) +
  geom_histogram() + 
  theme_minimal() + 
  labs(title = "Histogram for Baseline Kidney Volume",
       x = "Baseline Kidney Volume",
       y = "Count")
```

Kidney volume at baseline appears to not be normally distributed.

However, I learned in the last project that taking the change score (or percent) will make the outcome normally distributed, as it is a way of accounting for between subject variance. Let's check that that's the case this time.

#### Percent Change in Kidney Volume

```{r}
#| fig-height: 3.5
#| fig-width: 8
# Create histogram
hist_plot <- ggplot(data1_train, aes(x = kidvol_change)) +
  geom_histogram(binwidth = 4.5) +
  theme_minimal() +
  labs(title = "Histogram of Kidney Volume Change",
       y = "Count",
       x = "Kidney Volume Change (%)")

# Create qqplot
qq_plot <- ggplot(data1_train, aes(sample = kidvol_change)) +
  geom_qq() +
  geom_qq_line() +
  theme_minimal() +
  labs(title = "QQ Plot of Kidney Volume Change")

# Plot side by side
grid.arrange(hist_plot, qq_plot, ncol = 2)
```

`kidvol_change` is not quite normally distributed.

##### Log Transform

Let's check if a logarithmic or sqrt transformation will be better.

```{r}
# Use bestNormalize R package to select the best transformation
BNObject <- bestNormalize(data1_train$kidvol_change)
BNObject

# Create histogram
hist_plot <- ggplot(data1_train, aes(x = log(kidvol_change))) +
  geom_histogram(binwidth = 0.5) +
  theme_minimal() +
  labs(title = "Histogram of Log Kidney Volume Change",
       y = "Count",
       x = "Kidney Volume Change (%)")

# Create qqplot
qq_plot <- ggplot(data1_train, aes(sample = log(kidvol_change))) +
  geom_qq() +
  geom_qq_line() +
  theme_minimal() +
  labs(title = "QQ Plot of Log Kidney Volume Change")

# Plot side by side
grid.arrange(hist_plot, qq_plot, ncol = 2)
```

##### Square Root Transform

```{r}
# Create histogram
hist_plot <- ggplot(data1_train, aes(x = sqrt(kidvol_change))) +
  geom_histogram(binwidth = 0.5) +
  theme_minimal() +
  labs(title = "Histogram of Sqrt Kidney Volume Change",
       y = "Count",
       x = "Kidney Volume Change (%)")

# Create qqplot
qq_plot <- ggplot(data1_train, aes(sample = sqrt(kidvol_change))) +
  geom_qq() +
  geom_qq_line() +
  theme_minimal() +
  labs(title = "QQ Plot of Sqrt Kidney Volume Chage")

# Plot side by side
grid.arrange(hist_plot, qq_plot, ncol = 2)
```

The log transform appears the best for us.

```{r}
# Create log change in kidney volume
data1_train <- data1_train |> 
  mutate(kidvol_change_log = log(kidvol_change))
```

#### Outliers

It also appears that we have two outlier values. Let's assess using boxplots.

```{r}
# Calculate IQR and identify outliers
IQR_val <- IQR(data1_train$kidvol_change)
Q1 <- quantile(data1_train$kidvol_change, 0.25)
Q3 <- quantile(data1_train$kidvol_change, 0.75)
lower_bound <- Q1 - (1.5*IQR_val)
upper_bound <- Q3 + (1.5*IQR_val)

# Create boxplot and flag outliers
ggplot(data1_train, aes(x = "", y = kidvol_change)) +
  geom_boxplot(alpha = 0.8) +
  geom_text(aes(label = ifelse(kidvol_change < lower_bound | kidvol_change > upper_bound, Subject_ID, "")), hjust = -0.5, color = "red") +
  theme_minimal() +
  labs(title = "Boxplot of Kidney Volume Change",
       x = "",
       y = "Kidney Volume Change")
```

Patients 37 and 51 are flagging as potential outliers. Let's examine them.

```{r}
# Examine outlier patients
data1_train |> 
  arrange(desc(kidvol_change)) |> 
  head() |> 
  pretty_print()
```

Interesting, it looks like these patients also have high values for certain MRI image features, such as GLCM 1 and 2. Additionally, they do not have the highest visit 2 kidney volumes, they simply increases in size the most.

### Summary

##### Transformation

It appears that a log transform of `kidvol_change` gives us an approximately normal distribution, and this will be the value used for the outcome variable for task 1 going forward.

##### Outliers

While patients 37 and 51 are potential outliers on `kidvol_change`, these are NOT erroneous values. On the contrary, they are backed up by similar high values on other measurements such as `glcm2`, but are not universally high on all MRI image features. In other words, these appear to be very realistic values.

In summary, all values are derived from authentic biological data. They will be examined using the jackknife residuals to assess for leverage and influence, but we are motivated to retain them in the analysis.

[Top of Tabset](#Outcomes_Uni)

## Disease Progression

```{r}
# Create bar plot
ggplot(data1_train, aes(x = progression, fill = progression)) +
  geom_bar() +
  theme_minimal() +
  scale_fill_brewer(palette = "Pastel2") +
  ylim(0, 45) +
  labs(title = "Count of Slow vs Fast Disease Progression",
       x = "Disease Progression",
       y = "Count",
       fill = "Progression")

# Create Table
table(data1_train$progression) 
```

We have an equal amount of patients that had slow vs fast disease progression.

[Top of Tabset](#Outcomes_Uni)

## Summary

#### Kidney Volume Change

`kidvol_change` is approximately normally distributed following a log transform, with 2 potential outliers we will keep an eye on.

#### Slow vs Fast Disease Progression

We have an even count between patients who had slow vs fast disease progression.

[Top of Tabset](#Outcomes_Uni)
:::

# Feature Engineering {#Features_Uni}

An assumption for a linear regression is that there is a linear relationship between the predictor and the outcome variable.

Thus for the initial step, we will examine the distribution of our variables and create a quadratic term or take the log for each variable as necessary.

::::::::: panel-tabset
## Geometry

::: panel-tabset

## Geom1

```{r}
#| fig-height: 3.5
#| fig-width: 8
# Create histogram
hist_plot <- ggplot(data1_train, aes(x = geom1)) +
  geom_histogram(binwidth = 30) +
  theme_minimal() +
  labs(title = "Histogram of Geom1",
       x = "Geom1",
       y = "Count")

# Create qqplot
qq_plot <- ggplot(data1_train, aes(sample = geom1)) +
  geom_qq() +
  geom_qq_line() +
  theme_minimal() +
  labs(title = "QQ Plot of Geom1")

# Plot side by side
grid.arrange(hist_plot, qq_plot, ncol = 2)
```

There is 1 patient with a drastically lower value than everyone else. Let's examine.

```{r}
# Filter to most negative geom1 value
data1_train |> 
  arrange(geom1) |> 
  select(Subject_ID, geom1, geom2) |> 
  head() |> 
  pretty_print()
```

Patient 48 appears to be an outlier on `geom1`.

#### Perform Yeo Johnson Transformation

Since we have negatie we have to perform the yeo johnson transformation or we will lose observations as `NA` values.

```{r}
# Use bestNormalize R package to select the best transformation
BNObject <- bestNormalize(data1_train$geom1)
BNObject

# Create histogram
hist_plot <- ggplot(data1_train, aes(x = yeojohnson(geom1)$x.t)) +
  geom_histogram(binwidth = .4) +
  theme_minimal() +
  labs(title = "Histogram of Yeo Johnson Geom1",
       y = "Count",
       x = "Log Geom1")

# Create qqplot
qq_plot <- ggplot(data1_train, aes(sample = yeojohnson(geom1)$x.t)) +
  geom_qq() +
  geom_qq_line() +
  theme_minimal() +
  labs(title = "QQ Plot of Yeo Johnson Geom1")

# Plot side by side
grid.arrange(hist_plot, qq_plot, ncol = 2)

# Perform transform
data1_train <- data1_train |> 
  mutate(geom1_yeo = yeojohnson(geom1)$x.t)
```

`geom` is more normally distributed after a Yeo Johnson transform.

#### Summary

`geom1` is normally distributed following a Yeo Johnson transform. There is potentially one outlier on the negative side, but this seemed to not be a factor after transforming.

[Top of Tabset](#Features_Uni)

## Geom2

```{r}
#| fig-height: 3.5
#| fig-width: 8
# Create histogram
hist_plot <- ggplot(data1_train, aes(x = geom2)) +
  geom_histogram(binwidth = 300) +
  theme_minimal() +
  labs(title = "Histogram of Geom2",
       x = "Geom2",
       y = "Count")

# Create qqplot
qq_plot <- ggplot(data1_train, aes(sample = geom2)) +
  geom_qq() +
  geom_qq_line() +
  theme_minimal() +
  labs(title = "QQ Plot of Geom2")

# Plot side by side
grid.arrange(hist_plot, qq_plot, ncol = 2)
```

`geom2` appears to be logarithmic. Let's adjust.

#### Square Root Transformation

```{r}
#| fig-height: 3.5
#| fig-width: 8
# Create new variable
data1_train <- data1_train |> 
 mutate(geom2_sqrt = sqrt(geom2),
        geom2_log = log(geom2)) 

# Create histogram
hist_plot <- ggplot(data1_train, aes(x = geom2_sqrt)) +
  geom_histogram(binwidth = 20) +
  theme_minimal() +
  labs(title = "Histogram of Geom2_sqrt",
       x = "Geom2_sqrt",
       y = "Count")

# Create qqplot
qq_plot <- ggplot(data1_train, aes(sample = geom2_sqrt)) +
  geom_qq() +
  geom_qq_line() +
  theme_minimal() +
  labs(title = "QQ Plot of Geom2_sqrt")

# Plot side by side
grid.arrange(hist_plot, qq_plot, ncol = 2)
```

Sqrt kind of makes it better, but not quite.

#### Log Transformation

Let's examine the log.

```{r}

# Create histogram
hist_plot <- ggplot(data1_train, aes(x = log(geom2))) +
  geom_histogram(binwidth = 1) +
  theme_minimal() +
  labs(title = "Histogram of Geom2 log",
       x = "Geom2 log",
       y = "Count")

# Create qqplot
qq_plot <- ggplot(data1_train, aes(sample = log(geom2))) +
  geom_qq() +
  geom_qq_line() +
  theme_minimal() +
  labs(title = "QQ Plot of Geom2 log")

# Plot side by side
grid.arrange(hist_plot, qq_plot, ncol = 2)
```

#### Yeo Johnson Transform

A Yeo Johnson transformation

```{r}
# Use bestNormalize R package to select the best transformation
BNObject <- bestNormalize(data1_train$geom2)
BNObject

# Create histogram
hist_plot <- ggplot(data, aes(x = yeojohnson(geom2)$x.t)) +
  geom_histogram(binwidth = 1) +
  theme_minimal() +
  labs(title = "Histogram of Geom2 Yeo",
       x = "Geom2 Yeo",
       y = "Count")

# Create qqplot
qq_plot <- ggplot(data, aes(sample = yeojohnson(geom2)$x.t)) +
  geom_qq() +
  geom_qq_line() +
  theme_minimal() +
  labs(title = "QQ Plot of Geom2 Yeo")

# Plot side by side
grid.arrange(hist_plot, qq_plot, ncol = 2)

# Create the variable
data1_train <- data1_train |> 
  mutate(geom2_yeo = yeojohnson(geom2)$x.t)
```

I think log or yeo is our best bet here.

#### Summary

While there is no transformation that makes `geom2` normally distributed, log or yeojohnson transformation may help and will be considered. Let's select Yeo Johnson as it looks slightly better.

[Top of Tabset](#Features_Uni)
:::

## Gabor

::: panel-tabset
## Gabor1

```{r}
#| fig-height: 3.5
#| fig-width: 8
# Create histogram
hist_plot <- ggplot(data1_train, aes(x = gabor1)) +
  geom_histogram(binwidth = 0.2) +
  theme_minimal() +
  labs(title = "Histogram of Gabor1",
       x = "Gabor2",
       y = "Count")

# Create qqplot
qq_plot <- ggplot(data1_train, aes(sample = gabor1)) +
  geom_qq() +
  geom_qq_line() +
  theme_minimal() +
  labs(title = "QQ Plot of Gabor1")

# Plot side by side
grid.arrange(hist_plot, qq_plot, ncol = 2)
```

`gabor1` is normally distributed.

[Top of Tabset](#Features_Uni)

## Gabor2

```{r}
#| fig-height: 3.5
#| fig-width: 8
# Make plots
distr_plots(data1_train, "gabor2", 12)
```

`gabor2` appears approximately normally distributed.

```{r}
# Use bestNormalize R package to select the best transformation
BNObject <- bestNormalize(data1_train$gabor2)
BNObject
```

It looks like a yeo johnson transformation will be most appropriate here.

```{r}
data1_train <- data1_train |> 
  mutate(gabor2_yeo = yeojohnson(gabor2)$x.t)

distr_plots(data1_train, "gabor2_yeo", 10)
```

A yeo johnson transformation *slightly* improves the normality of `gabor2`.

[Top of Tabset](#Features_Uni)

## Gabor3

```{r}
#| fig-height: 3.5
#| fig-width: 8
# Make plots
distr_plots(data1_train, "gabor3", 30)
```

[Top of Tabset](#Features_Uni)

`gabor3` appears approximately normally distributed, with some potential outliers.

```{r}
# Use bestNormalize R package to select the best transformation
BNObject <- bestNormalize(data1_train$gabor3)
BNObject
```

Transformations don't seem to help much here. We want the values to be closest to 1 to indicate more normality.

#### Summary

Transformation doesn't seem to help much with `gabor3`.

[Top of Tabset](#Features_Uni)

## Gabor4

```{r}
#| fig-height: 3.5
#| fig-width: 8
# Make plots
distr_plots(data1_train, "gabor4", 30)
```

`gabor4` may be exponential.

```{r}
#| fig-height: 3.5
#| fig-width: 8
data1_train <- data1_train |> 
  mutate(gabor4_log = log(gabor4))

distr_plots(data1_train, "gabor4_log", 12)
```

`gabor4_log` appears to be normally distributed.

#### Summary

The log transform of `gabor4` made it approximately normal.

[Top of Tabset](#Features_Uni)

## Gabor5

```{r}
#| fig-height: 3.5
#| fig-width: 8
# Make plots
distr_plots(data1_train, "gabor5", 30)
```

`gabor5` looks logarithmic potentially.

#### Log Transform

```{r}
# Use bestNormalize R package to select the best transformation
BNObject <- bestNormalize(data1_train$gabor5)
BNObject

# Create log term
data1_train <- data1_train |> 
  mutate(gabor5_log = log(gabor5))
 
distr_plots(data1_train, "gabor5_log", 30)
```

That's kind of better.

```{r}
# Calculate IQR and identify outliers
IQR_val <- IQR(data1_train$gabor5_log)
Q1 <- quantile(data1_train$gabor5_log, 0.25)
Q3 <- quantile(data1_train$gabor5_log, 0.75)
lower_bound <- Q1 - (1.5*IQR_val)
upper_bound <- Q3 + (1.5*IQR_val)

# Create boxplot and flag outliers
ggplot(data1_train, aes(x = "", y = gabor5_log)) +
  geom_boxplot(alpha = 0.8) +
  geom_text(aes(label = ifelse(gabor5_log < lower_bound | gabor5_log > upper_bound, Subject_ID, "")), hjust = -0.5, color = "red") +
  theme_minimal() +
  labs(title = "Boxplot of Gabor5",
       x = "",
       y = "Gabor5")
```

#### Summary

`gabor5` is not normally distributed, and no transform seems to help much. It may be dropped from consideration.

[Top of Tabset](#Features_Uni)

:::

## GLCM

::: panel_tabset

## GLCM1

```{r}
#| fig-height: 3.5
#| fig-width: 8
# Make plots
distr_plots(data1_train, "glcm1", 10)
```

`glcm1` is approximately normally distributed.

[Top of Tabset](#Features_Uni)

## GLCM2

```{r}
#| fig-height: 3.5
#| fig-width: 8
# Make plots
distr_plots(data1_train, "glcm2", 10)
```

`glcm2` may be logarithmic.

#### Log Transform

```{r}
# Create log variable
data1_train <- data1_train |> 
  mutate(glcm2_log = log(glcm2))

#| fig-height: 3.5
#| fig-width: 8
# Make plots
distr_plots(data1_train, "glcm2_log", 10)
```

```{r}
# Use bestNormalize R package to select the best transformation
BNObject <- bestNormalize(data1_train$glcm2)
BNObject
```

It appears that this is the best transformation we will get.

#### Summary

The best transformation for `glcm2` is a log transform.

[Top of Tabset](#Features_Uni)

:::

## TXTI

::: panel-tabset
## TXT1

```{r}
#| fig-height: 3.5
#| fig-width: 8
# Make plots
distr_plots(data1_train, "txti1", 10)
```

#### Summary

`txti1` appears normally distributed

[Top of Tabset](#Features_Uni)

## TXTI2

```{r}
#| fig-height: 3.5
#| fig-width: 8
# Make plots
distr_plots(data1_train, "txti2", 10)
```

#### Summary

`txti2` appears normally distributed.

[Top of Tabset](#Features_Uni)

## TXTI3

```{r}
#| fig-height: 3.5
#| fig-width: 8
# Make plots
distr_plots(data1_train, "txti3", 10)
```

#### Summary

`txti3` appears normally distributed.

[Top of Tabset](#Features_Uni)

## TXTI4

```{r}
#| fig-height: 3.5
#| fig-width: 8
# Make plots
distr_plots(data1_train, "txti4", 10)
```

`txti4` may be logarithmic.

#### Log Transform

```{r}
#| fig-height: 3.5
#| fig-width: 8
data1_train <- data1_train |> 
  mutate(txti4_log = log(txti4))

# Make plots
distr_plots(data1_train, "txti4_log", 10)
```

That's better, still not perfect.

```{r}
# Use bestNormalize R package to select the best transformation
BNObject <- bestNormalize(data1_train$txti4)
BNObject
```

Log is better than sqrt so we will use log.

#### Summary

The best transformation for `txtxi4` is log, though it is still not perfect.

[Top of Tabset](#Features_Uni)

## TXTI5

```{r}
#| fig-height: 3.5
#| fig-width: 8
#| # Make plots
distr_plots(data1_train, "txti5", 10)
```

May be logarithmic.

#### Log Transform

```{r}
#| fig-height: 3.5
#| fig-width: 8
data1_train <- data1_train |> 
  mutate(txti5_log = log(txti5))

#| # Make plots
distr_plots(data1_train, "txti5_log", 10)
```

That looks better. Mostly.

```{r}
# Use bestNormalize R package to select the best transformation
BNObject <- bestNormalize(data1_train$txti5)
BNObject
```

Log performs better than sqrt, that's as good as we can get.

#### Summary

The best transformation for `txti5` is logarithmic.

[Top of Tabset](#Features_Uni)
:::

## LBP

:::: panel-tabset
## LBP1

```{r}
#| fig-height: 3.5
#| fig-width: 8
#| # Make plots
distr_plots(data1_train, "lbp1", 20)
```

Looks normally distributed with 1 outlier.

#### Remove Outlier

```{r}
# Examine LBP outlier
data1_train |> 
  arrange(desc(lbp3)) |> 
  select(Subject_ID, lbp1, lbp2, lbp3, lbp4, lbp5) |> 
  head() |> 
  pretty_print()
```

Patient 52 is a CLEAR outlier (lbp3 = 11, no othe patients have an lbp3 over 1).

We remove them.

```{r}
# Remove outlier patient
data1_train <- data1_train |> 
  filter(!Subject_ID == 52)
```

#### Remake Plots

```{r}
#| fig-height: 3.5
#| fig-width: 8
#| # Make plots
distr_plots(data1_train, "lbp1", 10)
```

That's close to normal, let's check if it could be better.

```{r}
# Use bestNormalize R package to select the best transformation
BNObject <- bestNormalize(data1_train$lbp1)
BNObject
```

#### Yeo Johnson Transform

It seems a yeo transformation will be the best (since we have negatives and a log and sqrt transformation will cause NA values)

```{r}
#| fig-height: 3.5
#| fig-width: 8
# Perform transform
data1_train <- data1_train |> 
  mutate(lbp1_yeo = yeojohnson(lbp1)$x.t)

#| # Make plots
distr_plots(data1_train, "lbp1_yeo", 10)
```

It's slightly better, but not by much.

#### Summary

A yeo johnson transformation helps a little bit.

We will choose to not transform this variable.

::: callout-note
Note: Patient 52 is a clear outlier and possibly erroneous value on the `lbp` features. Ensure to remove it from all data sets!
:::

[Top of Tabset](#Features_Uni)

## LBP2

```{r}
#| fig-height: 3.5
#| fig-width: 8
#| # Make plots
distr_plots(data1_train, "lbp2", 20)
```

We appear to have a different outlier for `lbp2`.

```{r}
# Examine outlier
data1_train |> 
  arrange(desc(lbp2)) |> 
  select(Subject_ID, lbp1, lbp2, lbp3, lbp4, lbp5) |> 
  head() |> 
  pretty_print()
```

Patient 48 is an outlier on `lbp2` (lbp2 = 3, all other patients are essentially below 1).

We remove them

```{r}
# Remove outlier
data1_train <- data1_train |> 
  filter(!Subject_ID == 48)
```

#### Remake Plots

```{r}
#| fig-height: 3.5
#| fig-width: 8
#| # Make plots
distr_plots(data1_train, "lbp2", 20)
```

#### 

`lbp2` is normally distributed

[Top of Tabset](#Features_Uni)

## LBP3

```{r}
#| fig-height: 3.5
#| fig-width: 8
#| # Make plots
distr_plots(data1_train, "lbp3", 20)
```

Doesn't look too good.

```{r}
# Use bestNormalize R package to select the best transformation
BNObject <- bestNormalize(data1_train$lbp3)
BNObject
```

None of the transformations help much here.

#### Summary

`lbp3` is not normally distributed and no transformations help. We keep the original unmodified values.

[Top of Tabset](#Features_Uni)

## LBP4

```{r}
#| fig-height: 3.5
#| fig-width: 8
#| # Make plots
distr_plots(data1_train, "lbp4", 20)
```

`lbp4` appears logarithmic.

```{r}
#| fig-height: 3.5
#| fig-width: 8
# Create log variable
data1_train <- data1_train |> 
  mutate(lbp4_log = log(lbp4))

#| # Make plots
distr_plots(data1_train, "lbp4_log", 20)
```

Looks better.

```{r}
# Use bestNormalize R package to select the best transformation
BNObject <- bestNormalize(data1_train$lbp4)
BNObject
```

The log transform also provides the best performance here.

#### Summary

The log transform provides the best distribution for `lbp4`

::: callout-note
Note: Patient 48 is a clear outlier and possibly erroneous value on the `lbp2` feature. Ensure to remove it from all data sets!
:::

[Top of Tabset](#Features_Uni)

## LBP5

```{r}
#| fig-height: 3.5
#| fig-width: 8
#| # Make plots
distr_plots(data1_train, "lbp5", 20)
```

`lbp5` looks logarithmic.

```{r}
#| fig-height: 3.5
#| fig-width: 8
# Create log variable
data1_train <- data1_train |> 
  mutate(lbp5_log = log(lbp5))

#| # Make plots
distr_plots(data1_train, "lbp5_log", 15)

# Create variable
data1_train <- data1_train |> 
  mutate(lbp5_log = log(lbp5))
```

```{r}
# Use bestNormalize R package to select the best transformation
BNObject <- bestNormalize(data1_train$lbp5)
BNObject
```

Looks better, and we confirm that the log transform performs the best.

#### Summary

Log transform is the best for `lbp5`.

[Top of Tabset](#Features_Uni)

::::

## Feature Engineering Summary

We determined that the following variables were approximately normally distributed after performing the transformations below:
-   `kidvol_change_log`
-   `geom1_yeo`
-   `geom2_yeo`
-   `gabor2_yeo`
-   `gabor4_log`
-   `glcm2_log`
-   `txti4_log`
-   `txti5_log`
-   `lbp4_log`
-   `lbp5_log`

`gabor5` is not normal even after transforming.

We also eliminated 2 outlier patients, `48` and `52`.

[Top of Tabset](#Features_Uni)

:::::::::

# Correlation Matrices

To assess for the best engineering of features and select the best covariates for this predictive model, I will create the following correlation matrices. 

 - With unaltered features
 - With most normalized features
 - With unaltered quadratic features
 - With most normalized quadratic features
 - With averages of all related terms
 - With averages of only the most significant related terms
 - With Interaction terms

## Matrix One

This matrix will contain the unaltered features in their original form, as well as the engineered features as determined by examination of distributions. 

```{r}
# Control matrix for better output
matrix_order <- c("kidvol_base", "kidvol_change", "kidvol_change_log", "progression", "geom1", "geom1_yeo", "geom2", "geom2_yeo", "gabor1", "gabor2", "gabor2_yeo", "gabor3", "gabor4", "gabor4_log", "gabor5", "glcm1", "glcm2", "glcm2_log", "txti1", "txti2", "txti3", "txti4", "txti4_log", "txti5", "txti5_log", "lbp1", "lbp2", "lbp3", "lbp4", "lbp4_log", "lbp5", "lbp5_log")
 
# Clean the output by making a trimmed dataset excluding extaneous variables
data_for_matrix <- data1_train |> 
  select(matrix_order)

# We factored our variables at the start. To make a correlation matrix we must reconvert those back to numeric
data_for_matrix <- data.frame(lapply(data_for_matrix, function(x) if (is.factor(x)) as.numeric(x) else x))

# Make a correlation matrix with all variables of the trimmed data set
correlation_matrix <- cor(data_for_matrix, use = "complete.obs")

# Plot correlation matrix
corrplot(correlation_matrix, method = "circle")
```
From this plot we can see a few relationships between our original and transformed features.

#### Geom

Yeo Johnson transforming `geom2` slightly improved the correlation.

#### Gabor

None of the original or transformed `gabor` features are correlated to change in `kidvol_change_log`. 

Interestingly, `gabor3` appears to have a strong correlation to the untransformed `kidvol_change`. Let's assess.

```{r}
# Create plot
ggplot(data1_train, aes(x = gabor3, y = kidvol_change)) +
  geom_smooth(method = "lm") +
  geom_point() +
  labs(title = "Change in Kidney Volume by Gabor3",
       x = "Gabor3",
       y = "Change in Kidney Volume (%)") +
  theme_minimal()
```

This relationship is driven entirely by that outlier on the far left!

```{r}
# Create plot
ggplot(data1_train, aes(x = gabor3, y = kidvol_change_log)) +
  geom_smooth(method = "lm") +
  geom_point() +
  labs(title = "Change in Log Kidney Volume by Gabor3",
       x = "Gabor3",
       y = "Change in Log Kidney Volume (%)") +
  theme_minimal()
```

We now have a slope of zero for the relationship between `gabor3` and `kidvol_change_log`, which appears to be more accurate to the truth. 

This finding and the fact that none of the `gabor` features were related to `kidvol_change_log` provides confidence that we were correct to transform this outcome variable.

#### GLCM

```{r}
# Create plot
ggplot(data1_train, aes(x = glcm2, y = kidvol_change)) +
  geom_smooth(method = "lm") +
  geom_point() +
  labs(title = "Change in Kidney Volume by GLCM2",
       x = "GLCM2",
       y = "Change in Kidney Volume (%)") +
  theme_minimal()
```

`GLCM2` was originally a strong predictor of `kidvol_change`, however we can clearly see that this was because `GLCM2` is a logarithmic variable.

```{r}
# Create plot
ggplot(data1_train, aes(x = glcm2_log, y = kidvol_change_log)) +
  geom_smooth(method = "lm") +
  geom_point() +
  labs(title = "Change in Log Kidney Volume by Log GLCM2",
       x = "Log GLCM2",
       y = "Change in Log Kidney Volume (%)") +
  theme_minimal()
```

After transforming both `GLCM2` and `kidvol_change`, we can see that the relationship between the two variables does not appear that strong.

#### Txti
`txti2` is a strong predictor of change in `kidvol` regardless if whether `kidvol` is transformed or not.

`txti` becomes correlated after we create `kidvol_change_log`, interestingly.

Let's assess.

```{r}
# Create plot
ggplot(data1_train, aes(x = txti1, y = kidvol_change)) +
  geom_smooth(method = "lm") +
  geom_point() +
  labs(title = "Change in Kidney Volume by Txti1",
       x = "Txti1",
       y = "Change in Kidey Volume (%)") +
  theme_minimal()
```

```{r}
# Create plot
ggplot(data1_train, aes(x = txti1, y = kidvol_change_log)) +
  geom_smooth(method = "lm") +
  geom_point() +
  labs(title = "Change in Log Kidney Volume by Txti1",
       x = "Txti1",
       y = "Change in Log Kidney Volume (%)") +
  theme_minimal()
```

The relationship between `txti1` and change in kidney volume appears slighly linear and positive in both version, just when we took the log of kidney volume change we seem to have drawn back some of those outliers.

```{r}
# Create plot
ggplot(data1_train, aes(x = txti2, y = kidvol_change_log)) +
  geom_smooth(method = "lm") +
  geom_point() +
  labs(title = "Change in Log Kidney Volume by Txti2",
       x = "Txti2",
       y = "Change in Log Kidney Volume (%)") +
  theme_minimal()
```

`txti2` is a strong predictor of change in kidney volume!

#### LBP

`lbp2` is strongly related to change in kidney volume.

```{r}
# Create plot
ggplot(data1_train, aes(x = lbp2, y = kidvol_change_log)) +
  geom_smooth(method = "lm") +
  geom_point() +
  labs(title = "Change in Log Kidney Volume by lbp2",
       x = "lbp2",
       y = "Change in Log Kidney Volume (%)") +
  theme_minimal()
```
`lbp2` is strongly positively associated with `kidvol_change_log`.

`lbp4` after log transforming is significantly associated with change in kidney volume.

```{r}
# Create plot
ggplot(data1_train, aes(x = lbp4, y = kidvol_change_log)) +
  geom_smooth(method = "lm") +
  geom_point() +
  labs(title = "Change in Log Kidney Volume by Lbp4",
       x = "Log Lbp2",
       y = "Change in Kidney Volume (%)") +
  theme_minimal()
```

We can tell that lbp2 increases exponentially.

```{r}
# Create plot
ggplot(data1_train, aes(x = lbp4_log, y = kidvol_change_log)) +
  geom_smooth(method = "lm") +
  geom_point() +
  labs(title = "Change in Log Kidney Volume by Log Lbp4",
       x = "Log Lbp4",
       y = "Change in Log Kidney Volume (%)") +
  theme_minimal()
```
Our scale is still weird after transforming, but it does appear to be negatively related.

`lbp5_log` is strongly correlated to change in kidney volume.

```{r}
# Create plot
ggplot(data1_train, aes(x = lbp5_log, y = kidvol_change_log)) +
  geom_smooth(method = "lm") +
  geom_point() +
  labs(title = "Change in Log Kidney Volume by Log Lbp5",
       x = "Log Lbp5",
       y = "Change in Log Kidney Volume (%)") +
  theme_minimal()
```

We have a pretty strong negative relationship here.

# Feature Engineering II

We saw in the correlation matrix that both `geom1` and `geom2` had pretty even correlations with change in kidney volume. 

In the interest of including as many variables as possible (at most we can include 5), and possibly uncovering a strong relationship, I will attempt to collapse these two variables by 

 - Taking their average
 - Taking their interaction term
 
I will also do the same for `lbp2`, `lbp4_log`, and `lbp5_log`

#### Create Average and Interaction Terms

```{r}
data1_train <- data1_train |> 
  mutate(geom_avg = (geom1_yeo + geom2_yeo)/2,
         geom_int = geom1_yeo*geom2_yeo,
         lbp_avg  = (lbp2 + lbp5_log),
         lbp_int  = lbp2*lbp5_log)
```


# Correlation Matrix II

Now we can rerun the correlation matrix and examine how those relationships changed

```{r}
# Control matrix for better output
matrix_order <- c("kidvol_base", "kidvol_change", "kidvol_change_log", "progression", "geom1", "geom1_yeo", "geom2", "geom2_yeo", "geom_avg", "geom_int", "gabor1", "gabor2", "gabor2_yeo", "gabor3", "gabor4", "gabor4_log", "gabor5", "glcm1", "glcm2", "glcm2_log", "txti1", "txti2", "txti3", "txti4", "txti4_log", "txti5", "txti5_log", "lbp1", "lbp2", "lbp3", "lbp4", "lbp4_log", "lbp5", "lbp5_log", "lbp_avg", "lbp_int")
 
# Clean the output by making a trimmed dataset excluding extaneous variables
data_for_matrix <- data1_train |> 
  select(matrix_order)

# We factored our variables at the start. To make a correlation matrix we must reconvert those back to numeric
data_for_matrix <- data.frame(lapply(data_for_matrix, function(x) if (is.factor(x)) as.numeric(x) else x))

# Make a correlation matrix with all variables of the trimmed data set
correlation_matrix <- cor(data_for_matrix, use = "complete.obs")

# Plot correlation matrix
corrplot(correlation_matrix, method = "circle")
```

#### Geom

The `geom` interaction and average terms do not seem to perform differently from each other. They *may* perform slightly better than `geom2_yeo` alone.

Let's check.

```{r}
# Create plot
ggplot(data1_train, aes(x = geom2_yeo, y = kidvol_change_log)) +
  geom_smooth(method = "lm") +
  geom_point() +
  labs(title = "Change in Log Kidney Volume by Geom2 Yeo",
       x = "Geom2_Yeo",
       y = "Change in Log Kidney Volume (%)") +
  theme_minimal()
```

```{r}
# Create plot
ggplot(data1_train, aes(x = geom_avg, y = kidvol_change_log)) +
  geom_smooth(method = "lm") +
  geom_point() +
  labs(title = "Change in Log Kidney Volume by geom_avg",
       x = "geom_avg",
       y = "Change in Log Kidney Volume (%)") +
  theme_minimal()
```

These relationships do not appear any different and `geom2_yeo` will be selected as potential covariate for the final model.


#### LBP

Similarly, the `lbp` average and interaction terms do not appear to perform any differently then `lbp5_log` alone.

```{r}
# Create plot
ggplot(data1_train, aes(x = lbp5_log, y = kidvol_change_log)) +
  geom_smooth(method = "lm") +
  geom_point() +
  labs(title = "Change in Log Kidney Volume by lbp5 log",
       x = "lb5p log",
       y = "Change in Log Kidney Volume (%)") +
  theme_minimal()
```


```{r}
# Create plot
ggplot(data1_train, aes(x = lbp_avg, y = kidvol_change_log)) +
  geom_smooth(method = "lm") +
  geom_point() +
  labs(title = "Change in Log Kidney Volume by lbp avg",
       x = "lbp_avg",
       y = "Change in Log Kidney Volume (%)") +
  theme_minimal()
```
These appear to be the same relationship and `lbp5_log` will be selected for simplicity.

# Feature Engineering III

Finally, we will consider square terms for each feature.

```{r}
# Create function to square specified columns and rename the new variables
square_selected_variables <- function(df, columns) {
  df <- df %>%
    mutate(across(all_of(columns), ~ .^2, .names = "{.col}_square"))
  return(df)
}

# Choose columns to get squared
columns <- data1_train |>
  select(geom1, geom1_yeo, geom2, geom2_yeo, geom_avg, geom_int, gabor1, gabor2, gabor2_yeo, gabor3, gabor4, gabor4_log, gabor5, glcm1, glcm2, glcm2_log, txti1, txti2, txti3, txti4, txti4_log, txti5, txti5_log, lbp1, lbp2, lbp3, lbp4, lbp4_log, lbp5, lbp5_log, lbp_avg, lbp_int) |>
  colnames()


# Square selected variables
data_square <- square_selected_variables(data1_train, columns)
  
# Control matrix for better output
matrix_order <- c("kidvol_base", "kidvol_change", "kidvol_change_log", "progression", "geom1_square", "geom1_yeo_square", "geom2_square", "geom2_yeo_square", "geom_avg_square", "geom_int_square", "gabor1_square", "gabor2_square", "gabor2_yeo_square", "gabor3_square", "gabor4_square", "gabor4_log_square", "gabor5_square", "glcm1_square", "glcm2_square", "glcm2_log_square", "txti1_square", "txti2_square", "txti3_square", "txti4_square", "txti4_log_square", "txti5_square", "txti5_log_square", "lbp1_square", "lbp2_square", "lbp3_square", "lbp4_square", "lbp4_log_square", "lbp5_square", "lbp5_log_square", "lbp_avg_square", "lbp_int_square")
 
# Clean the output by making a trimmed dataset excluding extraneous variables
data_for_matrix2 <- data_square |> 
  select(matrix_order)

# We factored our variables at the start. To make a correlation matrix we must reconvert those back to numeric
data_for_matrix2 <- data.frame(lapply(data_for_matrix2, function(x) if (is.factor(x)) as.numeric(x) else x))

# Make a correlation matrix with all variables of the trimmed data set
correlation_matrix <- cor(data_for_matrix2, use = "complete.obs")

# Plot correlation matrix
corrplot(correlation_matrix, method = "circle")

```
```{r}
ggplot(data_square, aes(x = gabor3_square, y = kidvol_change)) +
  geom_point()
```

```{r}
ggplot(data_square, aes(x = glcm2_square, y = kidvol_change)) +
  geom_point() +
  geom_smooth(method = "lm")
```
That kinda makes sense I guess?

Should I take that?

### LEAVING OFF HERE

```{r}
model <- lm(kidvol_change ~ kidvol_base, data = data_square)
summary(model)
plot(model)
```


```{r}
model <- lm(kidvol_change ~ txti2 + lbp5_log + glcm2_square + gabor3, data = data_square)
summary(model)
plot(model)
```


```{r}
model <- lm(kidvol_change ~ kidvol_base + txti2 + lbp5_log + glcm2_square + gabor3, data = data_square)
summary(model)
plot(model)
```

# Using log ki volume change makes the residuals normally distributed and meets the assumptions of a linear regression

```{r}
model <- lm(kidvol_change_log ~ kidvol_base + txti2 + lbp5_log + glcm2_square + gabor3, data = data_square)
summary(model)
plot(model)
```


#### Squaring only original terms

```{r}
# Create function to square specified columns and rename the new variables
square_selected_variables <- function(df, columns) {
  df <- df %>%
    mutate(across(all_of(columns), ~ .^2, .names = "{.col}_square"))
  return(df)
}

# Choose columns to get squared
columns <- data1_train |>
  select(geom1,geom2, gabor1, gabor2, gabor3, gabor4, gabor4_log, gabor5, glcm1, glcm2, txti1, txti2, txti3, txti4, txti5, lbp1, lbp2, lbp3, lbp4, lbp5) |>
  colnames()


# Square selected variables
data_square <- square_selected_variables(data1_train, columns)
  
# Control matrix for better output
matrix_order <- c("kidvol_base", "kidvol_change", "kidvol_change_log", "progression", "geom1_square", "geom2_square", "gabor1_square", "gabor2_square", "gabor3_square", "gabor4_square", "gabor5_square", "glcm1_square", "glcm2_square", "txti1_square", "txti2_square", "txti3_square", "txti4_square", "txti5_square", "lbp1_square", "lbp2_square", "lbp3_square", "lbp4_square", "lbp5_square")
 
# Clean the output by making a trimmed dataset excluding extraneous variables
data_for_matrix2 <- data_square |> 
  select(matrix_order)

# We factored our variables at the start. To make a correlation matrix we must reconvert those back to numeric
data_for_matrix2 <- data.frame(lapply(data_for_matrix2, function(x) if (is.factor(x)) as.numeric(x) else x))

# Make a correlation matrix with all variables of the trimmed data set
correlation_matrix <- cor(data_for_matrix2, use = "complete.obs")

# Plot correlation matrix
corrplot(correlation_matrix, method = "circle")

```



```{r}
model <- lm(kidvol_change ~ txti4_log, data = data1_train)
summary(model)
```

```{r}
model <- lm(kidvol_change ~ txti4_log, data = data1_train)
summary(model)
```

#### This is a matrix for Feature Engineered Data Set

```{r}
# # Function to square specified columns and rename the new variables
# square_selected_variables <- function(df, columns) {
#   df %>%
#     mutate(across(all_of(columns), ~ .^2, .names = "{.col}_square"))
# }
# 
# columns <- data |> 
#   select(-Subject_ID, -progression, - kidvol_base, -kidvol_visit2, -kidvol_change) |> 
#   colnames()
# 
# data_test <- square_selected_variables(data, columns)
# 
# 
# # Clean the output by making a trimmed dataset excluding extaneous variables
# data_for_matrix <- data_test |> 
#   select(kidvol_base, kidvol_visit2, kidvol_change, everything(), -Subject_ID, -progression)
# 
# # We factored our variables at the start. To make a correlation matrix we must reconvert those back to numeric
# data_for_matrix <- data.frame(lapply(data_for_matrix, function(x) if (is.factor(x)) as.numeric(x) else x))
# 
# # Make a correlation matrix with all variables of the trimmed data set
# correlation_matrix <- cor(data_for_matrix, use = "complete.obs")
# 
# # Plot correlation matrix
# corrplot(correlation_matrix, method = "circle")

```

```{r}
model <- lm(kidvol_change ~ gabor3, data = data1_train)
summary(model)
```

```{r}
ggplot(data1_train, aes(x = gabor1, y = kidvol_change)) +
  geom_point() +
  geom_smooth(method = "lm")

model <- lm(kidvol_change ~ gabor1, data = data1_train)
summary(model)
```

```{r}
model <- lm(kidvol_change ~kidvol_base, data = data1_train)
summary(model)

ggplot(data, aes(x = kidvol_base, y = kidvol_change, size = geom1, color = geom1)) +
  geom_point() +
  geom_smooth(method = "lm")

model <- lm(kidvol_change ~ kidvol_base + geom1 , data = data1_train)

summary(model)

library(car)

vif(model)
```

```{r}
ggplot(data1_train, aes(x = sqrt(geom2), y = kidvol_visit2)) +
  geom_point() +
  geom_smooth(method = "lm")
```

```{r}
ggplot(data1_train, aes(x = txti2, y = kidvol_change)) +
  geom_point() +
  geom_smooth(method = "lm")
```

```{r}
data1_train |> 
  arrange(desc(lbp3))

data_filtered <- data1_train |> 
  filter(!Subject_ID %in% c(52, 48, 37, 51))

ggplot(data_filtered, aes(x = gabor2, y = kidvol_change)) +
  geom_point() +
  geom_smooth(method = "lm")
```

# Proposal

I propose here an approach to model selection based on the findings in the correlation matrix.

Model 1) A model with the strongest relationships using only the untransformed variables.

So `geom1`, `geom2`, `gabor`, `gabor3`, `glcm2`, `txti2`, `lbp2`, `lbp4_log`,`lbp5_log`

Model 2) `geom1_yeo`, `geom2_yeo`, `txti2` and maaaybe `txti1`, `lpb2`, `lbp4_log`, and `lbp5_log`. on `kidvol_change_log`.
:::

# Bivariate Comparisons

```{r}
data |> 
  arrange(desc(lbp5)) |> 
  head() |> 
  pretty_print()
```

#### Scatterplots

Let's make a function here that will make the scatterplots with the layout that we prefer.

```{r}
#| warning: false

# Initialize empty list
plots <- list()

# Get column names of MRI features
features_gabor <- c("gabor1", "gabor2", "gabor3", "gabor4", "gabor5")
features_geom <- c("geom1", "geom2")
features_glcm <- c("glcm1", "glcm2")
features_txti <- c("txti1", "txti2", "txti3", "txti4", "txti5")
features_lbp <- c("lbp1", "lbp2", "lbp3", "lbp4", "lbp5")
  
# Plot scatterplots of each MRI feature against kidney volume change
plot_features_against_outcome <- function(features) {
for (feature in features) {
  p <- ggplot(data, aes_string(x = feature, y = "kidvol_change")) +
    geom_point() +
    theme_minimal() +
    labs(title = paste(feature),
                       x = feature,
                       y = "Kidney Volume Change (%)")
    plots[[feature]] <- p
}

# Combine all plots into a grid
combined_plot <- wrap_plots(plots, ncol = 3)
combined_plot
}
```

### Gabor Transform

```{r}
#| fig-width: 8
#| fig-height: 6
# Plot features against total change in kidney volume
plot_features_against_outcome(features_gabor)
```

It looks like `gabor4` and `gabor5` are exponential. We will get the square of those.

### Geom

```{r}
#| fig-width: 8
#| fig-height: 2.5
# Plot features against total change in kidney volume
plot_features_against_outcome(features_geom)
```

Geom 2 looks quadratic.

### Txti

```{r}
#| fig-width: 8
#| fig-height: 6
# Plot features against total change in kidney volume
plot_features_against_outcome(features_txti)
```

`txti4` and `txti5` look quadratic.

### LBP

```{r}
#| fig-width: 8
#| fig-height: 6
# Plot features against total change in kidney volume
plot_features_against_outcome(features_lbp)
```

# Feature Scaling

Here we will perform the scaling as necessary to place all variables on the same scale.

```{r}
# # Min-max normalization function
# min_max_norm <- function(x) {
#   (x - min(x)) / (max(x) - min(x))
# }
# 
# #############I was trying to features scale ecerything at once here.
# test <- scale(data$geom1)
# testly <- z_norm(data$geom1)

```

# To do

gabor3 has an outlier somewhere. reexamine.

Plot against outcome variable with transformed features as necessary (try with method - "lm", se = F)

Feature Scaling the training set

Then feature scale the test set based on the training set values (mean and SD)

Run the models 1a 1b

Take interactions of only same features, not between different features (so bascially just do like geom1 \* geom2)

2a 2b 2c

???

Profit

# References

[@fogel1989] [@kumar2020][@haralick1973] [@ojala2002]
