---
title: "Predictive Modelling"
subtitle: "Advanced Data Analysis - Project 3"
author: "Sean Vieau"
date: October 30, 2024
format: html
editor: visual
toc: true
bibliography: references.bib
theme: cerulean
---

```{r setup, include=FALSE}
# Sets the default for all chunks as echo = TRUE (ensures they show unless specified not to)
knitr::opts_chunk$set(echo = TRUE)

# Create a function to pretty print our dataframes
pretty_print <- function(x) {
  kable(x, format = "html") |>
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"))
}

# Create a function to pretty print useful parameters of a regression model
model_results <- function(x) {
  # Create a table of the coefficients of the model
  coefficients_ <- summary(x)$coefficients[]
  
  # Perform Bonferroni correction 
  p_values <- summary(x)$coefficients[,4] # This line selects the fourth column of the resulting coefficients      table from summary(model), which is the p-values
  p_adjusted <- p.adjust(p_values, method = "bonferroni")
  
  # Get the 95% CIs
  conf_intervals <- confint(x)
  
  # Compare adjusted p-values to unadjusted p-values, with 95% CI's
  model_output <- cbind(coefficients_, p_adjusted, conf_intervals)
  
  # Pretty print results
  pretty_print(model_output)
}

# Set options to avoid scientific notation
options(scipen = 999)

# Make a function for quick plotting histograms and qq plot
distr_plots <- function(data, variable, bins_choose = 30) {
  
hist_plot <- ggplot(data, aes_string(x = variable)) +
  geom_histogram(bins = bins_choose) +
  theme_minimal() +
  labs(title = paste("Histogram of", variable),
       x = variable,
       y = "Count")

# Create qqplot
qq_plot <- ggplot(data, aes_string(sample = variable)) +
  geom_qq() +
  geom_qq_line() +
  theme_minimal() +
  labs(title = paste("QQ Plot of", variable))

# Plot side by side
grid.arrange(hist_plot, qq_plot, ncol = 2)
}
```

# Introduction

The aim of the current study is to investigate whether MRI image features extracted from baseline kidney images can enhance the prediction of disease progression in young Autosomal Dominant Polycystic Kidney Disease (ADPKD) patients. This study is important because recent literature has demonstrated the inclusion of MRI image features improves the prognostic accuracy of models in adults, but this has not yet been demonstrated in children. Improving prediction models could greatly assist in diagnosing and predicting kidney disease outcomes in children, which is more challenging than in adults and could lead to improved treatment.

::: {style="text-align: center;"}
<img src="/Project_3_Predictive_Modelling/Project_3_R/Media/Project_Description.png" style="width:80%;"/>
:::

#### Task One

![](/Project_3_Predictive_Modelling/Project_3_R/Media/Task_One.png){fig-align="center" width="80%"}

#### Task Two

![](/Project_3_Predictive_Modelling/Project_3_R/Media/Task_Two.png){fig-align="center" width="80%"}

### Clinical Hypotheses

The two clinical hypotheses for this study are that including MRI image features will improve model performance compared to using baseline kidney volume alone in models predicting

1.  Percentage change of total kidney volume growth

2.  Classification of a patient as having fast or slow progression of the disease.

# Background

Here we will review background information on the methodology and variables provided in this data set.

## Gabor Transform

#### What is it

The Gabor Transform, named after Dennis Gabor, is a powerful technique for analyzing the [**texture**]{.underline} of MRI images. By convolving the image with a Gabor filter, it becomes possible to discriminate between features based on intensity differences. This allows the target region and background to be differentiated if they possess distinct features, as they will exhibit different intensity levels. Moreover, the Gabor Transform has tunable parameters, such as the frequency of the sinusoidal wave, which can be adjusted to extract specific textures from the images. Higher frequencies are ideal for capturing fine textures, while lower frequencies are better suited for coarse textures.

#### How it Works

The Gabor Filter first applies a Gaussian Envelope to focus on a small region of the image. It then applies a sinusoidal wave that oscillates at a specific frequency and captures the variation in intensities at that frequency and orientation within that region.

#### Example of Gabor Transform

![Example of applying a Gaussian, then Gabor, and Laplacian Filter (GGL) for differentiation between two different textures (+'s vs L's) ([2](https://link.springer.com/article/10.1007/BF00204594)).](images/clipboard-573562689.png){fig-align="center"}

#### Image Features Provided by the Gabor Transform

In general, Gabor functions can easily extract features of:

-   Spatial Frequency (e.g. how often pixel intensity changes in a given area)

-   Density (e.g. concentration of features within a certain area)

-   Orientation (e.g. Textures or edges at 0°, 45°, 90°, 135°)

-   Phase (e.g. alignment/distance of features)

-   Energy (e.g. overall intensity)

Sources: [1](https://link.springer.com/article/10.1007/s11042-020-09635-6), [2](https://link.springer.com/article/10.1007/BF00204594)

## Gray Level Co-Occurrence Matrix

#### What is it

The Gray Level Co-Occurrence Matrix (GLCM) is another powerful tool for extracting **texture** features from an MRI image. GLCM works under the assumption that the textural information in an image is contained in the "average" spatial relationship that the gray tones in the image have to each other. For example, when a small patch of the picture has little variation of features, the dominant property is tone; When a small patch of a picture has high variation, the dominant property is texture ([3](https://ieeexplore.ieee.org/document/4309314)).

#### How it Works

GLCM examines the spatial relationship between pairs of pixels in an image and calculates how often pairs of pixel values occur at a specified distance and orientation. It does this by computing a set of gray-tone spacial-dependence matrices for various angular relationships and distances between neighboring resolution cell pairs on the image ([3](https://ieeexplore.ieee.org/document/4309314)).

#### Example of GLCM

![Example of textural features extracted from two different land-use category images ([3](https://ieeexplore.ieee.org/document/4309314)).](images/clipboard-759848695.png){width="80%"}

#### Image Features Provided by GLCM

In general, GLCM provides information on the following features:

-   Homogeneity

-   Linear Structure

-   Contrast

-   Number and Nature of Boundaries

-   Complexity of the Image

## Local Binary Pattern

#### What is it

The Local Binary Pattern (LBP) is a third powerful method for extracting [**texture**]{.underline} features from an image. An important and fundamental property of texture is how uniform the patterns are, and LBP captures this by detecting these uniform patterns in circular neighborhoods at any rotation and spatial resolution. LBP is rotation invariant, meaning it does not matter what rotation the image is at; it will always extract very similar features) ([4](https://ieeexplore.ieee.org/document/1017623)).

#### How it Works

LBP works by comparing the intensity of a central pixel with its neighboring pixels and encoding this relationship into a binary pattern. For each neighbor, if it's intensity is greater or equal to the central pixel, it gets assigned a 1 (otherwise a 0). The binary values of all neighbors are then concatenated to form a binary number, and this number is converted into a decimal that represent the LBP for the central pixel ([4](https://ieeexplore.ieee.org/document/1017623)).

#### Example of LBP

![Example of the 36 unique comparisons that can be made between neighboring pixels ([4](https://ieeexplore.ieee.org/document/1017623)).](images/clipboard-3679699747.png){fig-align="center"}

#### Code Information

![](/Project_3_Predictive_Modelling/Project_3_R/Media/Code_Information.png){fig-align="center" width="80%"}


#### Image Features Provided by the LBP

In general, the LBP provides image features on:

-   Uniformity

-   Local Contrast

-   Texture Description

-   Spatial Patterns

-   Gray Level Distribution

# Method

## Study Design

The investigators recruited 71 young patients with ADPKD and collected MRI data at baseline and after 3 years. Additionally, the height corrected kidney volume for each patient was collected at baseline and 3 years by a physician, and the percentage change calculated. Patients were classified as having slow or fast progression of the disease based on this percentage change. Image features were extracted from the baseline MRI images including 2 image features on kidney geometric information, 5 features based on Gabor transform, 2 features based on gray level co-occurrence matrix, 5 features based on image textures, and 5 features based on local binary pattern.

### Statistical Hypotheses

1.  A **linear regression model** predicting **percentage change of total kidney volume growth** including MRI image features and baseline kidney volume will have better performance than a model with baseline kidney volume alone, as determined by specificity, sensitivity, PPV,  NPV,  accuracy, and AUC.

2.  A **logistic regression model** predicting **classification of disease progression as slow or fast** including MRI image features and baseline kidney volume will have better performance than a model with baseline kidney volume alone, as determined by specificity, sensitivity, PPV,  NPV,  accuracy, and AUC.

# Data Preparation

#### Load Packages

```{r, message = FALSE}
library(dplyr)
library(ggplot2)
library(tidyverse)
library(plotly) # Used for interactive plots
library(kableExtra) # Used for pretty printing (kable_styling)
library(ggridges) # Used for plotting distributions
library(naniar) # Used to visualize missingness
library(patchwork) # Used to chain ggplots together
library(gridExtra) # Used to plot ggplots side by side
library(corrplot) # Used for correlation matrix
```

#### Read .CSV

```{r}
# Read in data
data <- read_csv("C:/Users/sviea/Documents/Portfolio/Project_3_Predictive_Modelling/Project_3_R/Raw_Data/Project3_data.csv")
```

#### Examine Data Set

```{r}
# Examine data
pretty_print(head(data))

# Check data types
glimpse(data)
```

#### Create Factors

Everything is appropriately coded as a double. 

Let's just convert `progression` into a factor that contains the levels for "slow" or "fast and we're good to go. We will also rename some variables for convenience.

```{r}
# Convert progression into a factor
data <- data |> 
  mutate(progression = factor(progression,
         levels = c(0,1),
         labels = c("Slow", "Fast")))

# Rename Subject ID and kidney volume variables so they are easier to access
data <- data |> 
  rename(Subject_ID = `Subject ID`,
         kidvol_base = tkvht_base,
         kidvol_visit2 = tkvht_visit2,
         kidvol_change = tkvht_change) 

# Double check data types
glimpse(data)
```

#### Summary

Our data set consists of:

 - 71 patients 
 - 19 MRI image features 
 - 4 kidney volume variables

# Missingness

```{r}
# Examine missingness
gg_miss_var(data)

# Examine missingness percents
vis_miss(data)
```

We have 0 missing values!

# Data Examination

Here I will perform the intial examination of the distribution of our variables.

## Univariate Distributions 

### Outcome Variables {#Outcomes_Uni}

::: panel-tabset

## Kidney Volume

#### Baseline Kidney Volume

```{r}
# Visualize baseline kidney volume
ggplot(data, aes(x = kidvol_base)) +
  geom_histogram() + 
  theme_minimal() + 
  labs(title = "Histogram for Baseline Kidney Volume",
       x = "Baseline Kidney Volume",
       y = "Count")
```
Kidney volume at baseline appears to not be normally distributed.

However, I learned in the last project that taking the change score (or percent) will make the outcome normally distributed, as it is a way of accounting for between subject variance. Let's check that that's the case this time.

#### Percent Change in Kidney Volume

```{r}
#| fig-height: 3.5
#| fig-width: 8
# Create histogram
hist_plot <- ggplot(data, aes(x = kidvol_change)) +
  geom_histogram(binwidth = 4.5) +
  theme_minimal() +
  labs(title = "Histogram of Kidney Volume Change",
       y = "Count",
       x = "Kidney Volume Change (%)")

# Create qqplot
qq_plot <- ggplot(data, aes(sample = kidvol_change)) +
  geom_qq() +
  geom_qq_line() +
  theme_minimal() +
  labs(title = "QQ Plot of Kidney Volume Chage")

# Plot side by side
grid.arrange(hist_plot, qq_plot, ncol = 2)
```

`kidvol_change` is approximately normally distributed.

It also appears that we have two outlier values. Let's assess using boxplots.

```{r}
# Calculate IQR and identify outliers
IQR_val <- IQR(data$kidvol_change)
Q1 <- quantile(data$kidvol_change, 0.25)
Q3 <- quantile(data$kidvol_change, 0.75)
lower_bound <- Q1 - (1.5*IQR_val)
upper_bound <- Q3 + (1.5*IQR_val)

# Create boxplot and flag outliers
ggplot(data, aes(x = "", y = kidvol_change)) +
  geom_boxplot(alpha = 0.8) +
  geom_text(aes(label = ifelse(kidvol_change < lower_bound | kidvol_change > upper_bound, Subject_ID, "")), hjust = -0.5, color = "red") +
  theme_minimal() +
  labs(title = "Boxplot of Kidney Volume Change",
       x = "",
       y = "Kidney Volume Change")
```

Patients 37 and 51 are flagging as potential outliers. Let's examine them.

```{r}
# Examine outlier patients
data |> 
  arrange(desc(kidvol_change)) |> 
  head() |> 
  pretty_print()
```
Interesting, it looks like these patients also have high values for certain MRI image features, such as GLCM 1 and 2. Additionally, they do not have the highest visit 2 kidney volumes, they simply increases in size the most.

#### Summary

While patients 37 and 51 are potential outliers on `kidvol_change`, these are NOT erroneous values. On the contrary, they are backed up by similar high values on other measurements such as `glcm2`, but are not universally high on all MRI image features. In other words, these appear to be very realistic values.

In summary, all values are derived from authentic biological data. They will be examined using the jackknife residuals to assess for leverage and influence, but we are motivated to retain them in the analysis.

[Top of Tabset](#Outcomes_Uni)

## Disease Progression

####

```{r}
# Create bar plot
ggplot(data, aes(x = progression, fill = progression)) +
  geom_bar() +
  theme_minimal() +
  scale_fill_brewer(palette = "Pastel2") +
  ylim(0, 45) +
  labs(title = "Count of Slow vs Fast Disease Progression",
       x = "Disease Progression",
       y = "Count",
       fill = "Progression")

# Create Table
table(data$progression) 
```

We have an almost equal amount of patients that had slow vs fast disease progression.

[Top of Tabset](#Outcomes_Uni)

## Summary

#### Kidney Volume Change

`kidvol_change` is approximately normally distributed, with 2 potential outliers we will keep an eye on.

#### Slow vs Fast Disease Progression

We have an approximately even count between patients who had slow vs fast disease progression.

[Top of Tabset](#Outcomes_Uni)

:::

# Feature Engineering {#Features_Uni}

Here, we will examine the distribution of our variables and create quadratic terms for any variables that appear logarithmic.

::: panel-tabset

## Geometry

::: panel-tabset

## Geom1

```{r}
#| fig-height: 3.5
#| fig-width: 8
# Create histogram
hist_plot <- ggplot(data, aes(x = geom1)) +
  geom_histogram(binwidth = 30) +
  theme_minimal() +
  labs(title = "Histogram of Geom1",
       x = "Geom1",
       y = "Count")

# Create qqplot
qq_plot <- ggplot(data, aes(sample = geom1)) +
  geom_qq() +
  geom_qq_line() +
  theme_minimal() +
  labs(title = "QQ Plot of Geom1")

# Plot side by side
grid.arrange(hist_plot, qq_plot, ncol = 2)
```

There is 1 patient with a drastically lower value than everyone else. Let's examine.

```{r}
# Filter to most negative geom1 value
data |> 
  arrange(geom1)
```

Patient 48 appears to be an outlier on `geom1`.

#### Summary

`geom1` appears almost approximately normally distributed. There is potentially one outlier on the negative side, and the same two patients on the high end.

We do not need to consider `geom1**2`.

[Top of Tabset](#Features_Uni)

## Geom2

```{r}
#| fig-height: 3.5
#| fig-width: 8
# Create histogram
hist_plot <- ggplot(data, aes(x = geom2)) +
  geom_histogram(binwidth = 300) +
  theme_minimal() +
  labs(title = "Histogram of Geom2",
       x = "Geom2",
       y = "Count")

# Create qqplot
qq_plot <- ggplot(data, aes(sample = geom2)) +
  geom_qq() +
  geom_qq_line() +
  theme_minimal() +
  labs(title = "QQ Plot of Geom2")

# Plot side by side
grid.arrange(hist_plot, qq_plot, ncol = 2)
```
`geom2` appears to be logarithmic. Let's adjust.

```{r}
#| fig-height: 3.5
#| fig-width: 8
# Create new variable
data <- data |> 
 mutate(geom2_sqrt = sqrt(geom2),
        geom2_log = log(geom2)) 

# Create histogram
hist_plot <- ggplot(data, aes(x = geom2_sqrt)) +
  geom_histogram(binwidth = 20) +
  theme_minimal() +
  labs(title = "Histogram of Geom2_sqrt",
       x = "Geom2_sqrt",
       y = "Count")

# Create qqplot
qq_plot <- ggplot(data, aes(sample = geom2_sqrt)) +
  geom_qq() +
  geom_qq_line() +
  theme_minimal() +
  labs(title = "QQ Plot of Geom2_sqrt")

# Plot side by side
grid.arrange(hist_plot, qq_plot, ncol = 2)
```
Sqrt kind of makes it better, still not a normall distribution, not that it has to be...

Experimented with normalizing here.
```{r}
data |> 
  arrange(geom2_log)

ggplot(data, aes(x = geom2_log, y = kidvol_change)) +
  geom_point()

ggplot(data, aes(x = geom2, y = kidvol_change)) +
  geom_point()

# Create histogram
hist_plot <- ggplot(data, aes(x = geom2_norm)) +
  geom_histogram(binwidth = 0.2) +
  theme_minimal() +
  labs(title = "Histogram of Geom2_norm",
       x = "Geom2_norm",
       y = "Count")

```

We have an extremely low `geom2` value for patient 68...

[Top of Tabset](#Features_Uni)

:::

## Gabor

::: panel-tabset

## Gabor1

```{r}
#| fig-height: 3.5
#| fig-width: 8
# Create histogram
hist_plot <- ggplot(data, aes(x = gabor1)) +
  geom_histogram(binwidth = 0.2) +
  theme_minimal() +
  labs(title = "Histogram of Gabor1",
       x = "Gabor2",
       y = "Count")

# Create qqplot
qq_plot <- ggplot(data, aes(sample = gabor1)) +
  geom_qq() +
  geom_qq_line() +
  theme_minimal() +
  labs(title = "QQ Plot of Gabor1")

# Plot side by side
grid.arrange(hist_plot, qq_plot, ncol = 2)
```
`gabor1` is normally distributed.

[Top of Tabset](#Features_Uni)

## Gabor2

```{r}
#| fig-height: 3.5
#| fig-width: 8
# Make plots
distr_plots(data, "gabor2", 12)
```
`gabor2` appears approximately normally distributed.

[Top of Tabset](#Features_Uni)

### Gabor3

```{r}
#| fig-height: 3.5
#| fig-width: 8
# Make plots
distr_plots(data, "gabor3", 30)
```

[Top of Tabset](#Features_Uni)

`gabor3` appears approximately normally distributed, with some potential outliers.

## Gabor4

```{r}
#| fig-height: 3.5
#| fig-width: 8
# Make plots
distr_plots(data, "gabor4", 30)
```
`gabor4` may be exponential.

```{r}
#| fig-height: 3.5
#| fig-width: 8
data <- data |> 
  mutate(gabor4_log = log(gabor4))

distr_plots(data, "gabor4_log", 12)
```
`gabor4_log` appears to be normally distributed.

## Gabor5

```{r}
#| fig-height: 3.5
#| fig-width: 8
# Make plots
distr_plots(data, "gabor5", 30)
```

`gabor5` may be logarithmic?

```{r}
#| fig-height: 3.5
#| fig-width: 8
data <- data |> 
  mutate(gabor5_log = log(gabor5))

distr_plots(data, "gabor5_log", 30)
```

That's kind of better, don't really know what to do with that.

```{r}
# Calculate IQR and identify outliers
IQR_val <- IQR(data$gabor5_log)
Q1 <- quantile(data$gabor5_log, 0.25)
Q3 <- quantile(data$gabor5_log, 0.75)
lower_bound <- Q1 - (1.5*IQR_val)
upper_bound <- Q3 + (1.5*IQR_val)

# Create boxplot and flag outliers
ggplot(data, aes(x = "", y = gabor5_log)) +
  geom_boxplot(alpha = 0.8) +
  geom_text(aes(label = ifelse(gabor5_log < lower_bound | gabor5_log > upper_bound, Subject_ID, "")), hjust = -0.5, color = "red") +
  theme_minimal() +
  labs(title = "Boxplot of Gabor5",
       x = "",
       y = "Gabor5")
```


[Top of Tabset](#Features_Uni)

:::

## GLCM

::: panel_tabset 

## GLCM1

```{r}
#| fig-height: 3.5
#| fig-width: 8
# Make plots
distr_plots(data, "glcm1", 10)
```

`glcm1` is approximately normally distributed.

[Top of Tabset](#Features_Uni)

## GLCM2

```{r}
#| fig-height: 3.5
#| fig-width: 8
# Make plots
distr_plots(data, "glcm2", 10)
```

`glcm2` may be logarithmic.

```{r}
# Create log variable
data <- data |> 
  mutate(glcm2_log = log(glcm2))

#| fig-height: 3.5
#| fig-width: 8
# Make plots
distr_plots(data, "glcm2_log", 10)
```

That's better, with perhaps a negative outlier.

[Top of Tabset](#Features_Uni)

:::

## TXTI

::: panel-tabset

## TXT1

```{r}
#| fig-height: 3.5
#| fig-width: 8
# Make plots
distr_plots(data, "txti1", 10)
```
`txti1` appears normally distributed

[Top of Tabset](#Features_Uni)

## TXTI2

```{r}
#| fig-height: 3.5
#| fig-width: 8
# Make plots
distr_plots(data, "txti2", 10)
```

`txti2` appears normally distributed.

[Top of Tabset](#Features_Uni)

## TXTI3

```{r}
#| fig-height: 3.5
#| fig-width: 8
# Make plots
distr_plots(data, "txti3", 10)
```

`txti3` appears normally distributed.

[Top of Tabset](#Features_Uni)

## TXTI4

```{r}
#| fig-height: 3.5
#| fig-width: 8
# Make plots
distr_plots(data, "txti4", 10)
```

`txti4` may be logarithmic.

```{r}
#| fig-height: 3.5
#| fig-width: 8
data <- data |> 
  mutate(txti4_log = log(txti4))

# Make plots
distr_plots(data, "txti4_log", 10)
```

That's better?

[Top of Tabset](#Features_Uni)

## TXTI5

```{r}
#| fig-height: 3.5
#| fig-width: 8
#| # Make plots
distr_plots(data, "txti5", 10)
```

May be logarithmic?

```{r}
#| fig-height: 3.5
#| fig-width: 8
data <- data |> 
  mutate(txti5_log = log(txti5))

#| # Make plots
distr_plots(data, "txti5_log", 10)
```

That looks better. Mostly.

[Top of Tabset](#Features_Uni)

:::

## LBP

::: panel-tabset

## LBP1

```{r}
#| fig-height: 3.5
#| fig-width: 8
#| # Make plots
distr_plots(data, "lbp1", 20)
```

Looks normally distributed with 1 outlier.

#### Remove Outlier

```{r}
data |> 
  arrange(desc(lbp3)) |> 
  head() |> 
  pretty_print()
```

Patient 52 is a CLEAR outlier (lbp3 = 11, no othe patients have an lbp3 over 1).

We remove them.

```{r}
# Remove outlier patient
data <- data |> 
  filter(!Subject_ID == 52)
```

#### Remake Plots

```{r}
#| fig-height: 3.5
#| fig-width: 8
#| # Make plots
distr_plots(data, "lbp1", 20)
```

`lbp1` is normally distributed.

[Top of Tabset](#Features_Uni)

## LBP2

```{r}
#| fig-height: 3.5
#| fig-width: 8
#| # Make plots
distr_plots(data, "lbp2", 20)
```

We appear to have a different outlier for `lbp2`.

```{r}
# Examine outlier
data |> 
  arrange(desc(lbp2)) |> 
  head() |> 
  pretty_print()
```
Patient 48 is an outlier on `lbp2` (lbp2 = 3, all other patients are essentially below 1).

We remove them

```{r}
# Remove outlier
data <- data |> 
  filter(!Subject_ID == 48)
```

#### Remake Plots

```{r}
#| fig-height: 3.5
#| fig-width: 8
#| # Make plots
distr_plots(data, "lbp2", 20)
```

`lbp2` is normally distributed


[Top of Tabset](#Features_Uni)

## LBP3

```{r}
#| fig-height: 3.5
#| fig-width: 8
#| # Make plots
distr_plots(data, "lbp3", 20)
```

Approximately normalish.

[Top of Tabset](#Features_Uni)

## LBP4

```{r}
#| fig-height: 3.5
#| fig-width: 8
#| # Make plots
distr_plots(data, "lbp4", 20)
```

`lbp4` appears logarithmic.

```{r}
#| fig-height: 3.5
#| fig-width: 8
# Create log variable
data <- data |> 
  mutate(lbp4_log = log(lbp4))

#| # Make plots
distr_plots(data, "lbp4_log", 20)
```

Looks better.

[Top of Tabset](#Features_Uni)

## LBP5

```{r}
#| fig-height: 3.5
#| fig-width: 8
#| # Make plots
distr_plots(data, "lbp5", 20)
```

`lbp5` looks logarithmic.

```{r}
#| fig-height: 3.5
#| fig-width: 8
# Create log variable
data <- data |> 
  mutate(lbp5_log = log(lbp5))

#| # Make plots
distr_plots(data, "lbp5_log", 20)
```

Looks better, maybe 1 outlier.

```{r}
data |> 
  arrange(lbp5_log) |> 
  head()
```

[Top of Tabset](#Features_Uni)

:::

## Summary

We determined that the following variables were logarithmic:

 - `geom2`
 - `gabor4`
 - `gabor5`?
 - `glcm2`
 - `txti4`
 - `txti5`
 - `lbp4`
 - `lbp5`

We also eliminated 2 outlier patients, `48` and `52`.

[Top of Tabset](#Features_Uni)

:::

# Correlation Matrices

#### This is a matrix for Raw Data Set

```{r}
# Clean the output by making a trimmed dataset excluding extaneous variables
data_for_matrix <- data |> 
  select(kidvol_base, kidvol_visit2, kidvol_change, progression, everything(), -Subject_ID)

# We factored our variables at the start. To make a correlation matrix we must reconvert those back to numeric
data_for_matrix <- data.frame(lapply(data_for_matrix, function(x) if (is.factor(x)) as.numeric(x) else x))

# Make a correlation matrix with all variables of the trimmed data set
correlation_matrix <- cor(data_for_matrix, use = "complete.obs")

# Plot correlation matrix
corrplot(correlation_matrix, method = "circle")
```

```{r}
model <- lm(kidvol_change ~ txti4_log, data = data)
summary(model)
```

```{r}
model <- lm(kidvol_change ~ txti4_log, data = data)
summary(model)
```



#### This is a matrix for Feature Engineered Data Set
```{r}
# # Function to square specified columns and rename the new variables
# square_selected_variables <- function(df, columns) {
#   df %>%
#     mutate(across(all_of(columns), ~ .^2, .names = "{.col}_square"))
# }
# 
# columns <- data |> 
#   select(-Subject_ID, -progression, - kidvol_base, -kidvol_visit2, -kidvol_change) |> 
#   colnames()
# 
# data_test <- square_selected_variables(data, columns)
# 
# 
# # Clean the output by making a trimmed dataset excluding extaneous variables
# data_for_matrix <- data_test |> 
#   select(kidvol_base, kidvol_visit2, kidvol_change, everything(), -Subject_ID, -progression)
# 
# # We factored our variables at the start. To make a correlation matrix we must reconvert those back to numeric
# data_for_matrix <- data.frame(lapply(data_for_matrix, function(x) if (is.factor(x)) as.numeric(x) else x))
# 
# # Make a correlation matrix with all variables of the trimmed data set
# correlation_matrix <- cor(data_for_matrix, use = "complete.obs")
# 
# # Plot correlation matrix
# corrplot(correlation_matrix, method = "circle")

```
```{r}
ggplot(data, aes(x = gabor3, y = kidvol_change)) +
  geom_point() +
  geom_smooth(method = "lm")

model <- lm(kidvol_change ~ gabor3, data = data)
summary(model)
```

```{r}
ggplot(data, aes(x = gabor1, y = kidvol_change)) +
  geom_point() +
  geom_smooth(method = "lm")

model <- lm(kidvol_change ~ gabor1, data = data)
summary(model)
```

```{r}
ggplot(data, aes(x = kidvol_base, y = kidvol_change)) +
  geom_point() +
  geom_smooth(method = "lm")

model <- lm(kidvol_change ~kidvol_base, data = data)
summary(model)

ggplot(data, aes(x = kidvol_base, y = kidvol_change, size = geom1, color = geom1)) +
  geom_point() +
  geom_smooth(method = "lm")

model <- lm(kidvol_change ~ kidvol_base + geom1 , data = data)

summary(model)

library(car)

vif(model)
```




```{r}
ggplot(data, aes(x = sqrt(geom2), y = kidvol_visit2)) +
  geom_point() +
  geom_smooth(method = "lm")
```

```{r}
ggplot(data, aes(x = txti2, y = kidvol_change)) +
  geom_point() +
  geom_smooth(method = "lm")
```

```{r}
data |> 
  arrange(desc(lbp3))

data_filtered <- data |> 
  filter(!Subject_ID %in% c(52, 48, 37, 51))

ggplot(data_filtered, aes(x = gabor2, y = kidvol_change)) +
  geom_point() +
  geom_smooth(method = "lm")
```

:::

# Bivariate Comparisons

```{r}
data |> 
  arrange(desc(lbp5)) |> 
  head() |> 
  pretty_print()
```


#### Scatterplots

Let's make a function here that will make the scatterplots with the layout that we prefer.

```{r}
#| warning: false

# Initialize empty list
plots <- list()

# Get column names of MRI features
features_gabor <- c("gabor1", "gabor2", "gabor3", "gabor4", "gabor5")
features_geom <- c("geom1", "geom2")
features_glcm <- c("glcm1", "glcm2")
features_txti <- c("txti1", "txti2", "txti3", "txti4", "txti5")
features_lbp <- c("lbp1", "lbp2", "lbp3", "lbp4", "lbp5")
  
# Plot scatterplots of each MRI feature against kidney volume change
plot_features_against_outcome <- function(features) {
for (feature in features) {
  p <- ggplot(data, aes_string(x = feature, y = "kidvol_change")) +
    geom_point() +
    theme_minimal() +
    labs(title = paste(feature),
                       x = feature,
                       y = "Kidney Volume Change (%)")
    plots[[feature]] <- p
}

# Combine all plots into a grid
combined_plot <- wrap_plots(plots, ncol = 3)
combined_plot
}
```

### Gabor Transform

```{r}
#| fig-width: 8
#| fig-height: 6
# Plot features against total change in kidney volume
plot_features_against_outcome(features_gabor)
```

It looks like `gabor4` and `gabor5` are exponential. We will get the square of those.


### Geom

```{r}
#| fig-width: 8
#| fig-height: 2.5
# Plot features against total change in kidney volume
plot_features_against_outcome(features_geom)
```

Geom 2 looks quadratic.

### Txti

```{r}
#| fig-width: 8
#| fig-height: 6
# Plot features against total change in kidney volume
plot_features_against_outcome(features_txti)
```

`txti4` and `txti5` look quadratic.

### LBP

```{r}
#| fig-width: 8
#| fig-height: 6
# Plot features against total change in kidney volume
plot_features_against_outcome(features_lbp)
```

# Feature Scaling

Here we will perform the scaling as necessary to place all variables on the same scale.

```{r}
# Min-max normalization function
min_max_norm <- function(x) {
  (x - min(x)) / (max(x) - min(x))
}

#############I was trying to features scale ecerything at once here.
test <- scale(data$geom1)
testly <- z_norm(data$geom1)

data_scale <- data |> 
  select()

for (var in colnames(data)) {
  datascale(var)
}

```


# To do

Split Data Set - technically was supposed to do this right at start... How does k-cross fold come into play though.

Plot against outcome variable with transformed features as necessary (try with method - "lm", se = F)

Feature Scaling the training set

Then feature scale the test set based on the training set values (mean and SD)

Run the models
1a
1b
1c

2a
2b
2c

???

Profit





# References

[@fogel1989] [@kumar2020][@haralick1973] [@ojala2002]
