---
title: "Predictive Modelling"
subtitle: "Advanced Data Analysis - Project 3"
author: "Sean Vieau"
date: October 30, 2024
format: html
editor: visual
toc: true
bibliography: references.bib
---

```{r setup, include=FALSE}
# Sets the default for all chunks as echo = TRUE (ensures they show unless specified not to)
knitr::opts_chunk$set(echo = TRUE)

# Create a function to pretty print our dataframes
pretty_print <- function(x) {
  kable(x, format = "html") |>
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"))
}

# Create a function to pretty print useful parameters of a regression model
model_results <- function(x) {
  # Create a table of the coefficients of the model
  coefficients_ <- summary(x)$coefficients[]
  
  # Perform Bonferroni correction 
  p_values <- summary(x)$coefficients[,4] # This line selects the fourth column of the resulting coefficients      table from summary(model), which is the p-values
  p_adjusted <- p.adjust(p_values, method = "bonferroni")
  
  # Get the 95% CIs
  conf_intervals <- confint(x)
  
  # Compare adjusted p-values to unadjusted p-values, with 95% CI's
  model_output <- cbind(coefficients_, p_adjusted, conf_intervals)
  
  # Pretty print results
  pretty_print(model_output)
}

# Set options to avoid scientific notation
options(scipen = 999)

# Make a function for quick plotting histograms and qq plot
distr_plots <- function(data, variable, bins_choose = 30) {
  
hist_plot <- ggplot(data, aes_string(x = variable)) +
  geom_histogram(bins = bins_choose) +
  theme_minimal() +
  labs(title = paste("Histogram of", variable),
       x = variable,
       y = "Count")

# Create qqplot
qq_plot <- ggplot(data, aes_string(sample = variable)) +
  geom_qq() +
  geom_qq_line() +
  theme_minimal() +
  labs(title = paste("QQ Plot of", variable))

# Plot side by side
grid.arrange(hist_plot, qq_plot, ncol = 2)
}
```

# Introduction

The aim of the current study is to investigate whether MRI image features extracted from baseline kidney images can enhance the prediction of disease progression in young Autosomal Dominant Polycystic Kidney Disease (ADPKD) patients. This study is important because recent literature has demonstrated the inclusion of MRI image features improves the prognostic accuracy of models in adults, but this has not yet been demonstrated in children. Improving prediction models could greatly assist in diagnosing and predicting kidney disease outcomes in children, which is more challenging than in adults and could lead to improved treatment.

::: {style="text-align: center;"}
<img src="/Project_3_Predictive_Modelling/Project_3_R/Media/Project_Description.png" style="width:80%;"/>
:::

#### Task One

![](/Project_3_Predictive_Modelling/Project_3_R/Media/Task_One.png){fig-align="center" width="80%"}

#### Task Two

![](/Project_3_Predictive_Modelling/Project_3_R/Media/Task_Two.png){fig-align="center" width="80%"}

### Clinical Hypotheses

The two clinical hypotheses for this study are that including MRI image features will improve model performance compared to using baseline kidney volume alone in models predicting

1.  Percentage change of total kidney volume growth

2.  Classification of a patient as having fast or slow progression of the disease.

# Background

Here we will review background information on the methodology and variables provided in this data set.

## Gabor Transform

#### What is it

The Gabor Transform, named after Dennis Gabor, is a powerful technique for analyzing the [**texture**]{.underline} of MRI images. By convolving the image with a Gabor filter, it becomes possible to discriminate between features based on intensity differences. This allows the target region and background to be differentiated if they possess distinct features, as they will exhibit different intensity levels. Moreover, the Gabor Transform has tunable parameters, such as the frequency of the sinusoidal wave, which can be adjusted to extract specific textures from the images. Higher frequencies are ideal for capturing fine textures, while lower frequencies are better suited for coarse textures.

#### How it Works

The Gabor Filter first applies a Gaussian Envelope to focus on a small region of the image. It then applies a sinusoidal wave that oscillates at a specific frequency and captures the variation in intensities at that frequency and orientation within that region.

#### Example of Gabor Transform

![Example of applying a Gaussian, then Gabor, and Laplacian Filter (GGL) for differentiation between two different textures (+'s vs L's) ([2](https://link.springer.com/article/10.1007/BF00204594)).](images/clipboard-573562689.png){fig-align="center"}

#### Image Features Provided by the Gabor Transform

In general, Gabor functions can easily extract features of:

-   Spatial Frequency (e.g. how often pixel intensity changes in a given area)

-   Density (e.g. concentration of features within a certain area)

-   Orientation (e.g. Textures or edges at 0°, 45°, 90°, 135°)

-   Phase (e.g. alignment/distance of features)

-   Energy (e.g. overall intensity)

Sources: [1](https://link.springer.com/article/10.1007/s11042-020-09635-6), [2](https://link.springer.com/article/10.1007/BF00204594)

## Gray Level Co-Occurrence Matrix

#### What is it

The Gray Level Co-Occurrence Matrix (GLCM) is another powerful tool for extracting **texture** features from an MRI image. GLCM works under the assumption that the textural information in an image is contained in the "average" spatial relationship that the gray tones in the image have to each other. For example, when a small patch of the picture has little variation of features, the dominant property is tone; When a small patch of a picture has high variation, the dominant property is texture ([3](https://ieeexplore.ieee.org/document/4309314)).

#### How it Works

GLCM examines the spatial relationship between pairs of pixels in an image and calculates how often pairs of pixel values occur at a specified distance and orientation. It does this by computing a set of gray-tone spacial-dependence matrices for various angular relationships and distances between neighboring resolution cell pairs on the image ([3](https://ieeexplore.ieee.org/document/4309314)).

#### Example of GLCM

![Example of textural features extracted from two different land-use category images ([3](https://ieeexplore.ieee.org/document/4309314)).](images/clipboard-759848695.png){width="80%"}

#### Image Features Provided by GLCM

In general, GLCM provides information on the following features:

-   Homogeneity

-   Linear Structure

-   Contrast

-   Number and Nature of Boundaries

-   Complexity of the Image

## Local Binary Pattern

#### What is it

The Local Binary Pattern (LBP) is a third powerful method for extracting [**texture**]{.underline} features from an image. An important and fundamental property of texture is how uniform the patterns are, and LBP captures this by detecting these uniform patterns in circular neighborhoods at any rotation and spatial resolution. LBP is rotation invariant, meaning it does not matter what rotation the image is at; it will always extract very similar features) ([4](https://ieeexplore.ieee.org/document/1017623)).

#### How it Works

LBP works by comparing the intensity of a central pixel with its neighboring pixels and encoding this relationship into a binary pattern. For each neighbor, if it's intensity is greater or equal to the central pixel, it gets assigned a 1 (otherwise a 0). The binary values of all neighbors are then concatenated to form a binary number, and this number is converted into a decimal that represent the LBP for the central pixel ([4](https://ieeexplore.ieee.org/document/1017623)).

#### Example of LBP

![Example of the 36 unique comparisons that can be made between neighboring pixels ([4](https://ieeexplore.ieee.org/document/1017623)).](images/clipboard-3679699747.png){fig-align="center"}

#### Code Information

![](/Project_3_Predictive_Modelling/Project_3_R/Media/Code_Information.png){fig-align="center" width="80%"}

#### Image Features Provided by the LBP

In general, the LBP provides image features on:

-   Uniformity

-   Local Contrast

-   Texture Description

-   Spatial Patterns

-   Gray Level Distribution

# Method

## Study Design

The investigators recruited 71 young patients with ADPKD and collected MRI data at baseline and after 3 years. Additionally, the height corrected kidney volume for each patient was collected at baseline and 3 years by a physician, and the percentage change calculated. Patients were classified as having slow or fast progression of the disease based on this percentage change. Image features were extracted from the baseline MRI images including 2 image features on kidney geometric information, 5 features based on Gabor transform, 2 features based on gray level co-occurrence matrix, 5 features based on image textures, and 5 features based on local binary pattern.

### Statistical Hypotheses

1.  A **linear regression model** predicting **percentage change of total kidney volume growth** including MRI image features and baseline kidney volume will have better performance than a model with baseline kidney volume alone, as determined by specificity, sensitivity, PPV,  NPV,  accuracy, and AUC.

2.  A **logistic regression model** predicting **classification of disease progression as slow or fast** including MRI image features and baseline kidney volume will have better performance than a model with baseline kidney volume alone, as determined by specificity, sensitivity, PPV,  NPV,  accuracy, and AUC.

# Data Preparation

#### Load Packages

```{r, message = FALSE}
library(dplyr)
library(ggplot2)
library(tidyverse)
library(plotly) # Used for interactive plots
library(kableExtra) # Used for pretty printing (kable_styling)
library(ggridges) # Used for plotting distributions
library(naniar) # Used to visualize missingness
library(patchwork) # Used to chain ggplots together
library(gridExtra) # Used to plot ggplots side by side
library(corrplot) # Used for correlation matrix
library(caret) # Used for K-fold cross validation and machine learning
library(bestNormalize) # Used to find best transformation
library(olsrr) # used for backwards elimination model.
library(car) # Used for influenceplot function
library(reshape2) # Used for melting data for multi ROC plotting
```

#### Read .CSV

```{r}
# Read in data
data <- read_csv("C:/Users/sviea/Documents/Portfolio/Project_3_Predictive_Modelling/Project_3_R/Raw_Data/Project3_data.csv")
```

#### Examine Data Set

```{r}
# Examine data
pretty_print(head(data))

# Check data types
glimpse(data)
```

#### Create Factors

Everything is appropriately coded as a double.

Let's just convert `progression` into a factor that contains the levels for "slow" or "fast and we're good to go. We will also rename some variables for convenience.

```{r}
# Convert progression into a factor
data <- data |> 
  mutate(progression = factor(progression,
         levels = c(0,1),
         labels = c("Slow", "Fast")))

# Rename Subject ID and kidney volume variables so they are easier to access
data <- data |> 
  rename(Subject_ID = `Subject ID`,
         kidvol_base = tkvht_base,
         kidvol_visit2 = tkvht_visit2,
         kidvol_change = tkvht_change) 

# Double check data types
glimpse(data)
```

#### Summary

Our data set consists of:

-   71 patients
-   19 MRI image features
-   4 kidney volume variables

# Missingness

```{r}
# Examine missingness
gg_miss_var(data)

# Examine missingness percents
vis_miss(data)
```

We have 0 missing values!

# Data Split

#### Background

We have an N of 71, which is notably small.

In order to increase the power of the study, we will perform a 5-fold cross validation. Thus, all patients will be in both the training and validation sets, providing a comprehensive evaluation.

![](images/clipboard-802573844.png)

#### Perform the Split

To begin with, we will divide our data set into 5 even folds.

First we randomly shuffle the data so it is not organized by Subject ID

```{r}
# Set seed for reproducibility
set.seed(123)

# Shuffle the rows
data <- data |> slice_sample(n = nrow(data))
```

Then we create 5-folds of our data set.

```{r}
# Create labels for Folds1-5
data |> 
  mutate(fold_number = rep(c("Fold1", "Fold2", "Fold3", "Fold4", "Fold5"), length.out = n())) -> data

#### Create separate data frames for each fold

# Create Fold 1
data1_train <- data |> 
  filter(fold_number != "Fold1")
data1_test <- data |> 
  filter(fold_number == "Fold1")

# Create fold 2
data2_train <- data |> 
  filter(fold_number != "Fold2")
data2_test <- data |> 
  filter(fold_number == "Fold2")

# Create fold 3
data3_train <- data |> 
  filter(fold_number != "Fold3")
data3_test <- data |> 
  filter(fold_number == "Fold3")

# Create fold 4
data4_train <- data |> 
  filter(fold_number != "Fold4")
data4_test <- data |> 
  filter(fold_number == "Fold4")

# Create fold 5
data5_train <- data |> 
  filter(fold_number != "Fold5")
data5_test <- data |> 
  filter(fold_number == "Fold5")
```

And examine `Fold1` and ensure the dimensions check out.

```{r}
# Check dimensions of Fold1
dim(data1_train)

# Check dimensions of Fold1
dim(data1_test)

# Check for any overlapping patients
inner_join(data1_train, data1_test, by = "Subject_ID")
```

We have 56 patients in the fold 1 training set, and 15 in the test set, with no overlapping IDs. Perfect!

# Outcome Variables {#Outcomes_Uni}

Examining the distributions of the entire data set before splitting can cause subtle biases. For example, transformations or decisions based on the entire data set might inadvertently incorporate patterns from the test data into the training process!

We essentially want to mimic real world situations, where we would not know the distributions of our test set prior to analysis.

Thus, we will perform the initial examination of the distribution of our variables, using `Fold1_train`.

::: panel-tabset
## Kidney Volume

#### Baseline Kidney Volume

```{r}
# Visualize baseline kidney volume
ggplot(data1_train, aes(x = kidvol_base)) +
  geom_histogram() + 
  theme_minimal() + 
  labs(title = "Histogram for Baseline Kidney Volume",
       x = "Baseline Kidney Volume",
       y = "Count")

## QQ Plot
ggplot(data1_train, aes(sample = kidvol_base)) +
  geom_qq() +
  geom_qq_line() +
  theme_minimal() + 
  labs(title = "Histogram for Baseline Kidney Volume",
       x = "Baseline Kidney Volume",
       y = "Count")
```

Kidney volume at baseline appears to be logarithmic.

```{r}
# Visualize baseline kidney volume
ggplot(data1_train, aes(x = log(kidvol_base))) +
  geom_histogram() + 
  theme_minimal() + 
  labs(title = "Histogram forLog Baseline Kidney Volume",
       x = "Baseline Kidney Volume",
       y = "Count")

# QQ Plot
ggplot(data1_train, aes(sample = log(kidvol_base))) +
  geom_qq() +
  geom_qq_line() +
  theme_minimal() + 
  labs(title = "Histogram for Log Baseline Kidney Volume",
       x = "Baseline Kidney Volume",
       y = "Count")

# Create variable
data1_train <- data1_train |> 
  mutate(kidvol_base_log = log(kidvol_base))
```

`kidvol_base` is normally distributed approximately, after log transform and this variable will be used going forward.

#### Percent Change in Kidney Volume

```{r}
#| fig-height: 3.5
#| fig-width: 8
# Create histogram
hist_plot <- ggplot(data1_train, aes(x = kidvol_change)) +
  geom_histogram(binwidth = 4.5) +
  theme_minimal() +
  labs(title = "Histogram of Kidney Volume Change",
       y = "Count",
       x = "Kidney Volume Change (%)")

# Create qqplot
qq_plot <- ggplot(data1_train, aes(sample = kidvol_change)) +
  geom_qq() +
  geom_qq_line() +
  theme_minimal() +
  labs(title = "QQ Plot of Kidney Volume Change")

# Plot side by side
grid.arrange(hist_plot, qq_plot, ncol = 2)
```

`kidvol_change` is not quite normally distributed.

##### Log Transform

Let's check if a logarithmic or sqrt transformation will be better.

```{r}
# Use bestNormalize R package to select the best transformation
BNObject <- bestNormalize(data1_train$kidvol_change)
BNObject

# Create histogram
hist_plot <- ggplot(data1_train, aes(x = log(kidvol_change))) +
  geom_histogram(binwidth = 0.5) +
  theme_minimal() +
  labs(title = "Histogram of Log Kidney Volume Change",
       y = "Count",
       x = "Kidney Volume Change (%)")

# Create qqplot
qq_plot <- ggplot(data1_train, aes(sample = log(kidvol_change))) +
  geom_qq() +
  geom_qq_line() +
  theme_minimal() +
  labs(title = "QQ Plot of Log Kidney Volume Change")

# Plot side by side
grid.arrange(hist_plot, qq_plot, ncol = 2)
```

##### Square Root Transform

```{r}
# Create histogram
hist_plot <- ggplot(data1_train, aes(x = sqrt(kidvol_change))) +
  geom_histogram(binwidth = 0.5) +
  theme_minimal() +
  labs(title = "Histogram of Sqrt Kidney Volume Change",
       y = "Count",
       x = "Kidney Volume Change (%)")

# Create qqplot
qq_plot <- ggplot(data1_train, aes(sample = sqrt(kidvol_change))) +
  geom_qq() +
  geom_qq_line() +
  theme_minimal() +
  labs(title = "QQ Plot of Sqrt Kidney Volume Chage")

# Plot side by side
grid.arrange(hist_plot, qq_plot, ncol = 2)
```

The log transform appears the best for us.

However, after checking with the professor, it is very rare in machine learning that we engineer the outcome variable at all.

```{r}
# Create log change in kidney volume
data1_train <- data1_train |> 
  mutate(kidvol_change_log = log(kidvol_change))
```

#### Outliers

It also appears that we have two outlier values. Let's assess using boxplots.

```{r}
# Calculate IQR and identify outliers
IQR_val <- IQR(data1_train$kidvol_change)
Q1 <- quantile(data1_train$kidvol_change, 0.25)
Q3 <- quantile(data1_train$kidvol_change, 0.75)
lower_bound <- Q1 - (1.5*IQR_val)
upper_bound <- Q3 + (1.5*IQR_val)

# Create boxplot and flag outliers
ggplot(data1_train, aes(x = "", y = kidvol_change)) +
  geom_boxplot(alpha = 0.8) +
  geom_text(aes(label = ifelse(kidvol_change < lower_bound | kidvol_change > upper_bound, Subject_ID, "")), hjust = -0.5, color = "red") +
  theme_minimal() +
  labs(title = "Boxplot of Kidney Volume Change",
       x = "",
       y = "Kidney Volume Change")
```

Patients 37 and 51 are flagging as potential outliers. Let's examine them.

```{r}
# Examine outlier patients
data1_train |> 
  arrange(desc(kidvol_change)) |> 
  head() |> 
  pretty_print()
```

Interesting, it looks like these patients also have high values for certain MRI image features, such as GLCM 1 and 2. Additionally, they do not have the highest visit 2 kidney volumes, they simply increases in size the most.

### Summary

##### Transformation

It appears that a log transform of `kidvol_base` gives us an approximately normal distribution, and this will be the value used for this covariate going forward.

##### Outliers

While patients 37 and 51 are potential outliers on `kidvol_change`, these are NOT erroneous values. On the contrary, they are backed up by similar high values on other measurements such as `glcm2`, but are not universally high on all MRI image features. In other words, these appear to be very realistic values.

In summary, all values are derived from authentic biological data. They will be examined using the jackknife residuals to assess for leverage and influence, but we are motivated to retain them in the analysis.

[Top of Tabset](#Outcomes_Uni)

## Disease Progression

```{r}
# Create bar plot
ggplot(data1_train, aes(x = progression, fill = progression)) +
  geom_bar() +
  theme_minimal() +
  scale_fill_brewer(palette = "Pastel2") +
  ylim(0, 45) +
  labs(title = "Count of Slow vs Fast Disease Progression",
       x = "Disease Progression",
       y = "Count",
       fill = "Progression")

# Create Table
table(data1_train$progression) 
```

We have an equal amount of patients that had slow vs fast disease progression.

[Top of Tabset](#Outcomes_Uni)
:::

## Summary

#### Kidney Volume Change

`kidvol_change` is approximately normally distributed following a log transform, with 2 potential outliers we will keep an eye on.

#### Slow vs Fast Disease Progression

We have an even count between patients who had slow vs fast disease progression.

# Feature Engineering I {#Features_Uni}

An assumption for a linear regression is that there is a linear relationship between the predictor and the outcome variable.

Thus for the initial step, we will examine the distribution of our variables and create a quadratic term or take the log for each variable as necessary.

:::::::::: panel-tabset
## Geometry

::: panel-tabset
## Geom1

```{r}
#| fig-height: 3.5
#| fig-width: 8
# Create histogram
hist_plot <- ggplot(data1_train, aes(x = geom1)) +
  geom_histogram(binwidth = 30) +
  theme_minimal() +
  labs(title = "Histogram of Geom1",
       x = "Geom1",
       y = "Count")

# Create qqplot
qq_plot <- ggplot(data1_train, aes(sample = geom1)) +
  geom_qq() +
  geom_qq_line() +
  theme_minimal() +
  labs(title = "QQ Plot of Geom1")

# Plot side by side
grid.arrange(hist_plot, qq_plot, ncol = 2)
```

There is 1 patient with a drastically lower value than everyone else. Let's examine.

```{r}
# Filter to most negative geom1 value
data1_train |> 
  arrange(geom1) |> 
  select(Subject_ID, geom1, geom2) |> 
  head() |> 
  pretty_print()
```

Patient 48 appears to be an outlier on `geom1`.

#### Perform Yeo Johnson Transformation

Since we have negatie we have to perform the yeo johnson transformation or we will lose observations as `NA` values.

```{r}
# Use bestNormalize R package to select the best transformation
BNObject <- bestNormalize(data1_train$geom1)
BNObject

# Create histogram
hist_plot <- ggplot(data1_train, aes(x = yeojohnson(geom1)$x.t)) +
  geom_histogram(binwidth = .4) +
  theme_minimal() +
  labs(title = "Histogram of Yeo Johnson Geom1",
       y = "Count",
       x = "Log Geom1")

# Create qqplot
qq_plot <- ggplot(data1_train, aes(sample = yeojohnson(geom1)$x.t)) +
  geom_qq() +
  geom_qq_line() +
  theme_minimal() +
  labs(title = "QQ Plot of Yeo Johnson Geom1")

# Plot side by side
grid.arrange(hist_plot, qq_plot, ncol = 2)

# Perform transform
data1_train <- data1_train |> 
  mutate(geom1_yeo = yeojohnson(geom1)$x.t)
```

`geom` is more normally distributed after a Yeo Johnson transform.

#### Summary

`geom1` is normally distributed following a Yeo Johnson transform. There is potentially one outlier on the negative side, but this seemed to not be a factor after transforming.

[Top of Tabset](#Features_Uni)

## Geom2

```{r}
#| fig-height: 3.5
#| fig-width: 8
# Create histogram
hist_plot <- ggplot(data1_train, aes(x = geom2)) +
  geom_histogram(binwidth = 300) +
  theme_minimal() +
  labs(title = "Histogram of Geom2",
       x = "Geom2",
       y = "Count")

# Create qqplot
qq_plot <- ggplot(data1_train, aes(sample = geom2)) +
  geom_qq() +
  geom_qq_line() +
  theme_minimal() +
  labs(title = "QQ Plot of Geom2")

# Plot side by side
grid.arrange(hist_plot, qq_plot, ncol = 2)
```

`geom2` appears to be logarithmic. Let's adjust.

#### Square Root Transformation

```{r}
#| fig-height: 3.5
#| fig-width: 8
# Create new variable
data1_train <- data1_train |> 
 mutate(geom2_sqrt = sqrt(geom2),
        geom2_log = log(geom2)) 

# Create histogram
hist_plot <- ggplot(data1_train, aes(x = geom2_sqrt)) +
  geom_histogram(binwidth = 20) +
  theme_minimal() +
  labs(title = "Histogram of Geom2_sqrt",
       x = "Geom2_sqrt",
       y = "Count")

# Create qqplot
qq_plot <- ggplot(data1_train, aes(sample = geom2_sqrt)) +
  geom_qq() +
  geom_qq_line() +
  theme_minimal() +
  labs(title = "QQ Plot of Geom2_sqrt")

# Plot side by side
grid.arrange(hist_plot, qq_plot, ncol = 2)
```

Sqrt kind of makes it better, but not quite.

#### Log Transformation

Let's examine the log.

```{r}

# Create histogram
hist_plot <- ggplot(data1_train, aes(x = log(geom2))) +
  geom_histogram(binwidth = 1) +
  theme_minimal() +
  labs(title = "Histogram of Geom2 log",
       x = "Geom2 log",
       y = "Count")

# Create qqplot
qq_plot <- ggplot(data1_train, aes(sample = log(geom2))) +
  geom_qq() +
  geom_qq_line() +
  theme_minimal() +
  labs(title = "QQ Plot of Geom2 log")

# Plot side by side
grid.arrange(hist_plot, qq_plot, ncol = 2)
```

#### Yeo Johnson Transform

A Yeo Johnson transformation

```{r}
# Use bestNormalize R package to select the best transformation
BNObject <- bestNormalize(data1_train$geom2)
BNObject

# Create histogram
hist_plot <- ggplot(data, aes(x = yeojohnson(geom2)$x.t)) +
  geom_histogram(binwidth = 1) +
  theme_minimal() +
  labs(title = "Histogram of Geom2 Yeo",
       x = "Geom2 Yeo",
       y = "Count")

# Create qqplot
qq_plot <- ggplot(data, aes(sample = yeojohnson(geom2)$x.t)) +
  geom_qq() +
  geom_qq_line() +
  theme_minimal() +
  labs(title = "QQ Plot of Geom2 Yeo")

# Plot side by side
grid.arrange(hist_plot, qq_plot, ncol = 2)

# Create the variable
data1_train <- data1_train |> 
  mutate(geom2_yeo = yeojohnson(geom2)$x.t)
```

I think log or yeo is our best bet here.

#### Summary

While there is no transformation that makes `geom2` normally distributed, log or yeojohnson transformation may help and will be considered. Let's select Yeo Johnson as it looks slightly better.

[Top of Tabset](#Features_Uni)
:::

## Gabor

::: panel-tabset
## Gabor1

```{r}
#| fig-height: 3.5
#| fig-width: 8
# Create histogram
hist_plot <- ggplot(data1_train, aes(x = gabor1)) +
  geom_histogram(binwidth = 0.2) +
  theme_minimal() +
  labs(title = "Histogram of Gabor1",
       x = "Gabor2",
       y = "Count")

# Create qqplot
qq_plot <- ggplot(data1_train, aes(sample = gabor1)) +
  geom_qq() +
  geom_qq_line() +
  theme_minimal() +
  labs(title = "QQ Plot of Gabor1")

# Plot side by side
grid.arrange(hist_plot, qq_plot, ncol = 2)
```

`gabor1` is normally distributed.

[Top of Tabset](#Features_Uni)

## Gabor2

```{r}
#| fig-height: 3.5
#| fig-width: 8
# Make plots
distr_plots(data1_train, "gabor2", 12)
```

`gabor2` appears approximately normally distributed.

```{r}
# Use bestNormalize R package to select the best transformation
BNObject <- bestNormalize(data1_train$gabor2)
BNObject
```

It looks like a yeo johnson transformation will be most appropriate here.

```{r}
data1_train <- data1_train |> 
  mutate(gabor2_yeo = yeojohnson(gabor2)$x.t)

distr_plots(data1_train, "gabor2_yeo", 10)
```

A yeo johnson transformation *slightly* improves the normality of `gabor2`.

[Top of Tabset](#Features_Uni)

## Gabor3

```{r}
#| fig-height: 3.5
#| fig-width: 8
# Make plots
distr_plots(data1_train, "gabor3", 30)
```

[Top of Tabset](#Features_Uni)

`gabor3` appears approximately normally distributed, with some potential outliers.

```{r}
# Use bestNormalize R package to select the best transformation
BNObject <- bestNormalize(data1_train$gabor3)
BNObject
```

Transformations don't seem to help much here. We want the values to be closest to 1 to indicate more normality.

#### Summary

Transformation doesn't seem to help much with `gabor3`.

[Top of Tabset](#Features_Uni)

## Gabor4

```{r}
#| fig-height: 3.5
#| fig-width: 8
# Make plots
distr_plots(data1_train, "gabor4", 30)
```

`gabor4` may be exponential.

```{r}
#| fig-height: 3.5
#| fig-width: 8
data1_train <- data1_train |> 
  mutate(gabor4_log = log(gabor4))

distr_plots(data1_train, "gabor4_log", 12)
```

`gabor4_log` appears to be normally distributed.

#### Summary

The log transform of `gabor4` made it approximately normal.

[Top of Tabset](#Features_Uni)

## Gabor5

```{r}
#| fig-height: 3.5
#| fig-width: 8
# Make plots
distr_plots(data1_train, "gabor5", 30)
```

`gabor5` looks logarithmic potentially.

#### Log Transform

```{r}
# Use bestNormalize R package to select the best transformation
BNObject <- bestNormalize(data1_train$gabor5)
BNObject

# Create log term
data1_train <- data1_train |> 
  mutate(gabor5_log = log(gabor5))
 
distr_plots(data1_train, "gabor5_log", 30)
```

That's kind of better.

```{r}
# Calculate IQR and identify outliers
IQR_val <- IQR(data1_train$gabor5_log)
Q1 <- quantile(data1_train$gabor5_log, 0.25)
Q3 <- quantile(data1_train$gabor5_log, 0.75)
lower_bound <- Q1 - (1.5*IQR_val)
upper_bound <- Q3 + (1.5*IQR_val)

# Create boxplot and flag outliers
ggplot(data1_train, aes(x = "", y = gabor5_log)) +
  geom_boxplot(alpha = 0.8) +
  geom_text(aes(label = ifelse(gabor5_log < lower_bound | gabor5_log > upper_bound, Subject_ID, "")), hjust = -0.5, color = "red") +
  theme_minimal() +
  labs(title = "Boxplot of Gabor5",
       x = "",
       y = "Gabor5")
```

#### Summary

`gabor5` is not normally distributed, and no transform seems to help much. It may be dropped from consideration.

[Top of Tabset](#Features_Uni)
:::

## GLCM

::: panel_tabset
## GLCM1

```{r}
#| fig-height: 3.5
#| fig-width: 8
# Make plots
distr_plots(data1_train, "glcm1", 10)
```

`glcm1` is approximately normally distributed.

[Top of Tabset](#Features_Uni)

## GLCM2

```{r}
#| fig-height: 3.5
#| fig-width: 8
# Make plots
distr_plots(data1_train, "glcm2", 10)
```

`glcm2` may be logarithmic.

#### Log Transform

```{r}
# Create log variable
data1_train <- data1_train |> 
  mutate(glcm2_log = log(glcm2))

#| fig-height: 3.5
#| fig-width: 8
# Make plots
distr_plots(data1_train, "glcm2_log", 10)
```

```{r}
# Use bestNormalize R package to select the best transformation
BNObject <- bestNormalize(data1_train$glcm2)
BNObject
```

It appears that this is the best transformation we will get.

#### Summary

The best transformation for `glcm2` is a log transform.

[Top of Tabset](#Features_Uni)
:::

## TXTI

::: panel-tabset
## TXT1

```{r}
#| fig-height: 3.5
#| fig-width: 8
# Make plots
distr_plots(data1_train, "txti1", 10)
```

#### Summary

`txti1` appears normally distributed

[Top of Tabset](#Features_Uni)

## TXTI2

```{r}
#| fig-height: 3.5
#| fig-width: 8
# Make plots
distr_plots(data1_train, "txti2", 10)
```

#### Summary

`txti2` appears normally distributed.

[Top of Tabset](#Features_Uni)

## TXTI3

```{r}
#| fig-height: 3.5
#| fig-width: 8
# Make plots
distr_plots(data1_train, "txti3", 10)
```

#### Summary

`txti3` appears normally distributed.

[Top of Tabset](#Features_Uni)

## TXTI4

```{r}
#| fig-height: 3.5
#| fig-width: 8
# Make plots
distr_plots(data1_train, "txti4", 10)
```

`txti4` may be logarithmic.

#### Log Transform

```{r}
#| fig-height: 3.5
#| fig-width: 8
data1_train <- data1_train |> 
  mutate(txti4_log = log(txti4))

# Make plots
distr_plots(data1_train, "txti4_log", 10)
```

That's better, still not perfect.

```{r}
# Use bestNormalize R package to select the best transformation
BNObject <- bestNormalize(data1_train$txti4)
BNObject
```

Log is better than sqrt so we will use log.

#### Summary

The best transformation for `txtxi4` is log, though it is still not perfect.

[Top of Tabset](#Features_Uni)

## TXTI5

```{r}
#| fig-height: 3.5
#| fig-width: 8
#| # Make plots
distr_plots(data1_train, "txti5", 10)
```

May be logarithmic.

#### Log Transform

```{r}
#| fig-height: 3.5
#| fig-width: 8
data1_train <- data1_train |> 
  mutate(txti5_log = log(txti5))

#| # Make plots
distr_plots(data1_train, "txti5_log", 10)
```

That looks better. Mostly.

```{r}
# Use bestNormalize R package to select the best transformation
BNObject <- bestNormalize(data1_train$txti5)
BNObject
```

Log performs better than sqrt, that's as good as we can get.

#### Summary

The best transformation for `txti5` is logarithmic.

[Top of Tabset](#Features_Uni)
:::

## LBP

::::: panel-tabset
## LBP1

```{r}
#| fig-height: 3.5
#| fig-width: 8
#| # Make plots
distr_plots(data1_train, "lbp1", 20)
```

Looks normally distributed with 1 outlier.

#### Remove Outlier

```{r}
# Examine LBP outlier
data1_train |> 
  arrange(desc(lbp3)) |> 
  select(Subject_ID, lbp1, lbp2, lbp3, lbp4, lbp5) |> 
  head() |> 
  pretty_print()
```

Patient 52 is a CLEAR outlier (lbp3 = 11, no othe patients have an lbp3 over 1).

We remove them.

```{r}
# Remove outlier patient
data1_train <- data1_train |> 
  filter(!Subject_ID == 52)
```

#### Remake Plots

```{r}
#| fig-height: 3.5
#| fig-width: 8
#| # Make plots
distr_plots(data1_train, "lbp1", 10)
```

That's close to normal, let's check if it could be better.

```{r}
# Use bestNormalize R package to select the best transformation
BNObject <- bestNormalize(data1_train$lbp1)
BNObject
```

#### Yeo Johnson Transform

It seems a yeo transformation will be the best (since we have negatives and a log and sqrt transformation will cause NA values)

```{r}
#| fig-height: 3.5
#| fig-width: 8
# Perform transform
data1_train <- data1_train |> 
  mutate(lbp1_yeo = yeojohnson(lbp1)$x.t)

#| # Make plots
distr_plots(data1_train, "lbp1_yeo", 10)
```

It's slightly better, but not by much.

#### Summary

A yeo johnson transformation helps a little bit.

We will choose to not transform this variable.

::: callout-note
Note: Patient 52 is a clear outlier and possibly erroneous value on the `lbp` features. Ensure to remove it from all data sets!
:::

[Top of Tabset](#Features_Uni)

## LBP2

```{r}
#| fig-height: 3.5
#| fig-width: 8
#| # Make plots
distr_plots(data1_train, "lbp2", 20)
```

We appear to have a different outlier for `lbp2`.

```{r}
# Examine outlier
data1_train |> 
  arrange(desc(lbp2)) |> 
  select(Subject_ID, lbp1, lbp2, lbp3, lbp4, lbp5) |> 
  head() |> 
  pretty_print()
```

Patient 48 is an outlier on `lbp2` (lbp2 = 3, all other patients are essentially below 1).

We remove them

```{r}
# Remove outlier
data1_train <- data1_train |> 
  filter(!Subject_ID == 48)
```

#### Remake Plots

```{r}
#| fig-height: 3.5
#| fig-width: 8
#| # Make plots
distr_plots(data1_train, "lbp2", 20)
```

#### 

`lbp2` is normally distributed

[Top of Tabset](#Features_Uni)

## LBP3

```{r}
#| fig-height: 3.5
#| fig-width: 8
#| # Make plots
distr_plots(data1_train, "lbp3", 20)
```

Doesn't look too good.

```{r}
# Use bestNormalize R package to select the best transformation
BNObject <- bestNormalize(data1_train$lbp3)
BNObject
```

None of the transformations help much here.

#### Summary

`lbp3` is not normally distributed and no transformations help. We keep the original unmodified values.

[Top of Tabset](#Features_Uni)

## LBP4

```{r}
#| fig-height: 3.5
#| fig-width: 8
#| # Make plots
distr_plots(data1_train, "lbp4", 20)
```

`lbp4` appears logarithmic.

```{r}
#| fig-height: 3.5
#| fig-width: 8
# Create log variable
data1_train <- data1_train |> 
  mutate(lbp4_log = log(lbp4))

#| # Make plots
distr_plots(data1_train, "lbp4_log", 20)
```

Looks better.

```{r}
# Use bestNormalize R package to select the best transformation
BNObject <- bestNormalize(data1_train$lbp4)
BNObject
```

The log transform also provides the best performance here.

#### Summary

The log transform provides the best distribution for `lbp4`

::: callout-note
Note: Patient 48 is a clear outlier and possibly erroneous value on the `lbp2` feature. Ensure to remove it from all data sets!
:::

[Top of Tabset](#Features_Uni)

## LBP5

```{r}
#| fig-height: 3.5
#| fig-width: 8
#| # Make plots
distr_plots(data1_train, "lbp5", 20)
```

`lbp5` looks logarithmic.

```{r}
#| fig-height: 3.5
#| fig-width: 8
# Create log variable
data1_train <- data1_train |> 
  mutate(lbp5_log = log(lbp5))

#| # Make plots
distr_plots(data1_train, "lbp5_log", 15)

# Create variable
data1_train <- data1_train |> 
  mutate(lbp5_log = log(lbp5))
```

```{r}
# Use bestNormalize R package to select the best transformation
BNObject <- bestNormalize(data1_train$lbp5)
BNObject
```

Looks better, and we confirm that the log transform performs the best.

#### Summary

Log transform is the best for `lbp5`.

[Top of Tabset](#Features_Uni)
:::::

## Feature Engineering Summary

We determined that the following variables were approximately normally distributed after performing the transformations below: - `kidvol_base_log` - `kidvol_change_log` (this will be considered, but likely not used) - `geom1_yeo` - `geom2_yeo` - `gabor2_yeo` - `gabor4_log` - `glcm2_log` - `txti4_log` - `txti5_log` - `lbp4_log` - `lbp5_log`

These variables will be used for all comparisons moving forward to ensure we meet the assumption of linearity for a linear regression model.

`gabor5` is not normal even after transforming.

We also eliminated 2 outlier/possible erroneous values in the form of patients, `48` and `52`.

[Top of Tabset](#Features_Uni)
::::::::::

## Correlation Matrix I

To assess for the best engineering of features and select the most promising covariates for this predictive model, I will create the following correlation matrices.

-   With unaltered features & engineered features
-   With averages and interactions to aggregate features from the same class
-   With squared features

This process should uncover hidden relationships between features that are not immediately obvious (such as a squared feature being a significant predictor, but not in its original form).

This first matrix will contain the unaltered features in their original form, as well as the engineered features as determined by examination of distributions.

```{r}
#| fig-height: 8
#| fig-width: 8
# Control matrix for better output
matrix_order <- c("kidvol_base", "kidvol_change", "kidvol_change_log", "progression", "geom1", "geom1_yeo", "geom2", "geom2_yeo", "gabor1", "gabor2", "gabor2_yeo", "gabor3", "gabor4", "gabor4_log", "gabor5", "glcm1", "glcm2", "glcm2_log", "txti1", "txti2", "txti3", "txti4", "txti4_log", "txti5", "txti5_log", "lbp1", "lbp2", "lbp3", "lbp4", "lbp4_log", "lbp5", "lbp5_log")
 
# Clean the output by making a trimmed dataset excluding extaneous variables
data_for_matrix <- data1_train |> 
  select(matrix_order)

# We factored our variables at the start. To make a correlation matrix we must reconvert those back to numeric
data_for_matrix <- data.frame(lapply(data_for_matrix, function(x) if (is.factor(x)) as.numeric(x) else x))

# Make a correlation matrix with all variables of the trimmed data set
correlation_matrix <- cor(data_for_matrix, use = "complete.obs")

# Plot correlation matrix
corrplot(correlation_matrix, method = "color", addCoef.col = "black", number.cex = 0.4, insig = "blank")
```

From this plot we can see a few relationships between our original and transformed features.

## Main Observations {#Corr_One}

:::: panel-tabset
## Geometry

`geom1` (r = -0.17) and `geom2_yeo` (r = -0.15) have about equal correlation with `kidvol_change`

[Top of Tabset](#Corr_One)

## Gabor Transform

None of the original or transformed `gabor` features are correlated to change in `kidvol_change_log`.

Interestingly, `gabor3` appears to have a strong correlation to the untransformed `kidvol_change`, but not with `kidvol_change_log`. Let's assess.

```{r}
# Create plot
ggplot(data1_train, aes(x = gabor3, y = kidvol_change)) +
  geom_smooth(method = "lm") +
  geom_point() +
  labs(title = "Change in Kidney Volume by Gabor3",
       x = "Gabor3",
       y = "Change in Kidney Volume (%)") +
  theme_minimal()
```

This appears to be driven entirely by that outlier patient on the far left.

We previously determined that patients 37 and 51 were outliers on the boxplots on `kidvol_change`.

Let's assess how this relationship changes after removing them.

```{r}
# Remove outlier patients on kidvol change
data_out_rem <- data1_train |> 
  filter(!Subject_ID %in% c(37,51))

# Plot
ggplot(data_out_rem, aes(x = gabor3, y = kidvol_change)) +
  geom_smooth(method = "lm") +
  geom_point() +
  labs(title = "Change in Kidney Volume by Gabor3",
       x = "Gabor3",
       y = "Change in Kidney Volume (%)") +
  theme_minimal()
```

The relationship now looks appropriately linear when these outlier patients are removed.

We can also see that taking the log of the change in kidney volume reduces the influence of these outliers and makes the correlation non significant

```{r}
# Create plot
ggplot(data1_train, aes(x = gabor3, y = kidvol_change_log)) +
  geom_smooth(method = "lm") +
  geom_point() +
  labs(title = "Change in Log Kidney Volume by Gabor3",
       x = "Gabor3",
       y = "Change in Log Kidney Volume (%)") +
  theme_minimal()
```

::: callout-note
Note: Patients 37 and 51 may be outliers and will need to be assessed using leverage and influence after the final model is fit!
:::

[Top of Tabset](#Corr_One)

## Gray Level Co-Occurence Matrix

None of the MRI features from the gray level co-occurence matrix were strongly correlated with `kidvol_change`.

[Top of Tabset](#Corr_One)

## Txti

`txti2` is a strong predictor of change in `kidvol` regardless if whether `kidvol` is transformed or not.

```{r}
# Create plot
ggplot(data1_train, aes(x = txti2, y = kidvol_change)) +
  geom_smooth(method = "lm") +
  geom_point() +
  labs(title = "Change in Log Kidney Volume by Txti2",
       x = "Txti2",
       y = "Change in Log Kidney Volume (%)") +
  theme_minimal()
```

`txti2` is a strong predictor of change in kidney volume!

[Top of Tabset](#Corr_One)

## Local Binary Pattern

#### Lbp2

`lbp2` is weakly correlated to change in kidney volume.

```{r}
# Create plot
ggplot(data1_train, aes(x = lbp2, y = kidvol_change)) +
  geom_smooth(method = "lm") +
  geom_point() +
  labs(title = "Change in Log Kidney Volume by lbp2",
       x = "lbp2",
       y = "Change in Log Kidney Volume (%)") +
  theme_minimal()


model <- lm(kidvol_change ~ lbp2, data = data1_train)
summary(model)
```

#### Lbp4

`lbp4` after log transforming is weakly associated with change in kidney volume, but not strong enough to be significant.

```{r}
# Create plot
ggplot(data1_train, aes(x = lbp4_log, y = kidvol_change)) +
  geom_smooth(method = "lm") +
  geom_point() +
  labs(title = "Change in Log Kidney Volume by Lbp4",
       x = "Log Lbp4",
       y = "Change in Kidney Volume (%)") +
  theme_minimal()
```

#### Lbp Log

`lbp5_log` is strongly correlated to change in kidney volume.

```{r}
# Create plot
ggplot(data1_train, aes(x = lbp5_log, y = kidvol_change)) +
  geom_smooth(method = "lm") +
  geom_point() +
  labs(title = "Change in Log Kidney Volume by Log Lbp5",
       x = "Log Lbp5",
       y = "Change in Log Kidney Volume (%)") +
  theme_minimal()
```

We have a pretty strong negative relationship here, looks like it is slightly driven by those two outlier patients.

[Top of Tabset](#Corr_One)
::::

## Summary

The features that are most promising as covariates at this point are:

-   `geom2_yeo`
-   `gabor3`
-   `txt12`
-   `lbp5_log`

# Feature Engineering II

## Averages and Interaction Terms

We saw in the correlation matrix that both `geom1` and `geom2` had pretty even correlations with change in kidney volume.

In the interest of discovering underlying patterns (and minimizing the number of features we have, since we have a limit of 5 based on our sample size), I will attempt to collapse these two variables by

-   Taking their average
-   Taking their interaction term

I will also do the same for `lbp2`, and `lbp5_log`, which were correlated with the outcome.

#### Create Average and Interaction Terms

```{r}
# Create average and interaction terms
data1_train <- data1_train |> 
  mutate(geom_avg = (geom1_yeo + geom2_yeo)/2,
         geom_int = geom1_yeo*geom2_yeo,
         lbp_avg  = (lbp2 + lbp5_log),
         lbp_int  = lbp2*lbp5_log)
```

## Correlation Matrix II

Now we can rerun the correlation matrix and examine how those relationships changed

```{r}
#| fig-width: 8
#| fig-height: 8
# Control matrix for better output
matrix_order <- c("kidvol_base", "kidvol_change", "kidvol_change_log", "progression", "geom1", "geom1_yeo", "geom2", "geom2_yeo", "geom_avg", "geom_int", "lbp1", "lbp2", "lbp3", "lbp4", "lbp4_log", "lbp5", "lbp5_log", "lbp_avg", "lbp_int")
 
# Clean the output by making a trimmed dataset excluding extaneous variables
data_for_matrix <- data1_train |> 
  select(matrix_order)

# We factored our variables at the start. To make a correlation matrix we must reconvert those back to numeric
data_for_matrix <- data.frame(lapply(data_for_matrix, function(x) if (is.factor(x)) as.numeric(x) else x))

# Make a correlation matrix with all variables of the trimmed data set
correlation_matrix <- cor(data_for_matrix, use = "complete.obs")

# Plot correlation matrix
corrplot(correlation_matrix, method = "color", addCoef.col = "black", number.cex = 0.6)
```

## Main Observations {#Corr_Two}

::: panel-tabset
## Geometry

`geom_avg` (r = 0.20) has a higher correlation coefficient than `geom1_yeo` and `geom2_yeo` alone, and will thus be used as an aggregate covariate to capture the geometry of the MRIs going forward.

```{r}
# Create plot
ggplot(data1_train, aes(x = geom_avg, y = kidvol_change)) +
  geom_smooth(method = "lm") +
  geom_point() +
  labs(title = "Change in Kidney Volume by geom_avg",
       x = "geom_avg",
       y = "Change in Kidney Volume (%)") +
  theme_minimal()
```

Notably, this could be driven by those two outliers as mentioned previously.

[Top of Tabset](#Corr_Two)

## Local Binary Pattern

The average and interaction terms do not perform better than `lbp5_log` alone.

[Top of Tabset](#Corr_Two)
:::

### Summary

`geom_avg` increases the r by 0.03 over the individual non-combined features, and will be chosen as a candidate covariate.

`lbp_avg` does not perform better than `lbp5_log` alone, and thus `lbp5_log` will be chosen as a candidate covariate in the final model.

# Feature Engineering III

## Quadratic Terms

Finally, we will consider quadratic terms for each feature.

```{r}
#| fig-width: 10
#| fig-height: 10
# Create function to square specified columns and rename the new variables
square_selected_variables <- function(df, columns) {
  df <- df %>%
    mutate(across(all_of(columns), ~ .^2, .names = "{.col}_square"))
  return(df)
}

# Choose columns to get squared
columns <- data1_train |>
  select(geom1, geom1_yeo, geom2, geom2_yeo, geom_avg, geom_int, gabor1, gabor2, gabor2_yeo, gabor3, gabor4, gabor4_log, gabor5, glcm1, glcm2, glcm2_log, txti1, txti2, txti3, txti4, txti4_log, txti5, txti5_log, lbp1, lbp2, lbp3, lbp4, lbp4_log, lbp5, lbp5_log, lbp_avg, lbp_int) |>
  colnames()


# Square selected variables
data_square <- square_selected_variables(data1_train, columns)
  
```

## Correlation Matrix III

```{r}
# Control matrix for better output
matrix_order <- c("kidvol_base", "kidvol_change", "kidvol_change_log", "progression", "geom1_square", "geom1_yeo_square", "geom2_square", "geom2_yeo_square", "geom_avg_square", "geom_int_square", "gabor1_square", "gabor2_square", "gabor2_yeo_square", "gabor3_square", "gabor4_square", "gabor4_log_square", "gabor5_square", "glcm1_square", "glcm2_square", "glcm2_log_square", "txti1_square", "txti2_square", "txti3_square", "txti4_square", "txti4_log_square", "txti5_square", "txti5_log_square", "lbp1_square", "lbp2_square", "lbp3_square", "lbp4_square", "lbp4_log_square", "lbp5_square", "lbp5_log_square", "lbp_avg_square", "lbp_int_square")
 
# Clean the output by making a trimmed dataset excluding extraneous variables
data_for_matrix2 <- data_square |> 
  select(matrix_order)

# We factored our variables at the start. To make a correlation matrix we must reconvert those back to numeric
data_for_matrix2 <- data.frame(lapply(data_for_matrix2, function(x) if (is.factor(x)) as.numeric(x) else x))

# Make a correlation matrix with all variables of the trimmed data set
correlation_matrix <- cor(data_for_matrix2, use = "complete.obs")

# Plot correlation matrix
corrplot(correlation_matrix, method = "color", addCoef.col = "black", number.cex = 0.2)
```

## Main Observations {#Corr_Three}

::: panel-tabset
## Gabor Transform

#### Gabor3

The correlation coefficient for `gabor3_square` is higher than `gabor3`.

```{r}
ggplot(data_square, aes(x = gabor3_square, y = kidvol_change)) +
  geom_point() +
  geom_smooth(method = "lm")
```

Let's compare if we remove those outliers

```{r}
ggplot(data_out_rem, aes(x = gabor3**2, y = kidvol_change)) +
  geom_point() +
  geom_smooth(method = "lm")
```

This appears to be driven by that outlier patient.

[Top of Tabset](#Corr_Three)

## Gray Level Co-Occurence Matrix

`glcm2_square` has a strong correlation (r = 0.31). Let's examine.

```{r}
# Create the plot
ggplot(data_square, aes(x = glcm2_square, y = kidvol_change)) +
  geom_point() +
  geom_smooth(method = "lm")

# Check the model
model <- lm(kidvol_change ~ glcm2**2, data = data1_train)
summary(model)
```

This could be driven by the outlier, but we will include `glcm2_square` as a covariate during model selection.

[Top of Tabset](#Corr_Three)
:::

### Summary

`GLCM2_square` will be considered as a potential covariate during model selection.

# Feature Scaling

Before we perform our regression, we must standardize our variables to place them all on the same scale so that they contribute equally to the model.

This also makes it easier for the gradient descent process to converge faster and more efficiently.

![](images/clipboard-243237117.png){fig-align="center" width="80%"}

![](images/clipboard-2354431195.png){fig-align="center" width="80%"}

## Perform Scaling

We will perform z norm scaling using `caret`, which will appropriately scale the test set of each fold using the mean and standard deviation of each respective training set.

# Task One: Linear Regression

[This website](http://zevross.com/blog/2017/09/19/predictive-modeling-and-machine-learning-in-r-with-the-caret-package/) was used for coding guidance for the linear regression model.

## Cost Function

The cost function in a linear regression is Root Mean Square Error (RMSE):

$$ \text{RMSE} = \sqrt{\frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2} $$

Where y is the actual change in outcome variable and y-hat is the predicted change, and n is the number of observations.

#### Mapping Function

In the case of linear regression, the mapping function is essentially each beta in the model.

#### Goal

The goal is to select a mapping function that minimizes the cost function and thereby produces the best predictions for the outcome variable.

We will be using the `caret` package to perform model training using 5-fold cross validation and Ordinary Least Squares (OLS). Another option is to perform gradient descent.

## Model 1A: Baseline Kidney Volume {#1A}

::: panel-tabset
## Analysis

To answer the researcher's first question for Task 1, we will perform a predictive model that uses the baseline height-corrected total kidney volume to predict percent change in kidney volume.

$$ Kidney Volume Change = 𝛽_0 + 𝛽_1*Log Kidney Volume Baseline + e $$

```{r}
set.seed(123)

# Set up a 5-fold cross validation
tc <- trainControl(method = "cv", number = 5)

# Perform the linear regression using 5-fold CV
model1a <- train(kidvol_change ~ log(kidvol_base), 
                 data = data, 
                 method = "lm",
                 trControl = tc)
# Get RMSE
model1a

# Get model coefficients
summary(model1a)

# Examine RSME and R Squared of each fold
pretty_print(model1a$resample)
```

The model has an RMSE of 8.10.

[Top of Tabset](#1A)

## Visualization

#### Actual vs Predicted Values

```{r}
# Save predicted values to data set
data$predictions1a <- predict(model1a, newdata = data)

# Visualize predicted vs actual values
ggplot(data, aes(x = kidvol_change, y = predictions1a)) +
  geom_point() +
  geom_abline(intercept = 0, slope = 1, color = "blue", linetype = "dashed") +
  theme_minimal() +
  labs(title = "Predicted vs Actual Values for Baseline Kidney Volume",
       x = "Actual Values",
       y = "Predicted Values")
```

In a well performing model, the dots are close to a straight line (the blue dashed line), which would indicate perfect overlap between predicted and actual values.

#### Residuals

```{r}
# Calculate the residuals
data$residuals1a <- data$kidvol_change - data$predictions1a

# Plot
ggplot(data, aes(x = predictions1a, y = residuals1a)) +
  geom_point() +
  geom_hline(yintercept = 0, color = "blue", linetype = "dashed") +
  theme_minimal() +
  labs(title = "Residuals for Baseline Kidney Volume",
       x = "Predicted Values",
       y = "Residuals")
```

There's some pretty large differences in the predictions here, with some patients having predicted values that are 15% off or greater!

#### QQ Plot

```{r}
# Plot QQ Plot
ggplot(data, aes(sample = residuals1a)) +
  geom_qq() +
  geom_qq_line() +
  theme_minimal() +
  labs(title = "QQ Plot of Model 1A")
```

The QQ plot is almost normal, but not quite.

#### Histogram of Residuals

```{r}
# Create histogram of residuals
ggplot(data, aes(x = residuals1a)) + 
  geom_histogram(bins = 20) +
  labs(title = "Histogram of Residuals for Model 1A", 
       x = "Residuals", 
       y = "Count") +
  theme_minimal()
```

The residuals are almost normally distributed, but not quite.

[Top of Tabset](#1A)

## Summary

the RMSE of the model including `kidvol_base` alone is 8.11

The model does not perform too well, when looking at the plots of the predicted vs actual values.

The assumption of normality is almost, but not quite, met.

[Top of Tabset](#1A)
:::

## Model 1B: MRI Features {#1B}

To answer the researcher's second question for Task 1, we will run a predictive model that uses only MRI image features to predict percent change in kidney volume at year 3.

$$ Kidney Volume Change = 𝛽_0 + 𝛽_1*Txti2 + e $$

#### Model Selection

Model selection will be performed on Fold 1 to avoid data snooping on the test set.

The potential scaled covariates for our final model after feature engineering and interactive variable selection are:

-   `geom_avg`
-   `gabor3`
-   `txti2`
-   `lbp5_log`
-   `glcm2_square`

We will perform model selection using backwards elimination and BIC.

::: panel-tabset
## Backwards Elimination I

```{r}
# Create variables
data1_train <- data1_train |> 
  mutate(geom_avg_scale = scale((geom1_yeo+geom2_yeo)/2),
         gabor3_scale = scale(gabor3),
         txti2_scale = scale(txti2),
         lbp5_log_scale = scale(log(lbp5)),
         glcm2_square_scale = scale(glcm2**2))

# First build the model with all variables
model1b <- lm(kidvol_change ~ geom_avg_scale + gabor3_scale + txti2_scale + lbp5_log_scale + glcm2_square_scale, data = data1_train)
summary(model1b)

# Perform backward elimination 
ols_step_backward_sbc(model1b)
```

`geom_avg` and `lbp5_log` are removed based on BIC. AIC prefers the model with `lbp5_log`

However, we must note that we suspect patients 37 and 51 to be outliers on total kidney volume change, and we saw previously that the correlations for `gabor3` and `glcm2_square` may be being driven by these outlier patients.

[Top of Tabset](#1B)

## Analysis I

### Perform 5-Fold Cross Validation

The final model selected via backwards elimination includes `gabor3`, `txti2`, and `glcm2_square`

```{r}
set.seed(123)
# Create variables for original data set
data <- data |> 
  mutate(gabor3_scale = scale(gabor3),
         txti2_scale = scale(txti2),
         glcm2_square = glcm2**2,
         glcm2_square_scale = scale(glcm2**2),
         lbp5_log_scale = scale(log(lbp5)),
         geom_avg_scale = scale(geom1+geom2)/2)

# Perform the linear regression using 5-fold CV
model1b <- train(kidvol_change ~ gabor3 + txti2 + glcm2_square, 
                 data = data, 
                 method = "lm",
                 trControl = tc,
                 preProc = "scale")
# Get RMSE
model1b

# Get model coefficients
summary(model1b)
```

The RMSE is 9.03, worse than the model with `kidvol_base` alone.

[Top of Tabset](#1B)

## Visualization I

#### Actual vs Predicted Values

```{r}
# Save predicted valuest to data set
data$predictions1b <- predict(model1b, newdata = data)

# Visualize predicted vs actual values
ggplot(data, aes(x = kidvol_change, y = predictions1b)) +
  geom_point() +
  geom_abline(intercept = 0, slope = 1, color = "blue", linetype = "dashed") +
  theme_minimal() +
  labs(title = "Predicted vs Actual Values for MRI Features",
       x = "Actual Values",
       y = "Predicted Values")
```

#### Residuals

```{r}
# Calculate the residuals
data$residuals1b <- data$kidvol_change - data$predictions1b

# Plot
ggplot(data, aes(x = predictions1a, y = residuals1b)) +
  geom_point() +
  geom_hline(yintercept = 0, color = "blue", linetype = "dashed") +
  theme_minimal() +
  labs(title = "Residuals for MRI Features",
       x = "Predicted Values",
       y = "Residuals")
```

Our residuals are comparable in this model compared to modela 1A using `kidvol_base` alone. Some It appears that more residuals are closer to 0, but some predictions are as large as 30% off!

#### QQ Plot

```{r}
# Plot QQ Plot
ggplot(data, aes(sample = residuals1b)) +
  geom_qq() +
  geom_qq_line() +
  theme_minimal() +
  labs(title = "QQ Plot of Model 1B")
```

#### Histogram of Residuals

```{r}
# Create histogram of residuals
ggplot(data, aes(x = residuals1b)) + 
  geom_histogram(bins = 20) +
  labs(title = "Histogram of Residuals for Model 1B", 
       x = "Residuals", 
       y = "Count") +
  theme_minimal()
```

[Top of Tabset](#1B)

## Leverage and Influence

The cutoff for leverage (hat-value) is 2(p+1)/n, where p is the number of variables in the model, or 2(3+1)/71 = 0.112.

The cut off for Cook's D \> 1.0.

```{r}
# Extract model
model1b_ext <- model1b$finalModel

# Influence plot
influence_measures <- influencePlot(model1b_ext, main = "Influence Plot",
              sub = "Circle size is proportional to Cook's Distance")

# Get row numbers of outliers
influence_measures
```

We have four clear outliers that are points of high leverage and influence.

Let's identify them

```{r}
# Identify outlier patients
outliers <- data[c(2, 8, 9, 35),]

# Pretty print
pretty_print(head(outliers))
```

As predicted, patients `37` and `52` are points of high leverage and influence.

Also [as identified before](#Features_Uni), patients `48` and `52` are points of high leverage and influence, which can be denoted by their absurdly high values for `lbp1` and `lbp2`.

Since we previously identified that certain correlations such as with `gabor3` and `glcm2_square` were being driven by these outlier patients, we will re-run model selection again using backwards elimination, with these patients removed.

[Top of Tabset](#1B)

## Backwards Elimination II

#### Remove Outliers and Re-Run Variable Selection

We will perform backwards elimination again with outlier patients removed.

We can expect `gabor3` and `glcm2_square` to be removed since their correlations were driven by these outliers, as identified in earlier plots.

```{r}
# Remove outlier patients from data set
data1_train <- data1_train |> 
  filter(!Subject_ID %in% c(37, 48, 51, 52))

# Create model
model1b2 <- lm(kidvol_change ~ geom_avg_scale + gabor3_scale + txti2_scale + lbp5_log_scale + glcm2_square_scale, data = data1_train)

# Examine Model Summary
summary(model1b2)

# Perform backward elimination 
ols_step_forward_sbc(model1b2)
```

As predicted, when excluding these outlier patients, `gabor3` and `glcm2_square` are no longer significant.

The only selected now using BIC is `txti2`!

Let's perform 5-fold CV again using this model.

[Top of Tabset](#1B)

## Analysis II

#### Remove Outliers

```{r}
# Remove outlier patients from data set
data <- data |> 
  filter(!Subject_ID %in% c(37, 48, 51, 52))
```

### Perform Regression with 5-Fold Cross Validation

```{r}
set.seed(123)

# Set up a 5-fold cross validation
tc <- trainControl(method = "cv", number = 5)

# Perform the linear regression using 5-fold CV
model1b2 <- train(kidvol_change ~ txti2, 
                 data = data, 
                 method = "lm",
                 trControl = tc,
                 preProc = "scale")

# Get RMSE
model1b2

# Get model coefficients
summary(model1b2)
```

The RMSE is now 6.15, drastically lower than with `kidvol_base_log` alone (but we need to rerun that model without outliers).

This model predicts 17.0% of the variance in `kidvol_change`.

[Top of Tabset](#1B)

## Visualization II

#### Actual vs Predicted Values

```{r}
# Save predicted valuest to data set
data$predictions1b2 <- predict(model1b2, newdata = data)

# Visualize predicted vs actual values
ggplot(data, aes(x = kidvol_change, y = predictions1b2)) +
  geom_point() +
  geom_abline(intercept = 0, slope = 1, color = "blue", linetype = "dashed") +
  theme_minimal() +
  labs(title = "Predicted vs Actual Values for TXTI2",
       x = "Actual Values",
       y = "Predicted Values")
```

The predicted models are still not that close to the actual values.

#### Residuals

```{r}
# Calculate the residuals
data$residuals1b2 <- data$kidvol_change - data$predictions1b2

# Plot residuals
ggplot(data, aes(x = predictions1b2, y = residuals1b2)) +
  geom_point() +
  geom_hline(yintercept = 0, color = "blue", linetype = "dashed") +
  theme_minimal() +
  labs(title = "Residuals for TXTI2",
       x = "Predicted Values",
       y = "Residuals")
```

The residuals still look pretty large.

#### QQ Plot

```{r}
# Plot QQ Plot
ggplot(data, aes(sample = residuals1b2)) +
  geom_qq() +
  geom_qq_line() +
  theme_minimal() +
  labs(title = "QQ Plot of TXTI2")
```

#### Histogram of Residuals

```{r}
# Create histogram of residuals
ggplot(data, aes(x = residuals1b2)) + 
  geom_histogram(bins = 20) +
  labs(title = "Histogram of Residuals for TXTI2", 
       x = "Residuals", 
       y = "Count") +
  theme_minimal()
```

The residuals are now approximately normally distributed, and would be more so with a larger sample size.

### Leverage and Influence

```{r}
# Extract model
model1b2_ext <- model1b2$finalModel

# Influence plot
influence_measures <- influencePlot(model1b2_ext, main = "Influence Plot",
              sub = "Circle size is proportional to Cook's Distance")

# Get row numbers of outliers
influence_measures
```

We no longer have outliers in this model as assessed by cutoffs using leverage and influence.

[Top of Tabset](#1B)

## Summary

In this step we discovered that patiens 37, 48, 51, and 52 were outliers with high leverage and influence in our model.

After removing them, only `txti2` was selected as an MRI image feature predicting change in kidney volume.

The RMSE for model 1B after removing outliers was 6.15
:::

## Model 1C: Baseline Kidney Volume and TXTI2 {#1C}

::: panel-tabset
## Analysis

To answer the researcher’s third question, we will run a predictive model that uses the baseline height-corrected total kidney volume AND MRI image features to predict percent change in kidney volume.

$$ Kidney Volume Change = 𝛽_0 + 𝛽_{1}*Baseline Kidney Volume + 𝛽_{2}*Txti2 +  e $$

```{r}
set.seed(123)

# Perform the linear regression using 5-fold CV
model1c <- train(kidvol_change ~ log(kidvol_base) + txti2, 
                 data = data, 
                 method = "lm",
                 trControl = tc,
                 preProc = "scale",
                 metric = "RMSE")
# Get RMSE
model1c

# Get model coefficients
summary(model1c)
```

The RMSE for the combined model is 6.14, about the same as the model using just `txti2` alone.

[Top of Tabset](#1C)

## Visualization

#### Actual Vs Predicted

```{r}
# Save predicted valuest to data set
data$predictions1c <- predict(model1c, newdata = data)

# Visualize predicted vs actual values
ggplot(data, aes(x = kidvol_change, y = predictions1c)) +
  geom_point() +
  geom_abline(intercept = 0, slope = 1, color = "blue", linetype = "dashed") +
  theme_minimal() +
  labs(title = "Predicted vs Actual Values for Baseline Kidney Volumne and TXTI2",
       x = "Actual Values",
       y = "Predicted Values")
```

The model still does not have the best prediction.

#### Residuals

```{r}
# Calculate the residuals
data$residuals1c <- data$kidvol_change - data$predictions1c

# Plot residuals
ggplot(data, aes(x = predictions1c, y = residuals1c)) +
  geom_point() +
  geom_hline(yintercept = 0, color = "blue", linetype = "dashed") +
  theme_minimal() +
  labs(title = "Residuals for Baseline Kidney Volume and TXTI2",
       x = "Predicted Values",
       y = "Residuals")
```

The residuals are slightly better than the model with `kidvol_base` alone.

```{r}
# Plot QQ Plot
ggplot(data, aes(sample = residuals1c)) +
  geom_qq() +
  geom_qq_line() +
  theme_minimal() +
  labs(title = "QQ Plot of Baseline Kidney Volume and  TXTI2")
```

This is the best QQ plot we have had so far.

#### Histogram of Residuals

```{r}
# Create histogram of residuals
ggplot(data, aes(x = residuals1c)) + 
  geom_histogram(bins = 20) +
  labs(title = "Histogram of Residuals for Baseline Kidney Volume and TXTI2", 
       x = "Residuals", 
       y = "Count") +
  theme_minimal()
```

#### Leverage and Influence

```{r}
# Extract model
model1c_ext <- model1c$finalModel

# Influence plot
influence_measures <- influencePlot(model1c_ext, main = "Influence Plot",
              sub = "Circle size is proportional to Cook's Distance")

# Get row numbers of outliers
influence_measures
```

There are no outliers (some edge cases but we will retain them)

#### Plot Final Model

```{r}
# Plot final model
ggplot(data, aes(x = log(kidvol_base), y = kidvol_change)) +
  geom_point() +
  geom_smooth(method = "lm") +
  labs(title = "Model 1C: Baseline Kidney Volume and Txti2",
       x = "Baseline Kidney Volume (log)",
       y = "Change in Kidney Volume (%)") +
  theme_minimal()
```

This plot shows that generally, the larger your kidney volume at baseline, the less of an increase there was in size over 3 years.

[Top of Tabset](#1C)

## Summary

The RMSE for model 1C is 6.14, this is similar performance to model 1B.

Model 1C meets the assumptions of a linear regression (normality) the best.

[Top of Tabset](#1C)
:::

## Model Comparison

Here we will compare how models 1A, 1B, and 1C performed.

Note: We removed outliers over the course of our model inspection. Thus we will re-run each analysis to get the true RMSE for each model.

#### Model 1A

```{r}
set.seed(123)
##### Model 1A
# Perform the linear regression using 5-fold CV
model1a <- train(kidvol_change ~ log(kidvol_base), 
                 data = data, 
                 method = "lm",
                 trControl = tc)
# Get RMSE
model1a

```

#### Model 1B

```{r}
set.seed(123)
##### Model 1B
# Perform the linear regression using 5-fold CV
model1b2 <- train(kidvol_change ~ txti2, 
                 data = data, 
                 method = "lm",
                 trControl = tc,
                 preProc = "scale")
# Get RMSE
model1b2
```

#### Model 1C

```{r}
set.seed(123)
# Perform the linear regression using 5-fold CV
model1c <- train(kidvol_change ~ log(kidvol_base) + txti2_scale, 
                 data = data, 
                 method = "lm",
                 trControl = tc)
# Get RMSE
model1c
```

### Final Average RMSE

| Model                               | RMSE |
|-------------------------------------|------|
| 1A: Baseline Kidney Volume          | 6.52 |
| 1B: Txti2                           | 6.15 |
| 1C Baseline Kidney Volume and Txti2 | 6.14 |

### Compare Model Performance

We can also use the `caret` package to compare perfomance between these three models.

-   Upper Diagonal Values: These are the differences in the metric values between models. Positive values mean the first model has a higher metric value, while negative values mean the first model has a lower metric value.

-   Lower Diagonal Values: These are the p-values from hypothesis tests comparing the metric values between models. Small p-values (typically \< 0.05) indicate significant differences between the models for that metric.

```{r}
#| fig-width: 8
# Create list of models to compare
model_list <- list(`Kidney Volume at Baseline` = model1a, `Txti2` = model1b2, `Kidney Volume at Baseline and Txti2` = model1c)

# Compare models
results <- resamples(model_list)
summary(results)

# Compare the RMSEs 
diffs <- diff(results) 
summary(diffs)

# Plot RMSEs of each model
dotplot(results, metric = "RMSE", main = "Comparison of RMSE Across Models")
```

Model A had an RMSE that was 0.37 points higher than model B (p = 0.68), and 0.74 points higher than model C (p = 0.75)

# Results

To evaluate whether differences in model performance were statistically significant, RMSE for each model were compared using pairwise t-tests with Bonferroni p-value correction. The differences in model performance were not statistically significant. Model A had a 0.37 higher RMSE compared to model B (p = 0.68), and a 0.38 higher RMSE compared to model C (p = 0.75).

| Model                               | RMSE | P-Adjusted |        |
|-------------------------------------|------|------------|--------|
| 1A: Baseline Kidney Volume          | 6.52 | --         |        |
| 1B: Txti2                           | 6.15 | 0.68       | ---    |
| 1C Baseline Kidney Volume and Txti2 | 6.14 | 0.75       | 1.0000 |

## Conclusion

The differences in model performance for task one were not statistically significant. Model A had an RMSE that was 0.37 points higher than model B (p = 0.68), and 0.74 points higher than model C (p = 0.75). This may be due to small sample size however.

Thus, including MRI image features into the predictive model did not increase predictive capability above and beyond that of just using kidney volume measurements at baseline.

On the other hand, if it is true that a model with baseline kidney volume, a model with `txti2` alone, and a model with both included all perform similarly at predicting percent change in kidney volume after 3 years, then this could provide support for predicting change in kidney volume percent by EITHER MRI image features or baseline kidney volume.

That is, utilizing MRI image features alone may offer similar predictive capabilities to using kidney volume measurements taken by a practiced physician. Therefore, if one is easier or cheaper than the other to acquire, the easier alternative could be used in place of the harder alternative and still achieve similar predictive power. For example, if there are MRI records but perhaps kidney volume measurements were not taken, then the MRI images can be used instead for making predictions.

# Task Two: Logistic Regression

Here we will perform the logistic regression predicting disease progression as `fast` or `slow`.

## Model 2A {#2A}

### Baseline Kidney Volume

To answer the researcher’s first question for this task, we will run a predictive model that uses the baseline height-corrected total kidney volume to predict whether a patient had fast or slow disease progression.

$$ logit(Progression) = 𝛽_0 + 𝛽_{Baseline Kidney Volume} + e $$

::: panel-tabset
## Analysis

```{r}
library(plotROC)
# Convert progression into a factor
data <- data |>
  mutate(progression_num = as.numeric(progression)-1)
data$progression

set.seed(123)
# Create a training control object with 5-fold cross-validation and class probabilities 
tc <- trainControl(method = "cv", number = 5, classProbs = TRUE, summaryFunction = twoClassSummary)

# Perform the linear regression using 5-fold CV
model2a <- train(progression~ log(kidvol_base), 
                 data = data, 
                 method = "glm",
                 family = "binomial",
                 trControl = tc,
                 metric = "ROC",
                 preProc = "scale")

# Examine model
model2a
summary(model2a)
```

`kidvol_base_log` is not a significant predictor of change in kidney volume (p = 0.75).

The AUC for this model is 0.47, which is near guessing. The sensitivity is 0.3 and the specificity is 0.71

[Top of Tabset](#2A)

## Visualization

#### Confusion Matrix

```{r}
# Save predictions
data$predictions2a <- predict(model2a, newdata = data)
data$predictions2a_prob <- predict(model2a, newdata = data, type = "prob")[,2]
data$predictions2a_num <- as.numeric(data$predictions2a_prob)-1

# Create confusion matrix
cm <- table(Predicted = data$predictions2a, Actual = data$progression)

# Examine confusion matrix and performance
cm
```

We can see the poor performance clearly in the confusion matrix.

#### ROC Curve

```{r}
# Plot ROC Curve
roc2a <- ggplot(data, aes(m = predictions2a_num, d = progression_num)) +
  geom_roc(n.cuts = 10, labels = F, labelround = 4) +
  theme_minimal() +
  labs(title = "ROC Curve Model 2A: Baseline Kidney Volume",
       x = "False Positive Rate",
       y = "True Positive Rate") +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed", color = "blue")

# Visualize Plot
roc2a
```

We can see that the ROC curve is very poor, near guessing.

## Summary

The model using `kidvol_base_log` alone performed poorly.

AUC: 0.47

Sensitivity: 0.3

Specificity: 0.71

[Top of Tabset](#2A)
:::

## Model 2B: MRI Image Features {#2B}

To answer the researcher’s second question for this task, we will run a predictive model that uses the MRI features to predict whether a patient had fast or slow disease progression.

$$ logit(Progression) = 𝛽_0 + 𝛽_{MRI Feature 1} + 𝛽_{MRI Feature 1} + ... + e $$

As the correlation matrices revealed similar relationship between the MRI image features and `progression` as they did with `kidvol_change`, we will select the same candidate covariates for Task 2 as we did for Task 1.

These are:

-   `geom_avg`
-   `gabor3`
-   `txti2`
-   `lbp5_log`
-   `glcm2_square`

::: panel-tabset
## Model Selection

```{r}
# Create variable
data1_train <- data1_train |> 
  mutate(progression_num = as.numeric(progression)-1)

# Create model
model2b <- glm(progression_num ~ geom_avg_scale + gabor3_scale + txti2_scale + lbp5_log_scale + glcm2_square_scale, data = data1_train, family = "binomial")

# Examine Model Summary
summary(model2b)

# Perform backward elimination 
ols_step_forward_sbc(model2b)
```

The model with the smallest BIC is with `txti2` alone, thus that will be our final model.

[Top of Tabset](#2B)

## Analysis

```{r}
set.seed(123)

# Perform the linear regression using 5-fold CV
model2b <- train(progression ~ txti2, 
                 data = data, 
                 method = "glm",
                 family = "binomial",
                 trControl = tc,
                 metric = "ROC",
                 preProc = "scale")

# Examine model
model2b
summary(model2b)
```

`txti2` is a significant predictor of `kidvol_change` (p = 0.0062).

The AUC is 0.74, the senstivity is 0.57, and the specificity is 0.64

[Top of Tabset](#2B)

## Visualization

#### Confusion Matrix

```{r}
# Save predictions
data$predictions2b <- predict(model2b, newdata = data)
data$predictions2b_prob <- predict(model2b, newdata = data, type = "prob")[,2]
data$predictions2b_num <- as.numeric(data$predictions2b_prob)-1

# Create confusion matrix
cm <- table(Predicted = data$predictions2b, Actual = data$progression)

# Examine confusion matrix and performance
cm
```

We can see that we are getting more correct hits in the confusion matrix compared to using baseline kidney volume alone.

[Top of Tabset](#2B)

#### ROC Curve

```{r}
# Plot ROC Curve
roc2b <- ggplot(data, aes(m = predictions2b_num, d = progression_num)) +
  geom_roc(n.cuts = 10, labels = F, labelround = 4) +
  theme_minimal() +
  labs(title = "ROC Curve Model 2B: Txti2",
       x = "False Positive Rate",
       y = "True Positive Rate") +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed", color = "blue")

# Visualize Plot
roc2b
```

That's not a bad curve!

[Top of Tabset](#2B)

## Messing around with saturated model

```{r}
set.seed(123)

# Create variable
data <- data |> 
  mutate(geom_avg = (geom1+geom2/2))

# Perform the linear regression using 5-fold CV
model2b_sat <- train(progression ~ txti2 + geom_avg + log(lbp5) + gabor3, 
                 data = data, 
                 method = "glm",
                 family = "binomial",
                 trControl = tc,
                 metric = "ROC",
                 preProc = "scale")

# Examine model
model2b_sat
summary(model2b_sat)

# Save predictions
data$predictions2b_sat <- predict(model2b_sat, newdata = data)
data$predictions2b_prob_sat <- predict(model2b_sat, newdata = data, type = "prob")[,2]
data$predictions2b_num_sat <- as.numeric(data$predictions2b_prob_sat)-1

# Create confusion matrix
cm <- table(Predicted = data$predictions2b, Actual = data$progression)

# Examine confusion matrix and performance
cm

# Plot ROC Curve
roc2b <- ggplot(data, aes(m = predictions2b_num, d = progression_num)) +
  geom_roc(n.cuts = 10, labels = F, labelround = 4) +
  theme_minimal() +
  labs(title = "ROC Curve Model 2B: Txti2",
       x = "False Positive Rate",
       y = "True Positive Rate") +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed", color = "blue")

# Visualize Plot
roc2b

# Calculate area under the curve
calc_auc(roc2b)$AUC
```

Not that different from just txti2. I just wanted to check because I was getting the BIC backwards selection choosing this model.

## Summary

Model 2B performs much better than model 2A. We can especially see this when we look at the ROC curve.

AUC: 0.74

Sensitivity: 0.57

Specificity: 0.64

[Top of Tabset](#2B)
:::

## Model 2C: Baseline Kidney Volume and MRI Image Features {#2C}

To answer the researcher’s third question, we will run a predictive model that uses the baseline height-corrected total kidney volume AND MRI image features to predict slow vs fast disease progression.

$$ logit(Progression) = 𝛽_0 + 𝛽_{Baseline Kidney Volume} + 𝛽_{Txti2} + ... + e $$

::: panel-tabset
## Analysis

```{r}
set.seed(123)

# Perform the linear regression using 5-fold CV
model2c <- train(progression ~ log(kidvol_base) + txti2, 
                 data = data, 
                 method = "glm",
                 family = "binomial",
                 trControl = tc,
                 metric = "ROC",
                 preProc = "scale")

# Examine model
model2c
summary(model2c)
```

The model performs similarly to using `txti2` alone.

The AUC is 0.71, the sensitivity is 0.57, and the specificity is 0.59

[Top of Tabset](#2C)

## Visualization

#### Confusion Matrix

```{r}
# Save predictions
data$predictions2c <- predict(model2c, newdata = data)
data$predictions2c_prob <- predict(model2c, newdata = data, type = "prob")[,2]
data$predictions2c_num <- as.numeric(data$predictions2c_prob)-1

# Create confusion matrix
cm <- table(Predicted = data$predictions2c, Actual = data$progression)

# Examine confusion matrix and performance
cm
```

Not much different than model 2.

#### ROC Curve

```{r}
# Plot ROC Curve
roc2c <- ggplot(data, aes(m = predictions2c_num, d = progression_num)) +
  geom_roc(n.cuts = 10, labels = F, labelround = 4) +
  theme_minimal() +
  labs(title = "ROC Curve Model 2C: Baseline Kidney Volume and Txti2",
       x = "False Positive Rate",
       y = "True Positive Rate") +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed", color = "blue")

# Visualize Plot
roc2c
```

Not much different than just using `txti2` alone.

[Top of Tabset](#2C)

## Summary

Model C does not appear to perform much differently than model B.

AUC: 0.71

Sensitivity: 0.57

Specificity is 0.59

[Top of Tabset](#2C)
:::

## Model Comparison

```{r}
#| fig-width: 8

# Create list of models to compare
model_list <- list(`Kidney Volume at Baseline` = model2a, Txti2 = model2b, `Kidney Volume at Baseline and Txti2` = model2c)

# Compare models
results <- resamples(model_list)
summary(results)

results$models

# Compare the AUCs 
diffs <- diff(results) 
summary(diffs)

# Plot RMSEs of each model
dotplot(results, metric = "ROC",
        main = "Comparison of AUC Across Models")
dotplot(results, metric = "Sens",
        main = "Comparison of Sensitivity Across Models")
dotplot(results, metric = "Spec",
        main = "Comparison of Specificity Across Models")
```

The difference in performance between models was not statistically significant (report AUC's and 95% CI's here)

# Results

Table 2. Results of Pairwise T-Tests Comparing Model Performance Predicting Disease Progression using AUC

|                                  |      |             |             |              |
|---------------|---------------|---------------|---------------|---------------|
| Model                            | AUC  | Sensitivity | Specificity |   P-Adjusted |
| Baseline Kidney Volume           | 0.47 | 0.30        | 0.71        | —            |
| Txti2                            | 0.74 | 0.57        | 0.64        | 0.28         |
| Baseline Kidney Volume and Txti2 | 0.71 | 0.57        | 0.59        | 0.31         |

## Discussion

The model using `txti2` alone performed better at predicting disease progression than the model using `kidvol_base` alone (AUC = , 95% CI: *,*). However, this difference was not statistically significant (p = \_\_\_).

Additionally, the model incorporating both `txti2` and `kidvol_base` did not perform better than the model using just `txti` (AUC = , 95% CI: )

However, examining the ROC curves and the 95% CIs for all of the models, it does appear that there is a trend towards these difference being significant. It is possible that these may have not been statistically significant due to a small sample size (N = 67).

# ROC Curve

[Source](https://www.r-project.org/conferences/useR-2013/Tutorials/kuhn/user_caret_2up.pdf)

```{r}
#| fig-width: 10
#| fig-height: 6
# Example Data: Combine predictions into a single data frame
data_for_ROC <- data.frame(
  progression_num = data$progression_num,  # Your actual outcomes
  `Baseline Kidney Volume` = data$predictions2a_num,
  `Txti2` = data$predictions2b_num,
  `Baseline Kidney Volume and Txti2` = data$predictions2c_num
)

# Melt the data frame for ggplot
data_melted <- melt(data_for_ROC, id.vars = "progression_num", variable.name = "model", value.name = "predictions")
data_melted

# Plot ROC Curves
roc_plot <- ggplot(data_melted, aes(m = predictions, d = progression_num, color = model)) +
  geom_roc(n.cuts = 10, labels = FALSE, labelround = 4) +
  theme_minimal() +
  labs(title = "ROC Curves for Models Predicting Kidney Disease Progression",
       x = "False Positive Rate",
       y = "True Positive Rate",
       color = "Model") +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed", color = "black") +
  scale_color_brewer(palette = "Set2") +
  theme( text = element_text(size = 15))

# Visualize Plot
roc_plot
```

# Discussion

Using linear regression to answer task one, there appeared to not be much of a difference between models.

However when performing logistic regression, there appeared to be a difference in perforamance between models. Specifically using the MRI image feature of `txti2` increase accuracy, specificity, and sensitivty over that of just using baseline kidney volume alone. While this difference was not statistically, this may have been due to a small sample size.

The statisticians recommend collecting more data.

Add details here about how using `txti2` in conjunction with other strong predictors of ADPKD such as hypertension, elevated serum creatinine, sex could be incorporated into a single model to enhance predictive power.

# Bivariate Comparisons

```{r}
data |> 
  arrange(desc(lbp5)) |> 
  head() |> 
  pretty_print()
```

#### Scatterplots

Let's make a function here that will make the scatterplots with the layout that we prefer.

```{r}
#| warning: false

# Initialize empty list
plots <- list()

# Get column names of MRI features
features_gabor <- c("gabor1", "gabor2", "gabor3", "gabor4", "gabor5")
features_geom <- c("geom1", "geom2")
features_glcm <- c("glcm1", "glcm2")
features_txti <- c("txti1", "txti2", "txti3", "txti4", "txti5")
features_lbp <- c("lbp1", "lbp2", "lbp3", "lbp4", "lbp5")
  
# Plot scatterplots of each MRI feature against kidney volume change
plot_features_against_outcome <- function(features) {
for (feature in features) {
  p <- ggplot(data, aes_string(x = feature, y = "kidvol_change")) +
    geom_point() +
    theme_minimal() +
    labs(title = paste(feature),
                       x = feature,
                       y = "Kidney Volume Change (%)")
    plots[[feature]] <- p
}

# Combine all plots into a grid
combined_plot <- wrap_plots(plots, ncol = 3)
combined_plot
}
```

### Gabor Transform

```{r}
#| fig-width: 8
#| fig-height: 6
# Plot features against total change in kidney volume
plot_features_against_outcome(features_gabor)
```

It looks like `gabor4` and `gabor5` are exponential. We will get the square of those.

### Geom

```{r}
#| fig-width: 8
#| fig-height: 2.5
# Plot features against total change in kidney volume
plot_features_against_outcome(features_geom)
```

Geom 2 looks quadratic.

### Txti

```{r}
#| fig-width: 8
#| fig-height: 6
# Plot features against total change in kidney volume
plot_features_against_outcome(features_txti)
```

`txti4` and `txti5` look quadratic.

### LBP

```{r}
#| fig-width: 8
#| fig-height: 6
# Plot features against total change in kidney volume
plot_features_against_outcome(features_lbp)
```

# Feature Scaling

Here we will perform the scaling as necessary to place all variables on the same scale.

```{r}
# # Min-max normalization function
# min_max_norm <- function(x) {
#   (x - min(x)) / (max(x) - min(x))
# }
# 
# #############I was trying to features scale ecerything at once here.
# test <- scale(data$geom1)
# testly <- z_norm(data$geom1)

```

# Bonus

#### Messing around with plotting results of logistic regression to visualize

```{r}
ggplot(data, aes(y = log(kidvol_base), x = txti2, color = progression)) +
  geom_point() +
  theme_minimal()
```

## Gradient Descent

The purpose of gradient descent is to find a mapping function that minimizes the cost function.

The general process of gradient descent is:

1.  Initialize the betas (start with 0's or small values)

2.  Predict the outcome variable

3.  Calculate the error

4.  Calculate the gradient (The direction and magnitude of the steepest increase in the cost function)

5.  Adjust the betas in the opposite direction of the gradient

6.  Iterate (repeat the process until the cost function converges on a minimum value

# To do

I need to rerun the 5 fold CV with correctly centering the test sets based on the values from each training set.

As I did it, I centered them all together.

2a 2b 2c

???

Profit

# References

[@fogel1989] [@kumar2020][@haralick1973] [@ojala2002]

# Bonus

## Principle Components Analysis

```{r}
# Load required libraries
library(factoextra)

# Scale the data
data_scaled <- data |> 
  select(geom1, geom2, gabor1, gabor2, gabor3, gabor4, gabor5, glcm1, glcm2, txti1, txti2, txti3, txti4, txti5, lbp1, lbp2, lbp3, lbp4, lbp5) |> 
  scale()

# Perform PCA
pca_result <- prcomp(data_scaled, center = TRUE, scale. = TRUE)

# Summary of PCA
summary(pca_result)

# Visualize the variance explained by each component
fviz_eig(pca_result)

# Biplot of the first two principal components
fviz_pca_biplot(pca_result, geom = "point", label = "var")

# Project data onto the principal components
pca_transformed <- as.data.frame(pca_result$x)

# Select the first few principal components that explain sufficient variance
selected_components <- pca_transformed[, 1:2] # Example: select first 2 components

# Display selected principal components
head(selected_components)

# Extract loadings and scores 
loadings <- pca_result$rotation 
scores <- as.data.frame(pca_result$x) 

# View loadings 
print(loadings) 

# Visualize the variance explained by each component 
fviz_eig(pca_result) 

# Select the first few principal components that explain sufficient variance
selected_components <- scores[, 1:2] 

# Add the principal component scores to the original data for modeling 
data_with_pcs <- cbind(data$kidvol_change, selected_components) 
colnames(data_with_pcs) <- c("kidvol_change", "PC1", "PC2")

# Train a linear regression model using the principal components 
model_pca <- lm(kidvol_change ~ PC1 + PC2, data = data_with_pcs) 
summary(model_pca)
```
